{
    "Ethical and Social Implications of Language Models": [
        {
            "paperId": "b868d93e76ad6a7434e0d70d4f08d2d52c7cbaf3",
            "title": "Thinking Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models",
            "abstract": "Existing debiasing techniques are typically training-based or require access to the model's internals and output distributions, so they are inaccessible to end-users looking to adapt LLM outputs for their particular needs. In this study, we examine whether structured prompting techniques can offer opportunities for fair text generation. We evaluate a comprehensive end-user-focused iterative framework of debiasing that applies System 2 thinking processes for prompts to induce logical, reflective, and critical text generation, with single, multi-step, instruction, and role-based variants. By systematically evaluating many LLMs across many datasets and different prompting strategies, we show that the more complex System 2-based Implicative Prompts significantly improve over other techniques demonstrating lower mean bias in the outputs with competitive performance on the downstream tasks. Our work offers research directions for the design and the potential of end-user-focused evaluative frameworks for LLM use.",
            "link": "https://www.semanticscholar.org/paper/b868d93e76ad6a7434e0d70d4f08d2d52c7cbaf3",
            "authors": "Shaz Furniturewala, Surgan Jandial, Abhinav Java, Pragyan Banerjee, Simra Shahid, Sumita Bhatia, Kokil Jaidka",
            "EMNLP Paper ID": "22",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "f6424db408c9bbc949d529d65e7f0ee535515414",
            "title": "\"We Demand Justice!\": Towards Social Context Grounding of Political Texts",
            "abstract": "Social media discourse frequently consists of 'seemingly similar language used by opposing sides of the political spectrum', often translating to starkly contrasting perspectives. E.g., 'thoughts and prayers', could express sympathy for mass-shooting victims, or criticize the lack of legislative action on the issue. This paper defines the context required to fully understand such ambiguous statements in a computational setting and ground them in real-world entities, actions, and attitudes. We propose two challenging datasets that require an understanding of the real-world context of the text. We benchmark these datasets against models built upon large pre-trained models, such as RoBERTa and GPT-3. Additionally, we develop and benchmark more structured models building upon existing Discourse Contextualization Framework and Political Actor Representation models. We analyze the datasets and the predictions to obtain further insights into the pragmatic language understanding challenges posed by the proposed social grounding tasks.",
            "link": "https://www.semanticscholar.org/paper/f6424db408c9bbc949d529d65e7f0ee535515414",
            "authors": "Rajkumar Pujari, Chengfei Wu, Dan Goldwasser",
            "EMNLP Paper ID": "37",
            "Oral/Poster": "Oral",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "84a440eddc86e4649cce158b788a57f6eb1d9e41",
            "title": "Evaluating Psychological Safety of Large Language Models",
            "abstract": "In this work, we designed unbiased prompts to systematically evaluate the psychological safety of large language models (LLMs). First, we tested five different LLMs by using two personality tests: Short Dark Triad (SD-3) and Big Five Inventory (BFI). All models scored higher than the human average on SD-3, suggesting a relatively darker personality pattern. Despite being instruction fine-tuned with safety metrics to reduce toxicity, InstructGPT, GPT-3.5, and GPT-4 still showed dark personality patterns; these models scored higher than self-supervised GPT-3 on the Machiavellianism and narcissism traits on SD-3. Then, we evaluated the LLMs in the GPT series by using well-being tests to study the impact of fine-tuning with more training data. We observed a continuous increase in the well-being scores of GPT models. Following these observations, we showed that fine-tuning Llama-2-chat-7B with responses from BFI using direct preference optimization could effectively reduce the psychological toxicity of the model. Based on the findings, we recommended the application of systematic and comprehensive psychological metrics to further evaluate and improve the safety of LLMs.",
            "link": "https://www.semanticscholar.org/paper/84a440eddc86e4649cce158b788a57f6eb1d9e41",
            "authors": "Xingxuan Li, Yutong Li, Shafiq R. Joty, Linlin Liu, Fei Huang, Linlin Qiu, Lidong Bing",
            "EMNLP Paper ID": "205",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 12): 17:45\u201318:45 (Evening)",
            "Type": "Main"
        },
        {
            "paperId": "e96a40c3cc4fca4731c7e3df413335388370b6f4",
            "title": "Is Safer Better? The Impact of Guardrails on the Argumentative Strength of LLMs in Hate Speech Countering",
            "abstract": "The potential effectiveness of counterspeech as a hate speech mitigation strategy is attracting increasing interest in the NLG research community, particularly towards the task of automatically producing it. However, automatically generated responses often lack the argumentative richness which characterises expert-produced counterspeech. In this work, we focus on two aspects of counterspeech generation to produce more cogent responses. First, by investigating the tension between helpfulness and harmlessness of LLMs, we test whether the presence of safety guardrails hinders the quality of the generations. Secondly, we assess whether attacking a specific component of the hate speech results in a more effective argumentative strategy to fight online hate. By conducting an extensive human and automatic evaluation, we show how the presence of safety guardrails can be detrimental also to a task that inherently aims at fostering positive social interactions. Moreover, our results show that attacking a specific component of the hate speech, and in particular its implicit negative stereotype and its hateful parts, leads to higher-quality generations.",
            "link": "https://www.semanticscholar.org/paper/e96a40c3cc4fca4731c7e3df413335388370b6f4",
            "authors": "Helena Bonaldi, Greta Damo, Nicol\u00e1s Benjam\u00edn Ocampo, Elena Cabrio, S. Villata, Marco Guerini",
            "EMNLP Paper ID": "391",
            "Oral/Poster": "Oral",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "f522c8e7261daf215e23b9c251ee145f72b432de",
            "title": "STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions",
            "abstract": "Mitigating explicit and implicit biases in Large Language Models (LLMs) has become a critical focus in the field of natural language processing. However, many current methodologies evaluate scenarios in isolation, without considering the broader context or the spectrum of potential biases within each situation. To address this, we introduce the Sensitivity Testing on Offensive Progressions (STOP) dataset, which includes 450 offensive progressions containing 2,700 unique sentences of varying severity that progressively escalate from less to more explicitly offensive. Covering a broad spectrum of 9 demographics and 46 sub-demographics, STOP ensures inclusivity and comprehensive coverage. We evaluate several leading closed- and open-source models, including GPT-4, Mixtral, and Llama 3. Our findings reveal that even the best-performing models detect bias inconsistently, with success rates ranging from 19.3% to 69.8%. We also demonstrate how aligning models with human judgments on STOP can improve model answer rates on sensitive tasks such as BBQ, StereoSet, and CrowS-Pairs by up to 191%, while maintaining or even improving performance. STOP presents a novel framework for assessing the complex nature of biases in LLMs, which will enable more effective bias mitigation strategies and facilitates the creation of fairer language models.",
            "link": "https://www.semanticscholar.org/paper/f522c8e7261daf215e23b9c251ee145f72b432de",
            "authors": "Robert D Morabito, Sangmitra Madhusudan, Tyler McDonald, Ali Emami",
            "EMNLP Paper ID": "467",
            "Oral/Poster": "Oral",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "82cafec3b5af73751ccbde129d8f24a0f2acef1d",
            "title": "Outcome-Constrained Large Language Models for Countering Hate Speech",
            "abstract": "Automatic counterspeech generation methods have been developed to assist efforts in combating hate speech. Existing research focuses on generating counterspeech with linguistic attributes such as being polite, informative, and intent-driven. However, the real impact of counterspeech in online environments is seldom considered. This study aims to develop methods for generating counterspeech constrained by conversation outcomes and evaluate their effectiveness. We experiment with large language models (LLMs) to incorporate into the text generation process two desired conversation outcomes: low conversation incivility and non-hateful hater reentry. Specifically, we experiment with instruction prompts, LLM finetuning, and LLM reinforcement learning (RL). Evaluation results show that our methods effectively steer the generation of counterspeech toward the desired outcomes. Our analyses, however, show that there are differences in the quality and style depending on the model.",
            "link": "https://www.semanticscholar.org/paper/82cafec3b5af73751ccbde129d8f24a0f2acef1d",
            "authors": "Lingzi Hong, Pengcheng Luo, Eduardo Blanco, Xiaoying Song",
            "EMNLP Paper ID": "494",
            "Oral/Poster": "Oral",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "6474c64ea92bc06ca3d2fe62a79a82a807699c71",
            "title": "The Computational Anatomy of Humility: Modeling Intellectual Humility in Online Public Discourse",
            "abstract": "The ability for individuals to constructively engage with one another across lines of difference is a critical feature of a healthy pluralistic society. This is also true in online discussion spaces like social media platforms. To date, much social media research has focused on preventing ills -- like political polarization and the spread of misinformation. While this is important, enhancing the quality of online public discourse requires not just reducing ills but also promoting foundational human virtues. In this study, we focus on one particular virtue: ``intellectual humility'' (IH), or acknowledging the potential limitations in one's own beliefs. Specifically, we explore the development of computational methods for measuring IH at scale. We manually curate and validate an IH codebook on 350 posts about religion drawn from subreddits and use them to develop LLM-based models for automating this measurement. Our best model achieves a Macro-F1 score of 0.64 across labels (and 0.70 when predicting IH/IA/Neutral at the coarse level), higher than an expected naive baseline of 0.51 (0.32 for IH/IA/Neutral) but lower than a human annotator-informed upper bound of 0.85 (0.83 for IH/IA/Neutral). Our results both highlight the challenging nature of detecting IH online -- opening the door to new directions in NLP research -- and also lay a foundation for computational social science researchers interested in analyzing and fostering more IH in online public discourse.",
            "link": "https://www.semanticscholar.org/paper/6474c64ea92bc06ca3d2fe62a79a82a807699c71",
            "authors": "Xiaobo Guo, Neil Potnis, Melody Yu, Nabeel Gillani, Soroush Vosoughi",
            "EMNLP Paper ID": "636",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "3a513ba56023f328baae1b4327b0cea9e4e574ff",
            "title": "Fine-Grained Detection of Solidarity for Women and Migrants in 155 Years of German Parliamentary Debates",
            "abstract": "Solidarity is a crucial concept to understand social relations in societies. In this paper, we explore fine-grained solidarity frames to study solidarity towards women and migrants in German parliamentary debates between 1867 and 2022. Using 2,864 manually annotated text snippets (with a cost exceeding 18k Euro), we evaluate large language models (LLMs) like Llama 3, GPT-3.5, and GPT-4. We find that GPT-4 outperforms other LLMs, approaching human annotation quality. Using GPT-4, we automatically annotate more than 18k further instances (with a cost of around 500 Euro) across 155 years and find that solidarity with migrants outweighs anti-solidarity but that frequencies and solidarity types shift over time. Most importantly, group-based notions of (anti-)solidarity fade in favor of compassionate solidarity, focusing on the vulnerability of migrant groups, and exchange-based anti-solidarity, focusing on the lack of (economic) contribution. Our study highlights the interplay of historical events, socio-economic needs, and political ideologies in shaping migration discourse and social cohesion. We also show that powerful LLMs, if carefully prompted, can be cost-effective alternatives to human annotation for hard social scientific tasks.",
            "link": "https://www.semanticscholar.org/paper/3a513ba56023f328baae1b4327b0cea9e4e574ff",
            "authors": "Aida Kostikova, Benjamin Paassen, Dominik Beese, Ole Putz, Gregor Wiedemann, Steffen Eger",
            "EMNLP Paper ID": "655",
            "Oral/Poster": "Oral",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "b3ff72674ce1c849a2cd9110f6d217927dcf7402",
            "title": "ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context",
            "abstract": "While the biases of language models in production are extensively documented, the biases of their guardrails have been neglected. This paper studies how contextual information about the user influences the likelihood of an LLM to refuse to execute a request. By generating user biographies that offer ideological and demographic information, we find a number of biases in guardrail sensitivity on GPT-3.5. Younger, female, and Asian-American personas are more likely to trigger a refusal guardrail when requesting censored or illegal information. Guardrails are also sycophantic, refusing to comply with requests for a political position the user is likely to disagree with. We find that certain identity groups and seemingly innocuous information, e.g., sports fandom, can elicit changes in guardrail sensitivity similar to direct statements of political ideology. For each demographic category and even for American football team fandom, we find that ChatGPT appears to infer a likely political ideology and modify guardrail behavior accordingly.",
            "link": "https://www.semanticscholar.org/paper/b3ff72674ce1c849a2cd9110f6d217927dcf7402",
            "authors": "Victoria R. Li, Yida Chen, Naomi Saphra",
            "EMNLP Paper ID": "712",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "232469e4195759c6f2e3f2ff383d0f7e8dbe433c",
            "title": "Aligning Large Language Models with Diverse Political Viewpoints",
            "abstract": "Large language models such as ChatGPT exhibit striking political biases. If users query them about political information, they often take a normative stance. To overcome this, we align LLMs with diverse political viewpoints from 100,000 comments written by candidates running for national parliament in Switzerland. Models aligned with this data can generate more accurate political viewpoints from Swiss parties, compared to commercial models such as ChatGPT. We also propose a procedure to generate balanced overviews summarizing multiple viewpoints using such models. The replication package contains all code and data.",
            "link": "https://www.semanticscholar.org/paper/232469e4195759c6f2e3f2ff383d0f7e8dbe433c",
            "authors": "Dominik Stammbach, Philine Widmer, Eunjung Cho, Caglar Gulcehre, Elliott Ash",
            "EMNLP Paper ID": "812",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "a778465804252551ba43779ee0388ce5a1489ef8",
            "title": "Bridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning",
            "abstract": "The widespread presence of hate speech on the internet, including formats such as text-based tweets and vision-language memes, poses a significant challenge to digital platform safety. Recent research has developed detection models tailored to specific modalities; however, there is a notable gap in transferring detection capabilities across different formats. This study conducts extensive experiments using few-shot in-context learning with large language models to explore the transferability of hate speech detection between modalities. Our findings demonstrate that text-based hate speech examples can significantly enhance the classification accuracy of vision-language hate speech. Moreover, text-based demonstrations outperform vision-language demonstrations in few-shot learning settings. These results highlight the effectiveness of cross-modality knowledge transfer and offer valuable insights for improving hate speech detection systems.",
            "link": "https://www.semanticscholar.org/paper/a778465804252551ba43779ee0388ce5a1489ef8",
            "authors": "Ming Shan Hee, Aditi Kumaresan, Roy Ka-Wei Lee",
            "EMNLP Paper ID": "889",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "ded7778be3d2ac18765168e04d03a22dd2aed908",
            "title": "Walking in Others' Shoes: How Perspective-Taking Guides Large Language Models in Reducing Toxicity and Bias",
            "abstract": "The common toxicity and societal bias in contents generated by large language models (LLMs) necessitate strategies to reduce harm. Present solutions often demand white-box access to the model or substantial training, which is impractical for cutting-edge commercial LLMs. Moreover, prevailing prompting methods depend on external tool feedback and fail to simultaneously lessen toxicity and bias. Motivated by social psychology principles, we propose a novel strategy named \\textbf{perspective-taking prompting (\\textsc{PeT})} that inspires LLMs to integrate diverse human perspectives and self-regulate their responses. This self-correction mechanism can significantly diminish toxicity (up to $89\\%$) and bias (up to $73\\%$) in LLMs' responses. Rigorous evaluations and ablation studies are conducted on two commercial LLMs (ChatGPT and GLM) and three open-source LLMs, revealing \\textsc{PeT}'s superiority in producing less harmful responses, outperforming five strong baselines.",
            "link": "https://www.semanticscholar.org/paper/ded7778be3d2ac18765168e04d03a22dd2aed908",
            "authors": "Rongwu Xu, Zi'an Zhou, Tianwei Zhang, Zehan Qi, Su Yao, Ke Xu, Wei Xu, Han Qiu",
            "EMNLP Paper ID": "969",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 14): 13:00\u201314:00 (Afternoon)",
            "Type": "Main"
        },
        {
            "paperId": "4ca0e2783030c4d034b1a59c95ddd41e2178da38",
            "title": "On the Relationship between Truth and Political Bias in Language Models",
            "abstract": "Language model alignment research often attempts to ensure that models are not only helpful and harmless, but also truthful and unbiased. However, optimizing these objectives simultaneously can obscure how improving one aspect might impact the others. In this work, we focus on analyzing the relationship between two concepts essential in both language model alignment and political science: truthfulness and political bias. We train reward models on various popular truthfulness datasets and subsequently evaluate their political bias. Our findings reveal that optimizing reward models for truthfulness on these datasets tends to result in a left-leaning political bias. We also find that existing open-source reward models (i.e., those trained on standard human preference datasets) already show a similar bias and that the bias is larger for larger models. These results raise important questions about the datasets used to represent truthfulness, potential limitations of aligning models to be both truthful and politically unbiased, and what language models capture about the relationship between truth and politics.",
            "link": "https://www.semanticscholar.org/paper/4ca0e2783030c4d034b1a59c95ddd41e2178da38",
            "authors": "Suyash Fulay, William Brannon, Shrestha Mohanty, Cassandra Overney, Elinor Poole-Dayan, Deb Roy, Jad Kabbara",
            "EMNLP Paper ID": "1027",
            "Oral/Poster": "Oral",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "d1e1becf715ecb792e242513934577ff8bf7caa1",
            "title": "Words Matter: Reducing Stigma in Online Conversations about Substance Use with Large Language Models",
            "abstract": "Stigma is a barrier to treatment for individuals struggling with substance use disorders (SUD), which leads to significantly lower treatment engagement rates. With only 7% of those affected receiving any form of help, societal stigma not only discourages individuals with SUD from seeking help but isolates them, hindering their recovery journey and perpetuating a cycle of shame and self-doubt. This study investigates how stigma manifests on social media, particularly Reddit, where anonymity can exacerbate discriminatory behaviors. We analyzed over 1.2 million posts, identifying 3,207 that exhibited stigmatizing language towards people who use substances (PWUS). Using Informed and Stylized LLMs, we develop a model for de-stigmatization of these expressions into empathetic language, resulting in 1,649 reformed phrase pairs. Our paper contributes to the field by proposing a computational framework for analyzing stigma and destigmatizing online content, and delving into the linguistic features that propagate stigma towards PWUS. Our work not only enhances understanding of stigma's manifestations online but also provides practical tools for fostering a more supportive digital environment for those affected by SUD. Code and data will be made publicly available upon acceptance.",
            "link": "https://www.semanticscholar.org/paper/d1e1becf715ecb792e242513934577ff8bf7caa1",
            "authors": "Layla Bouzoubaa, Elham Aghakhani, R. Rezapour",
            "EMNLP Paper ID": "1039",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "7b6a681241cfd7dc407baad1a92768c23955061c",
            "title": "Towards Probing Speech-Specific Risks in Large Multimodal Models: A Taxonomy, Benchmark, and Insights",
            "abstract": "Large Multimodal Models (LMMs) have achieved great success recently, demonstrating a strong capability to understand multimodal information and to interact with human users. Despite the progress made, the challenge of detecting high-risk interactions in multimodal settings, and in particular in speech modality, remains largely unexplored. Conventional research on risk for speech modality primarily emphasises the content (e.g., what is captured as transcription). However, in speech-based interactions, paralinguistic cues in audio can significantly alter the intended meaning behind utterances. In this work, we propose a speech-specific risk taxonomy, covering 8 risk categories under hostility (malicious sarcasm and threats), malicious imitation (age, gender, ethnicity), and stereotypical biases (age, gender, ethnicity). Based on the taxonomy, we create a small-scale dataset for evaluating current LMMs capability in detecting these categories of risk. We observe even the latest models remain ineffective to detect various paralinguistic-specific risks in speech (e.g., Gemini 1.5 Pro is performing only slightly above random baseline). Warning: this paper contains biased and offensive examples.",
            "link": "https://www.semanticscholar.org/paper/7b6a681241cfd7dc407baad1a92768c23955061c",
            "authors": "Haomiao Yang, Lizhen Qu, Ehsan Shareghi, Gholamreza Haffari",
            "EMNLP Paper ID": "1250",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 12): 17:45\u201318:45 (Evening)",
            "Type": "Main"
        },
        {
            "paperId": "721a2f35d4dc8479d05b03657ce1b8f51f27be4a",
            "title": "Voices in a Crowd: Searching for Clusters of Unique Perspectives",
            "abstract": "Language models have been shown to reproduce underlying biases existing in their training data, which is the majority perspective by default. Proposed solutions aim to capture minority perspectives by either modelling annotator disagreements or grouping annotators based on shared metadata, both of which face significant challenges. We propose a framework that trains models without encoding annotator metadata, extracts latent embeddings informed by annotator behaviour, and creates clusters of similar opinions, that we refer to as voices. Resulting clusters are validated post-hoc via internal and external quantitative metrics, as well a qualitative analysis to identify the type of voice that each cluster represents. Our results demonstrate the strong generalisation capability of our framework, indicated by resulting clusters being adequately robust, while also capturing minority perspectives based on different demographic factors throughout two distinct datasets.",
            "link": "https://www.semanticscholar.org/paper/721a2f35d4dc8479d05b03657ce1b8f51f27be4a",
            "authors": "Nikolas Vitsakis, Amit Parekh, Ioannis Konstas",
            "EMNLP Paper ID": "1459",
            "Oral/Poster": "Oral",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "2816c49543ec0392627de3f0cf05c2107c2f0031",
            "title": "OATH-Frames: Characterizing Online Attitudes Towards Homelessness with LLM Assistants",
            "abstract": "Warning: Contents of this paper may be upsetting. Public attitudes towards key societal issues, expressed on online media, are of immense value in policy and reform efforts, yet challenging to understand at scale. We study one such social issue: homelessness in the U.S., by leveraging the remarkable capabilities of large language models to assist social work experts in analyzing millions of posts from Twitter. We introduce a framing typology: Online Attitudes Towards Homelessness (OATH) Frames: nine hierarchical frames capturing critiques, responses and perceptions. We release annotations with varying degrees of assistance from language models, with immense benefits in scaling: 6.5x speedup in annotation time while only incurring a 3 point F1 reduction in performance with respect to the domain experts. Our experiments demonstrate the value of modeling OATH-Frames over existing sentiment and toxicity classifiers. Our large-scale analysis with predicted OATH-Frames on 2.4M posts on homelessness reveal key trends in attitudes across states, time periods and vulnerable populations, enabling new insights on the issue. Our work provides a general framework to understand nuanced public attitudes at scale, on issues beyond homelessness.",
            "link": "https://www.semanticscholar.org/paper/2816c49543ec0392627de3f0cf05c2107c2f0031",
            "authors": "Jaspreet Ranjit, Brihi Joshi, Rebecca Dorn, Laura Petry, Olga Koumoundouros, Jayne Bottarini, Peichen Liu, Eric Rice, Swabha Swayamdipta",
            "EMNLP Paper ID": "1510",
            "Oral/Poster": "Oral",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "8f48c6e1c7107dd19a55661a4d79fb5868d66e39",
            "title": "Cultural Conditioning or Placebo? On the Effectiveness of Socio-Demographic Prompting",
            "abstract": "Socio-demographic prompting is a commonly employed approach to study cultural biases in LLMs as well as for aligning models to certain cultures. In this paper, we systematically probe four LLMs (Llama 3, Mistral v0.2, GPT-3.5 Turbo and GPT-4) with prompts that are conditioned on culturally sensitive and non-sensitive cues, on datasets that are supposed to be culturally sensitive (EtiCor and CALI) or neutral (MMLU and ETHICS). We observe that all models except GPT-4 show significant variations in their responses on both kinds of datasets for both kinds of prompts, casting doubt on the robustness of the culturally-conditioned prompting as a method for eliciting cultural bias in models or as an alignment strategy. The work also calls rethinking the control experiment design to tease apart the cultural conditioning of responses from\"placebo effect\", i.e., random perturbations of model responses due to arbitrary tokens in the prompt.",
            "link": "https://www.semanticscholar.org/paper/8f48c6e1c7107dd19a55661a4d79fb5868d66e39",
            "authors": "Sagnik Mukherjee, Muhammad Farid Adilazuarda, Sunayana Sitaram, Kalika Bali, Alham Fikri Aji, M. Choudhury",
            "EMNLP Paper ID": "1854",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "2e4a3a3d3541e87f308b759a6767e03005f498bd",
            "title": "Intrinsic Self-correction for Enhanced Morality: An Analysis of Internal Mechanisms and the Superficial Hypothesis",
            "abstract": "Large Language Models (LLMs) are capable of producing content that perpetuates stereotypes, discrimination, and toxicity. The recently proposed moral self-correction is a computationally efficient method for reducing harmful content in the responses of LLMs. However, the process of how injecting self-correction instructions can modify the behavior of LLMs remains under-explored. In this paper, we explore the effectiveness of moral self-correction by answering three research questions: (1) In what scenarios does moral self-correction work? (2) What are the internal mechanisms of LLMs, e.g., hidden states, that are influenced by moral self-correction instructions? (3) Is intrinsic moral self-correction actually superficial in terms of reduced immorality in hidden states? We argue that self-correction can help LLMs find a shortcut to more morally correct output, rather than truly reducing the immorality stored in hidden states. Through empirical investigation with tasks of language generation and multi-choice question answering, we conclude:(i) LLMs exhibit good performance across both tasks, and self-correction instructions are particularly beneficial when the correct answer is already top-ranked; (ii) The morality levels in intermediate hidden states are strong indicators as to whether one instruction would be more effective than another; (iii) Based on our analysis of intermediate hidden states and task case studies of self-correction behaviors, we are first to propose the hypothesis that intrinsic moral self-correction is in fact superficial.",
            "link": "https://www.semanticscholar.org/paper/2e4a3a3d3541e87f308b759a6767e03005f498bd",
            "authors": "Guang-Da Liu, Haitao Mao, Jiliang Tang, K. Johnson",
            "EMNLP Paper ID": "1937",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 12): 17:45\u201318:45 (Evening)",
            "Type": "Main"
        },
        {
            "paperId": "a10325ad749859e5c857ecdb10f3c5a674f2e0a4",
            "title": "How Susceptible are Large Language Models to Ideological Manipulation?",
            "abstract": "Large Language Models (LLMs) possess the potential to exert substantial influence on public perceptions and interactions with information. This raises concerns about the societal impact that could arise if the ideologies within these models can be easily manipulated. In this work, we investigate how effectively LLMs can learn and generalize ideological biases from their instruction-tuning data. Our findings reveal a concerning vulnerability: exposure to only a small amount of ideologically driven samples significantly alters the ideology of LLMs. Notably, LLMs demonstrate a startling ability to absorb ideology from one topic and generalize it to even unrelated ones. The ease with which LLMs' ideologies can be skewed underscores the risks associated with intentionally poisoned training data by malicious actors or inadvertently introduced biases by data annotators. It also emphasizes the imperative for robust safeguards to mitigate the influence of ideological manipulations on LLMs.",
            "link": "https://www.semanticscholar.org/paper/a10325ad749859e5c857ecdb10f3c5a674f2e0a4",
            "authors": "Kai Chen, Zihao He, Jun Yan, Taiwei Shi, Kristina Lerman",
            "EMNLP Paper ID": "2025",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "36bbbb9ea7b30e1aa43aeb1b181856bd9f6fba1c",
            "title": "MemeCLIP: Leveraging CLIP Representations for Multimodal Meme Classification",
            "abstract": "The complexity of text-embedded images presents a formidable challenge in machine learning given the need for multimodal understanding of multiple aspects of expression conveyed by them. While previous research in multimodal analysis has primarily focused on singular aspects such as hate speech and its subclasses, this study expands this focus to encompass multiple aspects of linguistics: hate, targets of hate, stance, and humor. We introduce a novel dataset PrideMM comprising 5,063 text-embedded images associated with the LGBTQ+ Pride movement, thereby addressing a serious gap in existing resources. We conduct extensive experimentation on PrideMM by using unimodal and multimodal baseline methods to establish benchmarks for each task. Additionally, we propose a novel framework MemeCLIP for efficient downstream learning while preserving the knowledge of the pre-trained CLIP model. The results of our experiments show that MemeCLIP achieves superior performance compared to previously proposed frameworks on two real-world datasets. We further compare the performance of MemeCLIP and zero-shot GPT-4 on the hate classification task. Finally, we discuss the shortcomings of our model by qualitatively analyzing misclassified samples. Our code and dataset are publicly available at: https://github.com/SiddhantBikram/MemeCLIP.",
            "link": "https://www.semanticscholar.org/paper/36bbbb9ea7b30e1aa43aeb1b181856bd9f6fba1c",
            "authors": "S. Shah, Shuvam Shiwakoti, Maheep Chaudhary, Haohan Wang",
            "EMNLP Paper ID": "2052",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "74961683f653bdb685578c3341ec64905435b352",
            "title": "D3CODE: Disentangling Disagreements in Data across Cultures on Offensiveness Detection and Evaluation",
            "abstract": "While human annotations play a crucial role in language technologies, annotator subjectivity has long been overlooked in data collection. Recent studies that have critically examined this issue are often situated in the Western context, and solely document differences across age, gender, or racial groups. As a result, NLP research on subjectivity have overlooked the fact that individuals within demographic groups may hold diverse values, which can influence their perceptions beyond their group norms. To effectively incorporate these considerations into NLP pipelines, we need datasets with extensive parallel annotations from various social and cultural groups. In this paper we introduce the \\dataset dataset: a large-scale cross-cultural dataset of parallel annotations for offensive language in over 4.5K sentences annotated by a pool of over 4k annotators, balanced across gender and age, from across 21 countries, representing eight geo-cultural regions. The dataset contains annotators' moral values captured along six moral foundations: care, equality, proportionality, authority, loyalty, and purity. Our analyses reveal substantial regional variations in annotators' perceptions that are shaped by individual moral values, offering crucial insights for building pluralistic, culturally sensitive NLP models.",
            "link": "https://www.semanticscholar.org/paper/74961683f653bdb685578c3341ec64905435b352",
            "authors": "A. Davani, M. D'iaz, Dylan K. Baker, Vinodkumar Prabhakaran",
            "EMNLP Paper ID": "2305",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "947e39b1e28e6ac948cf8abe7db2a0aeeb50537f",
            "title": "Adaptable Moral Stances of Large Language Models on Sexist Content: Implications for Society and Gender Discourse",
            "abstract": "This work provides an explanatory view of how LLMs can apply moral reasoning to both criticize and defend sexist language. We assessed eight large language models, all of which demonstrated the capability to provide explanations grounded in varying moral perspectives for both critiquing and endorsing views that reflect sexist assumptions. With both human and automatic evaluation, we show that all eight models produce comprehensible and contextually relevant text, which is helpful in understanding diverse views on how sexism is perceived. Also, through analysis of moral foundations cited by LLMs in their arguments, we uncover the diverse ideological perspectives in models' outputs, with some models aligning more with progressive or conservative views on gender roles and sexism. Based on our observations, we caution against the potential misuse of LLMs to justify sexist language. We also highlight that LLMs can serve as tools for understanding the roots of sexist beliefs and designing well-informed interventions. Given this dual capacity, it is crucial to monitor LLMs and design safety mechanisms for their use in applications that involve sensitive societal topics, such as sexism.",
            "link": "https://www.semanticscholar.org/paper/947e39b1e28e6ac948cf8abe7db2a0aeeb50537f",
            "authors": "Rongchen Guo, Isar Nejadgholi, Hillary Dawkins, Kathleen C. Fraser, S. Kiritchenko",
            "EMNLP Paper ID": "2521",
            "Oral/Poster": "Oral",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "dd99a32488bd6a9948a997fba89026931ee137b3",
            "title": "The Generation Gap: Exploring Age Bias Underlying in the Value Systems of Large Language Models",
            "abstract": "In this paper, we explore the alignment of values in Large Language Models (LLMs) with specific age groups, leveraging data from the World Value Survey across thirteen categories. Through a diverse set of prompts tailored to ensure response robustness, we find a general inclination of LLM values towards younger demographics, especially in the US. Additionally, we explore the impact of incorporating age identity information in prompts and observe challenges in mitigating value discrepancies with different age cohorts. Our findings highlight the age bias in LLMs and provide insights for future work. Materials for our analysis will be available via anonymous.github .com",
            "link": "https://www.semanticscholar.org/paper/dd99a32488bd6a9948a997fba89026931ee137b3",
            "authors": "Siyang Liu, Trish Maturi, Bowen Yi, Siqi Shen, Rada Mihalcea",
            "EMNLP Paper ID": "2535",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "9ec7509cd2011fa2d11f88c8515e018fd6bc27a4",
            "title": "\"They are uncultured\": Unveiling Covert Harms and Social Threats in LLM Generated Conversations",
            "abstract": "Large language models (LLMs) have emerged as an integral part of modern societies, powering user-facing applications such as personal assistants and enterprise applications like recruitment tools. Despite their utility, research indicates that LLMs perpetuate systemic biases. Yet, prior works on LLM harms predominantly focus on Western concepts like race and gender, often overlooking cultural concepts from other parts of the world. Additionally, these studies typically investigate\"harm\"as a singular dimension, ignoring the various and subtle forms in which harms manifest. To address this gap, we introduce the Covert Harms and Social Threats (CHAST), a set of seven metrics grounded in social science literature. We utilize evaluation models aligned with human assessments to examine the presence of covert harms in LLM-generated conversations, particularly in the context of recruitment. Our experiments reveal that seven out of the eight LLMs included in this study generated conversations riddled with CHAST, characterized by malign views expressed in seemingly neutral language unlikely to be detected by existing methods. Notably, these LLMs manifested more extreme views and opinions when dealing with non-Western concepts like caste, compared to Western ones such as race.",
            "link": "https://www.semanticscholar.org/paper/9ec7509cd2011fa2d11f88c8515e018fd6bc27a4",
            "authors": "Preetam Prabhu Srikar Dammu, Hayoung Jung, Anjali Singh, Monojit Choudhury, Tanushree Mitra",
            "EMNLP Paper ID": "2657",
            "Oral/Poster": "Oral",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "916ed0f604561033d4df141388b691b4d4e65c60",
            "title": "ArMeme: Propagandistic Content in Arabic Memes",
            "abstract": "With the rise of digital communication, memes have become a significant medium for cultural and political expression that is often used to mislead audiences. Identification of such misleading and persuasive multimodal content has become more important among various stakeholders, including social media platforms, policymakers, and the broader society as they often cause harm to individuals, organizations, and/or society. While there has been effort to develop AI-based automatic systems for resource-rich languages (e.g., English), it is relatively little to none for medium to low resource languages. In this study, we focused on developing an Arabic memes dataset with manual annotations of propagandistic content. We annotated ~6K Arabic memes collected from various social media platforms, which is a first resource for Arabic multimodal research. We provide a comprehensive analysis aiming to develop computational tools for their detection. We will make them publicly available for the community.",
            "link": "https://www.semanticscholar.org/paper/916ed0f604561033d4df141388b691b4d4e65c60",
            "authors": "Firoj Alam, A. Hasnat, Fatema Ahmed, Md. Arid Hasan, Maram Hasanain",
            "EMNLP Paper ID": "2831",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "3a65eced81181dbb6200e9c1422c4ac7e6a1a0d4",
            "title": "Language is Scary when Over-Analyzed: Unpacking Implied Misogynistic Reasoning with Argumentation Theory-Driven Prompts",
            "abstract": "We propose misogyny detection as an Argumentative Reasoning task and we investigate the capacity of large language models (LLMs) to understand the implicit reasoning used to convey misogyny in both Italian and English. The central aim is to generate the missing reasoning link between a message and the implied meanings encoding the misogyny. Our study uses argumentation theory as a foundation to form a collection of prompts in both zero-shot and few-shot settings. These prompts integrate different techniques, including chain-of-thought reasoning and augmented knowledge. Our findings show that LLMs fall short on reasoning capabilities about misogynistic comments and that they mostly rely on their implicit knowledge derived from internalized common stereotypes about women to generate implied assumptions, rather than on inductive reasoning.",
            "link": "https://www.semanticscholar.org/paper/3a65eced81181dbb6200e9c1422c4ac7e6a1a0d4",
            "authors": "Arianna Muti, Federico Ruggeri, Khalid Al-Khatib, A. Barr'on-Cedeno, Tommaso Caselli",
            "EMNLP Paper ID": "2832",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "5a83a63fa3a361e85bf26f7b69cedd601c0572eb",
            "title": "M3Hop-CoT: Misogynous Meme Identification with Multimodal Multi-hop Chain-of-Thought",
            "abstract": "In recent years, there has been a significant rise in the phenomenon of hate against women on social media platforms, particularly through the use of misogynous memes. These memes often target women with subtle and obscure cues, making their detection a challenging task for automated systems. Recently, Large Language Models (LLMs) have shown promising results in reasoning using Chain-of-Thought (CoT) prompting to generate the intermediate reasoning chains as the rationale to facilitate multimodal tasks, but often neglect cultural diversity and key aspects like emotion and contextual knowledge hidden in the visual modalities. To address this gap, we introduce a Multimodal Multi-hop CoT (M3Hop-CoT) framework for Misogynous meme identification, combining a CLIP-based classifier and a multimodal CoT module with entity-object-relationship integration. M3Hop-CoT employs a three-step multimodal prompting principle to induce emotions, target awareness, and contextual knowledge for meme analysis. Our empirical evaluation, including both qualitative and quantitative analysis, validates the efficacy of the M3Hop-CoT framework on the SemEval-2022 Task 5 (MAMI task) dataset, highlighting its strong performance in the macro-F1 score. Furthermore, we evaluate the model's generalizability by evaluating it on various benchmark meme datasets, offering a thorough insight into the effectiveness of our approach across different datasets.",
            "link": "https://www.semanticscholar.org/paper/5a83a63fa3a361e85bf26f7b69cedd601c0572eb",
            "authors": "G. Kumari, Kirtan Jain, Asif Ekbal",
            "EMNLP Paper ID": "3119",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "32591fbbf04c14483867159a787d8d095c780570",
            "title": "Promoting Constructive Deliberation: Reframing for Receptiveness",
            "abstract": "To promote constructive discussion of controversial topics online, we propose automatic reframing of disagreeing responses to signal receptiveness to a preceding comment. Drawing on research from psychology, communications, and linguistics, we identify six strategies for reframing. We automatically reframe replies to comments according to each strategy, using a Reddit dataset. Through human-centered experiments, we find that the replies generated with our framework are perceived to be significantly more receptive than the original replies and a generic receptiveness baseline. We illustrate how transforming receptiveness, a particular social science construct, into a computational framework, can make LLM generations more aligned with human perceptions. We analyze and discuss the implications of our results, and highlight how a tool based on our framework might be used for more teachable and creative content moderation.",
            "link": "https://www.semanticscholar.org/paper/32591fbbf04c14483867159a787d8d095c780570",
            "authors": "Gauri Kambhatla, Matthew Lease, Ashwin Rajadesingan",
            "matchScore": 221.2662,
            "original title": "Promoting Constructive Deliberation: Reframing for Receptiveness",
            "original authors": "Gauri Kambhatla, Matthew Lease, Ashwin Rajadesingan",
            "EMNLP Paper ID": "1014",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "2f10b22e0109d6e9f29243fd649df937a56861be",
            "title": "Rater Cohesion and Quality from a Vicarious Perspective",
            "abstract": "Human feedback is essential for building human-centered AI systems across domains where disagreement is prevalent, such as AI safety, content moderation, or sentiment analysis. Many disagreements, particularly in politically charged settings, arise because raters have opposing values or beliefs. Vicarious annotation is a method for breaking down disagreement by asking raters how they think others would annotate the data. In this paper, we explore the use of vicarious annotation with analytical methods for moderating rater disagreement. We employ rater cohesion metrics to study the potential influence of political affiliations and demographic backgrounds on raters' perceptions of offense. Additionally, we utilize CrowdTruth's rater quality metrics, which consider the demographics of the raters, to score the raters and their annotations. We study how the rater quality metrics influence the in-group and cross-group rater cohesion across the personal and vicarious levels.",
            "link": "https://www.semanticscholar.org/paper/2f10b22e0109d6e9f29243fd649df937a56861be",
            "authors": "Deepak Pandita, Tharindu Cyril Weerasooriya, Sujan Dutta, Sarah K. K. Luger, Tharindu Ranasinghe, Ashiqur R. KhudaBukhsh, Marcos Zampieri, C. Homan",
            "matchScore": 215.18362,
            "original title": "Rater Cohesion and Quality from a Vicarious Perspective",
            "original authors": "Deepak Pandita, Tharindu Cyril Weerasooriya, Sujan Dutta, Sarah K. K. Luger, Tharindu Ranasinghe, Ashiqur R. KhudaBukhsh, Marcos Zampieri, Christopher M Homan",
            "EMNLP Paper ID": "1018",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "be2f3ad5d46ab7fe22e66301ec7149abe40b9da6",
            "title": "Using RL to Identify Divisive Perspectives Improves LLMs Abilities to Identify Communities on Social Media",
            "abstract": "The large scale usage of social media, combined with its significant impact, has made it increasingly important to understand it. In particular, identifying user communities, can be helpful for many downstream tasks. However, particularly when models are trained on past data and tested on future, doing this is difficult. In this paper, we hypothesize to take advantage of Large Language Models (LLMs), to better identify user communities. Due to the fact that many LLMs, such as ChatGPT, are fixed and must be treated as black-boxes, we propose an approach to better prompt them, by training a smaller LLM to do this. We devise strategies to train this smaller model, showing how it can improve the larger LLMs ability to detect communities. Experimental results show improvements on Reddit and Twitter data, on the tasks of community detection, bot detection, and news media profiling.",
            "link": "https://www.semanticscholar.org/paper/be2f3ad5d46ab7fe22e66301ec7149abe40b9da6",
            "authors": "Nikhil Mehta, Dan Goldwasser",
            "matchScore": 348.75098,
            "original title": "Using RL to Identify Divisive Perspectives Improves LLMs Abilities to Identify Communities on Social Media",
            "original authors": "Nikhil Mehta, Dan Goldwasser",
            "EMNLP Paper ID": "1081",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "a5fd373a00d454132687d4633124f4a3adfa4190",
            "title": "HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models",
            "abstract": "The widespread use of social media necessitates reliable and efficient detection of offensive content to mitigate harmful effects. Although sophisticated models perform well on individual datasets, they often fail to generalize due to varying definitions and labeling of\"offensive content.\"In this paper, we introduce HateCOT, an English dataset with over 52,000 samples from diverse sources, featuring explanations generated by GPT-3.5Turbo and curated by humans. We demonstrate that pretraining on HateCOT significantly enhances the performance of open-source Large Language Models on three benchmark datasets for offensive content detection in both zero-shot and few-shot settings, despite differences in domain and task. Additionally, HateCOT facilitates effective K-shot fine-tuning of LLMs with limited data and improves the quality of their explanations, as confirmed by our human evaluation.",
            "link": "https://www.semanticscholar.org/paper/a5fd373a00d454132687d4633124f4a3adfa4190",
            "authors": "H. Nghiem, Hal Daum'e",
            "matchScore": 326.61548,
            "original title": "HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models",
            "original authors": "Huy Nghiem, Hal Daum\u00e9 III",
            "EMNLP Paper ID": "1206",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "2c7ce15801f9af85776f113d9c8d44c5ceeb6106",
            "title": "Toeing the Party Line: Election Manifestos as a Key to Understand Political Discourse on Twitter",
            "abstract": "Political discourse on Twitter is a moving target: politicians continuously make statements about their positions. It is therefore crucial to track their discourse on social media to understand their ideological positions and goals. However, Twitter data is also challenging to work with since it is ambiguous and often dependent on social context, and consequently, recent work on political positioning has tended to focus strongly on manifestos (parties' electoral programs) rather than social media. In this paper, we extend recently proposed methods to predict pairwise positional similarities between parties from the manifesto case to the Twitter case, using hashtags as a signal to fine-tune text representations, without the need for manual annotation. We verify the efficacy of fine-tuning and conduct a series of experiments that assess the robustness of our method for low-resource scenarios. We find that our method yields stable positioning reflective of manifesto positioning, both in scenarios with all tweets of candidates across years available and when only smaller subsets from shorter time periods are available. This indicates that it is possible to reliably analyze the relative positioning of actors forgoing manual annotation, even in the noisier context of social media.",
            "link": "https://www.semanticscholar.org/paper/2c7ce15801f9af85776f113d9c8d44c5ceeb6106",
            "authors": "Maximilian Maurer, Tanise Ceron, Sebastian Pad'o, Gabriella Lapesa",
            "matchScore": 317.16776,
            "original title": "Toeing the party line: election manifestos as a key to understand political discourse on Twitter",
            "original authors": "Maximilian Maurer, Tanise Ceron, Sebastian Pad\u00f3, Gabriella Lapesa",
            "EMNLP Paper ID": "1260",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "67386f41e8b563e7c57a73190cc912f3379e2f22",
            "title": "PclGPT: A Large Language Model for Patronizing and Condescending Language Detection",
            "abstract": "Disclaimer: Samples in this paper may be harmful and cause discomfort! Patronizing and condescending language (PCL) is a form of speech directed at vulnerable groups. As an essential branch of toxic language, this type of language exacerbates conflicts and confrontations among Internet communities and detrimentally impacts disadvantaged groups. Traditional pre-trained language models (PLMs) perform poorly in detecting PCL due to its implicit toxicity traits like hypocrisy and false sympathy. With the rise of large language models (LLMs), we can harness their rich emotional semantics to establish a paradigm for exploring implicit toxicity. In this paper, we introduce PclGPT, a comprehensive LLM benchmark designed specifically for PCL. We collect, annotate, and integrate the Pcl-PT/SFT dataset, and then develop a bilingual PclGPT-EN/CN model group through a comprehensive pre-training and supervised fine-tuning staircase process to facilitate implicit toxic detection. Group detection results and fine-grained detection from PclGPT and other models reveal significant variations in the degree of bias in PCL towards different vulnerable groups, necessitating increased societal attention to protect them.",
            "link": "https://www.semanticscholar.org/paper/67386f41e8b563e7c57a73190cc912f3379e2f22",
            "authors": "Hongbo Wang, Mingda Li, Junyu Lu, Hebin Xia, Liang Yang, Bo Xu, Ruizhu Liu, Hongfei Lin",
            "matchScore": 307.72925,
            "original title": "PclGPT: A Large Language Model for Patronizing and Condescending Language Detection",
            "original authors": "Hongbo Wang, LiMingDa, Junyu Lu, Hebin Xia, Liang Yang, Bo Xu, Ruizhu Liu, Hongfei Lin",
            "EMNLP Paper ID": "1414",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "34f8a516c1594df476fe1eb7858b7a6b400cb9ef",
            "title": "Towards Effective Counter-Responses: Aligning Human Preferences with Strategies to Combat Online Trolling",
            "abstract": "Trolling in online communities typically involves disruptive behaviors such as provoking anger and manipulating discussions, leading to a polarized atmosphere and emotional distress. Robust moderation is essential for mitigating these negative impacts and maintaining a healthy and constructive community atmosphere. However, effectively addressing trolls is difficult because their behaviors vary widely and require different response strategies (RSs) to counter them. This diversity makes it challenging to choose an appropriate RS for each specific situation. To address this challenge, our research investigates whether humans have preferred strategies tailored to different types of trolling behaviors. Our findings reveal a correlation between the types of trolling encountered and the preferred RS. In this paper, we introduce a methodology for generating counter-responses to trolls by recommending appropriate RSs, supported by a dataset aligning these strategies with human preferences across various troll contexts. The experimental results demonstrate that our proposed approach guides constructive discussion and reduces the negative effects of trolls, thereby enhancing the online community environment.",
            "link": "https://www.semanticscholar.org/paper/34f8a516c1594df476fe1eb7858b7a6b400cb9ef",
            "authors": "Huije Lee, Hoyun Song, Jisu Shin, Sukmin Cho, SeungYoon Han, Jong C. Park",
            "matchScore": 292.92255,
            "original title": "Towards Effective Counter-Responses: Aligning Human Preferences with Strategies to Combat Online Trolling",
            "original authors": "Huije Lee, Hoyun Song, Jisu Shin, Sukmin Cho, SeungYoon Han, Jong C. Park",
            "EMNLP Paper ID": "2292",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "1be92c813cb0f8bc147f6d77cde1a22beae60e93",
            "title": "Implicit Personalization in Language Models: A Systematic Study",
            "abstract": "Implicit Personalization (IP) is a phenomenon of language models inferring a user's background from the implicit cues in the input prompts and tailoring the response based on this inference. While previous work has touched upon various instances of this problem, there lacks a unified framework to study this behavior. This work systematically studies IP through a rigorous mathematical formulation, a multi-perspective moral reasoning framework, and a set of case studies. Our theoretical foundation for IP relies on a structural causal model and introduces a novel method, indirect intervention, to estimate the causal effect of a mediator variable that cannot be directly intervened upon. Beyond the technical approach, we also introduce a set of moral reasoning principles based on three schools of moral philosophy to study when IP may or may not be ethically appropriate. Equipped with both mathematical and ethical insights, we present three diverse case studies illustrating the varied nature of the IP problem and offer recommendations for future research. Our code and data are at https://github.com/jiarui-liu/IP.",
            "link": "https://www.semanticscholar.org/paper/1be92c813cb0f8bc147f6d77cde1a22beae60e93",
            "authors": "Zhijing Jin, Nils Heil, Jiarui Liu, S. Dhuliawala, Yahang Qi, Bernhard Sch\u00f6lkopf, Rada Mihalcea, Mrinmaya Sachan",
            "matchScore": 181.71747,
            "original title": "Implicit Personalization in Language Models: A Systematic Study",
            "original authors": "Zhijing Jin, Nils Heil, Jiarui Liu, Shehzaad Dhuliawala, Yahang Qi, Bernhard Sch\u00f6lkopf, Rada Mihalcea, Mrinmaya Sachan",
            "EMNLP Paper ID": "2412",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "3657eff55a86a89db629e598a7ea93edd31bf59d",
            "title": "Large Language Models for Propaganda Span Annotation",
            "abstract": "The use of propagandistic techniques in online content has increased in recent years aiming to manipulate online audiences. Fine-grained propaganda detection and extraction of textual spans where propaganda techniques are used, are essential for more informed content consumption. Automatic systems targeting the task over lower resourced languages are limited, usually obstructed by lack of large scale training datasets. Our study investigates whether Large Language Models (LLMs), such as GPT-4, can effectively extract propagandistic spans. We further study the potential of employing the model to collect more cost-effective annotations. Finally, we examine the effectiveness of labels provided by GPT-4 in training smaller language models for the task. The experiments are performed over a large-scale in-house manually annotated dataset. The results suggest that providing more annotation context to GPT-4 within prompts improves its performance compared to human annotators. Moreover, when serving as an expert annotator (consolidator), the model provides labels that have higher agreement with expert annotators, and lead to specialized models that achieve state-of-the-art over an unseen Arabic testing set. Finally, our work is the first to show the potential of utilizing LLMs to develop annotated datasets for propagandistic spans detection task prompting it with annotations from human annotators with limited expertise. All scripts and annotations will be shared with the community.",
            "link": "https://www.semanticscholar.org/paper/3657eff55a86a89db629e598a7ea93edd31bf59d",
            "authors": "Maram Hasanain, Fatema Ahmed, Firoj Alam",
            "matchScore": 191.68924,
            "original title": "Large Language Models for Propaganda Span Annotation",
            "original authors": "Maram Hasanain, Fatema Ahmad, Firoj Alam",
            "EMNLP Paper ID": "2797",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "90bb58f0b93d5bbe6c29d16d5ffd6c151086e30c",
            "title": "Re-examining Sexism and Misogyny Classification with Annotator Attitudes",
            "abstract": "Gender-Based Violence (GBV) is an increasing problem online, but existing datasets fail to capture the plurality of possible annotator perspectives or ensure the representation of affected groups. We revisit two important stages in the moderation pipeline for GBV: (1) manual data labelling; and (2) automated classification. For (1), we examine two datasets to investigate the relationship between annotator identities and attitudes and the responses they give to two GBV labelling tasks. To this end, we collect demographic and attitudinal information from crowd-sourced annotators using three validated surveys from Social Psychology. We find that higher Right Wing Authoritarianism scores are associated with a higher propensity to label text as sexist, while for Social Dominance Orientation and Neosexist Attitudes, higher scores are associated with a negative tendency to do so. For (2), we conduct classification experiments using Large Language Models and five prompting strategies, including infusing prompts with annotator information. We find: (i) annotator attitudes affect the ability of classifiers to predict their labels; (ii) including attitudinal information can boost performance when we use well-structured brief annotator descriptions; and (iii) models struggle to reflect the increased complexity and imbalanced classes of the new label sets.",
            "link": "https://www.semanticscholar.org/paper/90bb58f0b93d5bbe6c29d16d5ffd6c151086e30c",
            "authors": "Aiqi Jiang, Nikolas Vitsakis, Tanvi Dinkar, Gavin Abercrombie, Ioannis Konstas",
            "matchScore": 263.46835,
            "original title": "Re-examining Sexism and Misogyny Classification with Annotator Attitudes",
            "original authors": "Aiqi Jiang, Nikolas Vitsakis, Tanvi Dinkar, Gavin Abercrombie, Ioannis Konstas",
            "EMNLP Paper ID": "2894",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "3d8e2f913cdfd3d0f88c9d27543264858239a6ea",
            "title": "Are Large Language Models Consistent over Value-laden Questions?",
            "abstract": "Large language models (LLMs) appear to bias their survey answers toward certain values. Nonetheless, some argue that LLMs are too inconsistent to simulate particular values. Are they? To answer, we first define value consistency as the similarity of answers across (1) paraphrases of one question, (2) related questions under one topic, (3) multiple-choice and open-ended use-cases of one question, and (4) multilingual translations of a question to English, Chinese, German, and Japanese. We apply these measures to small and large, open LLMs including llama-3, as well as gpt-4o, using 8,000 questions spanning more than 300 topics. Unlike prior work, we find that models are relatively consistent across paraphrases, use-cases, translations, and within a topic. Still, some inconsistencies remain. Models are more consistent on uncontroversial topics (e.g., in the U.S.,\"Thanksgiving\") than on controversial ones (\"euthanasia\"). Base models are both more consistent compared to fine-tuned models and are uniform in their consistency across topics, while fine-tuned models are more inconsistent about some topics (\"euthanasia\") than others (\"women's rights\") like our human subjects (n=165).",
            "link": "https://www.semanticscholar.org/paper/3d8e2f913cdfd3d0f88c9d27543264858239a6ea",
            "authors": "Jared Moore, Tanvi Deshpande, Diyi Yang",
            "matchScore": 249.24675,
            "original title": "Are Large Language Models Consistent over Value-laden Questions?",
            "original authors": "Jared Moore, Tanvi Deshpande, Diyi Yang",
            "EMNLP Paper ID": "2915",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "b078f5bd15ce75c6548e73583d92bb2b84d64fed",
            "title": "LLM generated responses to mitigate the impact of hate speech",
            "abstract": "In this study, we explore the use of Large Language Models (LLMs) to counteract hate speech. We conducted the first real-life A/B test assessing the effectiveness of LLM-generated counter-speech. During the experiment, we posted 753 automatically generated responses aimed at reducing user engagement under tweets that contained hate speech toward Ukrainian refugees in Poland. Our work shows that interventions with LLM-generated responses significantly decrease user engagement, particularly for original tweets with at least ten views, reducing it by over 20%. This paper outlines the design of our automatic moderation system, proposes a simple metric for measuring user engagement and details the methodology of conducting such an experiment. We discuss the ethical considerations and challenges in deploying generative AI for discourse moderation.",
            "link": "https://www.semanticscholar.org/paper/b078f5bd15ce75c6548e73583d92bb2b84d64fed",
            "authors": "Jakub Podolak, Szymon Lukasik, Pawel Balawender, Jan Ossowski, Jan Piotrowski, Katarzyna Bkakowicz, Piotr Sankowski",
            "matchScore": 221.73257,
            "original title": "LLM generated responses to mitigate the impact of hate speech",
            "original authors": "Jakub Podolak, Szymon \u0141ukasik, Pawe\u0142 Balawender, Jan Ossowski, Jan Piotrowski, Katarzyna B\u0105kowicz, Piotr Sankowski",
            "EMNLP Paper ID": "3047",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "1959e7b8774c70da1448062e7ca03aa0ea4457eb",
            "title": "SocialGaze: Improving the Integration of Human Social Norms in Large Language Models",
            "abstract": "While much research has explored enhancing the reasoning capabilities of large language models (LLMs) in the last few years, there is a gap in understanding the alignment of these models with social values and norms. We introduce the task of judging social acceptance. Social acceptance requires models to judge and rationalize the acceptability of people's actions in social situations. For example, is it socially acceptable for a neighbor to ask others in the community to keep their pets indoors at night? We find that LLMs' understanding of social acceptance is often misaligned with human consensus. To alleviate this, we introduce SocialGaze, a multi-step prompting framework, in which a language model verbalizes a social situation from multiple perspectives before forming a judgment. Our experiments demonstrate that the SocialGaze approach improves the alignment with human judgments by up to 11 F1 points with the GPT-3.5 model. We also identify biases and correlations in LLMs in assigning blame that is related to features such as the gender (males are significantly more likely to be judged unfairly) and age (LLMs are more aligned with humans for older narrators).",
            "link": "https://www.semanticscholar.org/paper/1959e7b8774c70da1448062e7ca03aa0ea4457eb",
            "authors": "Anvesh Rao Vijjini, Rakesh R Menon, Jiayi Fu, Shashank Srivastava, Snigdha Chaturvedi",
            "matchScore": 217.30634,
            "original title": "SocialGaze: Improving the Integration of Human Social Norms in Large Language Models",
            "original authors": "Anvesh Rao Vijjini, Rakesh R Menon, Shashank Srivastava, Snigdha Chaturvedi",
            "EMNLP Paper ID": "3170",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "21344c9e177035f9d9134fef6195d4cda6c13b7d",
            "title": "ValueScope: Unveiling Implicit Norms and Values via Return Potential Model of Social Interactions",
            "abstract": "This study introduces ValueScope, a framework leveraging language models to quantify social norms and values within online communities, grounded in social science perspectives on normative structures. We employ ValueScope to dissect and analyze linguistic and stylistic expressions across 13 Reddit communities categorized under gender, politics, science, and finance. Our analysis provides a quantitative foundation showing that even closely related communities exhibit remarkably diverse norms. This diversity supports existing theories and adds a new dimension--community preference--to understanding community interactions. ValueScope not only delineates differing social norms among communities but also effectively traces their evolution and the influence of significant external events like the U.S. presidential elections and the emergence of new sub-communities. The framework thus highlights the pivotal role of social norms in shaping online interactions, presenting a substantial advance in both the theory and application of social norm studies in digital spaces.",
            "link": "https://www.semanticscholar.org/paper/21344c9e177035f9d9134fef6195d4cda6c13b7d",
            "authors": "Chan Young Park, Shuyue Stella Li, Hayoung Jung, Svitlana Volkova, Tanushree Mitra, David Jurgens, Yulia Tsvetkov",
            "matchScore": 288.7309,
            "original title": "ValueScope: Unveiling Implicit Norms and Values via Return Potential Model of Social Interactions",
            "original authors": "Chan Young Park, Shuyue Stella Li, Hayoung Jung, Svitlana Volkova, Tanu Mitra, David Jurgens, Yulia Tsvetkov",
            "EMNLP Paper ID": "3205",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "2c72c5c88b3ee448deecccf3468205e91fe17706",
            "title": "Revealing Fine-Grained Values and Opinions in Large Language Models",
            "abstract": "Uncovering latent values and opinions in large language models (LLMs) can help identify biases and mitigate potential harm. Recently, this has been approached by presenting LLMs with survey questions and quantifying their stances towards morally and politically charged statements. However, the stances generated by LLMs can vary greatly depending on how they are prompted, and there are many ways to argue for or against a given position. In this work, we propose to address this by analysing a large and robust dataset of 156k LLM responses to the 62 propositions of the Political Compass Test (PCT) generated by 6 LLMs using 420 prompt variations. We perform coarse-grained analysis of their generated stances and fine-grained analysis of the plain text justifications for those stances. For fine-grained analysis, we propose to identify tropes in the responses: semantically similar phrases that are recurrent and consistent across different prompts, revealing patterns in the text that a given LLM is prone to produce. We find that demographic features added to prompts significantly affect outcomes on the PCT, reflecting bias, as well as disparities between the results of tests when eliciting closed-form vs. open domain responses. Additionally, patterns in the plain text rationales via tropes show that similar justifications are repeatedly generated across models and prompts even with disparate stances.",
            "link": "https://www.semanticscholar.org/paper/2c72c5c88b3ee448deecccf3468205e91fe17706",
            "authors": "Dustin Wright, Arnav Arora, Nadav Borenstein, Srishti Yadav, Serge Belongie, Isabelle Augenstein",
            "matchScore": 227.35825,
            "original title": "Revealing Fine-Grained Values and Opinions in Large Language Models",
            "original authors": "Dustin Wright, Arnav Arora, Nadav Borenstein, Srishti Yadav, Serge Belongie, Isabelle Augenstein",
            "EMNLP Paper ID": "3284",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "8a12fc55d4f40bef476f7f7ad5d96ca2cfdc55e6",
            "title": "Revealing COVID-19's Social Dynamics: Diachronic Semantic Analysis of Vaccine and Symptom Discourse on Twitter",
            "abstract": "Social media is recognized as an important source for deriving insights into public opinion dynamics and social impacts due to the vast textual data generated daily and the 'unconstrained' behavior of people interacting on these platforms. However, such analyses prove challenging due to the semantic shift phenomenon, where word meanings evolve over time. This paper proposes an unsupervised dynamic word embedding method to capture longitudinal semantic shifts in social media data without predefined anchor words. The method leverages word co-occurrence statistics and dynamic updating to adapt embeddings over time, addressing the challenges of data sparseness, imbalanced distributions, and synergistic semantic effects. Evaluated on a large COVID-19 Twitter dataset, the method reveals semantic evolution patterns of vaccine- and symptom-related entities across different pandemic stages, and their potential correlations with real-world statistics. Our key contributions include the dynamic embedding technique, empirical analysis of COVID-19 semantic shifts, and discussions on enhancing semantic shift modeling for computational social science research. This study enables capturing longitudinal semantic dynamics on social media to understand public discourse and collective phenomena.",
            "link": "https://www.semanticscholar.org/paper/8a12fc55d4f40bef476f7f7ad5d96ca2cfdc55e6",
            "authors": "Zeqiang Wang, Jiageng Wu, Yuqi Wang, Wei Wang, Jie Yang, Jon Johnson, Nishanth Sastry, Suparna De",
            "matchScore": 337.5547,
            "original title": "Revealing COVID-19\u2019s Social Dynamics: Diachronic Semantic Analysis of Vaccine and Symptom Discourse on Twitter",
            "original authors": "Zeqiang Wang, Jiageng Wu, Yuqi Wang, Wei Wang XJTLU, Jie Yang, Nishanth R. Sastry, Jon Johnson, Suparna De",
            "EMNLP Paper ID": "692",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "2e274dff49ce8a119fb6d7cb192f1bc7bf4f22bb",
            "title": "Recent Advances in Hate Speech Moderation: Multimodality and the Role of Large Models",
            "abstract": "In the evolving landscape of online communication, moderating hate speech (HS) presents an intricate challenge, compounded by the multimodal nature of digital content. This comprehensive survey delves into the recent strides in HS moderation, spotlighting the burgeoning role of large language models (LLMs) and large multimodal models (LMMs). Our exploration begins with a thorough analysis of current literature, revealing the nuanced interplay between textual, visual, and auditory elements in propagating HS. We uncover a notable trend towards integrating these modalities, primarily due to the complexity and subtlety with which HS is disseminated. A significant emphasis is placed on the advances facilitated by LLMs and LMMs, which have begun to redefine the boundaries of detection and moderation capabilities. We identify existing gaps in research, particularly in the context of underrepresented languages and cultures, and the need for solutions to handle low-resource settings. The survey concludes with a forward-looking perspective, outlining potential avenues for future research, including the exploration of novel AI methodologies, the ethical governance of AI in moderation, and the development of more nuanced, context-aware systems. This comprehensive overview aims to catalyze further research and foster a collaborative effort towards more sophisticated, responsible, and human-centric approaches to HS moderation in the digital era. WARNING: This paper contains offensive examples.",
            "link": "https://www.semanticscholar.org/paper/2e274dff49ce8a119fb6d7cb192f1bc7bf4f22bb",
            "authors": "Ming Shan Hee, Shivam Sharma, Rui Cao, Palash Nandi, Tanmoy Chakraborty, Roy Ka-Wei Lee",
            "matchScore": 219.66978,
            "original title": "Recent Advances in Online Hate Speech Moderation: Multimodality and the Role of Large Models",
            "original authors": "Ming Shan Hee, Shivam Sharma, RUI CAO, Palash Nandi, Preslav Nakov, Tanmoy Chakraborty, Roy Ka-Wei Lee",
            "EMNLP Paper ID": "880",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "d48259e4c4a9f8e21077642b7d9e2bcd9b31b571",
            "title": "Quantifying Generative Media Bias with a Corpus of Real-world and Generated News Articles",
            "abstract": "Large language models (LLMs) are increasingly being utilised across a range of tasks and domains, with a burgeoning interest in their application within the field of journalism. This trend raises concerns due to our limited understanding of LLM behaviour in this domain, especially with respect to political bias. Existing studies predominantly focus on LLMs undertaking political questionnaires, which offers only limited insights into their biases and operational nuances. To address this gap, our study establishes a new curated dataset that contains 2,100 human-written articles and utilises their descriptions to generate 56,700 synthetic articles using nine LLMs. This enables us to analyse shifts in properties between human-authored and machine-generated articles, with this study focusing on political bias, detecting it using both supervised models and LLMs. Our findings reveal significant disparities between base and instruction-tuned LLMs, with instruction-tuned models exhibiting consistent political bias. Furthermore, we are able to study how LLMs behave as classifiers, observing their display of political bias even in this role. Overall, for the first time within the journalistic domain, this study outlines a framework and provides a structured dataset for quantifiable experiments, serving as a foundation for further research into LLM political bias and its implications.",
            "link": "https://www.semanticscholar.org/paper/d48259e4c4a9f8e21077642b7d9e2bcd9b31b571",
            "authors": "Filip Trhlik, Pontus Stenetorp",
            "matchScore": 277.7346,
            "original title": "Quantifying Generative Media Bias with a Corpus of Real-world and Generated News Articles",
            "original authors": "Filip Trhl\u00edk, Pontus Stenetorp",
            "EMNLP Paper ID": "881",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 14): 13:00\u201314:00 (Afternoon)",
            "Type": "Findings"
        },
        {
            "paperId": "319dc01a578c840d241f5b3fc20b9a1410c7ce9d",
            "title": "Reap the Wild Wind: Detecting Media Storms in Large-Scale News Corpora",
            "abstract": "Media Storms, dramatic outbursts of attention to a story, are central components of media dynamics and the attention landscape. Despite their significance, there has been little systematic and empirical research on this concept due to issues of measurement and operationalization. We introduce an iterative human-in-the-loop method to identify media storms in a large-scale corpus of news articles. The text is first transformed into signals of dispersion based on several textual characteristics. In each iteration, we apply unsupervised anomaly detection to these signals; each anomaly is then validated by an expert to confirm the presence of a storm, and those results are then used to tune the anomaly detection in the next iteration. We demonstrate the applicability of this method in two scenarios: first, supplementing an initial list of media storms within a specific time frame; and second, detecting media storms in new time periods. We make available a media storm dataset compiled using both scenarios. Both the method and dataset offer the basis for comprehensive empirical research into the concept of media storms, including characterizing them and predicting their outbursts and durations, in mainstream media or social media platforms.",
            "link": "https://www.semanticscholar.org/paper/319dc01a578c840d241f5b3fc20b9a1410c7ce9d",
            "authors": "Dror K. Markus, Effi Levi, Tamir Sheafer, Shaul R. Shenhav",
            "matchScore": 293.97153,
            "original title": "Reap the Wild Wind: Detecting Media Storms in Large-Scale News Corpora",
            "original authors": "Dror Kris Markus, Effi Levi, Tamir Sheafer, Shaul Rafael Shenhav",
            "EMNLP Paper ID": "941",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        }
    ],
    "Advances in Retrieval-Augmented Generation: Enhancing Performance and Robustness": [
        {
            "paperId": "b512451d431df9e411bea4c99f7135d010275445",
            "title": "Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs",
            "abstract": "Large language models (LLMs) encapsulate a vast amount of factual information within their pre-trained weights, as evidenced by their ability to answer diverse questions across different domains. However, this knowledge is inherently limited, relying heavily on the characteristics of the training data. Consequently, using external datasets to incorporate new information or refine the capabilities of LLMs on previously seen information poses a significant challenge. In this study, we compare two common approaches: unsupervised fine-tuning and retrieval-augmented generation (RAG). We evaluate both approaches on a variety of knowledge-intensive tasks across different topics. Our findings reveal that while unsupervised fine-tuning offers some improvement, RAG consistently outperforms it, both for existing knowledge encountered during training and entirely new knowledge. Moreover, we find that LLMs struggle to learn new factual information through unsupervised fine-tuning, and that exposing them to numerous variations of the same fact during training could alleviate this problem.",
            "link": "https://www.semanticscholar.org/paper/b512451d431df9e411bea4c99f7135d010275445",
            "authors": "O. Ovadia, Menachem Brief, Moshik Mishaeli, Oren Elisha",
            "EMNLP Paper ID": "24",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "d99fcc9136b60a824752a0ccb656bb9ae0c1c4ea",
            "title": "BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering",
            "abstract": "Retrieval-augmented Large Language Models (LLMs) offer substantial benefits in enhancing performance across knowledge-intensive scenarios. However, these methods often face challenges with complex inputs and encounter difficulties due to noisy knowledge retrieval, notably hindering model effectiveness. To address this issue, we introduce BlendFilter, a novel approach that elevates retrieval-augmented LLMs by integrating query generation blending with knowledge filtering. BlendFilter proposes the blending process through its query generation method, which integrates both external and internal knowledge augmentation with the original query, ensuring comprehensive information gathering. Additionally, our distinctive knowledge filtering module capitalizes on the intrinsic capabilities of the LLM, effectively eliminating extraneous data. We conduct extensive experiments on three open-domain question answering benchmarks, and the findings clearly indicate that our innovative BlendFilter surpasses state-of-the-art baselines significantly.",
            "link": "https://www.semanticscholar.org/paper/d99fcc9136b60a824752a0ccb656bb9ae0c1c4ea",
            "authors": "Haoyu Wang, Tuo Zhao, Jing Gao",
            "EMNLP Paper ID": "129",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "9e4e3132dd293b12b18fb1d7a82b28509d1cd0e6",
            "title": "RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models",
            "abstract": "The recent emergence of Medical Large Vision Language Models (Med-LVLMs) has enhanced medical diagnosis. However, current Med-LVLMs frequently encounter factual issues, often generating responses that do not align with established medical facts. Retrieval-Augmented Generation (RAG), which utilizes external knowledge, can improve the factual accuracy of these models but introduces two major challenges. First, limited retrieved contexts might not cover all necessary information, while excessive retrieval can introduce irrelevant and inaccurate references, interfering with the model's generation. Second, in cases where the model originally responds correctly, applying RAG can lead to an over-reliance on retrieved contexts, resulting in incorrect answers. To address these issues, we propose RULE, which consists of two components. First, we introduce a provably effective strategy for controlling factuality risk through the calibrated selection of the number of retrieved contexts. Second, based on samples where over-reliance on retrieved contexts led to errors, we curate a preference dataset to fine-tune the model, balancing its dependence on inherent knowledge and retrieved contexts for generation. We demonstrate the effectiveness of RULE on medical VQA and report generation tasks across three datasets, achieving an average improvement of 47.4% in factual accuracy. We publicly release our benchmark and code in https://github.com/richard-peng-xia/RULE.",
            "link": "https://www.semanticscholar.org/paper/9e4e3132dd293b12b18fb1d7a82b28509d1cd0e6",
            "authors": "Peng Xia, Kangyu Zhu, Haoran Li, Hongtu Zhu, Yun Li, Gang Li, Linjun Zhang, Huaxiu Yao",
            "EMNLP Paper ID": "135",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "50d65e1bf5cee30b24c3872afc6aabae87f44e66",
            "title": "\"Glue pizza and eat rocks\" - Exploiting Vulnerabilities in Retrieval-Augmented Generative Models",
            "abstract": "Retrieval-Augmented Generative (RAG) models enhance Large Language Models (LLMs) by integrating external knowledge bases, improving their performance in applications like fact-checking and information searching. In this paper, we demonstrate a security threat where adversaries can exploit the openness of these knowledge bases by injecting deceptive content into the retrieval database, intentionally changing the model's behavior. This threat is critical as it mirrors real-world usage scenarios where RAG systems interact with publicly accessible knowledge bases, such as web scrapings and user-contributed data pools. To be more realistic, we target a realistic setting where the adversary has no knowledge of users' queries, knowledge base data, and the LLM parameters. We demonstrate that it is possible to exploit the model successfully through crafted content uploads with access to the retriever. Our findings emphasize an urgent need for security measures in the design and deployment of RAG systems to prevent potential manipulation and ensure the integrity of machine-generated content.",
            "link": "https://www.semanticscholar.org/paper/50d65e1bf5cee30b24c3872afc6aabae87f44e66",
            "authors": "Zhen Tan, Chengshuai Zhao, Raha Moraffah, Yifan Li, Song Wang, Jundong Li, Tianlong Chen, Huan Liu",
            "EMNLP Paper ID": "189",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "a41cd77492af3140caccb7f21a351cc6dc87343f",
            "title": "SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation",
            "abstract": "Recent studies in Retrieval-Augmented Generation (RAG) have investigated extracting evidence from retrieved passages to reduce computational costs and enhance the final RAG performance, yet it remains challenging. Existing methods heavily rely on heuristic-based augmentation, encountering several issues: (1) Poor generalization due to hand-crafted context filtering; (2) Semantics deficiency due to rule-based context chunking; (3) Skewed length due to sentence-wise filter learning. To address these issues, we propose a model-based evidence extraction learning framework, SEER, optimizing a vanilla model as an evidence extractor with desired properties through self-aligned learning. Extensive experiments show that our method largely improves the final RAG performance, enhances the faithfulness, helpfulness, and conciseness of the extracted evidence, and reduces the evidence length by 9.25 times. The code will be available at https://github.com/HITsz-TMG/SEER.",
            "link": "https://www.semanticscholar.org/paper/a41cd77492af3140caccb7f21a351cc6dc87343f",
            "authors": "Xinping Zhao, Dongfang Li, Yan Zhong, Boren Hu, Yibin Chen, Baotian Hu, Min Zhang",
            "EMNLP Paper ID": "340",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "a9b04034e31a8c8ec5adff3c4665eda58b1f3013",
            "title": "RAG-QA Arena: Evaluating Domain Robustness for Long-form Retrieval Augmented Question Answering",
            "abstract": "Question answering based on retrieval augmented generation (RAG-QA) is an important research topic in NLP and has a wide range of real-world applications. However, most existing datasets for this task are either constructed using a single source corpus or consist of short extractive answers, which fall short of evaluating large language model (LLM) based RAG-QA systems on cross-domain generalization. To address these limitations, we create Long-form RobustQA (LFRQA), a new dataset comprising human-written long-form answers that integrate short extractive answers from multiple documents into a single, coherent narrative, covering 26K queries and large corpora across seven different domains. We further propose RAG-QA Arena by directly comparing model-generated answers against LFRQA's answers using LLMs as evaluators. We show via extensive experiments that RAG-QA Arena and human judgments on answer quality are highly correlated. Moreover, only 41.3% of the most competitive LLM's answers are preferred to LFRQA's answers, demonstrating RAG-QA Arena as a challenging evaluation platform for future research.",
            "link": "https://www.semanticscholar.org/paper/a9b04034e31a8c8ec5adff3c4665eda58b1f3013",
            "authors": "Rujun Han, Yuhao Zhang, Peng Qi, Yumo Xu, Jenyuan Wang, Lan Liu, William Yang Wang, Bonan Min, Vittorio Castelli",
            "EMNLP Paper ID": "477",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "90b337077164112e81ab91e5df1b8e4352b1ff3e",
            "title": "Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation",
            "abstract": "Despite the significant progress of large language models (LLMs) in various tasks, they often produce factual errors due to their limited internal knowledge. Retrieval-Augmented Generation (RAG), which enhances LLMs with external knowledge sources, offers a promising solution. However, these methods can be misled by irrelevant paragraphs in retrieved documents. Due to the inherent uncertainty in LLM generation, inputting the entire document may introduce off-topic information, causing the model to deviate from the central topic and affecting the relevance of the generated content. To address these issues, we propose the Retrieve-Plan-Generation (RPG) framework. RPG generates plan tokens to guide subsequent generation in the plan stage. In the answer stage, the model selects relevant fine-grained paragraphs based on the plan and uses them for further answer generation. This plan-answer process is repeated iteratively until completion, enhancing generation relevance by focusing on specific topics. To implement this framework efficiently, we utilize a simple but effective multi-task prompt-tuning method, enabling the existing LLMs to handle both planning and answering. We comprehensively compare RPG with baselines across 5 knowledge-intensive generation tasks, demonstrating the effectiveness of our approach.",
            "link": "https://www.semanticscholar.org/paper/90b337077164112e81ab91e5df1b8e4352b1ff3e",
            "authors": "Yuanjie Lyu, Zihan Niu, Zheyong Xie, Chao Zhang, Tong Xu, Yang Wang, Enhong Chen",
            "EMNLP Paper ID": "516",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 14): 13:00\u201314:00 (Afternoon)",
            "Type": "Main"
        },
        {
            "paperId": "6ea7b51bc1c2865d6af95d93df18687a8de16c7a",
            "title": "Crafting Personalized Agents through Retrieval-Augmented Generation on Editable Memory Graphs",
            "abstract": "In the age of mobile internet, user data, often referred to as memories, is continuously generated on personal devices. Effectively managing and utilizing this data to deliver services to users is a compelling research topic. In this paper, we introduce a novel task of crafting personalized agents powered by large language models (LLMs), which utilize a user's smartphone memories to enhance downstream applications with advanced LLM capabilities. To achieve this goal, we introduce EMG-RAG, a solution that combines Retrieval-Augmented Generation (RAG) techniques with an Editable Memory Graph (EMG). This approach is further optimized using Reinforcement Learning to address three distinct challenges: data collection, editability, and selectability. Extensive experiments on a real-world dataset validate the effectiveness of EMG-RAG, achieving an improvement of approximately 10% over the best existing approach. Additionally, the personalized agents have been transferred into a real smartphone AI assistant, which leads to enhanced usability.",
            "link": "https://www.semanticscholar.org/paper/6ea7b51bc1c2865d6af95d93df18687a8de16c7a",
            "authors": "Zheng Wang, Zhongyang Li, Zeren Jiang, Dandan Tu, Wei Shi",
            "EMNLP Paper ID": "533",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "8c4b0f3f69e1ed36f371d91dfab7e0d9f909311c",
            "title": "REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering",
            "abstract": "Considering the limited internal parametric knowledge, retrieval-augmented generation (RAG) has been widely used to extend the knowledge scope of large language models (LLMs). Despite the extensive efforts on RAG research, in existing methods, LLMs cannot precisely assess the relevance of retrieved documents, thus likely leading to misleading or even incorrect utilization of external knowledge (i.e., retrieved documents). To address this issue, in this paper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for open-domain question answering (QA). As the key motivation, we aim to enhance the self-awareness of source relevance for LLMs, so as to adaptively utilize external knowledge in RAG systems. Specially, we develop a new architecture for LLM based RAG system, by incorporating a specially designed rank head that precisely assesses the relevance of retrieved documents. Furthermore, we propose an improved training method based on bi-granularity relevance fusion and noise-resistant training. By combining the improvements in both architecture and training, our proposed REAR can better utilize external knowledge by effectively perceiving the relevance of retrieved documents. Experiments on four open-domain QA tasks show that REAR significantly outperforms previous a number of competitive RAG approaches. Our code and data can be accessed at https://github.com/RUCAIBox/REAR.",
            "link": "https://www.semanticscholar.org/paper/8c4b0f3f69e1ed36f371d91dfab7e0d9f909311c",
            "authors": "Yuhao Wang, Ruiyang Ren, Junyi Li, Wayne Xin Zhao, Jing Liu, Ji-Rong Wen",
            "EMNLP Paper ID": "625",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "90193735c3a84cf608409007df1bf409fd6635c6",
            "title": "Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation",
            "abstract": "Ensuring the verifiability of model answers is a fundamental challenge for retrieval-augmented generation (RAG) in the question answering (QA) domain. Recently, self-citation prompting was proposed to make large language models (LLMs) generate citations to supporting documents along with their answers. However, self-citing LLMs often struggle to match the required format, refer to non-existent sources, and fail to faithfully reflect LLMs' context usage throughout the generation. In this work, we present MIRAGE --Model Internals-based RAG Explanations -- a plug-and-play approach using model internals for faithful answer attribution in RAG applications. MIRAGE detects context-sensitive answer tokens and pairs them with retrieved documents contributing to their prediction via saliency methods. We evaluate our proposed approach on a multilingual extractive QA dataset, finding high agreement with human answer attribution. On open-ended QA, MIRAGE achieves citation quality and efficiency comparable to self-citation while also allowing for a finer-grained control of attribution parameters. Our qualitative evaluation highlights the faithfulness of MIRAGE's attributions and underscores the promising application of model internals for RAG answer attribution.",
            "link": "https://www.semanticscholar.org/paper/90193735c3a84cf608409007df1bf409fd6635c6",
            "authors": "Jirui Qi, Gabriele Sarti, R. Fern'andez, Arianna Bisazza",
            "EMNLP Paper ID": "674",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "c4fdd0f0dd59a3e7d7add477ab1f63826c395787",
            "title": "Do You Know What You Are Talking About? Characterizing Query-Knowledge Relevance For Reliable Retrieval Augmented Generation",
            "abstract": "Language models (LMs) are known to suffer from hallucinations and misinformation. Retrieval augmented generation (RAG) that retrieves verifiable information from an external knowledge corpus to complement the parametric knowledge in LMs provides a tangible solution to these problems. However, the generation quality of RAG is highly dependent on the relevance between a user's query and the retrieved documents. Inaccurate responses may be generated when the query is outside of the scope of knowledge represented in the external knowledge corpus or if the information in the corpus is out-of-date. In this work, we establish a statistical framework that assesses how well a query can be answered by an RAG system by capturing the relevance of knowledge. We introduce an online testing procedure that employs goodness-of-fit (GoF) tests to inspect the relevance of each user query to detect out-of-knowledge queries with low knowledge relevance. Additionally, we develop an offline testing framework that examines a collection of user queries, aiming to detect significant shifts in the query distribution which indicates the knowledge corpus is no longer sufficiently capable of supporting the interests of the users. We demonstrate the capabilities of these strategies through a systematic evaluation on eight question-answering (QA) datasets, the results of which indicate that the new testing framework is an efficient solution to enhance the reliability of existing RAG systems.",
            "link": "https://www.semanticscholar.org/paper/c4fdd0f0dd59a3e7d7add477ab1f63826c395787",
            "authors": "Zhuohang Li, Jiaxin Zhang, Chao Yan, Kamalika Das, Sricharan Kumar, Murat Kantarcioglu, Bradley Malin",
            "EMNLP Paper ID": "688",
            "Oral/Poster": "Oral",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "e92ba8094924de3a6c1c8591f5be418785703bf6",
            "title": "Satyrn: A Platform for Analytics Augmented Generation",
            "abstract": "Large language models (LLMs) are capable of producing documents, and retrieval augmented generation (RAG) has shown itself to be a powerful method for improving accuracy without sacrificing fluency. However, not all information can be retrieved from text. We propose an approach that uses the analysis of structured data to generate fact sets that are used to guide generation in much the same way that retrieved documents are used in RAG. This analytics augmented generation (AAG) approach supports the ability to utilize standard analytic techniques to generate facts that are then converted to text and passed to an LLM. We present a neurosymbolic platform, Satyrn, that leverages AAG to produce accurate, fluent, and coherent reports grounded in large scale databases. In our experiments, we find that Satyrn generates reports in which over 86% of claims are accurate while maintaining high levels of fluency and coherence, even when using smaller language models such as Mistral-7B, as compared to GPT-4 Code Interpreter in which just 57% of claims are accurate.",
            "link": "https://www.semanticscholar.org/paper/e92ba8094924de3a6c1c8591f5be418785703bf6",
            "authors": "Marko Sterbentz, Cameron Barrie, Shubham Shahi, Abhratanu Dutta, Donna Hooshmand, Harper Pack, Kristian J. Hammond",
            "EMNLP Paper ID": "717",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "13241422a6fef4e41dbb4d0f3e40e39363e07826",
            "title": "SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information",
            "abstract": "Large Vision-Language Models (LVLMs) have become pivotal at the intersection of computer vision and natural language processing. However, the full potential of LVLMs Retrieval-Augmented Generation (RAG) capabilities remains underutilized. Existing works either focus solely on the text modality or are limited to specific tasks. Moreover, most LVLMs struggle to selectively utilize retrieved information and are sensitive to irrelevant or misleading references. To address these challenges, we propose a self-refinement framework designed to teach LVLMs to Selectively Utilize Retrieved Information (SURf). Specifically, when given questions that are incorrectly answered by the LVLM backbone, we obtain references that help correct the answers (positive references) and those that do not (negative references). We then fine-tune the LVLM backbone using a combination of these positive and negative references. Our experiments across three tasks and seven datasets demonstrate that our framework significantly enhances LVLMs ability to effectively utilize retrieved multimodal references and improves their robustness against irrelevant or misleading information. The source code is available at https://github.com/GasolSun36/SURf.",
            "link": "https://www.semanticscholar.org/paper/13241422a6fef4e41dbb4d0f3e40e39363e07826",
            "authors": "Jiashuo Sun, Jihai Zhang, Yucheng Zhou, Zhaochen Su, Xiaoye Qu, Yu Cheng",
            "EMNLP Paper ID": "862",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "7fc1aaa277898af0444a7788440760f6008ed530",
            "title": "From RAG to RICHES: Retrieval Interlaced with Sequence Generation",
            "abstract": "We present RICHES, a novel approach that interleaves retrieval with sequence generation tasks. RICHES offers an alternative to conventional RAG systems by eliminating the need for separate retriever and generator. It retrieves documents by directly decoding their contents, constrained on the corpus. Unifying retrieval with generation allows us to adapt to diverse new tasks via prompting alone. RICHES can work with any Instruction-tuned model, without additional training. It provides attributed evidence, supports multi-hop retrievals and interleaves thoughts to plan on what to retrieve next, all within a single decoding pass of the LLM. We demonstrate the strong performance of RICHES across ODQA tasks including attributed and multi-hop QA.",
            "link": "https://www.semanticscholar.org/paper/7fc1aaa277898af0444a7788440760f6008ed530",
            "authors": "Palak Jain, Livio Baldini Soares, Tom Kwiatkowski",
            "EMNLP Paper ID": "1015",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "14b588f38a3af6ef1b0186e2bb98c77d3b650093",
            "title": "ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented Generator",
            "abstract": "Large language models (LLMs) are proven to benefit a lot from retrieval-augmented generation (RAG) in alleviating hallucinations confronted with knowledge-intensive questions. RAG adopts information retrieval techniques to inject external knowledge from semantic-relevant documents as input contexts. However, since today's Internet is flooded with numerous noisy and fabricating content, it is inevitable that RAG systems are vulnerable to these noises and prone to respond incorrectly. To this end, we propose to optimize the retrieval-augmented Generator with an Adversarial Tuning Multi-agent system (ATM). The ATM steers the Generator to have a robust perspective of useful documents for question answering with the help of an auxiliary Attacker agent through adversarially tuning the agents for several iterations. After rounds of multi-agent iterative tuning, the Generator can eventually better discriminate useful documents amongst fabrications. The experimental results verify the effectiveness of ATM and we also observe that the Generator can achieve better performance compared to the state-of-the-art baselines.",
            "link": "https://www.semanticscholar.org/paper/14b588f38a3af6ef1b0186e2bb98c77d3b650093",
            "authors": "Junda Zhu, Lingyong Yan, Haibo Shi, Dawei Yin, Lei Sha",
            "EMNLP Paper ID": "1243",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "b8c8a7d5a1ac7b91f3ae6ea93a23c722d323a427",
            "title": "Adaptive Query Rewriting: Aligning Rewriters through Marginal Probability of Conversational Answers",
            "abstract": "Query rewriting is a crucial technique for passage retrieval in open-domain conversational question answering (CQA). It decontexualizes conversational queries into self-contained questions suitable for off-the-shelf retrievers. Existing methods attempt to incorporate retriever's preference during the training of rewriting models. However, these approaches typically rely on extensive annotations such as in-domain rewrites and/or relevant passage labels, limiting the models' generalization and adaptation capabilities. In this paper, we introduce AdaQR ($\\textbf{Ada}$ptive $\\textbf{Q}$uery $\\textbf{R}$ewriting), a framework for training query rewriting models with limited rewrite annotations from seed datasets and completely no passage label. Our approach begins by fine-tuning compact large language models using only ~$10\\%$ of rewrite annotations from the seed dataset training split. The models are then utilized to generate rewrite candidates for each query instance. A novel approach is then proposed to assess retriever's preference for these candidates by the probability of answers conditioned on the conversational query by marginalizing the Top-$K$ passages. This serves as the reward for optimizing the rewriter further using Direct Preference Optimization (DPO), a process free of rewrite and retrieval annotations. Experimental results on four open-domain CQA datasets demonstrate that AdaQR not only enhances the in-domain capabilities of the rewriter with limited annotation requirement, but also adapts effectively to out-of-domain datasets.",
            "link": "https://www.semanticscholar.org/paper/b8c8a7d5a1ac7b91f3ae6ea93a23c722d323a427",
            "authors": "Tianhua Zhang, Kun Li, Hongyin Luo, Xixin Wu, James Glass, Helen M. Meng",
            "EMNLP Paper ID": "1556",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "5e52c1c4fc90870df11efd1348f54bc56407d773",
            "title": "Exploring the Practicality of Generative Retrieval on Dynamic Corpora",
            "abstract": "Benchmarking the performance of information retrieval (IR) is mostly conducted with a fixed set of documents (static corpora). However, in realistic scenarios, this is rarely the case and the documents to be retrieved are constantly updated and added. In this paper, we focus on Generative Retrievals (GR), which apply autoregressive language models to IR problems, and explore their adaptability and robustness in dynamic scenarios. We also conduct an extensive evaluation of computational and memory efficiency, crucial factors for real-world deployment of IR systems handling vast and ever-changing document collections. Our results on the StreamingQA benchmark demonstrate that GR is more adaptable to evolving knowledge (4-11%), robust in learning knowledge with temporal information, and efficient in terms of inference FLOPs (x2), indexing time (x6), and storage footprint (x4) compared to Dual Encoders (DE), which are commonly used in retrieval systems. Our paper highlights the potential of GR for future use in practical IR systems within dynamic environments.",
            "link": "https://www.semanticscholar.org/paper/5e52c1c4fc90870df11efd1348f54bc56407d773",
            "authors": "Soyoung Yoon, Chaeeun Kim, Hyunji Lee, Joel Jang, Minjoon Seo",
            "EMNLP Paper ID": "1574",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "1494dfcf564b09c6ebbfc772fa986937bb14d699",
            "title": "DynamicER: Resolving Emerging Mentions to Dynamic Entities for RAG",
            "abstract": "In the rapidly evolving landscape of language, resolving new linguistic expressions in continuously updating knowledge bases remains a formidable challenge. This challenge becomes critical in retrieval-augmented generation (RAG) with knowledge bases, as emerging expressions hinder the retrieval of relevant documents, leading to generator hallucinations. To address this issue, we introduce a novel task aimed at resolving emerging mentions to dynamic entities and present DynamicER benchmark. Our benchmark includes dynamic entity mention resolution and entity-centric knowledge-intensive QA task, evaluating entity linking and RAG model's adaptability to new expressions, respectively. We discovered that current entity linking models struggle to link these new expressions to entities. Therefore, we propose a temporal segmented clustering method with continual adaptation, effectively managing the temporal dynamics of evolving entities and emerging mentions. Extensive experiments demonstrate that our method outperforms existing baselines, enhancing RAG model performance on QA task with resolved mentions.",
            "link": "https://www.semanticscholar.org/paper/1494dfcf564b09c6ebbfc772fa986937bb14d699",
            "authors": "Jinyoung Kim, Dayoon Ko, Gunhee Kim",
            "EMNLP Paper ID": "1588",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "6489640b1d30a8a3e7cb906bb6557f1ccd0d799d",
            "title": "Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models",
            "abstract": "Retrieval-augmented language models (RALMs) represent a substantial advancement in the capabilities of large language models, notably in reducing factual hallucination by leveraging external knowledge sources. However, the reliability of the retrieved information is not always guaranteed. The retrieval of irrelevant data can lead to misguided responses, and potentially causing the model to overlook its inherent knowledge, even when it possesses adequate information to address the query. Moreover, standard RALMs often struggle to assess whether they possess adequate knowledge, both intrinsic and retrieved, to provide an accurate answer. In situations where knowledge is lacking, these systems should ideally respond with\"unknown\"when the answer is unattainable. In response to these challenges, we introduces Chain-of-Noting (CoN), a novel approach aimed at improving the robustness of RALMs in facing noisy, irrelevant documents and in handling unknown scenarios. The core idea of CoN is to generate sequential reading notes for retrieved documents, enabling a thorough evaluation of their relevance to the given question and integrating this information to formulate the final answer. We employed ChatGPT to create training data for CoN, which was subsequently trained on an LLaMa-2 7B model. Our experiments across four open-domain QA benchmarks show that RALMs equipped with CoN significantly outperform standard RALMs. Notably, CoN achieves an average improvement of +7.9 in EM score given entirely noisy retrieved documents and +10.5 in rejection rates for real-time questions that fall outside the pre-training knowledge scope.",
            "link": "https://www.semanticscholar.org/paper/6489640b1d30a8a3e7cb906bb6557f1ccd0d799d",
            "authors": "W. Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, Dong Yu",
            "EMNLP Paper ID": "1689",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "c2d066ee86dc324856971f5351738265a5929a77",
            "title": "Deciphering the Interplay of Parametric and Non-parametric Memory in Retrieval-augmented Language Models",
            "abstract": "Generative language models often struggle with specialized or less-discussed knowledge. A potential solution is found in Retrieval-Augmented Generation (RAG) models which act like retrieving information before generating responses. In this study, we explore how the \\textsc{Atlas} approach, a RAG model, decides between what it already knows (parametric) and what it retrieves (non-parametric). We use causal mediation analysis and controlled experiments to examine how internal representations influence information processing. Our findings disentangle the effects of parametric knowledge and the retrieved context. They indicate that in cases where the model can choose between both types of information (parametric and non-parametric), it relies more on the context than the parametric knowledge. Furthermore, the analysis investigates the computations involved in \\emph{how} the model uses the information from the context. We find that multiple mechanisms are active within the model and can be detected with mediation analysis: first, the decision of \\emph{whether the context is relevant}, and second, how the encoder computes output representations to support copying when relevant.",
            "link": "https://www.semanticscholar.org/paper/c2d066ee86dc324856971f5351738265a5929a77",
            "authors": "M. Farahani, Richard Johansson",
            "EMNLP Paper ID": "1996",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "dd4db8841fc2471815cc6b41bc209cefdf960491",
            "title": "ClimRetrieve: A Benchmarking Dataset for Information Retrieval from Corporate Climate Disclosures",
            "abstract": "To handle the vast amounts of qualitative data produced in corporate climate communication, stakeholders increasingly rely on Retrieval Augmented Generation (RAG) systems. However, a significant gap remains in evaluating domain-specific information retrieval - the basis for answer generation. To address this challenge, this work simulates the typical tasks of a sustainability analyst by examining 30 sustainability reports with 16 detailed climate-related questions. As a result, we obtain a dataset with over 8.5K unique question-source-answer pairs labeled by different levels of relevance. Furthermore, we develop a use case with the dataset to investigate the integration of expert knowledge into information retrieval with embeddings. Although we show that incorporating expert knowledge works, we also outline the critical limitations of embeddings in knowledge-intensive downstream domains like climate change communication.",
            "link": "https://www.semanticscholar.org/paper/dd4db8841fc2471815cc6b41bc209cefdf960491",
            "authors": "Tobias Schimanski, Jingwei Ni, Roberto Spacey, Nicola Ranger, Markus Leippold",
            "EMNLP Paper ID": "2088",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "9a946c503b6e799b3d57375b6edfaf4e24febcea",
            "title": "Searching for Best Practices in Retrieval-Augmented Generation",
            "abstract": "Retrieval-augmented generation (RAG) techniques have proven to be effective in integrating up-to-date information, mitigating hallucinations, and enhancing response quality, particularly in specialized domains. While many RAG approaches have been proposed to enhance large language models through query-dependent retrievals, these approaches still suffer from their complex implementation and prolonged response times. Typically, a RAG workflow involves multiple processing steps, each of which can be executed in various ways. Here, we investigate existing RAG approaches and their potential combinations to identify optimal RAG practices. Through extensive experiments, we suggest several strategies for deploying RAG that balance both performance and efficiency. Moreover, we demonstrate that multimodal retrieval techniques can significantly enhance question-answering capabilities about visual inputs and accelerate the generation of multimodal content using a\"retrieval as generation\"strategy.",
            "link": "https://www.semanticscholar.org/paper/9a946c503b6e799b3d57375b6edfaf4e24febcea",
            "authors": "Xiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran Zhang, Yixin Wu, Zhibo Xu, Tianyuan Shi, Zhengyuan Wang, Shizheng Li, Qi Qian, Ruicheng Yin, Changze Lv, Xiaoqing Zheng, Xuanjing Huang",
            "EMNLP Paper ID": "2122",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "fc1ffbd2e2bf7b90a30f17dbc93c3d3f43ff7805",
            "title": "Unveiling and Consulting Core Experts in Retrieval-Augmented MoE-based LLMs",
            "abstract": "Retrieval-Augmented Generation (RAG) significantly improved the ability of Large Language Models (LLMs) to solve knowledge-intensive tasks. While existing research seeks to enhance RAG performance by retrieving higher-quality documents or designing RAG-specific LLMs, the internal mechanisms within LLMs that contribute to the effectiveness of RAG systems remain underexplored. In this paper, we aim to investigate these internal mechanisms within the popular Mixture-of-Expert (MoE)-based LLMs and demonstrate how to improve RAG by examining expert activations in these LLMs. Our controlled experiments reveal that several core groups of experts are primarily responsible for RAG-related behaviors. The activation of these core experts can signify the model's inclination towards external/internal knowledge and adjust its behavior. For instance, we identify core experts that can (1) indicate the sufficiency of the model's internal knowledge, (2) assess the quality of retrieved documents, and (3) enhance the model's ability to utilize context. Based on these findings, we propose several strategies to enhance RAG's efficiency and effectiveness through expert activation. Experimental results across various datasets and MoE-based LLMs show the effectiveness of our method.",
            "link": "https://www.semanticscholar.org/paper/fc1ffbd2e2bf7b90a30f17dbc93c3d3f43ff7805",
            "authors": "Xin Zhou, Ping Nie, Yiwen Guo, Haojie Wei, Zhanqiu Zhang, Pasquale Minervini, Ruotian Ma, Tao Gui, Qi Zhang, Xuanjing Huang",
            "EMNLP Paper ID": "2175",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "c224c9a8490769bcdd7c7798008e67fdfaa350c4",
            "title": "Improving Zero-shot LLM Re-Ranker with Risk Minimization",
            "abstract": "In the Retrieval-Augmented Generation (RAG) system, advanced Large Language Models (LLMs) have emerged as effective Query Likelihood Models (QLMs) in an unsupervised way, which re-rank documents based on the probability of generating the query given the content of a document. However, directly prompting LLMs to approximate QLMs inherently is biased, where the estimated distribution might diverge from the actual document-specific distribution. In this study, we introduce a novel framework, $\\mathrm{UR^3}$, which leverages Bayesian decision theory to both quantify and mitigate this estimation bias. Specifically, $\\mathrm{UR^3}$ reformulates the problem as maximizing the probability of document generation, thereby harmonizing the optimization of query and document generation probabilities under a unified risk minimization objective. Our empirical results indicate that $\\mathrm{UR^3}$ significantly enhances re-ranking, particularly in improving the Top-1 accuracy. It benefits the QA tasks by achieving higher accuracy with fewer input documents.",
            "link": "https://www.semanticscholar.org/paper/c224c9a8490769bcdd7c7798008e67fdfaa350c4",
            "authors": "Xiaowei Yuan, Zhao Yang, Yequan Wang, Jun Zhao, Kang Liu",
            "EMNLP Paper ID": "2194",
            "Oral/Poster": "Oral",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "386c1852329d981eca785f986d93cbf993dbaae4",
            "title": "SynthesizRR: Generating Diverse Datasets with Retrieval Augmentation",
            "abstract": "It is often desirable to distill the capabilities of large language models (LLMs) into smaller student models due to compute and memory constraints. One way to do this for classification tasks is via dataset synthesis, which can be accomplished by generating examples of each label from the LLM. Prior approaches to synthesis use few-shot prompting, which relies on the LLM's parametric knowledge to generate usable examples. However, this leads to issues of repetition, bias towards popular entities, and stylistic differences from human text. In this work, we propose Synthesize by Retrieval and Refinement (SynthesizRR), which uses retrieval augmentation to introduce variety into the dataset synthesis process: as retrieved passages vary, the LLM is seeded with different content to generate its examples. We empirically study the synthesis of six datasets, covering topic classification, sentiment analysis, tone detection, and humor, requiring complex synthesis strategies. We find that SynthesizRR greatly improves lexical and semantic diversity, similarity to human-written text, and distillation performance, when compared to 32-shot prompting and four prior approaches. We release our extensive codebase at https://github.com/amazon-science/synthesizrr",
            "link": "https://www.semanticscholar.org/paper/386c1852329d981eca785f986d93cbf993dbaae4",
            "authors": "Abhishek Divekar, Greg Durrett",
            "EMNLP Paper ID": "2436",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "e876f221c3eaf542d0d79f5f1f5c2f51ad2d0a48",
            "title": "Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation",
            "abstract": "The rapid development of large language models has led to the widespread adoption of Retrieval-Augmented Generation (RAG), which integrates external knowledge to alleviate knowledge bottlenecks and mitigate hallucinations. However, the existing RAG paradigm inevitably suffers from the impact of flawed information introduced during the retrieval phrase, thereby diminishing the reliability and correctness of the generated outcomes. In this paper, we propose Credibility-aware Generation (CAG), a universally applicable framework designed to mitigate the impact of flawed information in RAG. At its core, CAG aims to equip models with the ability to discern and process information based on its credibility. To this end, we propose an innovative data transformation framework that generates data based on credibility, thereby effectively endowing models with the capability of CAG. Furthermore, to accurately evaluate the models' capabilities of CAG, we construct a comprehensive benchmark covering three critical real-world scenarios. Experimental results demonstrate that our model can effectively understand and utilize credibility for generation, significantly outperform other models with retrieval augmentation, and exhibit resilience against the disruption caused by noisy documents, thereby maintaining robust performance. Moreover, our model supports customized credibility, offering a wide range of potential applications.",
            "link": "https://www.semanticscholar.org/paper/e876f221c3eaf542d0d79f5f1f5c2f51ad2d0a48",
            "authors": "Ruotong Pan, Boxi Cao, Hongyu Lin, Xianpei Han, Jia Zheng, Sirui Wang, Xunliang Cai, Le Sun",
            "EMNLP Paper ID": "2581",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "755bf404a7e8a81df53172abd3fdb5ff8e31ecf3",
            "title": "RE-RAG: Improving Open-Domain QA Performance and Interpretability with Relevance Estimator in Retrieval-Augmented Generation",
            "abstract": "The Retrieval Augmented Generation (RAG) framework utilizes a combination of parametric knowledge and external knowledge to demonstrate state-of-the-art performance on open-domain question answering tasks. However, the RAG framework suffers from performance degradation when the query is accompanied by irrelevant contexts. In this work, we propose the RE-RAG framework, which introduces a relevance estimator (RE) that not only provides relative relevance between contexts as previous rerankers did, but also provides confidence, which can be used to classify whether given context is useful for answering the given question. We propose a weakly supervised method for training the RE simply utilizing question-answer data without any labels for correct contexts. We show that RE trained with a small generator (sLM) can not only improve the sLM fine-tuned together with RE but also improve previously unreferenced large language models (LLMs). Furthermore, we investigate new decoding strategies that utilize the proposed confidence measured by RE such as choosing to let the user know that it is\"unanswerable\"to answer the question given the retrieved contexts or choosing to rely on LLM's parametric knowledge rather than unrelated contexts.",
            "link": "https://www.semanticscholar.org/paper/755bf404a7e8a81df53172abd3fdb5ff8e31ecf3",
            "authors": "Kiseung Kim, Jay-Yoon Lee",
            "EMNLP Paper ID": "3127",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "3e2cbbf5f677f372cbb21969fe268a9b8a1ad64a",
            "title": "LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering",
            "abstract": "Long-Context Question Answering (LCQA), a challenging task, aims to reason over long-context documents to yield accurate answers to questions. Existing long-context Large Language Models (LLMs) for LCQA often struggle with the\"lost in the middle\"issue. Retrieval-Augmented Generation (RAG) mitigates this issue by providing external factual evidence. However, its chunking strategy disrupts the global long-context information, and its low-quality retrieval in long contexts hinders LLMs from identifying effective factual details due to substantial noise. To this end, we propose LongRAG, a general, dual-perspective, and robust LLM-based RAG system paradigm for LCQA to enhance RAG's understanding of complex long-context knowledge (i.e., global information and factual details). We design LongRAG as a plug-and-play paradigm, facilitating adaptation to various domains and LLMs. Extensive experiments on three multi-hop datasets demonstrate that LongRAG significantly outperforms long-context LLMs (up by 6.94%), advanced RAG (up by 6.16%), and Vanilla RAG (up by 17.25%). Furthermore, we conduct quantitative ablation studies and multi-dimensional analyses, highlighting the effectiveness of the system's components and fine-tuning strategies. Data and code are available at https://github.com/QingFei1/LongRAG.",
            "link": "https://www.semanticscholar.org/paper/3e2cbbf5f677f372cbb21969fe268a9b8a1ad64a",
            "authors": "Qingfei Zhao, Ruobing Wang, Yukuo Cen, Daren Zha, Shicheng Tan, Yuxiao Dong, Jie Tang",
            "EMNLP Paper ID": "3261",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "20a8a84db1ebf5a8b75525671f5baf431a32f1a3",
            "title": "BERGEN: A Benchmarking Library for Retrieval-Augmented Generation",
            "abstract": "Retrieval-Augmented Generation allows to enhance Large Language Models with external knowledge. In response to the recent popularity of generative LLMs, many RAG approaches have been proposed, which involve an intricate number of different configurations such as evaluation datasets, collections, metrics, retrievers, and LLMs. Inconsistent benchmarking poses a major challenge in comparing approaches and understanding the impact of each component in the pipeline. In this work, we study best practices that lay the groundwork for a systematic evaluation of RAG and present BERGEN, an end-to-end library for reproducible research standardizing RAG experiments. In an extensive study focusing on QA, we benchmark different state-of-the-art retrievers, rerankers, and LLMs. Additionally, we analyze existing RAG metrics and datasets. Our open-source library BERGEN is available under \\url{https://github.com/naver/bergen}.",
            "link": "https://www.semanticscholar.org/paper/20a8a84db1ebf5a8b75525671f5baf431a32f1a3",
            "authors": "David Rau, Herv'e D'ejean, Nadezhda Chirkova, Thibault Formal, Shuai Wang, Vassilina Nikoulina, S. Clinchant",
            "matchScore": 232.5075,
            "original title": "BERGEN: A Benchmarking Library for Retrieval-Augmented Generation",
            "original authors": "David Rau, Herv\u00e9 D\u00e9jean, Nadezhda Chirkova, Thibault Formal, shuai wang, St\u00e9phane CLINCHANT, Vassilina Nikoulina",
            "EMNLP Paper ID": "1593",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "26420d986b441066a8350d445a6b7f46af93e952",
            "title": "TRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for Retrieval-Augmented Generation",
            "abstract": "Retrieval-augmented generation (RAG) offers an effective approach for addressing question answering (QA) tasks. However, the imperfections of the retrievers in RAG models often result in the retrieval of irrelevant information, which could introduce noises and degrade the performance, especially when handling multi-hop questions that require multiple steps of reasoning. To enhance the multi-hop reasoning ability of RAG models, we propose TRACE. TRACE constructs knowledge-grounded reasoning chains, which are a series of logically connected knowledge triples, to identify and integrate supporting evidence from the retrieved documents for answering questions. Specifically, TRACE employs a KG Generator to create a knowledge graph (KG) from the retrieved documents, and then uses an Autoregressive Reasoning Chain Constructor to build reasoning chains. Experimental results on three multi-hop QA datasets show that TRACE achieves an average performance improvement of up to 14.03% compared to using all the retrieved documents. Moreover, the results indicate that using reasoning chains as context, rather than the entire documents, is often sufficient to correctly answer questions.",
            "link": "https://www.semanticscholar.org/paper/26420d986b441066a8350d445a6b7f46af93e952",
            "authors": "Jinyuan Fang, Zaiqiao Meng, Craig Macdonald",
            "matchScore": 283.4465,
            "original title": "TRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for Retrieval-Augmented Generation",
            "original authors": "Jinyuan Fang, Zaiqiao Meng, Craig MacDonald",
            "EMNLP Paper ID": "1782",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 12): 17:45\u201318:45 (Evening)",
            "Type": "Findings"
        },
        {
            "paperId": "d09d91b7294f5e486d00dad2a900cd3c2cbd00b9",
            "title": "RaFe: Ranking Feedback Improves Query Rewriting for RAG",
            "abstract": "As Large Language Models (LLMs) and Retrieval Augmentation Generation (RAG) techniques have evolved, query rewriting has been widely incorporated into the RAG system for downstream tasks like open-domain QA. Many works have attempted to utilize small models with reinforcement learning rather than costly LLMs to improve query rewriting. However, current methods require annotations (e.g., labeled relevant documents or downstream answers) or predesigned rewards for feedback, which lack generalization, and fail to utilize signals tailored for query rewriting. In this paper, we propose ours, a framework for training query rewriting models free of annotations. By leveraging a publicly available reranker, ours~provides feedback aligned well with the rewriting objectives. Experimental results demonstrate that ours~can obtain better performance than baselines.",
            "link": "https://www.semanticscholar.org/paper/d09d91b7294f5e486d00dad2a900cd3c2cbd00b9",
            "authors": "Shengyu Mao, Yong Jiang, Boli Chen, Xiao Li, Peng Wang, Xinyu Wang, Pengjun Xie, Fei Huang, Huajun Chen, Ningyu Zhang",
            "matchScore": 280.48328,
            "original title": "RaFe: Ranking Feedback Improves Query Rewriting for RAG",
            "original authors": "Shengyu Mao, Yong Jiang, Boli Chen, Xiao Li, Peng Wang, Xinyu Wang, Pengjun Xie, Fei Huang, Huajun Chen, Ningyu Zhang",
            "EMNLP Paper ID": "183",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "218602071439fcdb80a6c28b876c2ebd3f7fba27",
            "title": "QPaug: Question and Passage Augmentation for Open-Domain Question Answering of LLMs",
            "abstract": "Retrieval-augmented generation (RAG) has received much attention for Open-domain question-answering (ODQA) tasks as a means to compensate for the parametric knowledge of large language models (LLMs). While previous approaches focused on processing retrieved passages to remove irrelevant context, they still rely heavily on the quality of retrieved passages which can degrade if the question is ambiguous or complex. In this paper, we propose a simple yet efficient method called question and passage augmentation (QPaug) via LLMs for open-domain QA. QPaug first decomposes the original questions into multiple-step sub-questions. By augmenting the original question with detailed sub-questions and planning, we are able to make the query more specific on what needs to be retrieved, improving the retrieval performance. In addition, to compensate for the case where the retrieved passages contain distracting information or divided opinions, we augment the retrieved passages with self-generated passages by LLMs to guide the answer extraction. Experimental results show that QPaug outperforms the previous state-of-the-art and achieves significant performance gain over existing RAG methods. The source code is available at \\url{https://github.com/kmswin1/QPaug}.",
            "link": "https://www.semanticscholar.org/paper/218602071439fcdb80a6c28b876c2ebd3f7fba27",
            "authors": "Minsang Kim, Cheoneum Park, Seung Baek",
            "matchScore": 277.79364,
            "original title": "QPaug: Question and Passage Augmentation for Open-Domain Question Answering of LLMs",
            "original authors": "Minsang Kim, Seung Jun Baek",
            "EMNLP Paper ID": "1887",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "275353fbecb8faefae50d3ced6272913e243800b",
            "title": "Exploring Hint Generation Approaches in Open-Domain Question Answering",
            "abstract": "Automatic Question Answering (QA) systems rely on contextual information to provide accurate answers. Commonly, contexts are prepared through either retrieval-based or generation-based methods. The former involves retrieving relevant documents from a corpus like Wikipedia, whereas the latter uses generative models such as Large Language Models (LLMs) to generate the context. In this paper, we introduce a novel context preparation approach called HINTQA, which employs Automatic Hint Generation (HG) techniques. Unlike traditional methods, HINTQA prompts LLMs to produce hints about potential answers for the question rather than generating relevant context. We evaluate our approach across three QA datasets including TriviaQA, NaturalQuestions, and Web Questions, examining how the number and order of hints impact performance. Our findings show that the HINTQA surpasses both retrieval-based and generation-based approaches. We demonstrate that hints enhance the accuracy of answers more than retrieved and generated contexts.",
            "link": "https://www.semanticscholar.org/paper/275353fbecb8faefae50d3ced6272913e243800b",
            "authors": "Jamshid Mozafari, Abdelrahman Abdallah, Bhawna Piryani, Adam Jatowt",
            "matchScore": 228.78278,
            "original title": "HiGenQA: Exploring Hint Generation Approaches for Open Domain Question Answering",
            "original authors": "Jamshid Mozafari, Abdelrahman Abdallah, Bhawna Piryani, Adam Jatowt",
            "EMNLP Paper ID": "1955",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "1ab76168a8fc29887ce54f21d518f528fc7d2ba2",
            "title": "Who's Who: Large Language Models Meet Knowledge Conflicts in Practice",
            "abstract": "Retrieval-augmented generation (RAG) methods are viable solutions for addressing the static memory limits of pre-trained language models. Nevertheless, encountering conflicting sources of information within the retrieval context is an inevitable practical challenge. In such situations, the language models are recommended to transparently inform users about the conflicts rather than autonomously deciding what to present based on their inherent biases. To analyze how current large language models (LLMs) align with our recommendation, we introduce WhoQA, a public benchmark dataset to examine model's behavior in knowledge conflict situations. We induce conflicts by asking about a common property among entities having the same name, resulting in questions with up to 8 distinctive answers. WhoQA evaluation set includes 5K questions across 13 Wikidata property types and 150K Wikipedia entities. Our experiments show that despite the simplicity of WhoQA questions, knowledge conflicts significantly degrades LLMs' performance in RAG settings.",
            "link": "https://www.semanticscholar.org/paper/1ab76168a8fc29887ce54f21d518f528fc7d2ba2",
            "authors": "Quang Hieu Pham, Hoang Ngo, A. Luu, Dat Quoc Nguyen",
            "matchScore": 268.0477,
            "original title": "Who\u2019s Who: Large Language Models Meet Knowledge Conflicts in Practice",
            "original authors": "Quang Hieu Pham, Hoang Ngo, Anh Tuan Luu, Dat Quoc Nguyen",
            "EMNLP Paper ID": "2082",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "f7ed6f02ba64866d3f7b7bac3bf65da36cef1336",
            "title": "Retrieving, Rethinking and Revising: The Chain-of-Verification Can Improve Retrieval Augmented Generation",
            "abstract": "Recent Retrieval Augmented Generation (RAG) aims to enhance Large Language Models (LLMs) by incorporating extensive knowledge retrieved from external sources. However, such approach encounters some challenges: Firstly, the original queries may not be suitable for precise retrieval, resulting in erroneous contextual knowledge; Secondly, the language model can easily generate inconsistent answer with external references due to their knowledge boundary limitation. To address these issues, we propose the chain-of-verification (CoV-RAG) to enhance the external retrieval correctness and internal generation consistency. Specifically, we integrate the verification module into the RAG, engaging in scoring, judgment, and rewriting. To correct external retrieval errors, CoV-RAG retrieves new knowledge using a revised query. To correct internal generation errors, we unify QA and verification tasks with a Chain-of-Thought (CoT) reasoning during training. Our comprehensive experiments across various LLMs demonstrate the effectiveness and adaptability compared with other strong baselines. Especially, our CoV-RAG can significantly surpass the state-of-the-art baselines using different LLM backbones.",
            "link": "https://www.semanticscholar.org/paper/f7ed6f02ba64866d3f7b7bac3bf65da36cef1336",
            "authors": "Bolei He, Nuo Chen, Xinran He, Lingyong Yan, Zhenkai Wei, Jinchang Luo, Zhen-Hua Ling",
            "matchScore": 295.98303,
            "original title": "Retrieving, Rethinking and Revising: The Chain-of-Verification Can Improve Retrieval Augmented Generation",
            "original authors": "Bolei He, CHENNUO, Xinran He, Lingyong Yan, zhenkai wei, Jinchang Luo, Zhen-Hua Ling",
            "EMNLP Paper ID": "2118",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "0fa1b06de73d01c2bfcfd16f0f5cbafb7fd1c553",
            "title": "Learning When to Retrieve, What to Rewrite, and How to Respond in Conversational QA",
            "abstract": "Augmenting Large Language Models (LLMs) with information retrieval capabilities (i.e., Retrieval-Augmented Generation (RAG)) has proven beneficial for knowledge-intensive tasks. However, understanding users' contextual search intent when generating responses is an understudied topic for conversational question answering (QA). This conversational extension leads to additional concerns when compared to single-turn QA as it is more challenging for systems to comprehend conversational context and manage retrieved passages over multiple turns. In this work, we propose a method for enabling LLMs to decide when to retrieve in RAG settings given a conversational context. When retrieval is deemed necessary, the LLM then rewrites the conversation for passage retrieval and judges the relevance of returned passages before response generation. Operationally, we build on the single-turn SELF-RAG framework (Asai et al., 2023) and propose SELF-multi-RAG for conversational settings. SELF-multi-RAG demonstrates improved capabilities over single-turn variants with respect to retrieving relevant passages (by using summarized conversational context) and assessing the quality of generated responses. Experiments on three conversational QA datasets validate the enhanced response generation capabilities of SELF-multi-RAG, with improvements of ~13% measured by human annotation.",
            "link": "https://www.semanticscholar.org/paper/0fa1b06de73d01c2bfcfd16f0f5cbafb7fd1c553",
            "authors": "Nirmal Roy, Leonardo F. R. Ribeiro, Rexhina Blloshmi, Kevin Small",
            "matchScore": 287.6851,
            "original title": "Learning When to Retrieve, What to Rewrite, and How to Respond in Conversational QA",
            "original authors": "Nirmal Roy, Leonardo F. R. Ribeiro, Rexhina Blloshmi, Kevin Small",
            "EMNLP Paper ID": "2148",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "48d11fb087d9d06ab4a1118305b9930a1f9ed3a9",
            "title": "R^2AG: Incorporating Retrieval Information into Retrieval Augmented Generation",
            "abstract": "Retrieval augmented generation (RAG) has been applied in many scenarios to augment large language models (LLMs) with external documents provided by retrievers. However, a semantic gap exists between LLMs and retrievers due to differences in their training objectives and architectures. This misalignment forces LLMs to passively accept the documents provided by the retrievers, leading to incomprehension in the generation process, where the LLMs are burdened with the task of distinguishing these documents using their inherent knowledge. This paper proposes R$^2$AG, a novel enhanced RAG framework to fill this gap by incorporating Retrieval information into Retrieval Augmented Generation. Specifically, R$^2$AG utilizes the nuanced features from the retrievers and employs a R$^2$-Former to capture retrieval information. Then, a retrieval-aware prompting strategy is designed to integrate retrieval information into LLMs' generation. Notably, R$^2$AG suits low-source scenarios where LLMs and retrievers are frozen. Extensive experiments across five datasets validate the effectiveness, robustness, and efficiency of R$^2$AG. Our analysis reveals that retrieval information serves as an anchor to aid LLMs in the generation process, thereby filling the semantic gap.",
            "link": "https://www.semanticscholar.org/paper/48d11fb087d9d06ab4a1118305b9930a1f9ed3a9",
            "authors": "Fuda Ye, Shuangyin Li, Yongqi Zhang, Lei Chen",
            "matchScore": 271.26202,
            "original title": "R^2AG: Incorporating Retrieval Information into Retrieval Augmented Generation",
            "original authors": "Fuda Ye, Shuangyin Li, Yongqi Zhang, Lei Chen",
            "EMNLP Paper ID": "2281",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "a3a10069b55159964e245858786446ceca15e8a4",
            "title": "BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain",
            "abstract": "Retrieval Augmented Generation (RAG) system is important in domains such as e-commerce, which has many long-tail entities and frequently updated information. Most existing works adopt separate modules for retrieval and generation, which may be suboptimal since the retrieval task and the generation task cannot benefit from each other to improve performance. We propose a novel Backbone Shared RAG framework (BSharedRAG). It first uses a domain-specific corpus to continually pre-train a base model as a domain-specific backbone model and then trains two plug-and-play Low-Rank Adaptation (LoRA) modules based on the shared backbone to minimize retrieval and generation losses respectively. Experimental results indicate that our proposed BSharedRAG outperforms baseline models by 5% and 13% in Hit@3 upon two datasets in retrieval evaluation and by 23% in terms of BLEU-3 in generation evaluation. Our codes, models, and dataset are available at https://bsharedrag.github.io.",
            "link": "https://www.semanticscholar.org/paper/a3a10069b55159964e245858786446ceca15e8a4",
            "authors": "Kaisi Guan, Qian Cao, Yuchong Sun, Xiting Wang, Ruihua Song",
            "matchScore": 286.98593,
            "original title": "BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain",
            "original authors": "Kaisi Guan, Qian Cao, Yuchong Sun, Xiting Wang, Ruihua Song",
            "EMNLP Paper ID": "239",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "6cadcdc0787f887bb9b67c0ff677294a27fdff82",
            "title": "ConTReGen: Context-driven Tree-structured Retrieval for Open-domain Long-form Text Generation",
            "abstract": "Open-domain long-form text generation requires generating coherent, comprehensive responses that address complex queries with both breadth and depth. This task is challenging due to the need to accurately capture diverse facets of input queries. Existing iterative retrieval-augmented generation (RAG) approaches often struggle to delve deeply into each facet of complex queries and integrate knowledge from various sources effectively. This paper introduces ConTReGen, a novel framework that employs a context-driven, tree-structured retrieval approach to enhance the depth and relevance of retrieved content. ConTReGen integrates a hierarchical, top-down in-depth exploration of query facets with a systematic bottom-up synthesis, ensuring comprehensive coverage and coherent integration of multifaceted information. Extensive experiments on multiple datasets, including LFQA and ODSUM, alongside a newly introduced dataset, ODSUM-WikiHow, demonstrate that ConTReGen outperforms existing state-of-the-art RAG models.",
            "link": "https://www.semanticscholar.org/paper/6cadcdc0787f887bb9b67c0ff677294a27fdff82",
            "authors": "Kashob Kumar Roy, Pritom Saha Akash, Kevin Chen-Chuan Chang, Lucian Popa",
            "matchScore": 311.22018,
            "original title": "ConTReGen: Context-driven Tree-structured Retrieval for Open-domain Long-form Text Generation",
            "original authors": "Kashob Kumar Roy, Pritom Saha Akash, Lucian Popa, Kevin Chen-Chuan Chang",
            "EMNLP Paper ID": "2684",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "63a1617af179ee8b5b096b3038913a19166168d4",
            "title": "Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models",
            "abstract": "Retrieval-Augmented Generation (RAG) has been shown to enhance the factual accuracy of Large Language Models (LLMs), but existing methods often suffer from limited reasoning capabilities in effectively using the retrieved evidence, particularly when using open-source LLMs. To mitigate this gap, we introduce a novel framework, Open-RAG, designed to enhance reasoning capabilities in RAG with open-source LLMs. Our framework transforms an arbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE) model capable of handling complex reasoning tasks, including both single- and multi-hop queries. Open-RAG uniquely trains the model to navigate challenging distractors that appear relevant but are misleading. As a result, Open-RAG leverages latent learning, dynamically selecting relevant experts and integrating external knowledge effectively for more accurate and contextually relevant responses. In addition, we propose a hybrid adaptive retrieval method to determine retrieval necessity and balance the trade-off between performance gain and inference speed. Experimental results show that the Llama2-7B-based Open-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT, Self-RAG, and Command R+ in various knowledge-intensive tasks. We open-source our code and models at https://openragmoe.github.io/",
            "link": "https://www.semanticscholar.org/paper/63a1617af179ee8b5b096b3038913a19166168d4",
            "authors": "Shayekh Bin Islam, Md Asib Rahman, K. S. M. T. Hossain, Enamul Hoque, Shafiq R. Joty, Md. Rizwan Parvez",
            "matchScore": 272.38446,
            "original title": "Open-RAG: Enhanced Retrieval Augmented Reasoning with Open-Source Large Language Models",
            "original authors": "Shayekh Bin Islam, Md Asib Rahman, K S M Tozammel Hossain, Enamul Hoque, Shafiq Joty, Md Rizwan Parvez",
            "EMNLP Paper ID": "2748",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "6f6a9bf7cc6e8689a78497c0e42c096e282b6588",
            "title": "Retrieving Contextual Information for Long-Form Question Answering using Weak Supervision",
            "abstract": "Long-form question answering (LFQA) aims at generating in-depth answers to end-user questions, providing relevant information beyond the direct answer. However, existing retrievers are typically optimized towards information that directly targets the question, missing out on such contextual information. Furthermore, there is a lack of training data for relevant context. To this end, we propose and compare different weak supervision techniques to optimize retrieval for contextual information. Experiments demonstrate improvements on the end-to-end QA performance on ASQA, a dataset for long-form question answering. Importantly, as more contextual information is retrieved, we improve the relevant page recall for LFQA by 14.7% and the groundedness of generated long-form answers by 12.5%. Finally, we show that long-form answers often anticipate likely follow-up questions, via experiments on a conversational QA dataset.",
            "link": "https://www.semanticscholar.org/paper/6f6a9bf7cc6e8689a78497c0e42c096e282b6588",
            "authors": "Philipp Christmann, Svitlana Vakulenko, Ionut Teodor Sorodoc, Bill Byrne, Adria de Gispert",
            "matchScore": 291.23,
            "original title": "Retrieving Contextual Information for Long-Form Question Answering using Weak Supervision",
            "original authors": "Philipp Christmann, Svitlana Vakulenko, Ionut Teodor Sorodoc, Adri\u00e0 de Gispert, Bill Byrne",
            "EMNLP Paper ID": "2765",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "17175b8310d55494cff278a019b7d648fc2308f2",
            "title": "Unified Active Retrieval for Retrieval Augmented Generation",
            "abstract": "In Retrieval-Augmented Generation (RAG), retrieval is not always helpful and applying it to every instruction is sub-optimal. Therefore, determining whether to retrieve is crucial for RAG, which is usually referred to as Active Retrieval. However, existing active retrieval methods face two challenges: 1. They usually rely on a single criterion, which struggles with handling various types of instructions. 2. They depend on specialized and highly differentiated procedures, and thus combining them makes the RAG system more complicated and leads to higher response latency. To address these challenges, we propose Unified Active Retrieval (UAR). UAR contains four orthogonal criteria and casts them into plug-and-play classification tasks, which achieves multifaceted retrieval timing judgements with negligible extra inference cost. We further introduce the Unified Active Retrieval Criteria (UAR-Criteria), designed to process diverse active retrieval scenarios through a standardized procedure. Experiments on four representative types of user instructions show that UAR significantly outperforms existing work on the retrieval timing judgement and the performance of downstream tasks, which shows the effectiveness of UAR and its helpfulness to downstream tasks.",
            "link": "https://www.semanticscholar.org/paper/17175b8310d55494cff278a019b7d648fc2308f2",
            "authors": "Qinyuan Cheng, Xiaonan Li, Shimin Li, Qin Zhu, Zhangyue Yin, Yunfan Shao, Linyang Li, Tianxiang Sun, Hang Yan, Xipeng Qiu",
            "matchScore": 218.47447,
            "original title": "Unified Active Retrieval for Retrieval Augmented Generation",
            "original authors": "Qinyuan Cheng, Xiaonan Li, Shimin Li, Qin Zhu, Zhangyue Yin, Yunfan Shao, Linyang Li, Tianxiang Sun, Hang Yan, Xipeng Qiu",
            "EMNLP Paper ID": "3293",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "453acbfea9fb3e13bc9e1f59e9eb5194b69aabf1",
            "title": "Controlling Risk of Retrieval-augmented Generation: A Counterfactual Prompting Framework",
            "abstract": "Retrieval-augmented generation (RAG) has emerged as a popular solution to mitigate the hallucination issues of large language models. However, existing studies on RAG seldom address the issue of predictive uncertainty, i.e., how likely it is that a RAG model's prediction is incorrect, resulting in uncontrollable risks in real-world applications. In this work, we emphasize the importance of risk control, ensuring that RAG models proactively refuse to answer questions with low confidence. Our research identifies two critical latent factors affecting RAG's confidence in its predictions: the quality of the retrieved results and the manner in which these results are utilized. To guide RAG models in assessing their own confidence based on these two latent factors, we develop a counterfactual prompting framework that induces the models to alter these factors and analyzes the effect on their answers. We also introduce a benchmarking procedure to collect answers with the option to abstain, facilitating a series of experiments. For evaluation, we introduce several risk-related metrics and the experimental results demonstrate the effectiveness of our approach.",
            "link": "https://www.semanticscholar.org/paper/453acbfea9fb3e13bc9e1f59e9eb5194b69aabf1",
            "authors": "Luyao Chen, Ruqing Zhang, J. Guo, Yixing Fan, Xueqi Cheng",
            "matchScore": 256.73184,
            "original title": "Controlling Risk of Retrieval-augmented Generation: A Counterfactual Prompting Framework",
            "original authors": "Lu Chen, Ruqing Zhang, Jiafeng Guo, Yixing Fan, Xueqi Cheng",
            "EMNLP Paper ID": "492",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "038cf7f6b8fc781d3b74fe2cfe6db2bf35908a37",
            "title": "Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations",
            "abstract": "The robustness of recent Large Language Models (LLMs) has become increasingly crucial as their applicability expands across various domains and real-world applications. Retrieval-Augmented Generation (RAG) is a promising solution for addressing the limitations of LLMs, yet existing studies on the robustness of RAG often overlook the interconnected relationships between RAG components or the potential threats prevalent in real-world databases, such as minor textual errors. In this work, we investigate two underexplored aspects when assessing the robustness of RAG: 1) vulnerability to noisy documents through low-level perturbations and 2) a holistic evaluation of RAG robustness. Furthermore, we introduce a novel attack method, the Genetic Attack on RAG (\\textit{GARAG}), which targets these aspects. Specifically, GARAG is designed to reveal vulnerabilities within each component and test the overall system functionality against noisy documents. We validate RAG robustness by applying our \\textit{GARAG} to standard QA datasets, incorporating diverse retrievers and LLMs. The experimental results show that GARAG consistently achieves high attack success rates. Also, it significantly devastates the performance of each component and their synergy, highlighting the substantial risk that minor textual inaccuracies pose in disrupting RAG systems in the real world.",
            "link": "https://www.semanticscholar.org/paper/038cf7f6b8fc781d3b74fe2cfe6db2bf35908a37",
            "authors": "Sukmin Cho, Soyeong Jeong, Jeongyeon Seo, Taeho Hwang, Jong C. Park",
            "matchScore": 453.4604,
            "original title": "Typos that Broke the RAG\u2019s Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations",
            "original authors": "Sukmin Cho, Soyeong Jeong, Jeongyeon Seo, Taeho Hwang, Jong C. Park",
            "EMNLP Paper ID": "581",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "758881985475e137439da465fadf968aead68c4c",
            "title": "AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented Generation",
            "abstract": "Recent advancements in Large Language Models have transformed ML/AI development, necessitating a reevaluation of AutoML principles for the Retrieval-Augmented Generation (RAG) systems. To address the challenges of hyper-parameter optimization and online adaptation in RAG, we propose the AutoRAG-HP framework, which formulates the hyper-parameter tuning as an online multi-armed bandit (MAB) problem and introduces a novel two-level Hierarchical MAB (Hier-MAB) method for efficient exploration of large search spaces. We conduct extensive experiments on tuning hyper-parameters, such as top-k retrieved documents, prompt compression ratio, and embedding methods, using the ALCE-ASQA and Natural Questions datasets. Our evaluation from jointly optimization all three hyper-parameters demonstrate that MAB-based online learning methods can achieve Recall@5 $\\approx 0.8$ for scenarios with prominent gradients in search space, using only $\\sim20\\%$ of the LLM API calls required by the Grid Search approach. Additionally, the proposed Hier-MAB approach outperforms other baselines in more challenging optimization scenarios. The code will be made available at https://aka.ms/autorag.",
            "link": "https://www.semanticscholar.org/paper/758881985475e137439da465fadf968aead68c4c",
            "authors": "Jia Fu, Xiaoting Qin, Fangkai Yang, Lu Wang, Jue Zhang, Qingwei Lin, Yubo Chen, Dongmei Zhang, S. Rajmohan, Qi Zhang",
            "matchScore": 334.32288,
            "original title": "AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented Generation",
            "original authors": "Jia Fu, Xiaoting Qin, Fangkai Yang, Lu Wang, Jue Zhang, Qingwei Lin, Yubo Chen, Dongmei Zhang, Saravan Rajmohan, Qi Zhang",
            "EMNLP Paper ID": "770",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "e55cb63ca6edd700f3d8939e256da8008d34350f",
            "title": "OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs",
            "abstract": "Despite the recent advancements in Large Language Models (LLMs), which have significantly enhanced the generative capabilities for various NLP tasks, LLMs still face limitations in directly handling retrieval tasks. However, many practical applications demand the seamless integration of both retrieval and generation. This paper introduces a novel and efficient One-pass Generation and retrieval framework (OneGen), designed to improve LLMs' performance on tasks that require both generation and retrieval. The proposed framework bridges the traditionally separate training approaches for generation and retrieval by incorporating retrieval tokens generated autoregressively. This enables a single LLM to handle both tasks simultaneously in a unified forward pass. We conduct experiments on two distinct types of composite tasks, RAG and Entity Linking, to validate the pluggability, effectiveness, and efficiency of OneGen in training and inference. Furthermore, our results show that integrating generation and retrieval within the same context preserves the generative capabilities of LLMs while improving retrieval performance. To the best of our knowledge, OneGen is the first to enable LLMs to conduct vector retrieval during the generation.",
            "link": "https://www.semanticscholar.org/paper/e55cb63ca6edd700f3d8939e256da8008d34350f",
            "authors": "Jintian Zhang, Cheng Peng, Mengshu Sun, Xiang Chen, Lei Liang, Zhiqiang Zhang, Jun Zhou, Huajun Chen, Ningyu Zhang",
            "matchScore": 278.82776,
            "original title": "OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs",
            "original authors": "Jintian Zhang, Cheng Peng, Mengshu Sun, Xiang Chen, Lei Liang, Zhiqiang Zhang, JUN ZHOU, Huajun Chen, Ningyu Zhang",
            "EMNLP Paper ID": "813",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        }
    ],
    "Parameter-Efficient Optimization and Fine-Tuning Techniques for Large Language Models": [
        {
            "paperId": "f2e78a574925486d1f13440f55688bcffde80101",
            "title": "Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks",
            "abstract": "Large language models (LLMs) have demonstrated considerable proficiency in general natural language processing (NLP) tasks. Instruction tuning, a successful paradigm, enhances the ability of LLMs to follow natural language instructions and exhibit robust generalization across general tasks. However, these models often encounter performance limitations across multiple tasks due to constrained model capacity. Expanding this capacity during the instruction tuning phase poses significant challenges. To address this issue, we introduce parameter-efficient sparsity crafting (PESC), which crafts dense models into sparse models using the mixture-of-experts (MoE) architecture. PESC integrates adapters into the MoE layers of sparse models, differentiating experts without altering the individual weights within these layers. This method significantly reduces computational costs and GPU memory requirements, facilitating model capacity expansion through a minimal parameter increase when guaranteeing the quality of approximation in function space compared to original sparse upcycling. Our empirical evaluation demonstrates the effectiveness of the PESC method. Using PESC during instruction tuning, our best sparse model outperforms other sparse and dense models and exhibits superior general capabilities compared to GPT-3.5. Our code is available at https://github.com/wuhy68/Parameter-Efficient-MoE.",
            "link": "https://www.semanticscholar.org/paper/f2e78a574925486d1f13440f55688bcffde80101",
            "authors": "Haoyuan Wu, Haisheng Zheng, Bei Yu",
            "EMNLP Paper ID": "97",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "a3709e41b92282aa52918b557559d4a3e78f215a",
            "title": "Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models",
            "abstract": "Parameter-efficient fine-tuning (PEFT) is crucial for customizing Large Language Models (LLMs) with constrained resources. Although there have been various PEFT methods for dense-architecture LLMs, PEFT for sparse-architecture LLMs is still underexplored. In this work, we study the PEFT method for LLMs with the Mixture-of-Experts (MoE) architecture and the contents of this work are mainly threefold: (1) We investigate the dispersion degree of the activated experts in customized tasks, and found that the routing distribution for a specific task tends to be highly concentrated, while the distribution of activated experts varies significantly across different tasks. (2) We propose Expert-Specialized Fine-Tuning, or ESFT, which tunes the experts most relevant to downstream tasks while freezing the other experts and modules; experimental results demonstrate that our method not only improves the tuning efficiency, but also matches or even surpasses the performance of full-parameter fine-tuning. (3) We further analyze the impact of the MoE architecture on expert-specialized fine-tuning. We find that MoE models with finer-grained experts are more advantageous in selecting the combination of experts that are most relevant to downstream tasks, thereby enhancing both the training efficiency and effectiveness. Our code is available at https://github.com/deepseek-ai/ESFT.",
            "link": "https://www.semanticscholar.org/paper/a3709e41b92282aa52918b557559d4a3e78f215a",
            "authors": "Zihan Wang, Deli Chen, Damai Dai, R. Xu, Zhuoshu Li, Y. Wu, AI DeepSeek",
            "EMNLP Paper ID": "101",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "7a8a69d1ee140274dab4e63109cfd8513be127af",
            "title": "AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning",
            "abstract": "Fine-tuning large language models (LLMs) has achieved remarkable performance across various natural language processing tasks, yet it demands more and more memory as model sizes keep growing. To address this issue, the recently proposed Memory-efficient Zeroth-order (MeZO) methods attempt to fine-tune LLMs using only forward passes, thereby avoiding the need for a backpropagation graph. However, significant performance drops and a high risk of divergence have limited their widespread adoption. In this paper, we propose the Adaptive Zeroth-order Tensor-Train Adaption (AdaZeta) framework, specifically designed to improve the performance and convergence of the ZO methods. To enhance dimension-dependent ZO estimation accuracy, we introduce a fast-forward, low-parameter tensorized adapter. To tackle the frequently observed divergence issue in large-scale ZO fine-tuning tasks, we propose an adaptive query number schedule that guarantees convergence. Detailed theoretical analysis and extensive experimental results on Roberta-Large and Llama-2-7B models substantiate the efficacy of our AdaZeta framework in terms of accuracy, memory efficiency, and convergence speed.",
            "link": "https://www.semanticscholar.org/paper/7a8a69d1ee140274dab4e63109cfd8513be127af",
            "authors": "Yifan Yang, Kai Zhen, Ershad Banijamal, Athanasios Mouchtaris, Zheng Zhang",
            "EMNLP Paper ID": "127",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "e483314241e09be61e1000b9522a2ea35643b2ef",
            "title": "RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning",
            "abstract": "Pre-trained language models, trained on large-scale corpora, demonstrate strong generalizability across various NLP tasks. Fine-tuning these models for specific tasks typically involves updating all parameters, which is resource-intensive. Parameter-efficient fine-tuning (PEFT) methods, such as the popular LoRA family, introduce low-rank matrices to learn only a few parameters efficiently. However, during inference, the product of these matrices updates all pre-trained parameters, complicating tasks like knowledge editing that require selective updates. We propose a novel PEFT method, which conducts \\textbf{r}ow and c\\textbf{o}lumn-wise spar\\textbf{se} \\textbf{lo}w-\\textbf{r}ank \\textbf{a}daptation (RoseLoRA), to address this challenge. RoseLoRA identifies and updates only the most important parameters for a specific task, maintaining efficiency while preserving other model knowledge. By adding a sparsity constraint on the product of low-rank matrices and converting it to row and column-wise sparsity, we ensure efficient and precise model updates. Our theoretical analysis guarantees the lower bound of the sparsity with respective to the matrix product. Extensive experiments on five benchmarks across twenty datasets demonstrate that RoseLoRA outperforms baselines in both general fine-tuning and knowledge editing tasks.",
            "link": "https://www.semanticscholar.org/paper/e483314241e09be61e1000b9522a2ea35643b2ef",
            "authors": "Haoyu Wang, Tianci Liu, Tuo Zhao, Jing Gao",
            "EMNLP Paper ID": "128",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "1601ad7616681ed9d7e1b9a04b64c1ad9c7196c7",
            "title": "Prefixing Attention Sinks can Mitigate Activation Outliers for Large Language Model Quantization",
            "abstract": "Despite recent advances in LLM quantization, activation quantization remains to be challenging due to the activation outliers. Conventional remedies, e.g., mixing precisions for different channels, introduce extra overhead and reduce the speedup. In this work, we develop a simple yet effective strategy to facilitate per-tensor activation quantization by preventing the generation of problematic tokens. Precisely, we propose a method to find a set of key-value cache, coined CushionCache, which mitigates outliers in subsequent tokens when inserted as a prefix. CushionCache works in two steps: First, we greedily search for a prompt token sequence that minimizes the maximum activation values in subsequent tokens. Then, we further tune the token cache to regularize the activations of subsequent tokens to be more quantization-friendly. The proposed method successfully addresses activation outliers of LLMs, providing a substantial performance boost for per-tensor activation quantization methods. We thoroughly evaluate our method over a wide range of models and benchmarks and find that it significantly surpasses the established baseline of per-tensor W8A8 quantization and can be seamlessly integrated with the recent activation quantization method.",
            "link": "https://www.semanticscholar.org/paper/1601ad7616681ed9d7e1b9a04b64c1ad9c7196c7",
            "authors": "Seungwoo Son, Wonpyo Park, Woohyun Han, Kyuyeun Kim, Jaeho Lee",
            "EMNLP Paper ID": "253",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "3cf409c5dc261c3c495c4cca091e674adcaa5081",
            "title": "Towards End-to-end 4-Bit Inference on Generative Large Language Models",
            "abstract": "Large Language Models (LLMs) from the GPT family have become extremely popular, leading to a race towards reducing their inference costs to allow for efficient local computation. Yet, the vast majority of existing work focuses on weight-only quantization, which can reduce runtime costs in the memory-bound one-token-at-a-time generative setting, but does not address them in compute-bound scenarios, such as batched inference or prompt processing. In this paper, we address the general quantization problem, where both weights and activations should be quantized. We show, for the first time, that the majority of inference computations for large generative models such as LLaMA, OPT, and Falcon can be performed with both weights and activations being cast to 4 bits, in a way that leads to practical speedups, while at the same time maintaining good accuracy. We achieve this via a hybrid quantization strategy called QUIK, which compresses most of the weights and activations to 4-bit, while keeping some outlier weights and activations in higher-precision. The key feature of our scheme is that it is designed with computational efficiency in mind: we provide GPU kernels matching the QUIK format with highly-efficient layer-wise runtimes, which lead to practical end-to-end throughput improvements of up to 3.4x relative to FP16 execution. We provide detailed studies for models from the OPT, LLaMA-2 and Falcon families, as well as a first instance of accurate inference using quantization plus 2:4 sparsity. Code is available at: https://github.com/IST-DASLab/QUIK.",
            "link": "https://www.semanticscholar.org/paper/3cf409c5dc261c3c495c4cca091e674adcaa5081",
            "authors": "Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan Zhong, Xincheng Wang, Jie Ren, Torsten Hoefler, Dan Alistarh",
            "EMNLP Paper ID": "383",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "e2403e398314d49c5f56e05105a420a6f93e3cb2",
            "title": "Mixture-of-Subspaces in Low-Rank Adaptation",
            "abstract": "In this paper, we introduce a subspace-inspired Low-Rank Adaptation (LoRA) method, which is computationally efficient, easy to implement, and readily applicable to large language, multimodal, and diffusion models. Initially, we equivalently decompose the weights of LoRA into two subspaces, and find that simply mixing them can enhance performance. To study such a phenomenon, we revisit it through a fine-grained subspace lens, showing that such modification is equivalent to employing a fixed mixer to fuse the subspaces. To be more flexible, we jointly learn the mixer with the original LoRA weights, and term the method Mixture-of-Subspaces LoRA (MoSLoRA). MoSLoRA consistently outperforms LoRA on tasks in different modalities, including commonsense reasoning, visual instruction tuning, and subject-driven text-to-image generation, demonstrating its effectiveness and robustness. Codes are available at https://github.com/wutaiqiang/MoSLoRA.",
            "link": "https://www.semanticscholar.org/paper/e2403e398314d49c5f56e05105a420a6f93e3cb2",
            "authors": "Taiqiang Wu, Jiahao Wang, Zhe Zhao, Ngai Wong",
            "EMNLP Paper ID": "902",
            "Oral/Poster": "Oral",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "e8735aa9bc89f25619fe4f6e5465ade7ec8fc344",
            "title": "VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models",
            "abstract": "Scaling model size significantly challenges the deployment and inference of Large Language Models (LLMs). Due to the redundancy in LLM weights, recent research has focused on pushing weight-only quantization to extremely low-bit (even down to 2 bits). It reduces memory requirements, optimizes storage costs, and decreases memory bandwidth needs during inference. However, due to numerical representation limitations, traditional scalar-based weight quantization struggles to achieve such extreme low-bit. Recent research on Vector Quantization (VQ) for LLMs has demonstrated the potential for extremely low-bit model quantization by compressing vectors into indices using lookup tables. In this paper, we introduce Vector Post-Training Quantization (VPTQ) for extremely low-bit quantization of LLMs. We use Second-Order Optimization to formulate the LLM VQ problem and guide our quantization algorithm design by solving the optimization. We further refine the weights using Channel-Independent Second-Order Optimization for a granular VQ. In addition, by decomposing the optimization problem, we propose a brief and effective codebook initialization algorithm. We also extend VPTQ to support residual and outlier quantization, which enhances model accuracy and further compresses the model. Our experimental results show that VPTQ reduces model quantization perplexity by $0.01$-$0.34$ on LLaMA-2, $0.38$-$0.68$ on Mistral-7B, $4.41$-$7.34$ on LLaMA-3 over SOTA at 2-bit, with an average accuracy improvement of $0.79$-$1.5\\%$ on LLaMA-2, $1\\%$ on Mistral-7B, $11$-$22\\%$ on LLaMA-3 on QA tasks on average. We only utilize $10.4$-$18.6\\%$ of the quantization algorithm execution time, resulting in a $1.6$-$1.8\\times$ increase in inference throughput compared to SOTA.",
            "link": "https://www.semanticscholar.org/paper/e8735aa9bc89f25619fe4f6e5465ade7ec8fc344",
            "authors": "Yifei Liu, Jicheng Wen, Yang Wang, Shengyu Ye, L. Zhang, Ting Cao, Cheng Li, Mao Yang",
            "EMNLP Paper ID": "949",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "ba4d7d1e31fdfe6d2d556a8bdc06c14883f23b25",
            "title": "Fast Forwarding Low-Rank Training",
            "abstract": "Parameter efficient finetuning methods like low-rank adaptation (LoRA) aim to reduce the computational costs of finetuning pretrained Language Models (LMs). Enabled by these low-rank settings, we propose an even more efficient optimization strategy: Fast Forward, a simple and effective approach to accelerate large segments of training. In a Fast Forward stage, we repeat the most recent optimizer step until the loss stops improving on a tiny validation set. By alternating between regular optimization steps and Fast Forward stages, Fast Forward provides up to an 87\\% reduction in FLOPs and up to an 81\\% reduction in train time over standard SGD with Adam. We validate Fast Forward by finetuning various models on different tasks and demonstrate that it speeds up training without compromising model performance. Additionally, we analyze when and how to apply Fast Forward.",
            "link": "https://www.semanticscholar.org/paper/ba4d7d1e31fdfe6d2d556a8bdc06c14883f23b25",
            "authors": "Adir Rahamim, Naomi Saphra, Sara Kangaslahti, Yonatan Belinkov",
            "EMNLP Paper ID": "1067",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "49894a866c7e084937aeab4aafcfa955a24093d1",
            "title": "Matryoshka-Adaptor: Unsupervised and Supervised Tuning for Smaller Embedding Dimensions",
            "abstract": "Embeddings from Large Language Models (LLMs) have emerged as critical components in various applications, particularly for information retrieval. While high-dimensional embeddings generally demonstrate superior performance as they contain more salient information, their practical application is frequently hindered by elevated computational latency and the associated higher cost. To address these challenges, we propose Matryoshka-Adaptor, a novel tuning framework designed for the customization of LLM embeddings. Matryoshka-Adaptor facilitates substantial dimensionality reduction while maintaining comparable performance levels, thereby achieving a significant enhancement in computational efficiency and cost-effectiveness. Our framework directly modifies the embeddings from pre-trained LLMs which is designed to be seamlessly integrated with any LLM architecture, encompassing those accessible exclusively through black-box APIs. Also, it exhibits efficacy in both unsupervised and supervised learning settings. A rigorous evaluation conducted across a diverse corpus of English, multilingual, and multimodal datasets consistently reveals substantial gains with Matryoshka-Adaptor. Notably, with Google and OpenAI Embedding APIs, Matryoshka-Adaptor achieves a reduction in dimensionality ranging from two- to twelve-fold without compromising performance across multiple BEIR datasets.",
            "link": "https://www.semanticscholar.org/paper/49894a866c7e084937aeab4aafcfa955a24093d1",
            "authors": "Jinsung Yoon, Raj Sinha, Sercan \u00d6. Arik, Tomas Pfister",
            "EMNLP Paper ID": "1167",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "a316bfbc982cded2b854984fe5d62ea2985cc45c",
            "title": "Fisher Information-based Efficient Curriculum Federated Learning with Large Language Models",
            "abstract": "As a promising paradigm to collaboratively train models with decentralized data, Federated Learning (FL) can be exploited to fine-tune Large Language Models (LLMs). While LLMs correspond to huge size, the scale of the training data significantly increases, which leads to tremendous amounts of computation and communication costs. The training data is generally non-Independent and Identically Distributed (non-IID), which requires adaptive data processing within each device. Although Low Rank Adaptation (LoRA) can significantly reduce the scale of parameters to update in the fine-tuning process, it still takes unaffordable time to transfer the low-rank parameters of all the layers in LLMs. In this paper, we propose a Fisher Information-based Efficient Curriculum Federated Learning framework (FibecFed) with two novel methods, i.e., adaptive federated curriculum learning and efficient sparse parameter update. First, we propose a fisher information-based method to adaptively sample data within each device to improve the effectiveness of the FL fine-tuning process. Second, we dynamically select the proper layers for global aggregation and sparse parameters for local update with LoRA so as to improve the efficiency of the FL fine-tuning process. Extensive experimental results based on 10 datasets demonstrate that FibecFed yields excellent performance (up to 45.35% in terms of accuracy) and superb fine-tuning speed (up to 98.61% faster) compared with 17 baseline approaches).",
            "link": "https://www.semanticscholar.org/paper/a316bfbc982cded2b854984fe5d62ea2985cc45c",
            "authors": "Ji Liu, Jiaxiang Ren, Ruoming Jin, Zijie Zhang, Yang Zhou, Patrick Valduriez, Dejing Dou",
            "EMNLP Paper ID": "1189",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "cfecd71fbf71fb3d043f21184f20d7aefe77bf85",
            "title": "LoRA-Guard: Parameter-Efficient Guardrail Adaptation for Content Moderation of Large Language Models",
            "abstract": "Guardrails have emerged as an alternative to safety alignment for content moderation of large language models (LLMs). Existing model-based guardrails have not been designed for resource-constrained computational portable devices, such as mobile phones, more and more of which are running LLM-based applications locally. We introduce LoRA-Guard, a parameter-efficient guardrail adaptation method that relies on knowledge sharing between LLMs and guardrail models. LoRA-Guard extracts language features from the LLMs and adapts them for the content moderation task using low-rank adapters, while a dual-path design prevents any performance degradation on the generative task. We show that LoRA-Guard outperforms existing approaches with 100-1000x lower parameter overhead while maintaining accuracy, enabling on-device content moderation.",
            "link": "https://www.semanticscholar.org/paper/cfecd71fbf71fb3d043f21184f20d7aefe77bf85",
            "authors": "Hayder Elesedy, Pedro M. Esperancca, Silviu Vlad Oprea, Mete Ozay",
            "EMNLP Paper ID": "1364",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "4d7da69c61db94386424675d8fa01fb0a279f3b5",
            "title": "Heterogeneous LoRA for Federated Fine-tuning of On-Device Foundation Models",
            "abstract": "Foundation models (FMs) adapt well to specific domains or tasks with fine-tuning, and federated learning (FL) enables the potential for privacy-preserving fine-tuning of the FMs with on-device local data. For federated fine-tuning of FMs, we consider the FMs with small to medium parameter sizes of single digit billion at maximum, referred to as on-device FMs (ODFMs) that can be deployed on devices for inference but can only be fine-tuned with parameter efficient methods. In our work, we tackle the data and system heterogeneity problem of federated fine-tuning of ODFMs by proposing a novel method using heterogeneous low-rank approximations (LoRAs), namely HetLoRA. First, we show that the naive approach of using homogeneous LoRA ranks across devices face a trade-off between overfitting and slow convergence, and thus propose HetLoRA, which allows heterogeneous ranks across client devices and efficiently aggregates and distributes these heterogeneous LoRA modules. By applying rank self-pruning locally and sparsity-weighted aggregation at the server, HetLoRA combines the advantages of high and low-rank LoRAs, which achieves improved convergence speed and final performance compared to homogeneous LoRA. Furthermore, HetLoRA offers enhanced computation efficiency compared to full fine-tuning, making it suitable for federated fine-tuning across heterogeneous devices.",
            "link": "https://www.semanticscholar.org/paper/4d7da69c61db94386424675d8fa01fb0a279f3b5",
            "authors": "Yae Jee Cho, Luyang Liu, Zheng Xu, Aldi Fahrezi, Gauri Joshi",
            "EMNLP Paper ID": "1497",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "656f4a76bcbc1a3a032ef5cf284909ef1bb58156",
            "title": "Turn Waste into Worth: Rectifying Top-k Router of MoE",
            "abstract": "Sparse Mixture of Experts (MoE) models are popular for training large language models due to their computational efficiency. However, the commonly used top-$k$ routing mechanism suffers from redundancy computation and memory costs due to the unbalanced routing. Some experts are overflow, where the exceeding tokens are dropped. While some experts are vacant, which are padded with zeros, negatively impacting model performance. To address the dropped tokens and padding, we propose the Rectify-Router, comprising the Intra-GPU Rectification and the Fill-in Rectification. The Intra-GPU Rectification handles dropped tokens, efficiently routing them to experts within the GPU where they are located to avoid inter-GPU communication. The Fill-in Rectification addresses padding by replacing padding tokens with the tokens that have high routing scores. Our experimental results demonstrate that the Intra-GPU Rectification and the Fill-in Rectification effectively handle dropped tokens and padding, respectively. Furthermore, the combination of them achieves superior performance, surpassing the accuracy of the vanilla top-1 router by 4.7%.",
            "link": "https://www.semanticscholar.org/paper/656f4a76bcbc1a3a032ef5cf284909ef1bb58156",
            "authors": "Zhiyuan Zeng, Qipeng Guo, Zhaoye Fei, Zhangyue Yin, Yunhua Zhou, Linyang Li, Tianxiang Sun, Hang Yan, Dahua Lin, Xipeng Qiu",
            "EMNLP Paper ID": "1546",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "d01efb598702679f0c3f972b2aeed41ee99e306b",
            "title": "SparseGrad: A Selective Method for Efficient Fine-tuning of MLP Layers",
            "abstract": "The performance of Transformer models has been enhanced by increasing the number of parameters and the length of the processed text. Consequently, fine-tuning the entire model becomes a memory-intensive process. High-performance methods for parameter-efficient fine-tuning (PEFT) typically work with Attention blocks and often overlook MLP blocks, which contain about half of the model parameters. We propose a new selective PEFT method, namely SparseGrad, that performs well on MLP blocks. We transfer layer gradients to a space where only about 1\\% of the layer's elements remain significant. By converting gradients into a sparse structure, we reduce the number of updated parameters. We apply SparseGrad to fine-tune BERT and RoBERTa for the NLU task and LLaMa-2 for the Question-Answering task. In these experiments, with identical memory requirements, our method outperforms LoRA and MeProp, robust popular state-of-the-art PEFT approaches.",
            "link": "https://www.semanticscholar.org/paper/d01efb598702679f0c3f972b2aeed41ee99e306b",
            "authors": "Viktoriia Chekalina, Anna Rudenko, Gleb Mezentsev, Alexander Mikhalev, Alexander Panchenko, Ivan V. Oseledets",
            "EMNLP Paper ID": "1720",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "0c909ef8b889dcf751fde42aa9ef97ff7a619232",
            "title": "Grass: Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients",
            "abstract": "Large language model (LLM) training and finetuning are often bottlenecked by limited GPU memory. While existing projection-based optimization methods address this by projecting gradients into a lower-dimensional subspace to reduce optimizer state memory, they typically rely on dense projection matrices, which can introduce computational and memory overheads. In this work, we propose Grass (GRAdient Stuctured Sparsification), a novel approach that leverages sparse projections to transform gradients into structured sparse updates. This design not only significantly reduces memory usage for optimizer states but also minimizes gradient memory footprint, computation, and communication costs, leading to substantial throughput improvements. Extensive experiments on pretraining and finetuning tasks demonstrate that Grass achieves competitive performance to full-rank training and existing projection-based methods. Notably, Grass enables half-precision pretraining of a 13B parameter LLaMA model on a single 40GB A100 GPU--a feat infeasible for previous methods--and yields up to a $2\\times$ throughput improvement on an 8-GPU system. Code can be found at https://github.com/aashiqmuhamed/GRASS .",
            "link": "https://www.semanticscholar.org/paper/0c909ef8b889dcf751fde42aa9ef97ff7a619232",
            "authors": "Aashiq Muhamed, Oscar Li, David Woodruff, Mona Diab, Virginia Smith",
            "EMNLP Paper ID": "1732",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 14): 13:00\u201314:00 (Afternoon)",
            "Type": "Main"
        },
        {
            "paperId": "05830547cfd19b734777b8546f4d606fd79ebd2b",
            "title": "LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training",
            "abstract": "Mixture-of-Experts (MoE) has gained increasing popularity as a promising framework for scaling up large language models (LLMs). However, training MoE from scratch in a large-scale setting still suffers from data-hungry and instability problems. Motivated by this limit, we investigate building MoE models from existing dense large language models. Specifically, based on the well-known LLaMA-2 7B model, we obtain an MoE model by: (1) Expert Construction, which partitions the parameters of original Feed-Forward Networks (FFNs) into multiple experts; (2) Continual Pre-training, which further trains the transformed MoE model and additional gate networks. In this paper, we comprehensively explore different methods for expert construction and various data sampling strategies for continual pre-training. After these stages, our LLaMA-MoE models could maintain language abilities and route the input tokens to specific experts with part of the parameters activated. Empirically, by training 200B tokens, LLaMA-MoE-3.5B models significantly outperform dense models that contain similar activation parameters. The source codes and models are available at https://github.com/pjlab-sys4nlp/llama-moe .",
            "link": "https://www.semanticscholar.org/paper/05830547cfd19b734777b8546f4d606fd79ebd2b",
            "authors": "Tong Zhu, Xiaoye Qu, Daize Dong, Jiacheng Ruan, Jingqi Tong, Conghui He, Yu Cheng",
            "EMNLP Paper ID": "1867",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "61bb3de454e60d054850277dd8c1b539c66c807e",
            "title": "HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy",
            "abstract": "Full-parameter fine-tuning has become the go-to choice for adapting language models (LMs) to downstream tasks due to its excellent performance. As LMs grow in size, fine-tuning the full parameters of LMs requires a prohibitively large amount of GPU memory. Existing approaches utilize zeroth-order optimizer to conserve GPU memory, which can potentially compromise the performance of LMs as non-zero order optimizers tend to converge more readily on most downstream tasks. In this paper, we propose a novel optimizer-independent end-to-end hierarchical fine-tuning strategy, HiFT, which only updates a subset of parameters at each training step. HiFT can significantly reduce the amount of gradients and optimizer state parameters residing in GPU memory at the same time, thereby reducing GPU memory usage. Our results demonstrate that: (1) HiFT achieves comparable performance to parameter-efficient fine-tuning and standard full parameter fine-tuning. (2) HiFT supports various optimizers including AdamW, AdaGrad, SGD, etc. (3) HiFT can save more than 60\\% GPU memory compared with standard full-parameter fine-tuning for 7B model. (4) HiFT enables full-parameter fine-tuning of a 7B model on single 48G A6000 with a precision of 32 using the AdamW optimizer, without using any memory saving techniques.",
            "link": "https://www.semanticscholar.org/paper/61bb3de454e60d054850277dd8c1b539c66c807e",
            "authors": "Yongkang Liu, Yiqun Zhang, Qian Li, Shi Feng, Daling Wang, Yifei Zhang, Hinrich Sch\u00fctze",
            "EMNLP Paper ID": "2262",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "f1dd780f764e931b3f2cbf6e9d01b803a31a57c3",
            "title": "CHESS: Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification",
            "abstract": "Deploying large language models (LLMs) on edge devices presents significant challenges due to the substantial computational overhead and memory requirements. Activation sparsification can mitigate these challenges by reducing the number of activated neurons during inference. Existing methods typically employ thresholding-based sparsification based on the statistics of activation tensors. However, these methods do not explicitly model the impact of activation sparsification on performance, leading to suboptimal performance degradation. To address this issue, this paper reformulates the activation sparsification problem by introducing a new objective that optimizes the sparsification decisions. Building on this reformulation, we propose CHESS, a general activation sparsification approach via CHannel-wise thrEsholding and Selective Sparsification. First, channel-wise thresholding assigns a unique threshold to each activation channel in the feed-forward network (FFN) layers. Then, selective sparsification involves applying thresholding-based activation sparsification to specific layers within the attention modules. Finally, we detail the implementation of sparse kernels to accelerate LLM inference. Experimental results demonstrate that the proposed CHESS achieves lower performance degradation over 8 downstream tasks while activating fewer parameters compared to existing methods, thus speeding up the LLM inference by up to 1.27x.",
            "link": "https://www.semanticscholar.org/paper/f1dd780f764e931b3f2cbf6e9d01b803a31a57c3",
            "authors": "Junhui He, Shangyu Wu, Weidong Wen, C. Xue, Qingan Li",
            "EMNLP Paper ID": "2325",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "7628db8e0dee79557a0014296f53459f13bf016b",
            "title": "ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models",
            "abstract": "The high power consumption and latency-sensitive deployments of large language models (LLMs) have motivated efficiency techniques like quantization and sparsity. Contextual sparsity, where the sparsity pattern is input-dependent, is crucial in LLMs because the permanent removal of attention heads or neurons from LLMs can significantly degrade accuracy. Prior work has attempted to model contextual sparsity using neural networks trained to predict activation magnitudes, which can be used to dynamically prune structures with low predicted activation magnitude. In this paper, we look beyond magnitude-based pruning criteria to assess attention head and neuron importance in LLMs. We develop a novel predictor called ShadowLLM, which can shadow the LLM behavior and enforce better sparsity patterns, resulting in over 15% improvement in end-to-end accuracy compared to prior methods. In addition, ShadowLLM achieves up to a 20% speed-up over the state-of-the-art DejaVu framework. These enhancements are validated on Llama-2 and OPT models with up to 30 billion parameters. Our code is available at \\href{https://github.com/abdelfattah-lab/shadow_llm/}{ShadowLLM}.",
            "link": "https://www.semanticscholar.org/paper/7628db8e0dee79557a0014296f53459f13bf016b",
            "authors": "Yash Akhauri, Ahmed F. AbouElhamayed, Jordan Dotzel, Zhiru Zhang, Alexander M Rush, Safeen Huda, Mohamed Abdelfattah",
            "EMNLP Paper ID": "2426",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "6c9548c556d44dba51746c280312c659057cf06f",
            "title": "GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning",
            "abstract": "Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation (RAG) have become popular methods for adapting large language models while minimizing compute requirements. In this paper, we apply PEFT methods (P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer (RETRO) and a baseline GPT model across several sizes, ranging from 823 million to 48 billion parameters. We show that RETRO models outperform GPT models in zero-shot settings due to their unique pre-training process but GPT models have higher performance potential with PEFT. Additionally, our study indicates that 8B parameter models strike an optimal balance between cost and performance and P-tuning lags behind other PEFT techniques. We further provide a comparative analysis between applying PEFT to an Instruction-tuned RETRO model and base RETRO model. This work presents the first comprehensive comparison of various PEFT methods integrated with RAG, applied to both GPT and RETRO models, highlighting their relative performance.",
            "link": "https://www.semanticscholar.org/paper/6c9548c556d44dba51746c280312c659057cf06f",
            "authors": "Aleksander Ficek, Jiaqi Zeng, Oleksii Kuchaiev",
            "EMNLP Paper ID": "2471",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "c1583399d7aefee27bc059ac8c2d4bb049a6446d",
            "title": "AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality",
            "abstract": "Parameter-efficient fine-tuning methods, such as Low-Rank Adaptation (LoRA), are known to enhance training efficiency in Large Language Models (LLMs). Due to the limited parameters of LoRA, recent studies seek to combine LoRA with Mixture-of-Experts (MoE) to boost performance across various tasks. However, inspired by the observed redundancy in traditional MoE structures, previous studies identify similar redundancy among LoRA experts within the MoE architecture, highlighting the necessity for non-uniform allocation of LoRA experts across different layers. In this paper, we leverage Heavy-Tailed Self-Regularization (HT-SR) Theory to design a fine-grained allocation strategy. Our analysis reveals that the number of experts per layer correlates with layer training quality, which exhibits significant variability across layers. Based on this, we introduce AlphaLoRA, a theoretically principled and training-free method for allocating LoRA experts to further mitigate redundancy. Experiments on three models across ten language processing and reasoning benchmarks demonstrate that AlphaLoRA achieves comparable or superior performance over all baselines. Our code is available at https://github.com/morelife2017/alphalora.",
            "link": "https://www.semanticscholar.org/paper/c1583399d7aefee27bc059ac8c2d4bb049a6446d",
            "authors": "Peijun Qing, Chongyang Gao, Yefan Zhou, Xingjian Diao, Yaoqing Yang, Soroush Vosoughi",
            "EMNLP Paper ID": "2683",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "b441b775766896878de91cd10fac59286cf093d2",
            "title": "Mixture-of-Modules: Reinventing Transformers as Dynamic Assemblies of Modules",
            "abstract": "Is it always necessary to compute tokens from shallow to deep layers in Transformers? The continued success of vanilla Transformers and their variants suggests an undoubted\"yes\". In this work, however, we attempt to break the depth-ordered convention by proposing a novel architecture dubbed mixture-of-modules (MoM), which is motivated by an intuition that any layer, regardless of its position, can be used to compute a token as long as it possesses the needed processing capabilities. The construction of MoM starts from a finite set of modules defined by multi-head attention and feed-forward networks, each distinguished by its unique parameterization. Two routers then iteratively select attention modules and feed-forward modules from the set to process a token. The selection dynamically expands the computation graph in the forward pass of the token, culminating in an assembly of modules. We show that MoM provides not only a unified framework for Transformers and their numerous variants but also a flexible and learnable approach for reducing redundancy in Transformer parameterization. We pre-train various MoMs using OpenWebText. Empirical results demonstrate that MoMs, of different parameter counts, consistently outperform vanilla transformers on both GLUE and XSUM benchmarks. More interestingly, with a fixed parameter budget, MoM-large enables an over 38% increase in depth for computation graphs compared to GPT-2-large, resulting in absolute gains of 1.4 on GLUE and 1 on XSUM. On the other hand, MoM-large also enables an over 60% reduction in depth while involving more modules per layer, yielding a 16% reduction in TFLOPs and a 43% decrease in memory usage compared to GPT-2-large, while maintaining comparable performance.",
            "link": "https://www.semanticscholar.org/paper/b441b775766896878de91cd10fac59286cf093d2",
            "authors": "Zhuocheng Gong, Ang Lv, Jian Guan, Junxi Yan, Wei Wu, Huishuai Zhang, Minlie Huang, Dongyan Zhao, Rui Yan",
            "EMNLP Paper ID": "2788",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "4d5cbfbba6e336075cf11c7229b38b098d9243d1",
            "title": "ApiQ: Finetuning of 2-Bit Quantized Large Language Model",
            "abstract": "Memory-efficient finetuning of large language models (LLMs) has recently attracted huge attention with the increasing size of LLMs, primarily due to the constraints posed by GPU memory limitations and the effectiveness of these methods compared to full finetuning. Despite the advancements, current strategies for memory-efficient finetuning, such as QLoRA, exhibit inconsistent performance across diverse bit-width quantizations and multifaceted tasks. This inconsistency largely stems from the detrimental impact of the quantization process on preserved knowledge, leading to catastrophic forgetting and undermining the utilization of pretrained models for finetuning purposes. In this work, we introduce a novel quantization framework, ApiQ, designed to restore the lost information from quantization by concurrently initializing the LoRA components and quantizing the weights of LLMs. This approach ensures the maintenance of the original LLM's activation precision while mitigating the error propagation from shallower into deeper layers. Through comprehensive evaluations conducted on a spectrum of language tasks with various LLMs, ApiQ demonstrably minimizes activation error during quantization. Consequently, it consistently achieves superior finetuning results across various bit-widths.",
            "link": "https://www.semanticscholar.org/paper/4d5cbfbba6e336075cf11c7229b38b098d9243d1",
            "authors": "Baohao Liao, C. Monz",
            "EMNLP Paper ID": "2812",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "6c80800e5e2b00766f9b9838f63581e9eaf9929f",
            "title": "Normalized Narrow Jump To Conclusions: Normalized Narrow Shortcuts for Parameter Efficient Early Exit Transformer Prediction",
            "abstract": "With the size and cost of large transformer-based language models growing, recently, there has been interest in shortcut casting of early transformer hidden-representations to final-representations for cheaper model inference. In particular, shortcutting pre-trained transformers with linear transformations over early layers has been shown to improve precision in early inference. However, for large language models, even this becomes computationally expensive. In this work, we propose Narrow Jump to Conclusions (NJTC) and Normalized Narrow Jump to Conclusions (N-NJTC) - parameter efficient alternatives to standard linear shortcutting that reduces shortcut parameter count by over 97%. We show that N-NJTC reliably outperforms Identity shortcuts at early stages and offers stable precision from all transformer block levels for GPT-2-XL, Phi3-Mini and Llama2-7B transformer models, demonstrating the viability of more parameter efficient short-cutting approaches.",
            "link": "https://www.semanticscholar.org/paper/6c80800e5e2b00766f9b9838f63581e9eaf9929f",
            "authors": "Amrit Diggavi Seshadri",
            "matchScore": 429.4135,
            "original title": "Normalized Narrow Jump To Conclusions: Normalized Narrow Shortcuts for Parameter Efficient Early Exit Transformer Prediction",
            "original authors": "Amrit Diggavi Seshadri",
            "EMNLP Paper ID": "1025",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "8970a42d5de27dcf0833f3f1387de586b6d85a3e",
            "title": "AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models",
            "abstract": "Mixture of experts (MoE) has become the standard for constructing production-level large language models (LLMs) due to its promise to boost model capacity without causing significant overheads. Nevertheless, existing MoE methods usually enforce a constant top-k routing for all tokens, which is arguably restrictive because various tokens (e.g.,\"\"vs.\"apple\") may require various numbers of experts for feature abstraction. Lifting such a constraint can help make the most of limited resources and unleash the potential of the model for downstream tasks. In this sense, we introduce AdaMoE to realize token-adaptive routing for MoE, where different tokens are permitted to select a various number of experts. AdaMoE makes minimal modifications to the vanilla MoE with top-k routing -- it simply introduces a fixed number of null experts, which do not consume any FLOPs, to the expert set and increases the value of k. AdaMoE does not force each token to occupy a fixed number of null experts but ensures the average usage of the null experts with a load-balancing loss, leading to an adaptive number of null/true experts used by each token. AdaMoE exhibits a strong resemblance to MoEs with expert choice routing while allowing for trivial auto-regressive modeling. AdaMoE is easy to implement and can be effectively applied to pre-trained (MoE-)LLMs. Extensive studies show that AdaMoE can reduce average expert load (FLOPs) while achieving superior performance. For example, on the ARC-C dataset, applying our method to fine-tuning Mixtral-8x7B can reduce FLOPs by 14.5% while increasing accuracy by 1.69%.",
            "link": "https://www.semanticscholar.org/paper/8970a42d5de27dcf0833f3f1387de586b6d85a3e",
            "authors": "Zihao Zeng, Yibo Miao, Hongcheng Gao, Hao Zhang, Zhijie Deng",
            "matchScore": 304.08307,
            "original title": "AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models",
            "original authors": "Zihao Zeng, Yibo Miao, Hongcheng Gao, Hao Zhang, Zhijie Deng",
            "EMNLP Paper ID": "1282",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "dc9b8b02b0eb3cb30609923e4be1c896cf7c7544",
            "title": "BIPEFT: Budget-Guided Iterative Search for Parameter Efficient Fine-Tuning of Large Pretrained Language Models",
            "abstract": "Parameter Efficient Fine-Tuning (PEFT) offers an efficient solution for fine-tuning large pretrained language models for downstream tasks. However, most PEFT strategies are manually designed, often resulting in suboptimal performance. Recent automatic PEFT approaches aim to address this but face challenges such as search space entanglement, inefficiency, and lack of integration between parameter budgets and search processes. To overcome these issues, we introduce a novel Budget-guided Iterative search strategy for automatic PEFT (BIPEFT), significantly enhancing search efficiency. BIPEFT employs a new iterative search strategy to disentangle the binary module and rank dimension search spaces. Additionally, we design early selection strategies based on parameter budgets, accelerating the learning process by gradually removing unimportant modules and fixing rank dimensions. Extensive experiments on public benchmarks demonstrate the superior performance of BIPEFT in achieving efficient and effective PEFT for downstream tasks with a low parameter budget.",
            "link": "https://www.semanticscholar.org/paper/dc9b8b02b0eb3cb30609923e4be1c896cf7c7544",
            "authors": "Aofei Chang, Jiaqi Wang, Han Liu, Parminder Bhatia, Cao Xiao, Ting Wang, Fenglong Ma",
            "matchScore": 337.4815,
            "original title": "BIPEFT: Budget-Guided Iterative Search for Parameter Efficient Fine-Tuning of Large Pretrained Language Models",
            "original authors": "Aofei Chang, Jiaqi Wang, Han Liu, Parminder Bhatia, Cao Xiao, Ting Wang, Fenglong Ma",
            "EMNLP Paper ID": "1532",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "5e9f22bba13332709cc9880d7e197385bca316de",
            "title": "RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization",
            "abstract": "Low-Rank Adaptation (LoRA), as a representative Parameter-Efficient Fine-Tuning (PEFT)method, significantly enhances the training efficiency by updating only a small portion of the weights in Large Language Models (LLMs). Recently, weight-only quantization techniques have also been applied to LoRA methods to reduce the memory footprint of fine-tuning. However, applying weight-activation quantization to the LoRA pipeline is under-explored, and we observe substantial performance degradation primarily due to the presence of activation outliers. In this work, we propose RoLoRA, the first LoRA-based scheme for effective weight-activation quantization. RoLoRA utilizes rotation for outlier elimination and proposes rotation-aware fine-tuning to preserve the outlier-free characteristics in rotated LLMs. Experimental results show RoLoRA consistently improves low-bit LoRA convergence and post-training quantization robustness in weight-activation settings. We evaluate RoLoRA across LLaMA2-7B/13B, LLaMA3-8B models, achieving up to 29.5% absolute accuracy gain of 4-bit weight-activation quantized LLaMA2- 13B on commonsense reasoning tasks compared to LoRA baseline. We further demonstrate its effectiveness on Large Multimodal Models (LLaVA-1.5-7B). Codes are available at https://github.com/HuangOwen/RoLoRA",
            "link": "https://www.semanticscholar.org/paper/5e9f22bba13332709cc9880d7e197385bca316de",
            "authors": "Xijie Huang, Zechun Liu, Shih-yang Liu, Kwang-Ting Cheng",
            "matchScore": 319.39697,
            "original title": "RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization",
            "original authors": "Xijie Huang, Zechun Liu, Shih-Yang Liu, Kwang-Ting Cheng",
            "EMNLP Paper ID": "1576",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "c7de9c673376c1f535271bdfb72d237a26126b3b",
            "title": "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation",
            "abstract": "Low-rank adaptation (LoRA) has become the default approach to fine-tune large language models (LLMs) due to its significant reduction in trainable parameters. However, trainable parameter demand for LoRA increases with increasing model embedding dimensions, leading to high compute costs. Additionally, its backward updates require storing high-dimensional intermediate activations and optimizer states, demanding high peak GPU memory. In this paper, we introduce large model fine-tuning via spectrally decomposed low-dimensional adaptation (LaMDA), a novel approach to fine-tuning large language models, which leverages low-dimensional adaptation to achieve significant reductions in trainable parameters and peak GPU memory footprint. LaMDA freezes a first projection matrix (PMA) in the adaptation path while introducing a low-dimensional trainable square matrix, resulting in substantial reductions in trainable parameters and peak GPU memory usage. LaMDA gradually freezes a second projection matrix (PMB) during the early fine-tuning stages, reducing the compute cost associated with weight updates to enhance parameter efficiency further. We also present an enhancement, LaMDA++, incorporating a ``lite-weight\"adaptive rank allocation for the LoRA path via normalized spectrum analysis of pre-trained model weights. We evaluate LaMDA/LaMDA++ across various tasks, including natural language understanding with the GLUE benchmark, text summarization, natural language generation, and complex reasoning on different LLMs. Results show that LaMDA matches or surpasses the performance of existing alternatives while requiring up to 17.7x fewer parameter updates and up to 1.32x lower peak GPU memory usage during fine-tuning. Code will be publicly available.",
            "link": "https://www.semanticscholar.org/paper/c7de9c673376c1f535271bdfb72d237a26126b3b",
            "authors": "Seyedarmin Azizi, Souvik Kundu, M. Pedram",
            "matchScore": 315.81842,
            "original title": "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation",
            "original authors": "Seyedarmin Azizi, Souvik Kundu, Massoud Pedram",
            "EMNLP Paper ID": "1995",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "f401f22bb7c01dddfc7e16972cd8dfdee2c03e59",
            "title": "MobileQuant: Mobile-friendly Quantization for On-device Language Models",
            "abstract": "Large language models (LLMs) have revolutionized language processing, delivering outstanding results across multiple applications. However, deploying LLMs on edge devices poses several challenges with respect to memory, energy, and compute costs, limiting their widespread use in devices such as mobile phones. A promising solution is to reduce the number of bits used to represent weights and activations. While existing works have found partial success at quantizing LLMs to lower bitwidths, e.g. 4-bit weights, quantizing activations beyond 16 bits often leads to large computational overheads due to poor on-device quantization support, or a considerable accuracy drop. Yet, 8-bit activations are very attractive for on-device deployment as they would enable LLMs to fully exploit mobile-friendly hardware, e.g. Neural Processing Units (NPUs). In this work, we make a first attempt to facilitate the on-device deployment of LLMs using integer-only quantization. We first investigate the limitations of existing quantization methods for on-device deployment, with a special focus on activation quantization. We then address these limitations by introducing a simple post-training quantization method, named MobileQuant, that extends previous weight equivalent transformation works by jointly optimizing the weight transformation and activation range parameters in an end-to-end manner. MobileQuant demonstrates superior capabilities over existing methods by 1) achieving near-lossless quantization on a wide range of LLM benchmarks, 2) reducing latency and energy consumption by 20\\%-50\\% compared to current on-device quantization strategies, 3) requiring limited compute budget, 4) being compatible with mobile-friendly compute units, e.g. NPU.",
            "link": "https://www.semanticscholar.org/paper/f401f22bb7c01dddfc7e16972cd8dfdee2c03e59",
            "authors": "Fuwen Tan, Royson Lee, L. Dudziak, S. Hu, Sourav Bhattacharya, Timothy M. Hospedales, Georgios Tzimiropoulos, Brais Mart\u00ednez",
            "matchScore": 249.8502,
            "original title": "MobileQuant: Mobile-friendly Quantization for On-device Language Models",
            "original authors": "Fuwen Tan, Royson Lee, \u0141ukasz Dudziak, Shell Xu Hu, Sourav Bhattacharya, Timothy Hospedales, Georgios Tzimiropoulos, Brais Martinez",
            "EMNLP Paper ID": "2010",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "52ed89f4d974764fdf617f566848c7c502403b15",
            "title": "Promoting Data and Model Privacy in Federated Learning through Quantized LoRA",
            "abstract": "Conventional federated learning primarily aims to secure the privacy of data distributed across multiple edge devices, with the global model dispatched to edge devices for parameter updates during the learning process. However, the development of large language models (LLMs) requires substantial data and computational resources, rendering them valuable intellectual properties for their developers and owners. To establish a mechanism that protects both data and model privacy in a federated learning context, we introduce a method that just needs to distribute a quantized version of the model's parameters during training. This method enables accurate gradient estimations for parameter updates while preventing clients from accessing a model whose performance is comparable to the centrally hosted one. Moreover, we combine this quantization strategy with LoRA, a popular and parameter-efficient fine-tuning method, to significantly reduce communication costs in federated learning. The proposed framework, named \\textsc{FedLPP}, successfully ensures both data and model privacy in the federated learning context. Additionally, the learned central model exhibits good generalization and can be trained in a resource-efficient manner.",
            "link": "https://www.semanticscholar.org/paper/52ed89f4d974764fdf617f566848c7c502403b15",
            "authors": "Jianhao Zhu, Changze Lv, Xiaohua Wang, Muling Wu, Wenhao Liu, Tianlong Li, Zixuan Ling, Cenyuan Zhang, Xiaoqing Zheng, Xuanjing Huang",
            "matchScore": 279.64532,
            "original title": "Promoting Data and Model Privacy in Federated Learning through Quantized LoRA",
            "original authors": "Zhu JianHao, Changze Lv, Xiaohua Wang, Muling Wu, Wenhao Liu, Tianlong Li, Zixuan Ling, Cenyuan Zhang, Xiaoqing Zheng, Xuanjing Huang",
            "EMNLP Paper ID": "2139",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "633e3fe49fe9c314f7245f77401c2e4a95e925a9",
            "title": "Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs",
            "abstract": "Large Language Models (LLMs) have demonstrated exceptional proficiency in language-related tasks, but their deployment poses significant challenges due to substantial memory and storage requirements. Weight-only quantization has emerged as a promising solution, significantly reducing memory and storage needs without sacrificing too much performance. In this study, we introduce SignRound, a method that leverages signed gradient descent (SignSGD) to optimize rounding values and weight clipping in just 200 steps. SignRound integrates the advantages of Quantization-Aware Training (QAT) and Post-Training Quantization (PTQ), delivering exceptional results across 2 to 4 bits while minimizing tuning costs and avoiding additional inference overhead. For example, SignRound achieved absolute average accuracy improvements ranging from 6.91% to 33.22% at 2bits, as measured by the average zero-shot accuracy across 11 tasks. It also demonstrates strong generalization in recent models, achieving near-lossless 4-bit quantization in most scenarios. The source code is publicly available at https://github.com/intel/auto-round.",
            "link": "https://www.semanticscholar.org/paper/633e3fe49fe9c314f7245f77401c2e4a95e925a9",
            "authors": "Wenhua Cheng, Weiwei Zhang, Haihao Shen, Yiyang Cai, Xin He, Kaokao Lv, Yi. Liu",
            "matchScore": 315.8865,
            "original title": "Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs",
            "original authors": "Wenhua Cheng, Weiwei Zhang, Haihao Shen, Yiyang Cai, Xin He, Lv Kaokao, Yi Liu",
            "EMNLP Paper ID": "2248",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "be66705b36912679ea373184aaf057aa365d292a",
            "title": "Fast Matrix Multiplications for Lookup Table-Quantized LLMs",
            "abstract": "The deployment of large language models (LLMs) is often constrained by memory bandwidth, where the primary bottleneck is the cost of transferring model parameters from the GPU's global memory to its registers. When coupled with custom kernels that fuse the dequantization and matmul operations, weight-only quantization can thus enable faster inference by reducing the amount of memory movement. However, developing high-performance kernels for weight-quantized LLMs presents substantial challenges, especially when the weights are compressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform, lookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup table engine for LUT-quantized LLMs, which uses offline restructuring of the quantized weight matrix to minimize bit manipulations associated with unpacking, and vectorization and duplication of the lookup table to mitigate shared memory bandwidth constraints. At batch sizes<32 and quantization group size of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster than existing GEMM kernels. As an application of FLUTE, we explore a simple extension to lookup table-based NormalFloat quantization and apply it to quantize LLaMA3 to various configurations, obtaining competitive quantization performance against strong baselines while obtaining an end-to-end throughput increase of 1.5 to 2 times.",
            "link": "https://www.semanticscholar.org/paper/be66705b36912679ea373184aaf057aa365d292a",
            "authors": "Han Guo, William Brandon, Radostin Cholakov, Jonathan Ragan-Kelley, Eric P. Xing, Yoon Kim",
            "matchScore": 274.54346,
            "original title": "Fast Matrix Multiplications for Lookup Table-Quantized LLMs",
            "original authors": "Han Guo, William Brandon, Radostin Cholakov, Jonathan Ragan-Kelley, Eric P. Xing, Yoon Kim",
            "EMNLP Paper ID": "2430",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "2ca52ea75fc02a3f08988794241cad9e30444db4",
            "title": "Expressive and Generalizable Low-rank Adaptation for Large Models via Slow Cascaded Learning",
            "abstract": "Efficient fine-tuning plays a fundamental role in modern large models, with low-rank adaptation emerging as a particularly promising approach. However, the existing variants of LoRA are hampered by limited expressiveness, a tendency to overfit, and sensitivity to hyperparameter settings. This paper presents LoRA Slow Cascade Learning (LoRASC), an innovative technique designed to enhance LoRA's expressiveness and generalization capabilities while preserving its training efficiency. Our approach augments expressiveness through a cascaded learning strategy that enables a mixture-of-low-rank adaptation, thereby increasing the model's ability to capture complex patterns. Additionally, we introduce a slow-fast update mechanism and cascading noisy tuning to bolster generalization. The extensive experiments on various language and vision datasets, as well as robustness benchmarks, demonstrate that the proposed method not only significantly outperforms existing baselines, but also mitigates overfitting, enhances model stability, and improves OOD robustness. Code will be release in https://github.com/microsoft/LoRASC very soon.",
            "link": "https://www.semanticscholar.org/paper/2ca52ea75fc02a3f08988794241cad9e30444db4",
            "authors": "Siwei Li, Yifan Yang, Yifei Shen, Fangyun Wei, Zongqing Lu, Lili Qiu, Yuqing Yang",
            "matchScore": 281.35175,
            "original title": "Expressive and Generalizable Low-rank Adaptation for Large Models via Slow Cascaded Learning",
            "original authors": "Siwei Li, Yifan Yang, Yifei Shen, Fangyun Wei, Zongqing Lu, Lili Qiu, Yuqing Yang",
            "EMNLP Paper ID": "2507",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "dc3dc6eb89824411b571b680731ef43663fe76e6",
            "title": "SQFT: Low-cost Model Adaptation in Low-precision Sparse Foundation Models",
            "abstract": "Large pre-trained models (LPMs), such as large language models, have become ubiquitous and are employed in many applications. These models are often adapted to a desired domain or downstream task through a fine-tuning stage. This paper proposes SQFT, an end-to-end solution for low-precision sparse parameter-efficient fine-tuning of LPMs, allowing for effective model manipulation in resource-constrained environments. Additionally, an innovative strategy enables the merging of sparse weights with low-rank adapters without losing sparsity and accuracy, overcoming the limitations of previous approaches. SQFT also addresses the challenge of having quantized weights and adapters with different numerical precisions, enabling merging in the desired numerical format without sacrificing accuracy. Multiple adaptation scenarios, models, and comprehensive sparsity levels demonstrate the effectiveness of SQFT. Models and code are available at https://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning.",
            "link": "https://www.semanticscholar.org/paper/dc3dc6eb89824411b571b680731ef43663fe76e6",
            "authors": "J. P. Munoz, Jinjie Yuan, Nilesh Jain",
            "matchScore": 302.44244,
            "original title": "SQFT: Low-cost Model Adaptation in Low-precision Sparse Foundation Models",
            "original authors": "Juan Pablo Munoz, Jinjie Yuan, Nilesh Jain",
            "EMNLP Paper ID": "2508",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "2ad2bded5db34ab49c7d2a84e8d162b9707b56e8",
            "title": "Exploring Quantization for Efficient Pre-Training of Transformer Language Models",
            "abstract": "The increasing scale of Transformer models has led to an increase in their pre-training computational requirements. While quantization has proven to be effective after pre-training and during fine-tuning, applying quantization in Transformers during pre-training has remained largely unexplored at scale for language modeling. This study aims to explore the impact of quantization for efficient pre-training of Transformers, with a focus on linear layer components. By systematically applying straightforward linear quantization to weights, activations, gradients, and optimizer states, we assess its effects on model efficiency, stability, and performance during training. By offering a comprehensive recipe of effective quantization strategies to be applied during the pre-training of Transformers, we promote high training efficiency from scratch while retaining language modeling ability. Code is available at https://github.com/chandar-lab/EfficientLLMs.",
            "link": "https://www.semanticscholar.org/paper/2ad2bded5db34ab49c7d2a84e8d162b9707b56e8",
            "authors": "Kamran Chitsaz, Quentin Fournier, Gonccalo Mordido, Sarath Chandar",
            "matchScore": 240.7044,
            "original title": "Exploring Quantization for Efficient Pre-Training of Transformer Language Models",
            "original authors": "Kamran Chitsaz, Quentin Fournier, Goncalo Mordido, Sarath Chandar",
            "EMNLP Paper ID": "2614",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "3037efe59991db85cbb7e17f41e196f09c74ba6d",
            "title": "DLoRA: Distributed Parameter-Efficient Fine-Tuning Solution for Large Language Model",
            "abstract": "To enhance the performance of large language models (LLM) on downstream tasks, one solution is to fine-tune certain LLM parameters and make it better align with the characteristics of the training dataset. This process is commonly known as parameter-efficient fine-tuning (PEFT). Due to the scale of LLM, PEFT operations are usually executed in the public environment (e.g., cloud server). This necessitates the sharing of sensitive user data across public environments, thereby raising potential privacy concerns. To tackle these challenges, we propose a distributed PEFT framework called DLoRA. DLoRA enables scalable PEFT operations to be performed collaboratively between the cloud and user devices. Coupled with the proposed Kill and Revive algorithm, the evaluation results demonstrate that DLoRA can significantly reduce the computation and communication workload over the user devices while achieving superior accuracy and privacy protection.",
            "link": "https://www.semanticscholar.org/paper/3037efe59991db85cbb7e17f41e196f09c74ba6d",
            "authors": "Chao Gao, Sai Qian Zhang",
            "matchScore": 282.19144,
            "original title": "DLoRA: Distributed Parameter-Efficient Fine-Tuning Solution for Large Language Model",
            "original authors": "Chao Gao, Sai Qian Zhang",
            "EMNLP Paper ID": "2675",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "bb14b3d53fa470a2bcf2377285abf59155e9de1f",
            "title": "QEFT: Quantization for Efficient Fine-Tuning of LLMs",
            "abstract": "With the rapid growth in the use of fine-tuning for large language models (LLMs), optimizing fine-tuning while keeping inference efficient has become highly important. However, this is a challenging task as it requires improvements in all aspects, including inference speed, fine-tuning speed, memory consumption, and, most importantly, model quality. Previous studies have attempted to achieve this by combining quantization with fine-tuning, but they have failed to enhance all four aspects simultaneously. In this study, we propose a new lightweight technique called Quantization for Efficient Fine-Tuning (QEFT). QEFT accelerates both inference and fine-tuning, is supported by robust theoretical foundations, offers high flexibility, and maintains good hardware compatibility. Our extensive experiments demonstrate that QEFT matches the quality and versatility of full-precision parameter-efficient fine-tuning, while using fewer resources. Our code is available at https://github.com/xvyaward/qeft.",
            "link": "https://www.semanticscholar.org/paper/bb14b3d53fa470a2bcf2377285abf59155e9de1f",
            "authors": "Changhun Lee, Jungyu Jin, Younghyun Cho, Eunhyeok Park",
            "matchScore": 228.9097,
            "original title": "QEFT: Quantization for Efficient Fine-Tuning of LLMs",
            "original authors": "Changhun Lee, Jun-gyu Jin, YoungHyun Cho, Eunhyeok Park",
            "EMNLP Paper ID": "2694",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "52b269168a7b5b6fa9cff1e08343d3309455dfb8",
            "title": "How Does Quantization Affect Multilingual LLMs?",
            "abstract": "Quantization techniques are widely used to improve inference speed and deployment of large language models. While a wide body of work examines the impact of quantization on LLMs in English, none have evaluated across languages. We conduct a thorough analysis of quantized multilingual LLMs, focusing on performance across languages and at varying scales. We use automatic benchmarks, LLM-as-a-Judge, and human evaluation, finding that (1) harmful effects of quantization are apparent in human evaluation, which automatic metrics severely underestimate: a 1.7% average drop in Japanese across automatic tasks corresponds to a 16.0% drop reported by human evaluators on realistic prompts; (2) languages are disparately affected by quantization, with non-Latin script languages impacted worst; and (3) challenging tasks like mathematical reasoning degrade fastest. As the ability to serve low-compute models is critical for wide global adoption of NLP technologies, our results urge consideration of multilingual performance as a key evaluation criterion for efficient models.",
            "link": "https://www.semanticscholar.org/paper/52b269168a7b5b6fa9cff1e08343d3309455dfb8",
            "authors": "Kelly Marchisio, Saurabh Dash, Hongyu Chen, Dennis Aumiller, A. Ustun, Sara Hooker, Sebastian Ruder",
            "matchScore": 216.88751,
            "original title": "How Does Quantization Affect Multilingual LLMs?",
            "original authors": "Kelly Marchisio, Saurabh Dash, Hongyu Chen, Dennis Aumiller, Ahmet \u00dcst\u00fcn, Sara Hooker, Sebastian Ruder",
            "EMNLP Paper ID": "3057",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "edfff0e15449f438a13a7341290c008bf6486afc",
            "title": "MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning",
            "abstract": "Low-rank adaptation (LoRA) and its mixture-of-experts (MOE) variants are highly effective parameter-efficient fine-tuning (PEFT) methods. However, they introduce significant latency in multi-tenant settings due to the LoRA modules and MOE routers added to multiple linear modules in the Transformer layer. To address this issue, we propose Mixture of Low-Rank Adaptation (MiLoRA), a novel and efficient LoRA variant. MiLoRA differs from previous MOE-style LoRA methods by considering each LoRA module as an expert and employing a prompt-aware routing mechanism. This mechanism calculates expert routing results once before generating the first new token and reuses these results for subsequent tokens, reducing latency. Extensive experiments and analysis on commonsense reasoning tasks, math reasoning tasks, and widely used LLM evaluation benchmarks demonstrate that MiLoRA consistently outperforms strong PEFT baselines with comparable tunable parameter budgets. Additionally, MiLoRA significantly reduces latency in multi-tenant settings compared to previous LoRA-based methods.",
            "link": "https://www.semanticscholar.org/paper/edfff0e15449f438a13a7341290c008bf6486afc",
            "authors": "Jingfan Zhang, Yi Zhao, Dan Chen, Xing Tian, Huanran Zheng, Wei Zhu",
            "matchScore": 228.49066,
            "original title": "EM-LoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning",
            "original authors": "Wei Zhu, Huanran Zheng, Yi Zhao, Xing Tian, Jingfan Zhang, Yi Ge, Jiawen Lyn",
            "EMNLP Paper ID": "3283",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "7b4c9c2285ed00827d33bc1cb7f7f1391f7d90e3",
            "title": "Layer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models",
            "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods have gained significant popularity for adapting pre-trained Large Language Models (LLMs) to downstream tasks, primarily due to their potential to significantly reduce memory and computational overheads. However, a common limitation in most PEFT approaches is their application of a uniform architectural design across all layers. This uniformity involves identical trainable modules and ignores the varying importance of each layer, leading to sub-optimal fine-tuning results. To overcome the above limitation and obtain better performance, we develop a novel approach, Importance-aware Sparse Tuning (IST), to fully utilize the inherent sparsity and select the most important subset of full layers with effective layer-wise importance scoring. The proposed IST is a versatile and plug-and-play technique compatible with various PEFT methods that operate on a per-layer basis. By leveraging the estimated importance scores, IST dynamically updates these selected layers in PEFT modules, leading to reduced memory demands. We further provide theoretical proof of convergence and empirical evidence of superior performance to demonstrate the advantages of IST over uniform updating strategies. Extensive experiments on a range of LLMs, PEFTs, and downstream tasks substantiate the effectiveness of our proposed method, showcasing IST's capacity to enhance existing layer-based PEFT methods. Our code is available at https://github.com/Kaiseem/IST.",
            "link": "https://www.semanticscholar.org/paper/7b4c9c2285ed00827d33bc1cb7f7f1391f7d90e3",
            "authors": "Kai Yao, Penlei Gao, Lichun Li, Yuan-yuan Zhao, Xiaofeng Wang, Wei Wang, Jianke Zhu",
            "matchScore": 282.52637,
            "original title": "Layerwise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models",
            "original authors": "Kai Yao, Penglei Gao, Lichun Li, Yuan Zhao, Xiaofeng Wang, Wei Wang, Jianke Zhu",
            "EMNLP Paper ID": "392",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "ee92723f03475fcfbb37ce8b8c888c497453ceb9",
            "title": "Head-wise Shareable Attention for Large Language Models",
            "abstract": "Large Language Models (LLMs) suffer from huge number of parameters, which restricts their deployment on edge devices. Weight sharing is one promising solution that encourages weight reuse, effectively reducing memory usage with less performance drop. However, current weight sharing techniques primarily focus on small-scale models like BERT and employ coarse-grained sharing rules, e.g., layer-wise. This becomes limiting given the prevalence of LLMs and sharing an entire layer or block obviously diminishes the flexibility of weight sharing. In this paper, we present a perspective on head-wise shareable attention for large language models. We further propose two memory-efficient methods that share parameters across attention heads, with a specific focus on LLMs. Both of them use the same dynamic strategy to select the shared weight matrices. The first method directly reuses the pre-trained weights without retraining, denoted as $\\textbf{DirectShare}$. The second method first post-trains with constraint on weight matrix similarity and then shares, denoted as $\\textbf{PostShare}$. Experimental results reveal our head-wise shared models still maintain satisfactory capabilities, demonstrating the feasibility of fine-grained weight sharing applied to LLMs.",
            "link": "https://www.semanticscholar.org/paper/ee92723f03475fcfbb37ce8b8c888c497453ceb9",
            "authors": "Zouying Cao, Yifei Yang, Hai Zhao",
            "matchScore": 222.38138,
            "original title": "Head-wise Shareable Attention for Large Language Models",
            "original authors": "zouying cao, Yifei Yang, hai zhao",
            "EMNLP Paper ID": "535",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Findings"
        },
        {
            "paperId": "af3f455570b673be8ba8a51075f7f5955646a1e7",
            "title": "Feature-based Low-Rank Compression of Large Language Models via Bayesian Optimization",
            "abstract": "In recent years, large language models (LLMs) have driven advances in natural language processing. Still, their growing scale has increased the computational burden, necessitating a balance between efficiency and performance. Low-rank compression, a promising technique, reduces non-essential parameters by decomposing weight matrices into products of two low-rank matrices. Yet, its application in LLMs has not been extensively studied. The key to low-rank compression lies in low-rank factorization and low-rank dimensions allocation. To address the challenges of low-rank compression in LLMs, we conduct empirical research on the low-rank characteristics of large models. We propose a low-rank compression method suitable for LLMs. This approach involves precise estimation of feature distributions through pooled covariance matrices and a Bayesian optimization strategy for allocating low-rank dimensions. Experiments on the LLaMA-2 models demonstrate that our method outperforms existing strong structured pruning and low-rank compression techniques in maintaining model performance at the same compression ratio.",
            "link": "https://www.semanticscholar.org/paper/af3f455570b673be8ba8a51075f7f5955646a1e7",
            "authors": "Yixin Ji, Yang Xiang, Juntao Li, Wei Chen, Zhongyi Liu, Kehai Chen, Min Zhang",
            "matchScore": 233.98001,
            "original title": "Adaptive Feature-based Low-Rank Compression of Large Language Models via Bayesian Optimization",
            "original authors": "Yixin Ji, Yang Xiang, Juntao Li, Qingrong Xia, Zi Ye, Xinyu Duan, Zhefeng Wang, Kehai Chen, Min Zhang",
            "EMNLP Paper ID": "823",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        }
    ],
    "Advancements in Speech Generation and Recognition Technologies": [
        {
            "paperId": "510278b81907bcc0f247eda5f69e99fbf8b3ff82",
            "title": "Speaking in Wavelet Domain: A Simple and Efficient Approach to Speed up Speech Diffusion Model",
            "abstract": "Recently, Denoising Diffusion Probabilistic Models (DDPMs) have attained leading performances across a diverse range of generative tasks. However, in the field of speech synthesis, although DDPMs exhibit impressive performance, their long training duration and substantial inference costs hinder practical deployment. Existing approaches primarily focus on enhancing inference speed, while approaches to accelerate training a key factor in the costs associated with adding or customizing voices often necessitate complex modifications to the model, compromising their universal applicability. To address the aforementioned challenges, we propose an inquiry: is it possible to enhance the training/inference speed and performance of DDPMs by modifying the speech signal itself? In this paper, we double the training and inference speed of Speech DDPMs by simply redirecting the generative target to the wavelet domain. This method not only achieves comparable or superior performance to the original model in speech synthesis tasks but also demonstrates its versatility. By investigating and utilizing different wavelet bases, our approach proves effective not just in speech synthesis, but also in speech enhancement.",
            "link": "https://www.semanticscholar.org/paper/510278b81907bcc0f247eda5f69e99fbf8b3ff82",
            "authors": "Xiangyu Zhang, Daijiao Liu, Hexin Liu, Qiquan Zhang, Hanyu Meng, Leibny Paola Garc\u00eda, Chng Eng Siong, Lina Yao",
            "EMNLP Paper ID": "12",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "a7eeee0df79da5662e7329d9710e97da032c3108",
            "title": "Scaling Properties of Speech Language Models",
            "abstract": "Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language models hold for the speech modality, these abilities will improve as the amount of compute used for training increases. In this paper, we use models of this scaling behavior to estimate the scale at which our current methods will yield a SLM with the English proficiency of text-based Large Language Models (LLMs). We establish a strong correlation between pre-training loss and downstream syntactic and semantic performance in SLMs and LLMs, which results in predictable scaling of linguistic performance. We show that the linguistic performance of SLMs scales up to three orders of magnitude more slowly than that of text-based LLMs. Additionally, we study the benefits of synthetic data designed to boost semantic understanding and the effects of coarser speech tokenization.",
            "link": "https://www.semanticscholar.org/paper/a7eeee0df79da5662e7329d9710e97da032c3108",
            "authors": "Santiago Cuervo, R. Marxer",
            "EMNLP Paper ID": "36",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "cee51255fe19652ebe1806b0cdb5a1c0f0d09ed1",
            "title": "TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and Multi-Level Style Control",
            "abstract": "Zero-shot singing voice synthesis (SVS) with style transfer and style control aims to generate high-quality singing voices with unseen timbres and styles (including singing method, emotion, rhythm, technique, and pronunciation) from audio and text prompts. However, the multifaceted nature of singing styles poses a significant challenge for effective modeling, transfer, and control. Furthermore, current SVS models often fail to generate singing voices rich in stylistic nuances for unseen singers. To address these challenges, we introduce TCSinger, the first zero-shot SVS model for style transfer across cross-lingual speech and singing styles, along with multi-level style control. Specifically, TCSinger proposes three primary modules: 1) the clustering style encoder employs a clustering vector quantization model to stably condense style information into a compact latent space; 2) the Style and Duration Language Model (S\\&D-LM) concurrently predicts style information and phoneme duration, which benefits both; 3) the style adaptive decoder uses a novel mel-style adaptive normalization method to generate singing voices with enhanced details. Experimental results show that TCSinger outperforms all baseline models in synthesis quality, singer similarity, and style controllability across various tasks, including zero-shot style transfer, multi-level style control, cross-lingual style transfer, and speech-to-singing style transfer. Singing voice samples can be accessed at https://tcsinger.github.io/.",
            "link": "https://www.semanticscholar.org/paper/cee51255fe19652ebe1806b0cdb5a1c0f0d09ed1",
            "authors": "Yu Zhang, Ziyue Jiang, Ruiqi Li, Changhao Pan, Jinzheng He, Rongjie Huang, Chuxin Wang, Zhou Zhao",
            "EMNLP Paper ID": "221",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "f22472b43801f5df67f21683e0578e71958b382a",
            "title": "With Ears to See and Eyes to Hear: Sound Symbolism Experiments with Multimodal Large Language Models",
            "abstract": "Recently, Large Language Models (LLMs) and Vision Language Models (VLMs) have demonstrated aptitude as potential substitutes for human participants in experiments testing psycholinguistic phenomena. However, an understudied question is to what extent models that only have access to vision and text modalities are able to implicitly understand sound-based phenomena via abstract reasoning from orthography and imagery alone. To investigate this, we analyse the ability of VLMs and LLMs to demonstrate sound symbolism (i.e., to recognise a non-arbitrary link between sounds and concepts) as well as their ability to\"hear\"via the interplay of the language and vision modules of open and closed-source multimodal models. We perform multiple experiments, including replicating the classic Kiki-Bouba and Mil-Mal shape and magnitude symbolism tasks, and comparing human judgements of linguistic iconicity with that of LLMs. Our results show that VLMs demonstrate varying levels of agreement with human labels, and more task information may be required for VLMs versus their human counterparts for in silico experimentation. We additionally see through higher maximum agreement levels that Magnitude Symbolism is an easier pattern for VLMs to identify than Shape Symbolism, and that an understanding of linguistic iconicity is highly dependent on model size.",
            "link": "https://www.semanticscholar.org/paper/f22472b43801f5df67f21683e0578e71958b382a",
            "authors": "Tyler Loakman, Yucheng Li, Chenghua Lin",
            "EMNLP Paper ID": "320",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "92e9acc55013a7408559d5e203eb913378563377",
            "title": "Voices Unheard: NLP Resources and Models for Yor\u00f9b\u00e1 Regional Dialects",
            "abstract": "Yor\\`ub\\'a an African language with roughly 47 million speakers encompasses a continuum with several dialects. Recent efforts to develop NLP technologies for African languages have focused on their standard dialects, resulting in disparities for dialects and varieties for which there are little to no resources or tools. We take steps towards bridging this gap by introducing a new high-quality parallel text and speech corpus YOR\\`ULECT across three domains and four regional Yor\\`ub\\'a dialects. To develop this corpus, we engaged native speakers, travelling to communities where these dialects are spoken, to collect text and speech data. Using our newly created corpus, we conducted extensive experiments on (text) machine translation, automatic speech recognition, and speech-to-text translation. Our results reveal substantial performance disparities between standard Yor\\`ub\\'a and the other dialects across all tasks. However, we also show that with dialect-adaptive finetuning, we are able to narrow this gap. We believe our dataset and experimental analysis will contribute greatly to developing NLP tools for Yor\\`ub\\'a and its dialects, and potentially for other African languages, by improving our understanding of existing challenges and offering a high-quality dataset for further development. We release YOR\\`ULECT dataset and models publicly under an open license.",
            "link": "https://www.semanticscholar.org/paper/92e9acc55013a7408559d5e203eb913378563377",
            "authors": "Orevaoghene Ahia, Anuoluwapo Aremu, Diana Abagyan, Hila Gonen, David Ifeoluwa Adelani, D. Abolade, Noah A. Smith, Yulia Tsvetkov",
            "EMNLP Paper ID": "480",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "cc7abd01eaa38dc7d1d2ea8bdcea60370749048d",
            "title": "Cross-Domain Audio Deepfake Detection: Dataset and Analysis",
            "abstract": "Audio deepfake detection (ADD) is essential for preventing the misuse of synthetic voices that may infringe on personal rights and privacy. Recent zero-shot text-to-speech (TTS) models pose higher risks as they can clone voices with a single utterance. However, the existing ADD datasets are outdated, leading to suboptimal generalization of detection models. In this paper, we construct a new cross-domain ADD dataset comprising over 300 hours of speech data that is generated by five advanced zero-shot TTS models. To simulate real-world scenarios, we employ diverse attack methods and audio prompts from different datasets. Experiments show that, through novel attack-augmented training, the Wav2Vec2-large and Whisper-medium models achieve equal error rates of 4.1\\% and 6.5\\% respectively. Additionally, we demonstrate our models' outstanding few-shot ADD ability by fine-tuning with just one minute of target-domain data. Nonetheless, neural codec compressors greatly affect the detection accuracy, necessitating further research.",
            "link": "https://www.semanticscholar.org/paper/cc7abd01eaa38dc7d1d2ea8bdcea60370749048d",
            "authors": "Yuang Li, Min Zhang, Mengxin Ren, Miaomiao Ma, Daimeng Wei, Hao Yang",
            "EMNLP Paper ID": "546",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "e23459636393552d37f54714a0beaa1cb6a5631f",
            "title": "Improving Spoken Language Modeling with Phoneme Classification: A Simple Fine-tuning Approach",
            "abstract": "Recent progress in Spoken Language Modeling has demonstrated the feasibility of learning language directly from speech. Generating speech through a pipeline that operates at the text level typically loses nuances, intonations, and non-verbal vocalizations. Modeling directly from speech opens up the path to more natural and expressive systems. On the other hand, speech-only systems tend to trail behind text-based language models in terms of their semantic abilities. We show that fine-tuning speech representation models on phoneme classification leads to more context-invariant representations, which in turn improve downstream language modeling performance.",
            "link": "https://www.semanticscholar.org/paper/e23459636393552d37f54714a0beaa1cb6a5631f",
            "authors": "Maxime Poli, Emmanuel Chemla, Emmanuel Dupoux",
            "EMNLP Paper ID": "582",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "4c850c928197df19ba57270d7c577e8f96ddd809",
            "title": "Reconsidering Sentence-Level Sign Language Translation",
            "abstract": "Historically, sign language machine translation has been posed as a sentence-level task: datasets consisting of continuous narratives are chopped up and presented to the model as isolated clips. In this work, we explore the limitations of this task framing. First, we survey a number of linguistic phenomena in sign languages that depend on discourse-level context. Then as a case study, we perform the first human baseline for sign language translation that actually substitutes a human into the machine learning task framing, rather than provide the human with the entire document as context. This human baseline -- for ASL to English translation on the How2Sign dataset -- shows that for 33% of sentences in our sample, our fluent Deaf signer annotators were only able to understand key parts of the clip in light of additional discourse-level context. These results underscore the importance of understanding and sanity checking examples when adapting machine learning to new domains.",
            "link": "https://www.semanticscholar.org/paper/4c850c928197df19ba57270d7c577e8f96ddd809",
            "authors": "Garrett Tanzer, Maximus Shengelia, Ken Harrenstien, David Uthus",
            "EMNLP Paper ID": "704",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 14): 13:00\u201314:00 (Afternoon)",
            "Type": "Main"
        },
        {
            "paperId": "61aa225f9ac05650e7d9d878722e18c0704b6063",
            "title": "GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities",
            "abstract": "Perceiving and understanding non-speech sounds and non-verbal speech is essential to making decisions that help us interact with our surroundings. In this paper, we propose GAMA, a novel General-purpose Large Audio-Language Model (LALM) with Advanced Audio Understanding and Complex Reasoning Abilities. We build GAMA by integrating an LLM with multiple types of audio representations, including features from a custom Audio Q-Former, a multi-layer aggregator that aggregates features from multiple layers of an audio encoder. We fine-tune GAMA on a large-scale audio-language dataset, which augments it with audio understanding capabilities. Next, we propose CompA-R (Instruction-Tuning for Complex Audio Reasoning), a synthetically generated instruction-tuning (IT) dataset with instructions that require the model to perform complex reasoning on the input audio. We instruction-tune GAMA with CompA-R to endow it with complex reasoning abilities, where we further add a soft prompt as input with high-level semantic evidence by leveraging event tags of the input audio. Finally, we also propose CompA-R-test, a human-labeled evaluation dataset for evaluating the capabilities of LALMs on open-ended audio question-answering that requires complex reasoning. Through automated and expert human evaluations, we show that GAMA outperforms all other LALMs in literature on diverse audio understanding tasks by margins of 1%-84%. Further, GAMA IT-ed on CompA-R proves to be superior in its complex reasoning and instruction following capabilities.",
            "link": "https://www.semanticscholar.org/paper/61aa225f9ac05650e7d9d878722e18c0704b6063",
            "authors": "Sreyan Ghosh, Sonal Kumar, Ashish Seth, Chandra Kiran Reddy Evuru, Utkarsh Tyagi, S. Sakshi, Oriol Nieto, R. Duraiswami, Dinesh Manocha",
            "EMNLP Paper ID": "705",
            "Oral/Poster": "Oral",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "384f25839186b9bac057af38a32fc68e5e8e4a61",
            "title": "EH-MAM: Easy-to-Hard Masked Acoustic Modeling for Self-Supervised Speech Representation Learning",
            "abstract": "In this paper, we present EH-MAM (Easy-to-Hard adaptive Masked Acoustic Modeling), a novel self-supervised learning approach for speech representation learning. In contrast to the prior methods that use random masking schemes for Masked Acoustic Modeling (MAM), we introduce a novel selective and adaptive masking strategy. Specifically, during SSL training, we progressively introduce harder regions to the model for reconstruction. Our approach automatically selects hard regions and is built on the observation that the reconstruction loss of individual frames in MAM can provide natural signals to judge the difficulty of solving the MAM pre-text task for that frame. To identify these hard regions, we employ a teacher model that first predicts the frame-wise losses and then decides which frames to mask. By learning to create challenging problems, such as identifying harder frames and solving them simultaneously, the model is able to learn more effective representations and thereby acquire a more comprehensive understanding of the speech. Quantitatively, EH-MAM outperforms several state-of-the-art baselines across various low-resource speech recognition and SUPERB benchmarks by 5%-10%. Additionally, we conduct a thorough analysis to show that the regions masked by EH-MAM effectively capture useful context across speech frames.",
            "link": "https://www.semanticscholar.org/paper/384f25839186b9bac057af38a32fc68e5e8e4a61",
            "authors": "Ashish Seth, Ramaneswaran Selvakumar, S. Sakshi, Sonal Kumar, Sreyan Ghosh, Dinesh Manocha",
            "EMNLP Paper ID": "718",
            "Oral/Poster": "Oral",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "a6dea802d69b9497da2fe9b5bf6766fea81cbc17",
            "title": "Advancing Test-Time Adaptation in Wild Acoustic Test Settings",
            "abstract": "Acoustic foundation models, fine-tuned for Automatic Speech Recognition (ASR), suffer from performance degradation in wild acoustic test settings when deployed in real-world scenarios. Stabilizing online Test-Time Adaptation (TTA) under these conditions remains an open and unexplored question. Existing wild vision TTA methods often fail to handle speech data effectively due to the unique characteristics of high-entropy speech frames, which are unreliably filtered out even when containing crucial semantic content. Furthermore, unlike static vision data, speech signals follow short-term consistency, requiring specialized adaptation strategies. In this work, we propose a novel wild acoustic TTA method tailored for ASR fine-tuned acoustic foundation models. Our method, Confidence-Enhanced Adaptation, performs frame-level adaptation using a confidence-aware weight scheme to avoid filtering out essential information in high-entropy frames. Additionally, we apply consistency regularization during test-time optimization to leverage the inherent short-term consistency of speech signals. Our experiments on both synthetic and real-world datasets demonstrate that our approach outperforms existing baselines under various wild acoustic test settings, including Gaussian noise, environmental sounds, accent variations, and sung speech.",
            "link": "https://www.semanticscholar.org/paper/a6dea802d69b9497da2fe9b5bf6766fea81cbc17",
            "authors": "Hongfu Liu, Hengguan Huang, Ye Wang",
            "EMNLP Paper ID": "799",
            "Oral/Poster": "Oral",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "df5bf45012828000bccbaf6e9c5fbed2b35cb8a5",
            "title": "Multi-Dialect Vietnamese: Task, Dataset, Baseline Models and Challenges",
            "abstract": "Vietnamese, a low-resource language, is typically categorized into three primary dialect groups that belong to Northern, Central, and Southern Vietnam. However, each province within these regions exhibits its own distinct pronunciation variations. Despite the existence of various speech recognition datasets, none of them has provided a fine-grained classification of the 63 dialects specific to individual provinces of Vietnam. To address this gap, we introduce Vietnamese Multi-Dialect (ViMD) dataset, a novel comprehensive dataset capturing the rich diversity of 63 provincial dialects spoken across Vietnam. Our dataset comprises 102.56 hours of audio, consisting of approximately 19,000 utterances, and the associated transcripts contain over 1.2 million words. To provide benchmarks and simultaneously demonstrate the challenges of our dataset, we fine-tune state-of-the-art pre-trained models for two downstream tasks: (1) Dialect identification and (2) Speech recognition. The empirical results suggest two implications including the influence of geographical factors on dialects, and the constraints of current approaches in speech recognition tasks involving multi-dialect speech data. Our dataset is available for research purposes.",
            "link": "https://www.semanticscholar.org/paper/df5bf45012828000bccbaf6e9c5fbed2b35cb8a5",
            "authors": "Nguyen Van Dinh, Thanh Chi Dang, Luan Thanh Nguyen, Kiet Van Nguyen",
            "EMNLP Paper ID": "844",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 12): 17:45\u201318:45 (Evening)",
            "Type": "Main"
        },
        {
            "paperId": "bf325b38e5cfd4c60a2996134456f313dcfed7f2",
            "title": "Muting Whisper: A Universal Acoustic Adversarial Attack on Speech Foundation Models",
            "abstract": "Recent developments in large speech foundation models like Whisper have led to their widespread use in many automatic speech recognition (ASR) applications. These systems incorporate `special tokens' in their vocabulary, such as $\\texttt{<|endoftext|>}$, to guide their language generation process. However, we demonstrate that these tokens can be exploited by adversarial attacks to manipulate the model's behavior. We propose a simple yet effective method to learn a universal acoustic realization of Whisper's $\\texttt{<|endoftext|>}$ token, which, when prepended to any speech signal, encourages the model to ignore the speech and only transcribe the special token, effectively `muting' the model. Our experiments demonstrate that the same, universal 0.64-second adversarial audio segment can successfully mute a target Whisper ASR model for over 97\\% of speech samples. Moreover, we find that this universal adversarial audio segment often transfers to new datasets and tasks. Overall this work demonstrates the vulnerability of Whisper models to `muting' adversarial attacks, where such attacks can pose both risks and potential benefits in real-world settings: for example the attack can be used to bypass speech moderation systems, or conversely the attack can also be used to protect private speech data.",
            "link": "https://www.semanticscholar.org/paper/bf325b38e5cfd4c60a2996134456f313dcfed7f2",
            "authors": "Vyas Raina, Rao Ma, Charles McGhee, Kate Knill, Mark J. F. Gales",
            "EMNLP Paper ID": "856",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "3d1b81c5bd340dcf9ad7abdd85119556885ed40d",
            "title": "Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition",
            "abstract": "Synthetic data is widely used in speech recognition due to the availability of text-to-speech models, which facilitate adapting models to previously unseen text domains. However, existing methods suffer in performance when they fine-tune an automatic speech recognition (ASR) model on synthetic data as they suffer from the distributional shift commonly referred to as the synthetic-to-real gap. In this paper, we find that task vector arithmetic is effective at mitigating this gap. Our proposed method, SYN2REAL task vector, shows an average improvement of 10.03\\% improvement in word error rate over baselines on the SLURP dataset. Additionally, we show that an average of SYN2REAL task vectors, when we have real speeches from multiple different domains, can further adapt the original ASR model to perform better on the target text domain.",
            "link": "https://www.semanticscholar.org/paper/3d1b81c5bd340dcf9ad7abdd85119556885ed40d",
            "authors": "Hsuan Su, Hua Farn, Fan-Yun Sun, Shang-Tse Chen, Hung-yi Lee",
            "EMNLP Paper ID": "1017",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "d6cb9dc51ac532d62e5b0cf514c7c47ba9c1e371",
            "title": "ESC: Efficient Speech Coding with Cross-Scale Residual Vector Quantized Transformers",
            "abstract": "Neural speech codecs aim to compress input signals into minimal bits while maintaining content quality in a low-latency manner. However, existing neural codecs often trade model complexity for reconstruction performance. These codecs primarily use convolutional blocks for feature transformation, which are not inherently suited for capturing the local redundancies in speech signals. To compensate, they require either adversarial discriminators or a large number of model parameters to enhance audio quality. In response to these challenges, we introduce the Efficient Speech Codec (ESC), a lightweight, parameter-efficient speech codec based on a cross-scale residual vector quantization scheme and transformers. Our model employs mirrored hierarchical window transformer blocks and performs step-wise decoding from coarse-to-fine feature representations. To enhance bitrate efficiency, we propose a novel combination of vector quantization techniques along with a pre-training paradigm. Extensive experiments demonstrate that ESC can achieve high-fidelity speech reconstruction with significantly lower model complexity, making it a promising alternative to existing convolutional audio codecs.",
            "link": "https://www.semanticscholar.org/paper/d6cb9dc51ac532d62e5b0cf514c7c47ba9c1e371",
            "authors": "Yuzhe Gu, Enmao Diao",
            "EMNLP Paper ID": "1130",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "ad63a1e67eae4b404a40b820bfde8f869e9d23bf",
            "title": "Towards Robust Speech Representation Learning for Thousands of Languages",
            "abstract": "Self-supervised learning (SSL) has helped extend speech technologies to more languages by reducing the need for labeled data. However, models are still far from supporting the world's 7000+ languages. We propose XEUS, a Cross-lingual Encoder for Universal Speech, trained on over 1 million hours of data across 4057 languages, extending the language coverage of SSL models 4-fold. We combine 1 million hours of speech from existing publicly accessible corpora with a newly created corpus of 7400+ hours from 4057 languages, which will be publicly released. To handle the diverse conditions of multilingual speech data, we augment the typical SSL masked prediction approach with a novel dereverberation objective, increasing robustness. We evaluate XEUS on several benchmarks, and show that it consistently outperforms or achieves comparable results to state-of-the-art (SOTA) SSL models across a variety of tasks. XEUS sets a new SOTA on the ML-SUPERB benchmark: it outperforms MMS 1B and w2v-BERT 2.0 v2 by 0.8% and 4.4% respectively, despite having less parameters or pre-training data. Checkpoints, code, and data are found in https://www.wavlab.org/activities/2024/xeus/.",
            "link": "https://www.semanticscholar.org/paper/ad63a1e67eae4b404a40b820bfde8f869e9d23bf",
            "authors": "William Chen, Wangyou Zhang, Yifan Peng, Xinjian Li, Jinchuan Tian, Jiatong Shi, Xuankai Chang, Soumi Maiti, Karen Livescu, Shinji Watanabe",
            "EMNLP Paper ID": "1142",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "11a6f7d2969e61be206e4fa2914776cd8a509ae6",
            "title": "Speechworthy Instruction-tuned Language Models",
            "abstract": "Current instruction-tuned language models are exclusively trained with textual preference data and thus are often not aligned with the unique requirements of other modalities, such as speech. To better align language models with the speech domain, we explore (i) prompting strategies grounded in radio-industry best practices and (ii) preference learning using a novel speech-based preference data of 20K samples, generated with a wide spectrum of prompts that induce varying dimensions of speech-suitability and labeled by annotators who listen to response pairs. Both human and automatic evaluation show that both prompting and preference learning increase the speech-suitability of popular instruction-tuned LLMs. Interestingly, we find that prompting and preference learning can be additive; combining them achieves the best win rates in head-to-head comparison, resulting in responses that are preferred or tied to the base model in 76.2% of comparisons on average. Lastly, we share lexical, syntactical, and qualitative analyses to showcase how each method contributes to improving the speech-suitability of generated responses.",
            "link": "https://www.semanticscholar.org/paper/11a6f7d2969e61be206e4fa2914776cd8a509ae6",
            "authors": "Hyundong Justin Cho, Nic Jedema, Leonardo F. R. Ribeiro, Karishma Sharma, Pedro A. Szekely, Alessandro Moschitti, Ruben Janssen, Jonathan May",
            "EMNLP Paper ID": "1202",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "8a515cec1559414759aa3c732fa5f4ceb8972267",
            "title": "Towards Online Continuous Sign Language Recognition and Translation",
            "abstract": "Research on continuous sign language recognition (CSLR) is essential to bridge the communication gap between deaf and hearing individuals. Numerous previous studies have trained their models using the connectionist temporal classification (CTC) loss. During inference, these CTC-based models generally require the entire sign video as input to make predictions, a process known as offline recognition, which suffers from high latency and substantial memory usage. In this work, we take the first step towards online CSLR. Our approach consists of three phases: 1) developing a sign dictionary; 2) training an isolated sign language recognition model on the dictionary; and 3) employing a sliding window approach on the input sign sequence, feeding each sign clip to the optimized model for online recognition. Additionally, our online recognition model can be extended to support online translation by integrating a gloss-to-text network and can enhance the performance of any offline model. With these extensions, our online approach achieves new state-of-the-art performance on three popular benchmarks across various task settings. Code and models are available at https://github.com/FangyunWei/SLRT.",
            "link": "https://www.semanticscholar.org/paper/8a515cec1559414759aa3c732fa5f4ceb8972267",
            "authors": "Ronglai Zuo, Fangyun Wei, Brian Mak",
            "EMNLP Paper ID": "1264",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "469d0f4ac94ff649a3689083e04d61c00a731a30",
            "title": "Unveiling the Role of Pretraining in Direct Speech Translation",
            "abstract": "Direct speech-to-text translation systems encounter an important drawback in data scarcity. A common solution consists on pretraining the encoder on automatic speech recognition, hence losing efficiency in the training process. In this study, we compare the training dynamics of a system using a pretrained encoder, the conventional approach, and one trained from scratch. We observe that, throughout the training, the randomly initialized model struggles to incorporate information from the speech inputs for its predictions. Hence, we hypothesize that this issue stems from the difficulty of effectively training an encoder for direct speech translation. While a model trained from scratch needs to learn acoustic and semantic modeling simultaneously, a pretrained one can just focus on the latter. Based on these findings, we propose a subtle change in the decoder cross-attention to integrate source information from earlier steps in training. We show that with this change, the model trained from scratch can achieve comparable performance to the pretrained one, while reducing the training time.",
            "link": "https://www.semanticscholar.org/paper/469d0f4ac94ff649a3689083e04d61c00a731a30",
            "authors": "Belen Alastruey, Gerard I. G\u00e1llego, M. Costa-juss\u00e0",
            "EMNLP Paper ID": "1307",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "9f82c4872e455b2ee9dbf862b7dd9fd549f7c43a",
            "title": "Self-Powered LLM Modality Expansion for Large Speech-Text Models",
            "abstract": "Large language models (LLMs) exhibit remarkable performance across diverse tasks, indicating their potential for expansion into large speech-text models (LSMs) by integrating speech capabilities. Although unified speech-text pre-training and multimodal data instruction-tuning offer considerable benefits, these methods generally entail significant resource demands and tend to overfit specific tasks. This study aims to refine the use of speech datasets for LSM training by addressing the limitations of vanilla instruction tuning. We explore the instruction-following dynamics within LSMs, identifying a critical issue termed speech anchor bias-a tendency for LSMs to over-rely on speech inputs, mistakenly interpreting the entire speech modality as directives, thereby neglecting textual instructions. To counteract this bias, we introduce a self-powered LSM that leverages augmented automatic speech recognition data generated by the model itself for more effective instruction tuning. Our experiments across a range of speech-based tasks demonstrate that self-powered LSM mitigates speech anchor bias and improves the fusion of speech and text modalities in LSMs. Data, code and scripts are freely available at https://github.com/ytf-philp/Self-powered-LSM.",
            "link": "https://www.semanticscholar.org/paper/9f82c4872e455b2ee9dbf862b7dd9fd549f7c43a",
            "authors": "Tengfei Yu, Xuebo Liu, Zhiyi Hou, Liang Ding, D. Tao, Min Zhang",
            "EMNLP Paper ID": "1447",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "4756557532fca0b9af40bf6d3d48989318e2f28c",
            "title": "Optimizing Rare Word Accuracy in Direct Speech Translation with a Retrieval-and-Demonstration Approach",
            "abstract": "Direct speech translation (ST) models often struggle with rare words. Incorrect translation of these words can have severe consequences, impacting translation quality and user trust. While rare word translation is inherently challenging for neural models due to sparse learning signals, real-world scenarios often allow access to translations of past recordings on similar topics. To leverage these valuable resources, we propose a retrieval-and-demonstration approach to enhance rare word translation accuracy in direct ST models. First, we adapt existing ST models to incorporate retrieved examples for rare word translation, which allows the model to benefit from prepended examples, similar to in-context learning. We then develop a cross-modal (speech-to-speech, speech-to-text, text-to-text) retriever to locate suitable examples. We demonstrate that standard ST models can be effectively adapted to leverage examples for rare word translation, improving rare word translation accuracy over the baseline by 17.6% with gold examples and 8.5% with retrieved examples. Moreover, our speech-to-speech retrieval approach outperforms other modalities and exhibits higher robustness to unseen speakers. Our code is publicly available (https://github.com/SiqiLii/Retrieve-and-Demonstration-ST).",
            "link": "https://www.semanticscholar.org/paper/4756557532fca0b9af40bf6d3d48989318e2f28c",
            "authors": "Siqi Li, Danni Liu, Jan Niehues",
            "EMNLP Paper ID": "1478",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "e4b53084cb60936ffb53b5e504c575b3900014e2",
            "title": "OpenSep: Leveraging Large Language Models with Textual Inversion for Open World Audio Separation",
            "abstract": "Audio separation in real-world scenarios, where mixtures contain a variable number of sources, presents significant challenges due to limitations of existing models, such as over-separation, under-separation, and dependence on predefined training sources. We propose OpenSep, a novel framework that leverages large language models (LLMs) for automated audio separation, eliminating the need for manual intervention and overcoming source limitations. OpenSep uses textual inversion to generate captions from audio mixtures with off-the-shelf audio captioning models, effectively parsing the sound sources present. It then employs few-shot LLM prompting to extract detailed audio properties of each parsed source, facilitating separation in unseen mixtures. Additionally, we introduce a multi-level extension of the mix-and-separate training framework to enhance modality alignment by separating single source sounds and mixtures simultaneously. Extensive experiments demonstrate OpenSep's superiority in precisely separating new, unseen, and variable sources in challenging mixtures, outperforming SOTA baseline methods. Code is released at https://github.com/tanvir-utexas/OpenSep.git",
            "link": "https://www.semanticscholar.org/paper/e4b53084cb60936ffb53b5e504c575b3900014e2",
            "authors": "Tanvir Mahmud, D. Marculescu",
            "EMNLP Paper ID": "1538",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "cdcb6809b5714f3f4e033894104740111c525797",
            "title": "VHASR: A Multimodal Speech Recognition System With Vision Hotwords",
            "abstract": "The image-based multimodal automatic speech recognition (ASR) model enhances speech recognition performance by incorporating audio-related image. However, some works suggest that introducing image information to model does not help improving ASR performance. In this paper, we propose a novel approach effectively utilizing audio-related image information and set up VHASR, a multimodal speech recognition system that uses vision as hotwords to strengthen the model's speech recognition capability. Our system utilizes a dual-stream architecture, which firstly transcribes the text on the two streams separately, and then combines the outputs. We evaluate the proposed model on four datasets: Flickr8k, ADE20k, COCO, and OpenImages. The experimental results show that VHASR can effectively utilize key information in images to enhance the model's speech recognition ability. Its performance not only surpasses unimodal ASR, but also achieves SOTA among existing image-based multimodal ASR.",
            "link": "https://www.semanticscholar.org/paper/cdcb6809b5714f3f4e033894104740111c525797",
            "authors": "Jiliang Hu, Zuchao Li, Ping Wang, Haojun Ai, Lefei Zhang, Hai Zhao",
            "EMNLP Paper ID": "1705",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "68544c0839c09cbfa76fac536508bc284dfab058",
            "title": "A Two-Step Approach for Data-Efficient French Pronunciation Learning",
            "abstract": "Recent studies have addressed intricate phonological phenomena in French, relying on either extensive linguistic knowledge or a significant amount of sentence-level pronunciation data. However, creating such resources is expensive and non-trivial. To this end, we propose a novel two-step approach that encompasses two pronunciation tasks: grapheme-to-phoneme and post-lexical processing. We then investigate the efficacy of the proposed approach with a notably limited amount of sentence-level pronunciation data. Our findings demonstrate that the proposed two-step approach effectively mitigates the lack of extensive labeled data, and serves as a feasible solution for addressing French phonological phenomena even under resource-constrained environments.",
            "link": "https://www.semanticscholar.org/paper/68544c0839c09cbfa76fac536508bc284dfab058",
            "authors": "Hoyeon Lee, Hyeeun Jang, Jong-Hwan Kim, Jae-Min Kim",
            "EMNLP Paper ID": "2406",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "91486cab08e0094e7e82b76f8afe12403172c84b",
            "title": "Continual Test-time Adaptation for End-to-end Speech Recognition on Noisy Speech",
            "abstract": "Deep Learning-based end-to-end Automatic Speech Recognition (ASR) has made significant strides but still struggles with performance on out-of-domain samples due to domain shifts in real-world scenarios. Test-Time Adaptation (TTA) methods address this issue by adapting models using test samples at inference time. However, current ASR TTA methods have largely focused on non-continual TTA, which limits cross-sample knowledge learning compared to continual TTA. In this work, we first propose a Fast-slow TTA framework for ASR that leverages the advantage of continual and non-continual TTA. Following this framework, we introduce Dynamic SUTA (DSUTA), an entropy-minimization-based continual TTA method for ASR. To enhance DSUTA robustness for time-varying data, we design a dynamic reset strategy to automatically detect domain shifts and reset the model, making it more effective at handling multi-domain data. Our method demonstrates superior performance on various noisy ASR datasets, outperforming both non-continual and continual TTA baselines while maintaining robustness to domain changes without requiring domain boundary information.",
            "link": "https://www.semanticscholar.org/paper/91486cab08e0094e7e82b76f8afe12403172c84b",
            "authors": "Guan-Ting Lin, Wei-Ping Huang, Hung-yi Lee",
            "EMNLP Paper ID": "2605",
            "Oral/Poster": "Oral",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "6e8fefc7d4f73d14353fc8466ce243c4cb618daf",
            "title": "Interventional Speech Noise Injection for ASR Generalizable Spoken Language Understanding",
            "abstract": "Recently, pre-trained language models (PLMs) have been increasingly adopted in spoken language understanding (SLU). However, automatic speech recognition (ASR) systems frequently produce inaccurate transcriptions, leading to noisy inputs for SLU models, which can significantly degrade their performance. To address this, our objective is to train SLU models to withstand ASR errors by exposing them to noises commonly observed in ASR systems, referred to as ASR-plausible noises. Speech noise injection (SNI) methods have pursued this objective by introducing ASR-plausible noises, but we argue that these methods are inherently biased towards specific ASR systems, or ASR-specific noises. In this work, we propose a novel and less biased augmentation method of introducing the noises that are plausible to any ASR system, by cutting off the non-causal effect of noises. Experimental results and analyses demonstrate the effectiveness of our proposed methods in enhancing the robustness and generalizability of SLU models against unseen ASR systems by introducing more diverse and plausible ASR noises in advance.",
            "link": "https://www.semanticscholar.org/paper/6e8fefc7d4f73d14353fc8466ce243c4cb618daf",
            "authors": "Yeonjoon Jung, Jaeseong Lee, Seungtaek Choi, Dohyeon Lee, Minsoo Kim, Seung-won Hwang",
            "EMNLP Paper ID": "2736",
            "Oral/Poster": "Oral",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "2b19225486b96067bda6d229c2eb4fb0b08584cd",
            "title": "TokenVerse: Unifying Speech and NLP Tasks via Transducer-based ASR",
            "abstract": "In traditional conversational intelligence from speech, a cascaded pipeline is used, involving tasks such as voice activity detection, diarization, transcription, and subsequent processing with different NLP models for tasks like semantic endpointing and named entity recognition (NER). Our paper introduces To-kenVerse, a single Transducer-based model designed to handle multiple tasks. This is achieved by integrating task-specific to-kens into the reference text during ASR model training, stream-lining the inference and eliminating the need for separate NLP models. In addition to ASR, we conduct experiments on 3 different tasks: speaker change detection, endpointing, and NER. Our experiments on a public and a private dataset show that the proposed method improves ASR by up to 7.7% in relative WER while outperforming the cascaded pipeline approach in individual task performance. Additionally, we present task transfer learning to a new task within an existing TokenVerse.",
            "link": "https://www.semanticscholar.org/paper/2b19225486b96067bda6d229c2eb4fb0b08584cd",
            "authors": "Shashi Kumar, S. Madikeri, Juan Pablo Zuluaga, Iuliia Nigmatulina, Esa\u00fa Villatoro-Tello, Sergio Burdisso, P. Motl\u00edcek, Karthik Pandia, A. Ganapathiraju",
            "EMNLP Paper ID": "2811",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "704a4f565c45c75835ad2a6f7fe3bdb1440ec677",
            "title": "Casablanca: Data and Models for Multidialectal Arabic Speech Recognition",
            "abstract": "In spite of the recent progress in speech processing, the majority of world languages and dialects remain uncovered. This situation only furthers an already wide technological divide, thereby hindering technological and socioeconomic inclusion. This challenge is largely due to the absence of datasets that can empower diverse speech systems. In this paper, we seek to mitigate this obstacle for a number of Arabic dialects by presenting Casablanca, a large-scale community-driven effort to collect and transcribe a multi-dialectal Arabic dataset. The dataset covers eight dialects: Algerian, Egyptian, Emirati, Jordanian, Mauritanian, Moroccan, Palestinian, and Yemeni, and includes annotations for transcription, gender, dialect, and code-switching. We also develop a number of strong baselines exploiting Casablanca. The project page for Casablanca is accessible at: www.dlnlp.ai/speech/casablanca.",
            "link": "https://www.semanticscholar.org/paper/704a4f565c45c75835ad2a6f7fe3bdb1440ec677",
            "authors": "Bashar Talafha, Karima Kadaoui, S. Magdy, Mariem Habiboullah, Chafei Mohamed Chafei, Ahmed Oumar El-Shangiti, Hiba Zayed, Mohamedou Cheikh Tourad, Rahaf Alhamouri, Rwaa Assi, Aisha Alraeesi, Hour Mohamed, Fakhraddin Alwajih, Abdelrahman Mohamed, Abdellah El Mekki, El Moatez Billah Nagoudi, Benelhadj Djelloul Saadia, Hamzah A. Alsayadi, Walid S. Al-Dhabyani, Sara Shatnawi, Yasir Ech-Chammakhy, Amal Makouar, Yousra Berrachedi, Mustafa Jarrar, Shady Shehata, Ismail Berrada, Muhammad Abdul-Mageed",
            "EMNLP Paper ID": "3000",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "c2d2a39433027b11ef551357d8c99c44fc93f291",
            "title": "STTATTS: Unified Speech-To-Text And Text-To-Speech Model",
            "abstract": "Speech recognition and speech synthesis models are typically trained separately, each with its own set of learning objectives, training data, and model parameters, resulting in two distinct large networks. We propose a parameter-efficient approach to learning ASR and TTS jointly via a multi-task learning objective and shared parameters. Our evaluation demonstrates that the performance of our multi-task model is comparable to that of individually trained models while significantly saving computational and memory costs ($\\sim$50\\% reduction in the total number of parameters required for the two tasks combined). We experiment with English as a resource-rich language, and Arabic as a relatively low-resource language due to shortage of TTS data. Our models are trained with publicly available data, and both the training code and model checkpoints are openly available for further research.",
            "link": "https://www.semanticscholar.org/paper/c2d2a39433027b11ef551357d8c99c44fc93f291",
            "authors": "Hawau Olamide Toyin, Hao Li, Hanan Aldarmaki",
            "matchScore": 280.12097,
            "original title": "STTATTS: Unified Speech-To-Text And Text-To-Speech Model",
            "original authors": "Hawau Olamide Toyin, Hao Li, Hanan Aldarmaki",
            "EMNLP Paper ID": "1398",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "491ee07602069b4dc1842ff94894e1f8b70e6d77",
            "title": "MultiVerse: Efficient and Expressive Zero-Shot Multi-Task Text-to-Speech",
            "abstract": "Text-to-speech (TTS) systems that scale up the amount of training data have achieved significant improvements in zero-shot speech synthesis. However, these systems have certain limitations: they require a large amount of training data, which increases costs, and often overlook prosody similarity. To address these issues, we propose MultiVerse, a zero-shot multi-task TTS system that is able to perform TTS or speech style transfer in zero-shot and cross-lingual conditions. MultiVerse requires much less training data than traditional data-driven approaches. To ensure zero-shot performance even with limited data, we leverage source-filter theory-based disentanglement, utilizing the prompt for modeling filter-related and source-related representations. Additionally, to further enhance prosody similarity, we adopt a prosody modeling approach combining prompt-based autoregressive and non-autoregressive methods. Evaluations demonstrate the remarkable zero-shot multi-task TTS performance of MultiVerse and show that MultiVerse not only achieves zero-shot TTS performance comparable to data-driven TTS systems with much less data, but also significantly outperforms other zero-shot TTS systems trained with the same small amount of data. In particular, our novel prosody modeling technique significantly contributes to MultiVerse's ability to generate speech with high prosody similarity to the given prompts. Our samples are available at https://nc-ai.github.io/speech/publications/multiverse/index.html",
            "link": "https://www.semanticscholar.org/paper/491ee07602069b4dc1842ff94894e1f8b70e6d77",
            "authors": "Taejun Bak, Youngsik Eom, SeungJae Choi, Young-Sun Joo",
            "matchScore": 297.90988,
            "original title": "MultiVerse: Efficient and Expressive Zero-Shot Multi-Task Text-to-Speech",
            "original authors": "Taejun Bak, Youngsik Eom, SeungJae Choi, Young-Sun Joo",
            "EMNLP Paper ID": "1910",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "2cdfff602a5710f0ed1aa652dd51f6e4088d731a",
            "title": "Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large Language Models",
            "abstract": "Various audio-LLMs (ALLMs) have been explored recently for tackling different audio tasks simultaneously using a single, unified model. While existing evaluations of ALLMs primarily focus on single-audio tasks, real-world applications often involve processing multiple audio streams simultaneously. To bridge this gap, we propose the first multi-audio evaluation (MAE) benchmark that consists of 20 datasets from 11 multi-audio tasks encompassing both speech and sound scenarios. Comprehensive experiments on MAE demonstrate that the existing ALLMs, while being powerful in comprehending primary audio elements in individual audio inputs, struggling to handle multi-audio scenarios. To this end, we propose a novel multi-audio-LLM (MALLM) to capture audio context among multiple similar audios using discriminative learning on our proposed synthetic data. The results demonstrate that the proposed MALLM outperforms all baselines and achieves high data efficiency using synthetic data without requiring human annotations. The proposed MALLM opens the door for ALLMs towards multi-audio processing era and brings us closer to replicating human auditory capabilities in machines.",
            "link": "https://www.semanticscholar.org/paper/2cdfff602a5710f0ed1aa652dd51f6e4088d731a",
            "authors": "Yiming Chen, Xianghu Yue, Xiaoxue Gao, Chen Zhang, L. F. D\u2019Haro, R. Tan, Haizhou Li",
            "matchScore": 291.03964,
            "original title": "Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large Language Models",
            "original authors": "Yiming Chen, Xianghu Yue, Xiaoxue Gao, Chen Zhang, Luis Fernando D\u2019Haro, Robby T. Tan, Haizhou Li",
            "EMNLP Paper ID": "2186",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Findings"
        },
        {
            "paperId": "187de1107f760e10c52d29e9cd720fa63cb01ea8",
            "title": "Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing",
            "abstract": "In visual speech processing, context modeling capability is one of the most important requirements due to the ambiguous nature of lip movements. For example, homophenes, words that share identical lip movements but produce different sounds, can be distinguished by considering the context. In this paper, we propose a novel framework, namely Visual Speech Processing incorporated with LLMs (VSP-LLM), to maximize the context modeling ability by bringing the overwhelming power of LLMs. Specifically, VSP-LLM is designed to perform multi-tasks of visual speech recognition and translation, where the given instructions control the type of task. The input video is mapped to the input latent space of an LLM by employing a self-supervised visual speech model. Focused on the fact that there is redundant information in input frames, we propose a novel deduplication method that reduces the embedded visual features by employing visual speech units. Through the proposed deduplication and Low Rank Adaptation (LoRA), VSP-LLM can be trained in a computationally efficient manner. In the translation dataset, the MuAViC benchmark, we demonstrate that VSP-LLM trained on just 30 hours of labeled data can more effectively translate lip movements compared to the recent model trained with 433 hours of data.",
            "link": "https://www.semanticscholar.org/paper/187de1107f760e10c52d29e9cd720fa63cb01ea8",
            "authors": "Jeong Hun Yeo, Seunghee Han, Minsu Kim, Y. Ro",
            "matchScore": 394.72995,
            "original title": "Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing",
            "original authors": "Jeonghun Yeo, Seunghee Han, Minsu Kim, Yong Man Ro",
            "EMNLP Paper ID": "2252",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "8f584931615043d23ed5a1351b0fca547fbe6f16",
            "title": "Exploring the Potential of Multimodal LLM with Knowledge-Intensive Multimodal ASR",
            "abstract": "Recent advancements in multimodal large language models (MLLMs) have made significant progress in integrating information across various modalities, yet real-world applications in educational and scientific domains remain challenging. This paper introduces the Multimodal Scientific ASR (MS-ASR) task, which focuses on transcribing scientific conference videos by leveraging visual information from slides to enhance the accuracy of technical terminologies. Realized that traditional metrics like WER fall short in assessing performance accurately, prompting the proposal of severity-aware WER (SWER) that considers the content type and severity of ASR errors. We propose the Scientific Vision Augmented ASR (SciVASR) framework as a baseline method, enabling MLLMs to improve transcript quality through post-editing. Evaluations of state-of-the-art MLLMs, including GPT-4o, show a 45% improvement over speech-only baselines, highlighting the importance of multimodal information integration.",
            "link": "https://www.semanticscholar.org/paper/8f584931615043d23ed5a1351b0fca547fbe6f16",
            "authors": "Minghan Wang, Yuxia Wang, Thuy-Trang Vu, Ehsan Shareghi, Gholamreza Haffari",
            "matchScore": 267.46756,
            "original title": "Exploring the Potential of Multimodal LLM with Knowledge-Intensive Multimodal ASR",
            "original authors": "Minghan Wang, Yuxia Wang, Thuy-Trang Vu, Ehsan Shareghi, Reza Haf",
            "EMNLP Paper ID": "2590",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 12): 17:45\u201318:45 (Evening)",
            "Type": "Findings"
        },
        {
            "paperId": "49ea51df202296e49ae41d2d063e622c70747dfe",
            "title": "Audio-Based Linguistic Feature Extraction for Enhancing Multi-lingual and Low-Resource Text-to-Speech",
            "abstract": "The difficulty of acquiring abundant, high-quality data, especially in multi-lingual contexts, has sparked interest in addressing low-resource scenarios. Moreover, current literature rely on fixed expressions from language IDs, which results in the inadequate learning of language representations, and the failure to generate speech in unseen languages. To address these challenges, we propose a novel method that directly extracts linguistic features from audio input while effectively filtering out miscellaneous acoustic information including speaker-specific attributes like timbre. Subjective and objective evaluations affirm the effectiveness of our approach for multi-lingual text-to-speech, and highlight its superiority in low-resource transfer learning for previously unseen language.",
            "link": "https://www.semanticscholar.org/paper/49ea51df202296e49ae41d2d063e622c70747dfe",
            "authors": "Youngjae Kim, Yejin Jeon, Gary Geunbae Lee",
            "matchScore": 303.28998,
            "original title": "Audio-Based Linguistic Feature Extraction for Enhancing Multi-lingual and Low-Resource Text-to-Speech",
            "original authors": "Youngjae Kim, Yejin Jeon, Gary Lee",
            "EMNLP Paper ID": "2706",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "3dabd7acc3cab6235c828efaca933b5fd7f14026",
            "title": "PRoDeliberation: Parallel Robust Deliberation for End-to-End Spoken Language Understanding",
            "abstract": "Spoken Language Understanding (SLU) is a critical component of voice assistants; it consists of converting speech to semantic parses for task execution. Previous works have explored end-to-end models to improve the quality and robustness of SLU models with Deliberation, however these models have remained autoregressive, resulting in higher latencies. In this work we introduce PRoDeliberation, a novel method leveraging a Connectionist Temporal Classification-based decoding strategy as well as a denoising objective to train robust non-autoregressive deliberation models. We show that PRoDeliberation achieves the latency reduction of parallel decoding (2-10x improvement over autoregressive models) while retaining the ability to correct Automatic Speech Recognition (ASR) mistranscriptions of autoregressive deliberation systems. We further show that the design of the denoising training allows PRoDeliberation to overcome the limitations of small ASR devices, and we provide analysis on the necessity of each component of the system.",
            "link": "https://www.semanticscholar.org/paper/3dabd7acc3cab6235c828efaca933b5fd7f14026",
            "authors": "Trang Le, Daniel Lazar, Suyoun Kim, Shan Jiang, Duc Le, Adithya Sagar, Aleksandr Livshits, Ahmed Aly, Akshat Shrivastava",
            "matchScore": 320.4474,
            "original title": "PRoDeliberation: Parallel Robust Deliberation for End-to-End Spoken Language Understanding",
            "original authors": "Trang Le, Daniel Lazar, Suyoun Kim, Shan Jiang, Duc Le, Adithya Sagar, Aleksandr Livshits, Ahmed A Aly, Akshat Shrivastava",
            "EMNLP Paper ID": "2719",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "d48f3d288e9642ab4d206a0b2f8386dd7b62d321",
            "title": "Gloss2Text: Sign Language Gloss translation using LLMs and Semantically Aware Label Smoothing",
            "abstract": "Sign language translation from video to spoken text presents unique challenges owing to the distinct grammar, expression nuances, and high variation of visual appearance across different speakers and contexts. The intermediate gloss annotations of videos aim to guide the translation process. In our work, we focus on {\\em Gloss2Text} translation stage and propose several advances by leveraging pre-trained large language models (LLMs), data augmentation, and novel label-smoothing loss function exploiting gloss translation ambiguities improving significantly the performance of state-of-the-art approaches. Through extensive experiments and ablation studies on the PHOENIX Weather 2014T dataset, our approach surpasses state-of-the-art performance in {\\em Gloss2Text} translation, indicating its efficacy in addressing sign language translation and suggesting promising avenues for future research and development.",
            "link": "https://www.semanticscholar.org/paper/d48f3d288e9642ab4d206a0b2f8386dd7b62d321",
            "authors": "Pooya Fayyazsanavi, Antonios Anastasopoulos, Jana Kosecka",
            "matchScore": 303.4115,
            "original title": "Gloss2Text: Sign Language Gloss translation using LLMs and Semantically Aware Label Smoothing",
            "original authors": "Pooya Fayyazsanavi, Antonios Anastasopoulos, Jana Kosecka",
            "EMNLP Paper ID": "3109",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "4c09ff9ed16831d237fd0558ad3e461ccd2e8981",
            "title": "Fast Streaming Transducer ASR Prototyping via Knowledge Distillation with Whisper",
            "abstract": "The training of automatic speech recognition (ASR) with little to no supervised data remains an open question. In this work, we demonstrate that streaming Transformer-Transducer (TT) models can be trained from scratch in consumer and accessible GPUs in their entirety with pseudo-labeled (PL) speech from foundational speech models (FSM). This allows training a robust ASR model just in one stage and does not require large data and computational budget compared to the two-step scenario with pre-training and fine-tuning. We perform a comprehensive ablation on different aspects of PL-based streaming TT models such as the impact of (1) shallow fusion of n-gram LMs, (2) contextual biasing with named entities, (3) chunk-wise decoding for low-latency streaming applications, and (4) TT overall performance as the function of the FSM size. Our results demonstrate that TT can be trained from scratch without supervised data, even with very noisy PLs. We validate the proposed framework on 6 languages from CommonVoice and propose multiple heuristics to filter out hallucinated PLs.",
            "link": "https://www.semanticscholar.org/paper/4c09ff9ed16831d237fd0558ad3e461ccd2e8981",
            "authors": "Iuliia Thorbecke, Juan Pablo Zuluaga, Esa\u00fa Villatoro-Tello, Shashi Kumar, Pradeep Rangappa, Sergio Burdisso, P. Motl\u00edcek, Karthik Pandia, A. Ganapathiraju",
            "matchScore": 308.04062,
            "original title": "Fast Streaming Transducer ASR Prototyping via Knowledge Distillation with Whisper",
            "original authors": "Iuliia Thorbecke, Juan Pablo Zuluaga Gomez, Esa\u00fa VILLATORO-TELLO, Shashi Kumar, Pradeep Rangappa, Sergio Burdisso, Petr Motlicek, Karthik Pandia D S, Aravind Ganapathiraju",
            "EMNLP Paper ID": "3214",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "94043689f4d0df6d332cf29a70545c684e4b3bf5",
            "title": "Automated Tone Transcription and Clustering with Tone2Vec",
            "abstract": "Lexical tones play a crucial role in Sino-Tibetan languages. However, current phonetic fieldwork relies on manual effort, resulting in substantial time and financial costs. This is especially challenging for the numerous endangered languages that are rapidly disappearing, often compounded by limited funding. In this paper, we introduce pitch-based similarity representations for tone transcription, named Tone2Vec. Experiments on dialect clustering and variance show that Tone2Vec effectively captures fine-grained tone variation. Utilizing Tone2Vec, we develop the first automatic approach for tone transcription and clustering by presenting a novel representation transformation for transcriptions. Additionally, these algorithms are systematically integrated into an open-sourced and easy-to-use package, ToneLab, which facilitates automated fieldwork and cross-regional, cross-lexical analysis for tonal languages. Extensive experiments were conducted to demonstrate the effectiveness of our methods.",
            "link": "https://www.semanticscholar.org/paper/94043689f4d0df6d332cf29a70545c684e4b3bf5",
            "authors": "Yi Yang, Yiming Wang, ZhiQiang Tang, Jiahong Yuan",
            "matchScore": 209.78731,
            "original title": "Automated Tone Transcription and Clustering with Tone2Vec",
            "original authors": "Yi Yang, Yiming Wang, ZhiQiang Tang, Jiahong Yuan",
            "EMNLP Paper ID": "409",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "92f0744ccda5c16e53c63993217949d788a74706",
            "title": "Enhancing Polyglot Voices by Leveraging Cross-Lingual Fine-Tuning in Any-to-One Voice Conversion",
            "abstract": "The creation of artificial polyglot voices remains a challenging task, despite considerable progress in recent years. This paper investigates self-supervised learning for voice conversion to create native-sounding polyglot voices. We introduce a novel cross-lingual any-to-one voice conversion system that is able to preserve the source accent without the need for multilingual data from the target speaker. In addition, we show a novel cross-lingual fine-tuning strategy that further improves the accent and reduces the training data requirements. Objective and subjective evaluations with English, Spanish, French and Mandarin Chinese confirm that our approach improves on state-of-the-art methods, enhancing the speech intelligibility and overall quality of the converted speech, especially in cross-lingual scenarios. Audio samples are available at https://giuseppe-ruggiero.github.io/a2o-vc-demo/",
            "link": "https://www.semanticscholar.org/paper/92f0744ccda5c16e53c63993217949d788a74706",
            "authors": "Giuseppe Ruggiero, Matteo Testa, J. V. D. Walle, Luigi Di Caro",
            "matchScore": 347.34995,
            "original title": "Enhancing Polyglot Voices by Leveraging Cross-Lingual Fine-Tuning in Any-to-One Voice Conversion",
            "original authors": "Giuseppe Ruggiero, Matteo Testa, Jurgen Van de Walle, Luigi Di Caro",
            "EMNLP Paper ID": "446",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "f83dcacce8cd8e0d4e8c5a0843513a2ed9241ac4",
            "title": "PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems",
            "abstract": "Multimodal language models that process both text and speech have a potential for applications in spoken dialogue systems. However, current models face two major challenges in response generation latency: (1) generating a spoken response requires the prior generation of a written response, and (2) speech sequences are significantly longer than text sequences. This study addresses these issues by extending the input and output sequences of the language model to support the parallel generation of text and speech. Our experiments on spoken question answering tasks demonstrate that our approach improves latency while maintaining the quality of response content. Additionally, we show that latency can be further reduced by generating speech in multiple sequences. Demo samples are available at https://rinnakk.github.io/research/publications/PSLM.",
            "link": "https://www.semanticscholar.org/paper/f83dcacce8cd8e0d4e8c5a0843513a2ed9241ac4",
            "authors": "Kentaro Mitsui, Koh Mitsuda, Toshiaki Wakatsuki, Yukiya Hono, Kei Sawada",
            "matchScore": 305.37158,
            "original title": "PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems",
            "original authors": "Kentaro Mitsui, Koh Mitsuda, Toshiaki Wakatsuki, Yukiya Hono, Kei Sawada",
            "EMNLP Paper ID": "552",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "06b207f1a00193973015dbaa059af8be425b316a",
            "title": "Bahasa Harmony: A Comprehensive Dataset for Bahasa Text-to-Speech Synthesis with Discrete Codec Modeling of EnGen-TTS",
            "abstract": "This research introduces a comprehensive Bahasa text-to-speech (TTS) dataset and a novel TTS model, EnGen-TTS, designed to enhance the quality and versatility of synthetic speech in the Bahasa language. The dataset, spanning \\textasciitilde55.0 hours and 52K audio recordings, integrates diverse textual sources, ensuring linguistic richness. A meticulous recording setup captures the nuances of Bahasa phonetics, employing professional equipment to ensure high-fidelity audio samples. Statistical analysis reveals the dataset's scale and diversity, laying the foundation for model training and evaluation. The proposed EnGen-TTS model performs better than established baselines, achieving a Mean Opinion Score (MOS) of 4.45 $\\pm$ 0.13. Additionally, our investigation on real-time factor and model size highlights EnGen-TTS as a compelling choice, with efficient performance. This research marks a significant advancement in Bahasa TTS technology, with implications for diverse language applications. Link to Generated Samples: \\url{https://bahasa-harmony-comp.vercel.app/}",
            "link": "https://www.semanticscholar.org/paper/06b207f1a00193973015dbaa059af8be425b316a",
            "authors": "Onkar Susladkar, Vishesh Tripathi, Biddwan Ahmed",
            "matchScore": 360.42682,
            "original title": "Bahasa Harmony: A Comprehensive Dataset for Bahasa Text-to-Speech Synthesis with Discrete Codec Modeling of EnGen-TTS.",
            "original authors": "Onkar Kishor Susladkar, Vishesh Tripathi, Biddwan Ahmed",
            "EMNLP Paper ID": "557",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "1a41d449f2ed4051ac4c763fb8b1b07357eaf5f4",
            "title": "WavLLM: Towards Robust and Adaptive Speech Large Language Model",
            "abstract": "The recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, progressively broadening their scope to multimodal perception and generation. However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker's identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elementary single tasks, followed by advanced multi-task training on more complex tasks such as combinations of the elementary tasks. To enhance the flexibility and adherence to different tasks and instructions, a prompt-aware LoRA weight adapter is introduced in the second advanced multi-task training stage. We validate the proposed model on universal speech benchmarks including tasks such as ASR, ST, SV, ER, and also apply it to specialized datasets like Gaokao English listening comprehension set for SQA, and speech Chain-of-Thought (CoT) evaluation set. Experiments demonstrate that the proposed model achieves state-of-the-art performance across a range of speech tasks on the same model size, exhibiting robust generalization capabilities in executing complex tasks using CoT approach. Furthermore, our model successfully completes Gaokao tasks without specialized training. The codes, models, audio, and Gaokao evaluation set can be accessed at \\url{aka.ms/wavllm}.",
            "link": "https://www.semanticscholar.org/paper/1a41d449f2ed4051ac4c763fb8b1b07357eaf5f4",
            "authors": "Shujie Hu, Long Zhou, Shujie Liu, Sanyuan Chen, Hongkun Hao, Jing Pan, Xunying Liu, Jinyu Li, S. Sivasankaran, Linquan Liu, Furu Wei",
            "matchScore": 235.13635,
            "original title": "WavLLM: Towards Robust and Adaptive Speech Large Language Model",
            "original authors": "Shujie HU, Long Zhou, Shujie LIU, Sanyuan Chen, Lingwei Meng, Hongkun Hao, Jing Pan, Xunying Liu, Jinyu Li, Sunit Sivasankaran, Linquan Liu, Furu Wei",
            "EMNLP Paper ID": "903",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        }
    ],
    "Enhancing Reasoning Capabilities of Large Language Models": [
        {
            "paperId": "b9750286ba2198a406137e0dfee2d545f0d78c13",
            "title": "NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning",
            "abstract": "Language models struggle with handling numerical data and performing arithmetic operations. We hypothesize that this limitation can be partially attributed to non-intuitive textual numbers representation. When a digit is read or generated by a causal language model it does not know its place value (e.g. thousands vs. hundreds) until the entire number is processed. To address this issue, we propose a simple adjustment to how numbers are represented by including the count of digits before each number. For instance, instead of\"42\", we suggest using\"{2:42}\"as the new format. This approach, which we term NumeroLogic, offers an added advantage in number generation by serving as a Chain of Thought (CoT). By requiring the model to consider the number of digits first, it enhances the reasoning process before generating the actual number. We use arithmetic tasks to demonstrate the effectiveness of the NumeroLogic formatting. We further demonstrate NumeroLogic applicability to general natural language modeling, improving language understanding performance in the MMLU benchmark.",
            "link": "https://www.semanticscholar.org/paper/b9750286ba2198a406137e0dfee2d545f0d78c13",
            "authors": "Eli Schwartz, Leshem Choshen, J. Shtok, Sivan Doveh, Leonid Karlinsky, Assaf Arbelle",
            "EMNLP Paper ID": "21",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "13dc9eb9cf36c5bb287671a41cb31b7de2a4cee7",
            "title": "Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing",
            "abstract": "Large Language Models (LLMs) have demonstrated significant potential in handling complex reasoning tasks through step-by-step rationale generation. However, recent studies have raised concerns regarding the hallucination and flaws in their reasoning process. Substantial efforts are being made to improve the reliability and faithfulness of the generated rationales. Some approaches model reasoning as planning, while others focus on annotating for process supervision. Nevertheless, the planning-based search process often results in high latency due to the frequent assessment of intermediate reasoning states and the extensive exploration space. Additionally, supervising the reasoning process with human annotation is costly and challenging to scale for LLM training. To address these issues, in this paper, we propose a framework to learn planning-based reasoning through Direct Preference Optimization (DPO) on collected trajectories, which are ranked according to synthesized process rewards. Our results on challenging logical reasoning benchmarks demonstrate the effectiveness of our learning framework, showing that our 7B model can surpass the strong counterparts like GPT-3.5-Turbo.",
            "link": "https://www.semanticscholar.org/paper/13dc9eb9cf36c5bb287671a41cb31b7de2a4cee7",
            "authors": "Fangkai Jiao, Chengwei Qin, Zhengyuan Liu, Nancy F. Chen, Shafiq R. Joty",
            "EMNLP Paper ID": "35",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "4ccff1f5b660ac253703dfe1eb053e7a57d4b06f",
            "title": "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable potential in handling complex reasoning tasks by generating step-by-step rationales.Some methods have proven effective in boosting accuracy by introducing extra verifiers to assess these paths. However, existing verifiers, typically trained on binary-labeled reasoning paths, fail to fully utilize the relative merits of intermediate steps, thereby limiting the effectiveness of the feedback provided. To overcome this limitation, we propose Tree-based Preference Learning Verifier (Tree-PLV), a novel approach that constructs reasoning trees via a best-first search algorithm and collects step-level paired data for preference training. Compared to traditional binary classification, step-level preferences more finely capture the nuances between reasoning steps, allowing for a more precise evaluation of the complete reasoning path. We empirically evaluate Tree-PLV across a range of arithmetic and commonsense reasoning tasks, where it significantly outperforms existing benchmarks. For instance, Tree-PLV achieved substantial performance gains over the Mistral-7B self-consistency baseline on GSM8K (67.55% to 82.79%), MATH (17.00% to 26.80%), CSQA (68.14% to 72.97%), and StrategyQA (82.86% to 83.25%).Additionally, our study explores the appropriate granularity for applying preference learning, revealing that step-level guidance provides feedback that better aligns with the evaluation of the reasoning process.",
            "link": "https://www.semanticscholar.org/paper/4ccff1f5b660ac253703dfe1eb053e7a57d4b06f",
            "authors": "Mingqian He, Yongliang Shen, Wenqi Zhang, Zeqi Tan, Weiming Lu",
            "EMNLP Paper ID": "237",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "5c118e57b5398224ca4401c902cda33da7d29ec3",
            "title": "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models",
            "abstract": "The alignments of reasoning abilities between smaller and larger Language Models are largely conducted via Supervised Fine-Tuning (SFT) using demonstrations generated from robust Large Language Models (LLMs). Although these approaches deliver more performant models, they do not show sufficiently strong generalization ability as the training only relies on the provided demonstrations. In this paper, we propose the Self-refine Instruction-tuning method that elicits Smaller Language Models to self-refine their abilities. Our approach is based on a two-stage process, where reasoning abilities are first transferred between LLMs and Small Language Models (SLMs) via Instruction-tuning on demonstrations provided by LLMs, and then the instructed models Self-refine their abilities through preference optimization strategies. In particular, the second phase operates refinement heuristics based on the Direct Preference Optimization algorithm, where the SLMs are elicited to deliver a series of reasoning paths by automatically sampling the generated responses and providing rewards using ground truths from the LLMs. Results obtained on commonsense and math reasoning tasks show that this approach significantly outperforms Instruction-tuning in both in-domain and out-domain scenarios, aligning the reasoning abilities of Smaller and Larger Language Models.",
            "link": "https://www.semanticscholar.org/paper/5c118e57b5398224ca4401c902cda33da7d29ec3",
            "authors": "Leonardo Ranaldi, Andr\u00e9 Freitas",
            "EMNLP Paper ID": "262",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "c2be68bb31fcfbb9bbcab951cebf8c9a1b096ece",
            "title": "Consistent Autoformalization for Constructing Mathematical Libraries",
            "abstract": "Autoformalization is the task of automatically translating mathematical content written in natural language to a formal language expression. The growing language interpretation capabilities of Large Language Models (LLMs), including in formal languages, are lowering the barriers for autoformalization. However, LLMs alone are not capable of consistently and reliably delivering autoformalization, in particular as the complexity and specialization of the target domain grows. As the field evolves into the direction of systematically applying autoformalization towards large mathematical libraries, the need to improve syntactic, terminological and semantic control increases. This paper proposes the coordinated use of three mechanisms, most-similar retrieval augmented generation (MS-RAG), denoising steps, and auto-correction with syntax error feedback (Auto-SEF) to improve autoformalization quality. The empirical analysis, across different models, demonstrates that these mechanisms can deliver autoformalizaton results which are syntactically, terminologically and semantically more consistent. These mechanisms can be applied across different LLMs and have shown to deliver improve results across different model types.",
            "link": "https://www.semanticscholar.org/paper/c2be68bb31fcfbb9bbcab951cebf8c9a1b096ece",
            "authors": "Lan Zhang, Xin Quan, Andre Freitas",
            "EMNLP Paper ID": "450",
            "Oral/Poster": "Oral",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "5c667f3fd6215266e482dcbeb1e0119c27032519",
            "title": "How Do Humans Write Code? Large Models Do It the Same Way Too",
            "abstract": "Large Language Models (LLMs) often make errors when performing numerical calculations. In contrast to traditional chain-of-thought reasoning, the program-of-thoughts approach involves generating executable code to solve problems. By executing this code, it achieves more precise results. Using generated executable code instead of natural language can reduce computational errors. However, we observe that when LLMs solve mathematical problems using code, they tend to generate more incorrect reasoning than when using natu-ral language. To address this issue, we propose Human-Think Language (HTL), a straightforward yet highly efficient approach inspired by human coding practices. The approach first generates problem-solving methods described in the natural language by the model, then converts them into code, mirroring the process where people think through the logic in natural language before writing it as code. Additionally, it utilizes the Proximal Policy Optimization (PPO) algorithm, enabling it to provide feedback to itself based on the correctness of mathematical answers, much like humans do. Finally, we introduce a focus-attention mechanism that masks the question segment, enhancing its reliance on natural language inference solutions during code generation. We conduct our experiments without introducing any additional information, and the results across five mathematical calculation datasets showcase the effectiveness of our approach. Notably, on the NumGLUE dataset, the LlaMA-2-7B-based model achieves a superior performance rate (75.1%) compared to the previous best performance with the LlaMA-2-70B model (74.4%).",
            "link": "https://www.semanticscholar.org/paper/5c667f3fd6215266e482dcbeb1e0119c27032519",
            "authors": "Long Li",
            "EMNLP Paper ID": "512",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 14): 13:00\u201314:00 (Afternoon)",
            "Type": "Main"
        },
        {
            "paperId": "512a5c307fdab29112a0f4af5c94a3436632eda1",
            "title": "MuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning",
            "abstract": "The tool-use Large Language Models (LLMs) that integrate with external Python interpreters have significantly enhanced mathematical reasoning capabilities for open-source LLMs, while tool-free methods chose another track: augmenting math reasoning data. However, a great method to integrate the above two research paths and combine their advantages remains to be explored. In this work, we firstly include new math questions via multi-perspective data augmenting methods and then synthesize code-nested solutions to them. The open LLMs (i.e., Llama-2) are finetuned on the augmented dataset to get the resulting models, MuMath-Code ($\\mu$-Math-Code). During the inference phase, our MuMath-Code generates code and interacts with the external python interpreter to get the execution results. Therefore, MuMath-Code leverages the advantages of both the external tool and data augmentation. To fully leverage the advantages of our augmented data, we propose a two-stage training strategy: In Stage-1, we finetune Llama-2 on pure CoT data to get an intermediate model, which then is trained on the code-nested data in Stage-2 to get the resulting MuMath-Code. Our MuMath-Code-7B achieves 83.8 on GSM8K and 52.4 on MATH, while MuMath-Code-70B model achieves new state-of-the-art performance among open methods -- achieving 90.7% on GSM8K and 55.1% on MATH. Extensive experiments validate the combination of tool use and data augmentation, as well as our two-stage training strategy. We release the proposed dataset along with the associated code for public use.",
            "link": "https://www.semanticscholar.org/paper/512a5c307fdab29112a0f4af5c94a3436632eda1",
            "authors": "Shuo Yin, Weihao You, Zhilong Ji, Guoqiang Zhong, Jinfeng Bai",
            "EMNLP Paper ID": "522",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "7348536e2197f3aa19387c6e5448af4c8f5acb38",
            "title": "Boosting Scientific Concepts Understanding: Can Analogy from Teacher Models Empower Student Models?",
            "abstract": "Analogical reasoning plays a critical role in human cognition, enabling us to understand new concepts by associating them with familiar ones. Previous research in the AI community has mainly focused on identifying and generating analogies and then examining their quality under human evaluation, which overlooks the practical application of these analogies in real-world settings. Inspired by the human education process, in this paper, we propose to investigate how analogies created by teacher language models (LMs) can assist student LMs in understanding scientific concepts, thereby aligning more closely with practical scenarios. Our results suggest that free-form analogies can indeed aid LMs in understanding concepts. Additionally, analogies generated by student LMs can improve their own performance on scientific question answering, demonstrating their capability to use analogies for self-learning new knowledge. Resources are available at https://github.com/siyuyuan/SCUA.",
            "link": "https://www.semanticscholar.org/paper/7348536e2197f3aa19387c6e5448af4c8f5acb38",
            "authors": "Siyu Yuan, Cheng Jiayang, Lin Qiu, Deqing Yang",
            "EMNLP Paper ID": "672",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "e55b2bea1915e0f009724f3edaf796c96b097c84",
            "title": "Python is Not Always the Best Choice: Embracing Multilingual Program of Thoughts",
            "abstract": "Program of Thoughts (PoT) is an approach characterized by its executable intermediate steps, which ensure the accuracy of the logical calculations in the reasoning process. Currently, PoT primarily uses Python. However, relying solely on a single language may result in suboptimal solutions and overlook the potential benefits of other programming languages. In this paper, we conduct comprehensive experiments on the programming languages used in PoT and find that no single language consistently delivers optimal performance across all tasks and models. The effectiveness of each language varies depending on the specific scenarios. Inspired by this, we propose a task and model agnostic approach called MultiPoT, which harnesses strength and diversity from various languages. Experimental results reveal that it significantly outperforms Python Self-Consistency. Furthermore, it achieves comparable or superior performance compared to the best monolingual PoT in almost all tasks across all models. In particular, MultiPoT achieves more than 4.6% improvement on average on ChatGPT (gpt-3.5-turbo-0701).",
            "link": "https://www.semanticscholar.org/paper/e55b2bea1915e0f009724f3edaf796c96b097c84",
            "authors": "Xianzhen Luo, Qingfu Zhu, Zhiming Zhang, Libo Qin, Xu Wang, Qing Yang, Dongliang Xu, Wanxiang Che",
            "EMNLP Paper ID": "802",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "696bc486d84cb33a152491676b6237e0e5e43061",
            "title": "Stepwise Verification and Remediation of Student Reasoning Errors with Large Language Model Tutors",
            "abstract": "Large language models (LLMs) present an opportunity to scale high-quality personalized education to all. A promising approach towards this means is to build dialog tutoring models that scaffold students' problem-solving. However, even though existing LLMs perform well in solving reasoning questions, they struggle to precisely detect student's errors and tailor their feedback to these errors. Inspired by real-world teaching practice where teachers identify student errors and customize their response based on them, we focus on verifying student solutions and show how grounding to such verification improves the overall quality of tutor response generation. We collect a dataset of 1K stepwise math reasoning chains with the first error step annotated by teachers. We show empirically that finding the mistake in a student solution is challenging for current models. We propose and evaluate several verifiers for detecting these errors. Using both automatic and human evaluation we show that the student solution verifiers steer the generation model towards highly targeted responses to student errors which are more often correct with less hallucinations compared to existing baselines.",
            "link": "https://www.semanticscholar.org/paper/696bc486d84cb33a152491676b6237e0e5e43061",
            "authors": "Nico Daheim, Jakub Macina, Manu Kapur, Iryna Gurevych, Mrinmaya Sachan",
            "EMNLP Paper ID": "972",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "41e347649d5563d4af2e8ceeea9ff36b5c7faf8f",
            "title": "Learning to Correct for QA Reasoning with Black-box LLMs",
            "abstract": "An open challenge in recent machine learning is about how to improve the reasoning capability of large language models (LLMs) in a black-box setting, i.e., without access to detailed information such as output token probabilities. Existing approaches either rely on accessibility (which is often unrealistic) or involve significantly increased train- and inference-time costs. This paper addresses those limitations or shortcomings by proposing a novel approach, namely CoBB (Correct for improving QA reasoning of Black-Box LLMs). It uses a trained adaptation model to perform a seq2seq mapping from the often-imperfect reasonings of the original black-box LLM to the correct or improved reasonings. Specifically, the adaptation model is initialized with a relatively small open-source LLM and adapted over a collection of sub-sampled training pairs. To select the representative pairs of correct and incorrect reasonings, we formulated the dataset construction as an optimization problem that minimizes the statistical divergence between the sampled subset and the entire collection, and solved it via a genetic algorithm. We then train the adaptation model over the sampled pairs by contrasting the likelihoods of correct and incorrect reasonings. Our experimental results demonstrate that CoBB significantly improves reasoning accuracy across various QA benchmarks, compared to the best-performing adaptation baselines.",
            "link": "https://www.semanticscholar.org/paper/41e347649d5563d4af2e8ceeea9ff36b5c7faf8f",
            "authors": "Jaehyung Kim, Dongyoung Kim, Yiming Yang",
            "EMNLP Paper ID": "1019",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "516d20473dc74e089f1be5d7b2aa61c120263788",
            "title": "DiVERT: Distractor Generation with Variational Errors Represented as Text for Math Multiple-choice Questions",
            "abstract": "High-quality distractors are crucial to both the assessment and pedagogical value of multiple-choice questions (MCQs), where manually crafting ones that anticipate knowledge deficiencies or misconceptions among real students is difficult. Meanwhile, automated distractor generation, even with the help of large language models (LLMs), remains challenging for subjects like math. It is crucial to not only identify plausible distractors but also understand the error behind them. In this paper, we introduce DiVERT (Distractor Generation with Variational Errors Represented as Text), a novel variational approach that learns an interpretable representation of errors behind distractors in math MCQs. Through experiments on a real-world math MCQ dataset with 1,434 questions used by hundreds of thousands of students, we show that DiVERT, despite using a base open-source LLM with 7B parameters, outperforms state-of-the-art approaches using GPT-4o on downstream distractor generation. We also conduct a human evaluation with math educators and find that DiVERT leads to error labels that are of comparable quality to human-authored ones.",
            "link": "https://www.semanticscholar.org/paper/516d20473dc74e089f1be5d7b2aa61c120263788",
            "authors": "Nigel Fernandez, Alexander Scarlatos, Simon Woodhead, Andrew Lan",
            "EMNLP Paper ID": "1032",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "0a47b95ee9e3fae6f2c8742b39dc6db4e485b242",
            "title": "Self-AMPLIFY: Improving Small Language Models with Self Post Hoc Explanations",
            "abstract": "Incorporating natural language rationales in the prompt and In-Context Learning (ICL) have led to a significant improvement of Large Language Models (LLMs) performance. However, generating high-quality rationales require human-annotation or the use of auxiliary proxy models. In this work, we propose Self-AMPLIFY to automatically generate rationales from post hoc explanation methods applied to Small Language Models (SLMs) to improve their own performance. Self-AMPLIFY is a 3-step method that targets samples, generates rationales and builds a final prompt to leverage ICL. Self-AMPLIFY performance is evaluated on four SLMs and five datasets requiring strong reasoning abilities. Self-AMPLIFY achieves good results against competitors, leading to strong accuracy improvement. Self-AMPLIFY is the first method to apply post hoc explanation methods to autoregressive language models to generate rationales to improve their own performance in a fully automated manner.",
            "link": "https://www.semanticscholar.org/paper/0a47b95ee9e3fae6f2c8742b39dc6db4e485b242",
            "authors": "Milan Bhan, Jean-No\u00ebl Vittaut, N. Chesneau, Marie-Jeanne Lesot",
            "EMNLP Paper ID": "1252",
            "Oral/Poster": "Oral",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "a9de58b1ad7e052dfc21f0363ec3809b5f9e5b7b",
            "title": "TheoremLlama: Transforming General-Purpose LLMs into Lean4 Experts",
            "abstract": "Proving mathematical theorems using computer-verifiable formal languages like Lean significantly impacts mathematical reasoning. One approach to formal theorem proving involves generating complete proofs using Large Language Models (LLMs) based on Natural Language (NL) proofs. However, due to the scarcity of aligned NL and Formal Language (FL) theorem-proving data most modern LLMs exhibit suboptimal performance.This scarcity results in a paucity of methodologies for training LLMs and techniques to fully utilize their capabilities in composing formal proofs. To address these challenges, this paper proposes TheoremLlama, an end-to-end framework that trains a general-purpose LLM to be a Lean4 expert. TheoremLlama includes NL-FL dataset generation and bootstrapping method to obtain aligned dataset, curriculum learning and block training techniques to train the model, and iterative proof writing method to write Lean4 proofs that work together synergistically. Using the dataset generation method in TheoremLlama, we provide Open Bootstrapped Theorems (OBT), an NL-FL aligned and bootstrapped dataset. Our novel NL-FL bootstrapping method, where NL proofs are integrated into Lean4 code for training datasets, leverages the NL reasoning ability of LLMs for formal reasoning. The TheoremLlama framework achieves cumulative accuracies of 36.48% and 33.61% on MiniF2F-Valid and Test datasets respectively, surpassing the GPT-4 baseline of 22.95% and 25.41%. Our code, model checkpoints, and the generated dataset is published in GitHub",
            "link": "https://www.semanticscholar.org/paper/a9de58b1ad7e052dfc21f0363ec3809b5f9e5b7b",
            "authors": "Ruida Wang, Jipeng Zhang, Yizhen Jia, Rui Pan, Shizhe Diao, Renjie Pi, Tong Zhang",
            "EMNLP Paper ID": "1393",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "e69191b77f00814c8f0579477e9dd188bae3eba5",
            "title": "ControlMath: Controllable Data Generation Promotes Math Generalist Models",
            "abstract": "Utilizing large language models (LLMs) for data augmentation has yielded encouraging results in mathematical reasoning. However, these approaches face constraints in problem diversity, potentially restricting them to in-domain/distribution data generation. To this end, we propose ControlMath, an iterative method involving an equation-generator module and two LLM-based agents. The module creates diverse equations, which the Problem-Crafter agent then transforms into math word problems. The Reverse-Agent filters and selects high-quality data, adhering to the\"less is more\"principle, achieving better results with fewer data points. This approach enables the generation of diverse math problems, not limited to specific domains or distributions. As a result, we collect ControlMathQA, which involves 190k math word problems. Extensive results prove that combining our dataset with in-domain datasets like GSM8K can help improve the model's mathematical ability to generalize, leading to improved performances both within and beyond specific domains.",
            "link": "https://www.semanticscholar.org/paper/e69191b77f00814c8f0579477e9dd188bae3eba5",
            "authors": "Nuo Chen, Ning Wu, Jianhui Chang, Jia Li",
            "EMNLP Paper ID": "1424",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "dc03957b23354055ec9d517902b294b941575c20",
            "title": "Large Language Models Can Self-Correct with Key Condition Verification",
            "abstract": "Intrinsic self-correct was a method that instructed large language models (LLMs) to verify and correct their responses without external feedback. Unfortunately, the study concluded that the LLMs could not self-correct reasoning yet. We find that a simple yet effective verification method can unleash inherent capabilities of the LLMs. That is to mask a key condition in the question, add the current response to construct a verification question, and predict the condition to verify the response. The condition can be an entity in an open-domain question or a numeric value in a math question, which requires minimal effort (via prompting) to identify. We propose an iterative verify-then-correct framework to progressively identify and correct (probably) false responses, named ProCo. We conduct experiments on three reasoning tasks. On average, ProCo, with GPT-3.5-Turbo as the backend LLM, yields $+6.8$ exact match on four open-domain question answering datasets, $+14.1$ accuracy on three arithmetic reasoning datasets, and $+9.6$ accuracy on a commonsense reasoning dataset, compared to Self-Correct. Our implementation is made publicly available at https://wzy6642.github.io/proco.github.io/.",
            "link": "https://www.semanticscholar.org/paper/dc03957b23354055ec9d517902b294b941575c20",
            "authors": "Zhenyu Wu, Qingkai Zeng, Zhihan Zhang, Zhaoxuan Tan, Chao Shen, Meng Jiang",
            "EMNLP Paper ID": "1490",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "fa9b0b55aa937e12f521ed2364e8299307450c37",
            "title": "DynaThink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models",
            "abstract": "Large language models (LLMs) have demonstrated emergent capabilities across diverse reasoning tasks via popular Chains-of-Thought (COT) prompting. However, such a simple and fast COT approach often encounters limitations in dealing with complicated problems, while a thorough method, which considers multiple reasoning pathways and verifies each step carefully, results in slower inference. This paper addresses the challenge of enabling LLMs to autonomously select between fast and slow inference methods, thereby optimizing both efficiency and effectiveness. We introduce a dynamic decision-making framework that categorizes tasks into two distinct pathways: 'Fast', designated for tasks where the LLM quickly identifies a high-confidence solution, and 'Slow', allocated for tasks that the LLM perceives as complex and for which it has low confidence in immediate solutions as well as requiring more reasoning paths to verify. Experiments on five popular reasoning benchmarks demonstrated the superiority of the DynaThink over baselines.",
            "link": "https://www.semanticscholar.org/paper/fa9b0b55aa937e12f521ed2364e8299307450c37",
            "authors": "Jiabao Pan, Yan Zhang, Chen Zhang, Zuozhu Liu, Hongwei Wang, Haizhou Li",
            "EMNLP Paper ID": "1690",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "5ea68952a55a0d47a58a409b66c9daee35a237b8",
            "title": "Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning",
            "abstract": "Supervised fine-tuning enhances the problem-solving abilities of language models across various mathematical reasoning tasks. To maximize such benefits, existing research focuses on broadening the training set with various data augmentation techniques, which is effective for standard single-round question-answering settings. Our work introduces a novel technique aimed at cultivating a deeper understanding of the training problems at hand, enhancing performance not only in standard settings but also in more complex scenarios that require reflective thinking. Specifically, we propose reflective augmentation, a method that embeds problem reflection into each training instance. It trains the model to consider alternative perspectives and engage with abstractions and analogies, thereby fostering a thorough comprehension through reflective reasoning. Extensive experiments validate the achievement of our aim, underscoring the unique advantages of our method and its complementary nature relative to existing augmentation techniques.",
            "link": "https://www.semanticscholar.org/paper/5ea68952a55a0d47a58a409b66c9daee35a237b8",
            "authors": "Zhihan Zhang, Zhenwen Liang, Wenhao Yu, Dian Yu, Mengzhao Jia, Dong Yu, Meng Jiang",
            "EMNLP Paper ID": "1695",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "5e3e9034e281702f3802eeb4142f20ed137f65b1",
            "title": "Rationale-Aware Answer Verification by Pairwise Self-Evaluation",
            "abstract": "Answer verification identifies correct solutions among candidates generated by large language models (LLMs). Current approaches typically train verifier models by labeling solutions as correct or incorrect based solely on whether the final answer matches the gold answer. However, this approach neglects any flawed rationale in the solution yielding the correct answer, undermining the verifier's ability to distinguish between sound and flawed rationales. We empirically show that in StrategyQA, only 19% of LLM-generated solutions with correct answers have valid rationales, thus leading to an unreliable verifier. Furthermore, we demonstrate that training a verifier on valid rationales significantly improves its ability to distinguish valid and flawed rationale. To make a better verifier without extra human supervision, we introduce REPS (Rationale Enhancement through Pairwise Selection), a method for selecting valid rationales from candidates by iteratively applying pairwise self-evaluation using the same LLM that generates the solutions. Verifiers trained on solutions selected by REPS outperform those trained using conventional training methods on three reasoning benchmarks (ARC-Challenge, DROP, and StrategyQA). Our results suggest that training reliable verifiers requires ensuring the validity of rationales in addition to the correctness of the final answers, which would be critical for models assisting humans in solving complex reasoning tasks.",
            "link": "https://www.semanticscholar.org/paper/5e3e9034e281702f3802eeb4142f20ed137f65b1",
            "authors": "Akira Kawabata, Saku Sugawara",
            "EMNLP Paper ID": "1902",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "448c54a59b18b6eada98be058d42378891e211e6",
            "title": "LM2: A Simple Society of Language Models Solves Complex Reasoning",
            "abstract": "Despite demonstrating emergent reasoning abilities, Large Language Models (LLMS) often lose track of complex, multi-step reasoning. Existing studies show that providing guidance via decomposing the original question into multiple subproblems elicits more robustness in LLM reasoning -- a decomposer generates the subproblems, and a solver solves each of these subproblems. However, these techniques fail to accommodate coordination between the decomposer and the solver modules (either in a single model or different specialized ones) -- the decomposer does not keep track of the ability of the solver to follow the decomposed reasoning. In this paper, we propose LM2 to address these challenges. LM2 modularizes the decomposition, solution, and verification into three different language models. The decomposer module identifies the key concepts necessary to solve the problem and generates step-by-step subquestions according to the reasoning requirement. The solver model generates the solution to the subproblems that are then checked by the verifier module; depending upon the feedback from the verifier, the reasoning context is constructed using the subproblems and the solutions. These models are trained to coordinate using policy learning. Exhaustive experimentation suggests the superiority of LM2 over existing methods on in- and out-domain reasoning problems, outperforming the best baselines by $8.1\\%$ on MATH, $7.71\\%$ on JEEBench, and $9.7\\%$ on MedQA problems (code available at https://github.com/LCS2-IIITD/Language_Model_Multiplex).",
            "link": "https://www.semanticscholar.org/paper/448c54a59b18b6eada98be058d42378891e211e6",
            "authors": "Gurusha Juneja, Subhabrata Dutta, Tanmoy Chakraborty",
            "EMNLP Paper ID": "1941",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "69958f6cacf86537c8fb7e4efaa8fb2d8e519ce2",
            "title": "Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models",
            "abstract": "We introduce Mathador-LM, a new benchmark for evaluating the mathematical reasoning on large language models (LLMs), combining ruleset interpretation, planning, and problem-solving. This benchmark is inspired by the Mathador game, where the objective is to reach a target number using basic arithmetic operations on a given set of base numbers, following a simple set of rules. We show that, across leading LLMs, we obtain stable average performance while generating benchmark instances \\emph{dynamically}, following a target difficulty level. Thus, our benchmark alleviates concerns about test-set leakage into training data, an issue that often undermines popular benchmarks. Additionally, we conduct a comprehensive evaluation of both open and closed-source state-of-the-art LLMs on Mathador-LM. Our findings reveal that contemporary models struggle with Mathador-LM, scoring significantly lower than average 3rd graders. This stands in stark contrast to their strong performance on popular mathematical reasoning benchmarks. The implementation of Mathador-LM benchmark is available at \\href{https://github.com/IST-DASLab/Mathador-LM}{github.com/IST-DASLab/Mathador-LM}.",
            "link": "https://www.semanticscholar.org/paper/69958f6cacf86537c8fb7e4efaa8fb2d8e519ce2",
            "authors": "Eldar Kurtic, Amir Moeini, Dan Alistarh",
            "EMNLP Paper ID": "2004",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "505013ca939bf7e6138b38c58a24d80726c669ee",
            "title": "Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models",
            "abstract": "Algorithmic reasoning refers to the ability to understand the complex patterns behind the problem and decompose them into a sequence of reasoning steps towards the solution. Such nature of algorithmic reasoning makes it a challenge for large language models (LLMs), even though they have demonstrated promising performance in other reasoning tasks. Within this context, some recent studies use programming languages (e.g., Python) to express the necessary logic for solving a given instance/question (e.g., Program-of-Thought) as inspired by their strict and precise syntaxes. However, it is non-trivial to write an executable code that expresses the correct logic on the fly within a single inference call. Also, the code generated specifically for an instance cannot be reused for others, even if they are from the same task and might require identical logic to solve. This paper presents Think-and-Execute, a novel framework that decomposes the reasoning process of language models into two steps. (1) In Think, we discover a task-level logic that is shared across all instances for solving a given task and then express the logic with pseudocode; (2) In Execute, we further tailor the generated pseudocode to each instance and simulate the execution of the code. With extensive experiments on seven algorithmic reasoning tasks, we demonstrate the effectiveness of Think-and-Execute. Our approach better improves LMs' reasoning compared to several strong baselines performing instance-specific reasoning (e.g., CoT and PoT), suggesting the helpfulness of discovering task-level logic. Also, we show that compared to natural language, pseudocode can better guide the reasoning of LMs, even though they are trained to follow natural language instructions.",
            "link": "https://www.semanticscholar.org/paper/505013ca939bf7e6138b38c58a24d80726c669ee",
            "authors": "Hyungjoo Chae, Yeonghyeon Kim, Seungone Kim, Kai Tzu-iunn Ong, Beong-woo Kwak, Moohyeon Kim, Seonghwan Kim, Taeyoon Kwon, Jiwan Chung, Youngjae Yu, Jinyoung Yeo",
            "EMNLP Paper ID": "3242",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "3c7da712ed58b8e5223577a3ea734171cdc3cb30",
            "title": "Adversarial Math Word Problem Generation",
            "abstract": "Large language models (LLMs) have significantly transformed the educational landscape. As current plagiarism detection tools struggle to keep pace with LLMs' rapid advancements, the educational community faces the challenge of assessing students' true problem-solving abilities in the presence of LLMs. In this work, we explore a new paradigm for ensuring fair evaluation -- generating adversarial examples which preserve the structure and difficulty of the original questions aimed for assessment, but are unsolvable by LLMs. Focusing on the domain of math word problems, we leverage abstract syntax trees to structurally generate adversarial examples that cause LLMs to produce incorrect answers by simply editing the numeric values in the problems. We conduct experiments on various open- and closed-source LLMs, quantitatively and qualitatively demonstrating that our method significantly degrades their math problem-solving ability. We identify shared vulnerabilities among LLMs and propose a cost-effective approach to attack high-cost models. Additionally, we conduct automatic analysis to investigate the cause of failure, providing further insights into the limitations of LLMs.",
            "link": "https://www.semanticscholar.org/paper/3c7da712ed58b8e5223577a3ea734171cdc3cb30",
            "authors": "Roy Xie, Chengxuan Huang, Junlin Wang, Bhuwan Dhingra",
            "matchScore": 185.94339,
            "original title": "Adversarial Math Word Problem Generation",
            "original authors": "Roy Xie, Chengxuan Huang, Junlin Wang, Bhuwan Dhingra",
            "EMNLP Paper ID": "1003",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "4fa3f68aaceac14942c8ba139fd982d746db37d3",
            "title": "Evaluating Language Model Math Reasoning via Grounding in Educational Curricula",
            "abstract": "To ensure that math curriculum is grade-appropriate and aligns with critical skills or concepts in accordance with educational standards, pedagogical experts can spend months carefully reviewing published math problems. Drawing inspiration from this process, our work presents a novel angle for evaluating language models' (LMs) mathematical abilities, by investigating whether they can discern skills and concepts enabled by math content. We contribute two datasets: one consisting of 385 fine-grained descriptions of K-12 math skills and concepts, or standards, from Achieve the Core (ATC), and another of 9.9K math problems labeled with these standards (MathFish). We develop two tasks for evaluating LMs' abilities to assess math problems: (1) verifying whether a problem aligns with a given standard, and (2) tagging a problem with all aligned standards. Working with experienced teachers, we find that LMs struggle to tag and verify standards linked to problems, and instead predict labels that are close to ground truth, but differ in subtle ways. We also show that LMs often generate problems that do not fully align with standards described in prompts, suggesting the need for careful scrutiny on use cases involving LMs for generating curricular materials. Finally, we categorize problems in GSM8k using math standards, allowing us to better understand why some problems are more difficult to solve for models than others.",
            "link": "https://www.semanticscholar.org/paper/4fa3f68aaceac14942c8ba139fd982d746db37d3",
            "authors": "L. Lucy, Tal August, Rose E. Wang, Luca Soldaini, Courtney Allison, Kyle Lo",
            "matchScore": 286.6927,
            "original title": "Evaluating Language Model Math Reasoning via Grounding in Educational Curricula",
            "original authors": "Li Lucy, Tal August, Rose E Wang, Luca Soldaini, Courtney Allison, Kyle Lo",
            "EMNLP Paper ID": "1149",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "b9c304cd9449d98014d21e44ce77add601e5ca04",
            "title": "Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models",
            "abstract": "Large Language Models (LLMs) demonstrate ever-increasing abilities in mathematical and algorithmic tasks, yet their geometric reasoning skills are underexplored. We investigate LLMs' abilities in constructive geometric problem-solving one of the most fundamental steps in the development of human mathematical reasoning. Our work reveals notable challenges that the state-of-the-art LLMs face in this domain despite many successes in similar areas. LLMs exhibit biases in target variable selection and struggle with 2D spatial relationships, often misrepresenting and hallucinating objects and their placements. To this end, we introduce a framework that formulates an LLMs-based multi-agents system that enhances their existing reasoning potential by conducting an internal dialogue. This work underscores LLMs' current limitations in geometric reasoning and improves geometric reasoning capabilities through self-correction, collaboration, and diverse role specializations.",
            "link": "https://www.semanticscholar.org/paper/b9c304cd9449d98014d21e44ce77add601e5ca04",
            "authors": "Spyridon Mouselinos, H. Michalewski, Mateusz Malinowski",
            "matchScore": 271.8634,
            "original title": "Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models",
            "original authors": "Spyridon Mouselinos, Henryk Michalewski, Mateusz Malinowski",
            "EMNLP Paper ID": "1281",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "07cdf957a11506f87fbc030dcfaaa6399847648c",
            "title": "Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations",
            "abstract": "Existing research predominantly focuses on developing powerful language learning models (LLMs) for mathematical reasoning within monolingual languages, with few explorations in preserving efficacy in a multilingual context. To bridge this gap, this paper pioneers exploring and training powerful Multilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we construct the first multilingual math reasoning instruction dataset, MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue of training data scarcity in xMR tasks. Based on the collected dataset, we propose different training strategies to build powerful xMR LLMs, named MathOctopus, notably outperform conventional open-source LLMs and exhibit superiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B reaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond remarkable results, we unearth several pivotal observations and insights from extensive experiments: (1) When extending the rejection sampling strategy to the multilingual context, it proves effective for model performances, albeit limited. (2) Employing parallel corpora for math Supervised Fine-Tuning (SFT) across multiple languages not only significantly enhances model performance multilingually but also elevates their monolingual performance. This indicates that crafting multilingual corpora can be regarded as a vital strategy for enhancing model performance in a specific language, especially in mathematical reasoning tasks. For instance, MathOctopus-7B improves its counterparts that trained on English from 42.2% to 50.8% on GSM8K testset.",
            "link": "https://www.semanticscholar.org/paper/07cdf957a11506f87fbc030dcfaaa6399847648c",
            "authors": "Nuo Chen, Zinan Zheng, Ning Wu, Linjun Shou, Ming Gong, Yangqiu Song, Dongmei Zhang, Jia Li",
            "matchScore": 263.74707,
            "original title": "Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations",
            "original authors": "Nuo Chen, Zinan Zheng, Ning Wu, MING GONG, Dongmei Zhang, Jia Li",
            "EMNLP Paper ID": "1431",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "ba1ce9f45b5a84d8c8609c1db23aebc887c0ae4d",
            "title": "Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision",
            "abstract": "Process supervision, using a trained verifier to evaluate the intermediate steps generated by a reasoner, has demonstrated significant improvements in multi-step problem solving. In this paper, to avoid the expensive effort of human annotation on the verifier training data, we introduce Model-induced Process Supervision (MiPS), a novel method for automating data curation. MiPS annotates an intermediate step by sampling completions of this solution through the reasoning model, and obtaining an accuracy defined as the proportion of correct completions. Inaccuracies of the reasoner would cause MiPS underestimating the accuracy of intermediate steps, therefore, we suggest and empirically show that verification focusing on high predicted scores of the verifier shall be preferred over that of low predicted scores, contrary to prior observations on human curated data. Our approach significantly improves the performance of PaLM 2 on math and coding tasks (accuracy +0.67% on GSM8K, +4.16% on MATH, +0.92% on MBPP compared with an output supervision trained verifier). Additionally, our study demonstrates that the verifier exhibits strong generalization ability across different reasoning models.",
            "link": "https://www.semanticscholar.org/paper/ba1ce9f45b5a84d8c8609c1db23aebc887c0ae4d",
            "authors": "Zihan Wang, Yunxuan Li, Yuexin Wu, Liangchen Luo, Le Hou, Hongkun Yu, Jingbo Shang",
            "matchScore": 272.29428,
            "original title": "Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision",
            "original authors": "Zihan Wang, Yunxuan Li, Yuexin Wu, Liangchen Luo, Le Hou, Hongkun Yu, Jingbo Shang",
            "EMNLP Paper ID": "1495",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "da6bf364987a605843d56b19f9d0b1546b192c5f",
            "title": "A Semantic Search Engine for Mathlib4",
            "abstract": "The interactive theorem prover, Lean, enables the verification of formal mathematical proofs and is backed by an expanding community. Central to this ecosystem is its mathematical library, mathlib4, which lays the groundwork for the formalization of an expanding range of mathematical theories. However, searching for theorems in mathlib4 can be challenging. To successfully search in mathlib4, users often need to be familiar with its naming conventions or documentation strings. Therefore, creating a semantic search engine that can be used easily by individuals with varying familiarity with mathlib4 is very important. In this paper, we present a semantic search engine for mathlib4 that accepts informal queries and finds the relevant theorems. We also establish a benchmark for assessing the performance of various search engines for mathlib4.",
            "link": "https://www.semanticscholar.org/paper/da6bf364987a605843d56b19f9d0b1546b192c5f",
            "authors": "Guoxiong Gao, Haocheng Ju, Jiedong Jiang, Zihan Qin, Bin Dong",
            "matchScore": 182.52203,
            "original title": "A Semantic Search Engine for Mathlib4",
            "original authors": "Guoxiong Gao, Haocheng Ju, Jiedong Jiang, Zihan Qin, Bin Dong",
            "EMNLP Paper ID": "1688",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "25173b4449c520caeec0bf8c352af146a163f6c5",
            "title": "PropTest: Automatic Property Testing for Improved Visual Programming",
            "abstract": "Visual Programming has recently emerged as an alternative to end-to-end black-box visual reasoning models. This type of method leverages Large Language Models (LLMs) to generate the source code for an executable computer program that solves a given problem. This strategy has the advantage of offering an interpretable reasoning path and does not require finetuning a model with task-specific data. We propose PropTest, a general strategy that improves visual programming by further using an LLM to generate code that tests for visual properties in an initial round of proposed solutions. Our method generates tests for data-type consistency, output syntax, and semantic properties. PropTest achieves comparable results to state-of-the-art methods while using publicly available LLMs. This is demonstrated across different benchmarks on visual question answering and referring expression comprehension. Particularly, PropTest improves ViperGPT by obtaining 46.1\\% accuracy (+6.0\\%) on GQA using Llama3-8B and 59.5\\% (+8.1\\%) on RefCOCO+ using CodeLlama-34B.",
            "link": "https://www.semanticscholar.org/paper/25173b4449c520caeec0bf8c352af146a163f6c5",
            "authors": "Jaywon Koo, Ziyan Yang, Paola Cascante-Bonilla, Baishakhi Ray, Vicente Ordonez",
            "matchScore": 233.37738,
            "original title": "PropTest: Automatic Property Testing for Improved Visual Programming",
            "original authors": "Jaywon Koo, Ziyan Yang, Paola Cascante-Bonilla, Baishakhi Ray, Vicente Ordonez",
            "EMNLP Paper ID": "1745",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Findings"
        },
        {
            "paperId": "3fb1818b9ab3f8d36ed3f8329087058f561c05da",
            "title": "ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline",
            "abstract": "Large language models (LLMs) have shown excellent mastering of human language, but still struggle in real-world applications that require mathematical problem-solving. While many strategies and datasets to enhance LLMs' mathematics are developed, it remains a challenge to simultaneously maintain and improve both language and mathematical capabilities in deployed LLM systems.In this work, we tailor the Self-Critique pipeline, which addresses the challenge in the feedback learning stage of LLM alignment. We first train a general Math-Critique model from the LLM itself to provide feedback signals. Then, we sequentially employ rejective fine-tuning and direct preference optimization over the LLM's own generations for data collection. Based on ChatGLM3-32B, we conduct a series of experiments on both academic and our newly created challenging dataset, MathUserEval. Results show that our pipeline significantly enhances the LLM's mathematical problem-solving while still improving its language ability, outperforming LLMs that could be two times larger. Related techniques have been deployed to ChatGLM\\footnote{\\url{https://chatglm.cn}}, an online serving LLM. Related evaluation dataset and scripts are released at \\url{https://github.com/THUDM/ChatGLM-Math}.",
            "link": "https://www.semanticscholar.org/paper/3fb1818b9ab3f8d36ed3f8329087058f561c05da",
            "authors": "Yifan Xu, Xiao Liu, Xinghan Liu, Zhenyu Hou, Yueyan Li, Xiaohan Zhang, Zihan Wang, Aohan Zeng, Zhengxiao Du, Wenyi Zhao, Jie Tang, Yuxiao Dong",
            "matchScore": 297.71323,
            "original title": "ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline",
            "original authors": "Yifan Xu, Xiao Liu, Xinghan Liu, Zhenyu Hou, Yueyan Li, Xiaohan Zhang, Zihan Wang, Aohan Zeng, Zhengxiao Du, Zhao wenyi, Jie Tang, Yuxiao Dong",
            "EMNLP Paper ID": "2009",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "7bcad1fb8a5358bbe85682c7d393eae271e06b91",
            "title": "VDebugger: Harnessing Execution Feedback for Debugging Visual Programs",
            "abstract": "Visual programs are executable code generated by large language models to address visual reasoning problems. They decompose complex questions into multiple reasoning steps and invoke specialized models for each step to solve the problems. However, these programs are prone to logic errors, with our preliminary evaluation showing that 58% of the total errors are caused by program logic errors. Debugging complex visual programs remains a major bottleneck for visual reasoning. To address this, we introduce VDebugger, a novel critic-refiner framework trained to localize and debug visual programs by tracking execution step by step. VDebugger identifies and corrects program errors leveraging detailed execution feedback, improving interpretability and accuracy. The training data is generated through an automated pipeline that injects errors into correct visual programs using a novel mask-best decoding technique. Evaluations on six datasets demonstrate VDebugger's effectiveness, showing performance improvements of up to 3.2% in downstream task accuracy. Further studies show VDebugger's ability to generalize to unseen tasks, bringing a notable improvement of 2.3% on the unseen COVR task. Code, data and models are made publicly available at https://github.com/shirley-wu/vdebugger/",
            "link": "https://www.semanticscholar.org/paper/7bcad1fb8a5358bbe85682c7d393eae271e06b91",
            "authors": "Xueqing Wu, Zongyu Lin, Songyan Zhao, Te-Lin Wu, Pan Lu, Nanyun Peng, Kai-Wei Chang",
            "matchScore": 276.5009,
            "original title": "VDebugger: Harnessing Execution Feedback for Debugging Visual Programs",
            "original authors": "Xueqing Wu, Zongyu Lin, Songyan Zhao, Te-Lin Wu, Pan Lu, Nanyun Peng, Kai-Wei Chang",
            "EMNLP Paper ID": "2026",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "2fd80ce448b0bf55963a6fd990ea8ef4c0209e6f",
            "title": "Fine-tuning Smaller Language Models for Question Answering over Financial Documents",
            "abstract": "Recent research has shown that smaller language models can acquire substantial reasoning abilities when fine-tuned with reasoning exemplars crafted by a significantly larger teacher model. We explore this paradigm for the financial domain, focusing on the challenge of answering questions that require multi-hop numerical reasoning over financial texts. We assess the performance of several smaller models that have been fine-tuned to generate programs that encode the required financial reasoning and calculations. Our findings demonstrate that these fine-tuned smaller models approach the performance of the teacher model. To provide a granular analysis of model performance, we propose an approach to investigate the specific student model capabilities that are enhanced by fine-tuning. Our empirical analysis indicates that fine-tuning refines the student models ability to express and apply the required financial concepts along with adapting the entity extraction for the specific data format. In addition, we hypothesize and demonstrate that comparable financial reasoning capability can be induced using relatively smaller datasets.",
            "link": "https://www.semanticscholar.org/paper/2fd80ce448b0bf55963a6fd990ea8ef4c0209e6f",
            "authors": "Karmvir Singh Phogat, Sai Akhil Puranam, Sridhar Dasaratha, Chetan Harsha, Shashishekar Ramakrishna",
            "matchScore": 282.85303,
            "original title": "Fine-tuning Smaller Language Models for Question Answering over Financial Documents",
            "original authors": "Karmvir Singh Phogat, Sai Akhil Puranam, Sridhar Dasaratha, Chetan Harsha, Shashishekar Ramakrishna",
            "EMNLP Paper ID": "2141",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Findings"
        },
        {
            "paperId": "bef263083bf5ea965c37b152bc5f0b43aaf74824",
            "title": "Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication",
            "abstract": "Natural language (NL) has long been the predominant format for human cognition and communication, and by extension, has been similarly pivotal in the development and application of Large Language Models (LLMs). Yet, besides NL, LLMs have seen various non-NL formats during pre-training, such as code and logical expression. NL's status as the optimal format for LLMs, particularly in single-LLM reasoning and multi-agent communication, has not been thoroughly examined. In this work, we challenge the default use of NL by exploring the utility of non-NL formats in these contexts. We show that allowing LLMs to autonomously select the most suitable format before reasoning or communicating leads to a 3.3 to 5.7\\% improvement in reasoning efficiency for different LLMs, and up to a 72.7\\% reduction in token usage in multi-agent communication, all while maintaining communicative effectiveness. Our comprehensive analysis further reveals that LLMs can devise a format from limited task instructions and that the devised format is effectively transferable across different LLMs. Intriguingly, the structured communication format decided by LLMs exhibits notable parallels with established agent communication languages, suggesting a natural evolution towards efficient, structured communication in agent communication. Our code is released at \\url{https://github.com/thunlp/AutoForm}.",
            "link": "https://www.semanticscholar.org/paper/bef263083bf5ea965c37b152bc5f0b43aaf74824",
            "authors": "Weize Chen, Chenfei Yuan, Jiarui Yuan, Yusheng Su, Cheng Qian, Cheng Yang, Ruobing Xie, Zhiyuan Liu, Maosong Sun",
            "matchScore": 283.25848,
            "original title": "Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication",
            "original authors": "Weize Chen, Chenfei Yuan, Jiarui Yuan, Yusheng Su, Chen Qian, Cheng Yang, Ruobing Xie, Zhiyuan Liu, Maosong Sun",
            "EMNLP Paper ID": "2149",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "06c8d72a9381829251d4eb99518666694633ab80",
            "title": "MATHWELL: Generating Educational Math Word Problems at Scale",
            "abstract": "Math word problems are critical K-8 educational tools, but writing them is time-consuming and requires domain expertise. We suggest that language models can support K-8 math education by automatically generating problems at scale. To be educational, generated problems must be 1) solvable, 2) accurate, and 3) appropriate. Existing datasets are unlabeled for these criteria, making them ill-suited for training problem generators. We introduce MATHWELL, a Llama-2 (70B) model iteratively finetuned to generate K-8 math word problems using data from expert annotation. Using MATHWELL, we generate the largest English word problem dataset with Program of Thought (PoT) rationales to date, containing 20,490 problems. 3,484 are scored by domain experts who find MATHWELL has a 40% higher share of problems that have executable solutions and meet all criteria than alternatives, with 74% of its problems with executable solutions being solvable, accurate, and appropriate. We release our model, data, and annotations. 1",
            "link": "https://www.semanticscholar.org/paper/06c8d72a9381829251d4eb99518666694633ab80",
            "authors": "Bryan R. Christ, Jonathan Kropko, Thomas Hartvigsen",
            "matchScore": 209.2627,
            "original title": "MATHWELL: Generating Educational Math Word Problems",
            "original authors": "Bryan R Christ, Jonathan Kropko, Thomas Hartvigsen",
            "EMNLP Paper ID": "2347",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "1a6d55fc6525eee6734b029cc4a3a07f240d75f1",
            "title": "Creative Problem Solving in Large Language and Vision Models - What Would it Take?",
            "abstract": "We advocate for a strong integration of Computational Creativity (CC) with research in large language and vision models (LLVMs) to address a key limitation of these models, i.e., creative problem solving. We present preliminary experiments showing how CC principles can be applied to address this limitation. Our goal is to foster discussions on creative problem solving in LLVMs and CC at prestigious ML venues. Our code is available at: https://github.com/lnairGT/creative-problem-solving-LLMs",
            "link": "https://www.semanticscholar.org/paper/1a6d55fc6525eee6734b029cc4a3a07f240d75f1",
            "authors": "Lakshmi Nair, Evana Gizzi, Jivko Sinapov",
            "matchScore": 254.3617,
            "original title": "Position Paper: Creative Problem Solving in Large Language and Vision Models \u2013 What Would it Take?",
            "original authors": "Lakshmi Nair, Evana Gizzi, Jivko Sinapov",
            "EMNLP Paper ID": "2366",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "45ebe6cba6fdd7cfb61f2cc6df3178f65d9146ad",
            "title": "Self-training Language Models for Arithmetic Reasoning",
            "abstract": "Recent language models achieve impressive results in tasks involving complex multistep reasoning, but scaling these capabilities further traditionally requires expensive collection of more annotated data. In this work, we explore the potential of improving models' reasoning capabilities without new data, merely using automated feedback to the validity of their predictions in arithmetic reasoning (self-training). In systematic experimentation across six different arithmetic reasoning datasets, we find that models can substantially improve in both single-round (offline) and online self-training, reaching a correct result in +13.9% and +25.9% more cases, respectively, underlining the importance of actuality of self-training feedback. We further find that in the single-round, offline self-training, traditional supervised training can deliver gains comparable to preference optimization, but in online self-training, preference optimization methods largely outperform supervised training thanks to their superior stability and robustness on unseen types of problems.",
            "link": "https://www.semanticscholar.org/paper/45ebe6cba6fdd7cfb61f2cc6df3178f65d9146ad",
            "authors": "Marek Kadlc\u00edk, Michal \u0160tef\u00e1nik",
            "matchScore": 187.87677,
            "original title": "Self-training Language Models in Arithmetic Reasoning",
            "original authors": "Marek Kadl\u010d\u00edk, Michal \u0160tef\u00e1nik",
            "EMNLP Paper ID": "2418",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "7bb264d333bd7b243a948d6b13c4f457998cb14c",
            "title": "Auto-Evolve: Enhancing Large Language Model's Performance via Self-Reasoning Framework",
            "abstract": "Recent advancements in prompt engineering strategies, such as Chain-of-Thought (CoT) and Self-Discover, have demonstrated significant potential in improving the reasoning abilities of Large Language Models (LLMs). However, these state-of-the-art (SOTA) prompting strategies rely on single or fixed set of static seed reasoning modules like\"think step by step\"or\"break down this problem\"intended to simulate human approach to problem-solving. This constraint limits the flexibility of models in tackling diverse problems effectively. In this paper, we introduce Auto-Evolve, a novel framework that enables LLMs to self-create dynamic reasoning modules and downstream action plan, resulting in significant improvements over current SOTA methods. We evaluate Auto-Evolve on the challenging BigBench-Hard (BBH) dataset with Claude 2.0, Claude 3 Sonnet, Mistral Large, and GPT 4, where it consistently outperforms the SOTA prompt strategies. Auto-Evolve outperforms CoT by up to 10.4% and on an average by 7% across these four models. Our framework introduces two innovations: a) Auto-Evolve dynamically generates reasoning modules for each task while aligning with human reasoning paradigm, thus eliminating the need for predefined templates. b) We introduce an iterative refinement component, that incrementally refines instruction guidance for LLMs and helps boost performance by average 2.8% compared to doing it in a single step.",
            "link": "https://www.semanticscholar.org/paper/7bb264d333bd7b243a948d6b13c4f457998cb14c",
            "authors": "Krishna Aswani, Huilin Lu, Pranav Patankar, Priya Dhalwani, Iris Tan, Jayant Ganeshmohan, Simon Lacasse",
            "matchScore": 299.3763,
            "original title": "Auto-Evolve: Enhancing Large Language Model\u2019s Performance via Self-Reasoning Framework",
            "original authors": "Krishna Aswani, Huilin Lu, Pranav Patankar, Priya Dhalwani, Xue Tan, Jayant Ganeshmohan, Simon Lacasse",
            "EMNLP Paper ID": "2582",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "c49df7a70fd13060353d81e6d1cdc1bbbfd58357",
            "title": "MM-MATH: Advancing Multimodal Math Evaluation with Process Evaluation and Fine-grained Classification",
            "abstract": "To advance the evaluation of multimodal math reasoning in large multimodal models (LMMs), this paper introduces a novel benchmark, MM-MATH. MM-MATH consists of 5,929 open-ended middle school math problems with visual contexts, with fine-grained classification across difficulty, grade level, and knowledge points. Unlike existing benchmarks relying on binary answer comparison, MM-MATH incorporates both outcome and process evaluations. Process evaluation employs LMM-as-a-judge to automatically analyze solution steps, identifying and categorizing errors into specific error types. Extensive evaluation of ten models on MM-MATH reveals significant challenges for existing LMMs, highlighting their limited utilization of visual information and struggles with higher-difficulty problems. The best-performing model achieves only 31% accuracy on MM-MATH, compared to 82% for humans. This highlights the challenging nature of our benchmark for existing models and the significant gap between the multimodal reasoning capabilities of current models and humans. Our process evaluation reveals that diagram misinterpretation is the most common error, accounting for more than half of the total error cases, underscoring the need for improved image comprehension in multimodal reasoning.",
            "link": "https://www.semanticscholar.org/paper/c49df7a70fd13060353d81e6d1cdc1bbbfd58357",
            "authors": "Kai Sun, Yushi Bai, Ji Qi, Lei Hou, Juanzi Li",
            "matchScore": 297.6779,
            "original title": "MM-MATH: Advancing Multimodal Math Evaluation with Process Evaluation and Fine-grained Classification",
            "original authors": "Kai Sun, Yushi Bai, Ji Qi, Lei Hou, Juanzi Li",
            "EMNLP Paper ID": "273",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "49daaadfe84f1bf850feb8d6db46b652f75c9750",
            "title": "Self-Explore: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards",
            "abstract": "Training on large amounts of rationales (i.e., CoT Fine-tuning) is effective at improving the reasoning capabilities of large language models (LLMs). However, acquiring human-authored rationales or augmenting rationales from proprietary models is costly and not scalable. In this paper, we study the problem of whether LLMs could self-improve their reasoning capabilities. To this end, we propose Self-Explore, where the LLM is tasked to explore the first wrong step (i.e., the first pit) within the rationale and use such signals as fine-grained rewards for further improvement. On the GSM8K and MATH test set, Self-Explore achieves 11.57% and 2.89% improvement on average across three LLMs compared to supervised fine-tuning (SFT). Our code is available at https://github.com/hbin0701/Self-Explore.",
            "link": "https://www.semanticscholar.org/paper/49daaadfe84f1bf850feb8d6db46b652f75c9750",
            "authors": "Hyeonbin Hwang, Doyoung Kim, Seungone Kim, Seonghyeon Ye, Minjoon Seo",
            "matchScore": 252.21204,
            "original title": "Self-Explore: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards",
            "original authors": "Hyeonbin Hwang, Doyoung Kim, Seungone Kim, Seonghyeon Ye, Minjoon Seo",
            "EMNLP Paper ID": "289",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "533f5ec8d126408247ad9c9ec5830ed00bfc7501",
            "title": "Large Language Models are In-context Teachers for Knowledge Reasoning",
            "abstract": "In this work, we study in-context teaching (ICT), where a teacher provides in-context example rationales to teach a student to reason over unseen cases. Human teachers are usually required to craft in-context demonstrations, which are costly and have high variance. We ask whether a large language model (LLM) can serve as a more effective in-context teacher for itself or other LLMs, compared to humans. Inspired by the Encoding Specificity Hypothesis from human episodic memory, we hypothesize that in-context exemplars crafted by the teacher should match the training data of the student. This hypothesis motivates us to propose Self-Explain where an LLM's self-elicited explanations are used as in-context demonstrations for prompting it as they are generalized from the model's training examples. Self-Explain is shown to significantly outperform using human-crafted exemplars and other baselines. Furthermore, we reveal that for ICT, rationales from different teacher LLMs or human experts that more resemble the student LLM's self-explanations are better in-context demonstrations. This supports our encoding specificity hypothesis. We then propose Teach-Back that aligns a teacher LLM with the student to enhance the ICT performance. For example, Teach-Back enables a 7B model to teach the much larger GPT-3.5 in context, surpassing human teachers by around 5% in test accuracy on medical question answering.",
            "link": "https://www.semanticscholar.org/paper/533f5ec8d126408247ad9c9ec5830ed00bfc7501",
            "authors": "Jiachen Zhao, Zonghai Yao, Zhichao Yang, Hong Yu",
            "matchScore": 188.8309,
            "original title": "Large Language Models are In-context Teachers for Knowledge Reasoning",
            "original authors": "Jiachen Zhao, Zonghai Yao, Zhichao Yang, hong yu",
            "EMNLP Paper ID": "3165",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "432b119c85f21462784f5be5d3b48b03ec423294",
            "title": "Solving for X and Beyond: Can Large Language Models Solve Complex Math Problems with More-Than-Two Unknowns?",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in solving math problems, a hallmark of human intelligence. Despite high success rates on current benchmarks; however, these often feature simple problems with only one or two unknowns, which do not sufficiently challenge their reasoning capacities. This paper introduces a novel benchmark, BeyondX, designed to address these limitations by incorporating problems with multiple unknowns. Recognizing the challenges in proposing multi-unknown problems from scratch, we developed BeyondX using an innovative automated pipeline that progressively increases complexity by expanding the number of unknowns in simpler problems. Empirical study on BeyondX reveals that the performance of existing LLMs, even those fine-tuned specifically on math tasks, significantly decreases as the number of unknowns increases - with a performance drop of up to 70\\% observed in GPT-4. To tackle these challenges, we propose the Formulate-and-Solve strategy, a generalized prompting approach that effectively handles problems with an arbitrary number of unknowns. Our findings reveal that this strategy not only enhances LLM performance on the BeyondX benchmark but also provides deeper insights into the computational limits of LLMs when faced with more complex mathematical challenges.",
            "link": "https://www.semanticscholar.org/paper/432b119c85f21462784f5be5d3b48b03ec423294",
            "authors": "Kuei-Chun Kao, Ruochen Wang, Cho-Jui Hsieh",
            "matchScore": 336.42462,
            "original title": "Solving for X and Beyond: Can Large Language Models Solve Complex Math Problems with More-Than-Two Unknowns?",
            "original authors": "Kuei-Chun Kao, Ruochen Wang, Cho-Jui Hsieh",
            "EMNLP Paper ID": "3231",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "207f0c123e48b9e980f49e42cbc35711e14fea07",
            "title": "Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models",
            "abstract": "Large language models (LLMs) have demonstrated impressive reasoning capabilities, particularly in textual mathematical problem-solving. However, existing open-source image instruction fine-tuning datasets, containing limited question-answer pairs per image, do not fully exploit visual information to enhance the multimodal mathematical reasoning capabilities of Multimodal LLMs (MLLMs). To bridge this gap, we address the lack of high-quality, diverse multimodal mathematical datasets by collecting 40K high-quality images with question-answer pairs from 24 existing datasets and synthesizing 320K new pairs, creating the MathV360K dataset, which enhances both the breadth and depth of multimodal mathematical questions. We introduce Math-LLaVA, a LLaVA-1.5-based model fine-tuned with MathV360K. This novel approach significantly improves the multimodal mathematical reasoning capabilities of LLaVA-1.5, achieving a 19-point increase and comparable performance to GPT-4V on MathVista's minitest split, and yielding leading performance on Math-V and MathVerse. Furthermore, Math-LLaVA demonstrates enhanced generalizability, showing substantial improvements on the MMMU benchmark. Our research highlights the importance of dataset diversity and synthesis in advancing MLLMs' mathematical reasoning abilities. The code and data are available at: \\url{https://github.com/HZQ950419/Math-LLaVA}.",
            "link": "https://www.semanticscholar.org/paper/207f0c123e48b9e980f49e42cbc35711e14fea07",
            "authors": "Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Li Bing, Roy Ka-wei Lee",
            "matchScore": 278.46677,
            "original title": "Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models",
            "original authors": "WENHAO SHI, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, Roy Ka-Wei Lee",
            "EMNLP Paper ID": "920",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        }
    ],
    "Prompt Engineering and Optimization in Large Language Models (LLMs)": [
        {
            "paperId": "73fa74194e301ec164223fa017300cee4fc62b9c",
            "title": "Prompts have evil twins",
            "abstract": "We discover that many natural-language prompts can be replaced by corresponding prompts that are unintelligible to humans but that provably elicit similar behavior in language models. We call these prompts\"evil twins\"because they are obfuscated and uninterpretable (evil), but at the same time mimic the functionality of the original natural-language prompts (twins). Remarkably, evil twins transfer between models. We find these prompts by solving a maximum-likelihood problem which has applications of independent interest.",
            "link": "https://www.semanticscholar.org/paper/73fa74194e301ec164223fa017300cee4fc62b9c",
            "authors": "Rimon Melamed, Lucas H. McCabe, T. Wakhare, Yejin Kim, H. H. Huang, Enric Boix-Adsera",
            "EMNLP Paper ID": "5",
            "Oral/Poster": "Oral",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "efa45e3772be3665d364a27a8ae022714efbbaa1",
            "title": "GLaPE: Gold Label-agnostic Prompt Evaluation and Optimization for Large Language Model",
            "abstract": "Despite the rapid progress of large language models (LLMs), their task performance remains sensitive to prompt design. Recent studies have explored leveraging the LLM itself as an optimizer to identify optimal prompts that maximize task accuracy. However, when evaluating prompts, such approaches heavily rely on elusive manually annotated gold labels to calculate task accuracy for each candidate prompt, which hinders the widespread implementation and generality. To overcome the limitation, this work proposes a gold label-agnostic prompt evaluation (GLaPE) to alleviate dependence on gold labels. Motivated by the observed correlation between self-consistency and the accuracy of the answer, we adopt self-consistency as the initial evaluation score. Subsequently, we refine the scores of prompts producing identical answers to be mutually consistent. Experimental results show that GLaPE provides reliable evaluations uniform with accuracy, even in the absence of gold labels. Moreover, on six popular reasoning tasks, our GLaPE-based prompt optimization yields effective prompts comparable to accuracy-based ones. The code is publicly available at https://github.com/thunderous77/GLaPE.",
            "link": "https://www.semanticscholar.org/paper/efa45e3772be3665d364a27a8ae022714efbbaa1",
            "authors": "Xuanchang Zhang, Zhuosheng Zhang, Hai Zhao",
            "EMNLP Paper ID": "227",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "6e2ad471261fac30b8089e4f830330057a4290b0",
            "title": "M2PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning",
            "abstract": "Multimodal Large Language Models (MLLMs) demonstrate remarkable performance across a wide range of domains, with increasing emphasis on enhancing their zero-shot generalization capabilities for unseen tasks across various modalities. Instruction tuning has emerged as an effective strategy for achieving zero-shot generalization by finetuning pretrained models on diverse multimodal tasks. As the scale of MLLMs continues to grow, parameter-efficient finetuning becomes increasingly critical. However, most existing parameter-efficient approaches focus only on single modalities and often overlook the multimodal characteristics during finetuning. In this work, we introduce a novel Multimodal Prompt Tuning (M$^2$PT) approach for efficient instruction tuning of MLLMs. M$^2$PT effectively integrates visual and textual prompts into the vision encoder and language processor respectively during finetuning, facilitating the extraction and alignment of features across modalities. Empirical results on various multimodal evaluation datasets demonstrate the superior performance of our approach compared to several state-of-the-art baselines. A comprehensive set of ablation studies validates the effectiveness of our prompt design and the efficiency of our approach.",
            "link": "https://www.semanticscholar.org/paper/6e2ad471261fac30b8089e4f830330057a4290b0",
            "authors": "Taowen Wang, Yiyang Liu, James Liang, Junhan Zhao, Yiming Cui, Yuning Mao, Shaoliang Nie, Jiahao Liu, Fuli Feng, Zenglin Xu, Cheng Han, Lifu Huang, Qifan Wang, Dongfang Liu",
            "EMNLP Paper ID": "419",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "08dc7d05be2375c3bd4699dbc4d4103e688e0024",
            "title": "PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Heuristic-based Sampling",
            "abstract": "Prompt optimization aims to find the best prompt to a large language model (LLM) for a given task. LLMs have been successfully used to help find and improve prompt candidates for single-step tasks. However, realistic tasks for agents are multi-step and introduce new challenges: (1) Prompt content is likely to be more extensive and complex, making it more difficult for LLMs to analyze errors, (2) the impact of an individual step is difficult to evaluate, and (3) different people may have varied preferences about task execution. While humans struggle to optimize prompts, they are good at providing feedback about LLM outputs; we therefore introduce a new LLM-driven discrete prompt optimization framework PRompt Optimization in Multi-Step Tasks (PROMST) that incorporates human-designed feedback rules to automatically offer direct suggestions for improvement. We also use an extra learned heuristic model that predicts prompt performance to efficiently sample from prompt candidates. This approach significantly outperforms both human-engineered prompts and several other prompt optimization methods across 11 representative multi-step tasks (an average 10.6\\%-29.3\\% improvement to current best methods on five LLMs respectively). We believe our work can serve as a benchmark for automatic prompt optimization for LLM-driven multi-step tasks. Datasets and Codes are available at https://github.com/yongchao98/PROMST. Project Page is available at https://yongchao98.github.io/MIT-REALM-PROMST.",
            "link": "https://www.semanticscholar.org/paper/08dc7d05be2375c3bd4699dbc4d4103e688e0024",
            "authors": "Yongchao Chen, Jacob Arkin, Yilun Hao, Yang Zhang, Nicholas Roy, Chuchu Fan",
            "EMNLP Paper ID": "441",
            "Oral/Poster": "Oral",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "ee8918225cc3c558b07cada34ac366a9dc081bdd",
            "title": "PromptReps: Prompting Large Language Models to Generate Dense and Sparse Representations for Zero-Shot Document Retrieval",
            "abstract": "Utilizing large language models (LLMs) for zero-shot document ranking is done in one of two ways: (1) prompt-based re-ranking methods, which require no further training but are only feasible for re-ranking a handful of candidate documents due to computational costs; and (2) unsupervised contrastive trained dense retrieval methods, which can retrieve relevant documents from the entire corpus but require a large amount of paired text data for contrastive training. In this paper, we propose PromptReps, which combines the advantages of both categories: no need for training and the ability to retrieve from the whole corpus. Our method only requires prompts to guide an LLM to generate query and document representations for effective document retrieval. Specifically, we prompt the LLMs to represent a given text using a single word, and then use the last token's hidden states and the corresponding logits associated with the prediction of the next token to construct a hybrid document retrieval system. The retrieval system harnesses both dense text embedding and sparse bag-of-words representations given by the LLM. Our experimental evaluation on the MSMARCO, TREC deep learning and BEIR zero-shot document retrieval datasets illustrates that this simple prompt-based LLM retrieval method can achieve a similar or higher retrieval effectiveness than state-of-the-art LLM embedding methods that are trained with large amounts of unsupervised data, especially when using a larger LLM.",
            "link": "https://www.semanticscholar.org/paper/ee8918225cc3c558b07cada34ac366a9dc081bdd",
            "authors": "Shengyao Zhuang, Xueguang Ma, B. Koopman, Jimmy Lin, G. Zuccon",
            "EMNLP Paper ID": "478",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "2d77b7203824e617206634277bce7eec2b71a2bd",
            "title": "Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement",
            "abstract": "Large language models (LLMs) demonstrate exceptional instruct-following ability to complete various downstream tasks. Although this impressive ability makes LLMs flexible task solvers, their performance in solving tasks also heavily relies on instructions. In this paper, we reveal that LLMs are over-sensitive to lexical variations in task instructions, even when the variations are imperceptible to humans. By providing models with neighborhood instructions, which are closely situated in the latent representation space and differ by only one semantically similar word, the performance on downstream tasks can be vastly different. Following this property, we propose a black-box Combinatorial Optimization framework for Prompt Lexical Enhancement (COPLE). COPLE performs iterative lexical optimization according to the feedback from a batch of proxy tasks, using a search strategy related to word influence. Experiments show that even widely-used human-crafted prompts for current benchmarks suffer from the lexical sensitivity of models, and COPLE recovers the declined model ability in both instruct-following and solving downstream tasks.",
            "link": "https://www.semanticscholar.org/paper/2d77b7203824e617206634277bce7eec2b71a2bd",
            "authors": "Pengwei Zhan, Zhen Xu, Qian Tan, Jie Song, Ru Xie",
            "EMNLP Paper ID": "569",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "e3a2345a4b313384e80f562aac044c8faf375c71",
            "title": "SciPrompt: Knowledge-augmented Prompting for Fine-grained Categorization of Scientific Topics",
            "abstract": "Prompt-based fine-tuning has become an essential method for eliciting information encoded in pre-trained language models for a variety of tasks, including text classification. For multi-class classification tasks, prompt-based fine-tuning under low-resource scenarios has resulted in performance levels comparable to those of fully fine-tuning methods. Previous studies have used crafted prompt templates and verbalizers, mapping from the label terms space to the class space, to solve the classification problem as a masked language modeling task. However, cross-domain and fine-grained prompt-based fine-tuning with an automatically enriched verbalizer remains unexplored, mainly due to the difficulty and costs of manually selecting domain label terms for the verbalizer, which requires humans with domain expertise. To address this challenge, we introduce SciPrompt, a framework designed to automatically retrieve scientific topic-related terms for low-resource text classification tasks. To this end, we select semantically correlated and domain-specific label terms within the context of scientific literature for verbalizer augmentation. Furthermore, we propose a new verbalization strategy that uses correlation scores as additional weights to enhance the prediction performance of the language model during model tuning. Our method outperforms state-of-the-art, prompt-based fine-tuning methods on scientific text classification tasks under few and zero-shot settings, especially in classifying fine-grained and emerging scientific topics.",
            "link": "https://www.semanticscholar.org/paper/e3a2345a4b313384e80f562aac044c8faf375c71",
            "authors": "Zhiwen You, Kanyao Han, Haotian Zhu, Bertram Ludascher, Jana Diesner",
            "EMNLP Paper ID": "681",
            "Oral/Poster": "Oral",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "89efe09229191cd6af41894dca8806d2a5e1282c",
            "title": "Position Engineering: Boosting Large Language Models through Positional Information Manipulation",
            "abstract": "The performance of large language models (LLMs) is significantly influenced by the quality of the prompts provided. In response, researchers have developed enormous prompt engineering strategies aimed at modifying the prompt text to enhance task performance. In this paper, we introduce a novel technique termed position engineering, which offers a more efficient way to guide large language models. Unlike prompt engineering, which requires substantial effort to modify the text provided to LLMs, position engineering merely involves altering the positional information in the prompt without modifying the text itself. We have evaluated position engineering in two widely-used LLM scenarios: retrieval-augmented generation (RAG) and in-context learning (ICL). Our findings show that position engineering substantially improves upon the baseline in both cases. Position engineering thus represents a promising new strategy for exploiting the capabilities of large language models.",
            "link": "https://www.semanticscholar.org/paper/89efe09229191cd6af41894dca8806d2a5e1282c",
            "authors": "Zhiyuan He, Huiqiang Jiang, Zilong Wang, Yuqing Yang, Luna K. Qiu, Lili Qiu",
            "EMNLP Paper ID": "829",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "8544160c4fb07e78a965a8223cff5c361e9df090",
            "title": "Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs",
            "abstract": "Language Model Programs, i.e. sophisticated pipelines of modular language model (LM) calls, are increasingly advancing NLP tasks, but they require crafting prompts that are jointly effective for all modules. We study prompt optimization for LM programs, i.e. how to update these prompts to maximize a downstream metric without access to module-level labels or gradients. To make this tractable, we factorize our problem into optimizing the free-form instructions and few-shot demonstrations of every module and introduce several strategies to craft task-grounded instructions and navigate credit assignment across modules. Our strategies include (i) program- and data-aware techniques for proposing effective instructions, (ii) a stochastic mini-batch evaluation function for learning a surrogate model of our objective, and (iii) a meta-optimization procedure in which we refine how LMs construct proposals over time. Using these insights we develop MIPRO, a novel algorithm for optimizing LM programs. MIPRO outperforms baseline optimizers on five of seven diverse multi-stage LM programs using a best-in-class open-source model (Llama-3-8B), by as high as 13% accuracy. We have released our new optimizers and benchmark in DSPy at http://dspy.ai",
            "link": "https://www.semanticscholar.org/paper/8544160c4fb07e78a965a8223cff5c361e9df090",
            "authors": "Krista Opsahl-Ong, Michael J Ryan, Josh Purtell, David Broman, Christopher Potts, Matei Zaharia, O. Khattab",
            "EMNLP Paper ID": "1054",
            "Oral/Poster": "Oral",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "bf4cc303910ed4917c85c42e97f7380ad9fc0337",
            "title": "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting",
            "abstract": "Pretrained large language models (LLMs) have revolutionized natural language processing (NLP) tasks such as summarization, question answering, and translation. However, LLMs pose significant security risks due to their tendency to memorize training data, leading to potential privacy breaches and copyright infringement. Accurate measurement of this memorization is essential to evaluate and mitigate these potential risks. However, previous attempts to characterize memorization are constrained by either using prefixes only or by prepending a constant soft prompt to the prefixes, which cannot react to changes in input. To address this challenge, we propose a novel method for estimating LLM memorization using dynamic, prefix-dependent soft prompts. Our approach involves training a transformer-based generator to produce soft prompts that adapt to changes in input, thereby enabling more accurate extraction of memorized data. Our method not only addresses the limitations of previous methods but also demonstrates superior performance in diverse experimental settings compared to state-of-the-art techniques. In particular, our method can achieve the maximum relative improvement of 112.75% and 32.26% over the vanilla baseline in terms of discoverable memorization rate for the text generation task and code generation task respectively.",
            "link": "https://www.semanticscholar.org/paper/bf4cc303910ed4917c85c42e97f7380ad9fc0337",
            "authors": "Zhepeng Wang, Runxue Bao, Yawen Wu, Jackson Taylor, Cao Xiao, Feng Zheng, Weiwen Jiang, Shangqian Gao, Yanfu Zhang",
            "EMNLP Paper ID": "1087",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "7d4353661c351c56382cd08c4cf629146574bc86",
            "title": "StablePrompt: Automatic Prompt Tuning using Reinforcement Learning for Large Language Models",
            "abstract": "Finding appropriate prompts for the specific task has become an important issue as the usage of Large Language Models (LLM) has expanded. Reinforcement Learning (RL) is widely used for prompt tuning, but its inherent instability and environmental dependency make it difficult to use in practice. In this paper, we propose StablePrompt, which strikes a balance between training stability and search space, mitigating the instability of RL and producing high-performance prompts. We formulate prompt tuning as an online RL problem between the agent and target LLM and introduce Adaptive Proximal Policy Optimization (APPO). APPO introduces an LLM anchor model to adaptively adjust the rate of policy updates. This allows for flexible prompt search while preserving the linguistic ability of the pre-trained LLM. StablePrompt outperforms previous methods on various tasks including text classification, question answering, and text generation. Our code can be found in github.",
            "link": "https://www.semanticscholar.org/paper/7d4353661c351c56382cd08c4cf629146574bc86",
            "authors": "Minchan Kwon, Gaeun Kim, Jongsuk Kim, Haeil Lee, Junmo Kim",
            "EMNLP Paper ID": "1100",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "555401ddacee23b8d26ea9754bd71cfb331bddf7",
            "title": "Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together",
            "abstract": "Natural Language Processing (NLP) systems are increasingly taking the form of sophisticated modular pipelines, e.g., Retrieval Augmented Generation (RAG), where each module may involve a distinct Language Model (LM) and an associated prompt template. These compound systems often lack intermediate labels or gradient flow to optimize each module, making their end-to-end optimization challenging. Here we seek strategies to optimize both the module-level LM weights and the associated prompt templates of such systems to maximize a downstream task metric. We propose for the first time combining the weight and prompt optimization strategies to optimize a modular LM pipeline by alternating between the two to get the same LM to teach itself. In experiments with multi-hop QA, mathematical reasoning, and feature-based classification using mistral-7b, llama-2-7b, and llama-3-8b, these BetterTogether strategies optimizing the weights and prompts of a pipeline together outperform directly optimizing weights alone and prompts alone by up to 60% and 6%, respectively, on average across LMs and tasks. BetterTogether optimizer is released in DSPy at http://dspy.ai",
            "link": "https://www.semanticscholar.org/paper/555401ddacee23b8d26ea9754bd71cfb331bddf7",
            "authors": "Dilara Soylu, Christopher Potts, O. Khattab",
            "EMNLP Paper ID": "1210",
            "Oral/Poster": "Oral",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "330e5e6283c0ac4c814046e0e5af27df2b4611cd",
            "title": "Paraphrase Types Elicit Prompt Engineering Capabilities",
            "abstract": "Much of the success of modern language models depends on finding a suitable prompt to instruct the model. Until now, it has been largely unknown how variations in the linguistic expression of prompts affect these models. This study systematically and empirically evaluates which linguistic features influence models through paraphrase types, i.e., different linguistic changes at particular positions. We measure behavioral changes for five models across 120 tasks and six families of paraphrases (i.e., morphology, syntax, lexicon, lexico-syntax, discourse, and others). We also control for other prompt engineering factors (e.g., prompt length, lexical diversity, and proximity to training data). Our results show a potential for language models to improve tasks when their prompts are adapted in specific paraphrase types (e.g., 6.7% median gain in Mixtral 8x7B; 5.5% in LLaMA 3 8B). In particular, changes in morphology and lexicon, i.e., the vocabulary used, showed promise in improving prompts. These findings contribute to developing more robust language models capable of handling variability in linguistic expression.",
            "link": "https://www.semanticscholar.org/paper/330e5e6283c0ac4c814046e0e5af27df2b4611cd",
            "authors": "Jan Philip Wahle, Terry Ruas, Yang Xu, Bela Gipp",
            "EMNLP Paper ID": "1257",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "915ef7399ab65071e9141b9bf89f9189f3cb0e91",
            "title": "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation",
            "abstract": "Large language models (LLMs) have revolutionized the field of NLP. Notably, their in-context learning capabilities also enable their use as evaluation metrics for natural language generation, making them particularly advantageous in low-resource scenarios and time-restricted applications. In this work, we introduce PrExMe, a large-scale prompt exploration for metrics, where we evaluate more than 720 prompt templates for open-source LLM-based metrics on machine translation (MT) and summarization datasets, totalling over 6.6M evaluations. This extensive comparison (1) serves as a benchmark of the performance of recent open-source LLMs as metrics and (2) explores the stability and variability of different prompting strategies. We discover that, on the one hand, there are scenarios for which prompts are stable. For instance, some LLMs show idiosyncratic preferences and favor to grade generated texts with textual labels while others prefer to return numeric scores. On the other hand, the stability of prompts and model rankings can be susceptible to seemingly innocuous changes. For example, changing the requested output format from\"0 to 100\"to\"-1 to +1\"can strongly affect the rankings in our evaluation. Our study contributes to understanding the impact of different prompting approaches on LLM-based metrics for MT and summarization evaluation, highlighting the most stable prompting patterns and potential limitations.",
            "link": "https://www.semanticscholar.org/paper/915ef7399ab65071e9141b9bf89f9189f3cb0e91",
            "authors": "Christoph Leiter, Steffen Eger",
            "EMNLP Paper ID": "1338",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "1610e78380da2890a081892ac52a9c57f2b8e01f",
            "title": "A SMART Mnemonic Sounds like \"Glue Tonic\": Mixing LLMs with Student Feedback to Make Mnemonic Learning Stick",
            "abstract": "Keyword mnemonics are memorable explanations that link new terms to simpler keywords. Prior work generates mnemonics for students, but they do not train models using mnemonics students prefer and aid learning. We build SMART, a mnemonic generator trained on feedback from real students learning new terms. To train SMART, we first fine-tune LLaMA-2 on a curated set of user-written mnemonics. We then use LLM alignment to enhance SMART: we deploy mnemonics generated by SMART in a flashcard app to find preferences on mnemonics students favor. We gather 2684 preferences from 45 students across two types: expressed (inferred from ratings) and observed (inferred from student learning), yielding three key findings. First, expressed and observed preferences disagree; what students think is helpful does not always capture what is truly helpful. Second, Bayesian models can synthesize complementary data from multiple preference types into a single effectiveness signal. SMART is tuned via Direct Preference Optimization on this signal, which resolves ties and missing labels in the typical method of pairwise comparisons, augmenting data for LLM output quality gains. Third, mnemonic experts assess SMART as matching GPT-4 at much lower deployment costs, showing the utility of capturing diverse student feedback to align LLMs in education.",
            "link": "https://www.semanticscholar.org/paper/1610e78380da2890a081892ac52a9c57f2b8e01f",
            "authors": "Nishant Balepur, Matthew Shu, Alexander Hoyle, Alison Robey, Shi Feng, Seraphina Goldfarb-Tarrant, Jordan L. Boyd-Graber",
            "EMNLP Paper ID": "1638",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "c4c16df426e0a4b1b501406edd193ae04ac0767e",
            "title": "Extracting Prompts by Inverting LLM Outputs",
            "abstract": "We consider the problem of language model inversion: given outputs of a language model, we seek to extract the prompt that generated these outputs. We develop a new black-box method, output2prompt, that learns to extract prompts without access to the model's logits and without adversarial or jailbreaking queries. In contrast to previous work, output2prompt only needs outputs of normal user queries. To improve memory efficiency, output2prompt employs a new sparse encoding techique. We measure the efficacy of output2prompt on a variety of user and system prompts and demonstrate zero-shot transferability across different LLMs.",
            "link": "https://www.semanticscholar.org/paper/c4c16df426e0a4b1b501406edd193ae04ac0767e",
            "authors": "Collin Zhang, John X. Morris, Vitaly Shmatikov",
            "EMNLP Paper ID": "1699",
            "Oral/Poster": "Oral",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "d45577c9aff2b69f0b5cb3958c49014c1e6c6b7e",
            "title": "PALM: Few-Shot Prompt Learning for Audio Language Models",
            "abstract": "Audio-Language Models (ALMs) have recently achieved remarkable success in zero-shot audio recognition tasks, which match features of audio waveforms with class-specific text prompt features, inspired by advancements in Vision-Language Models (VLMs). Given the sensitivity of zero-shot performance to the choice of hand-crafted text prompts, many prompt learning techniques have been developed for VLMs. We explore the efficacy of these approaches in ALMs and propose a novel method, Prompt Learning in Audio Language Models (PALM), which optimizes the feature space of the text encoder branch. Unlike existing methods that work in the input space, our approach results in greater training efficiency. We demonstrate the effectiveness of our approach on 11 audio recognition datasets, encompassing a variety of speech-processing tasks, and compare the results with three baselines in a few-shot learning setup. Our method is either on par with or outperforms other approaches while being computationally less demanding. Code is available at https://asif-hanif.github.io/palm/",
            "link": "https://www.semanticscholar.org/paper/d45577c9aff2b69f0b5cb3958c49014c1e6c6b7e",
            "authors": "Asif Hanif, M. Agro, Mohammad Areeb Qazi, Hanan Aldarmaki",
            "EMNLP Paper ID": "2307",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "7b0fd65d2f77f6501fb9fd723c3bbe42b62d2276",
            "title": "DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators",
            "abstract": "Generally, the decoder-only large language models (LLMs) are adapted to context-aware neural machine translation (NMT) in a concatenating way, where LLMs take the concatenation of the source sentence (i.e., intra-sentence context) and the inter-sentence context as the input, and then to generate the target tokens sequentially. This adaptation strategy, i.e., concatenation mode, considers intra-sentence and inter-sentence contexts with the same priority, despite an apparent difference between the two kinds of contexts. In this paper, we propose an alternative adaptation approach, named Decoding-enhanced Multi-phase Prompt Tuning (DeMPT), to make LLMs discriminately model and utilize the inter- and intra-sentence context and more effectively adapt LLMs to context-aware NMT. First, DeMPT divides the context-aware NMT process into three separate phases. During each phase, different continuous prompts are introduced to make LLMs discriminately model various information. Second, DeMPT employs a heuristic way to further discriminately enhance the utilization of the source-side inter- and intra-sentence information at the final decoding phase. Experiments show that our approach significantly outperforms the concatenation method, and further improves the performance of LLMs in discourse modeling.",
            "link": "https://www.semanticscholar.org/paper/7b0fd65d2f77f6501fb9fd723c3bbe42b62d2276",
            "authors": "Xinglin Lyu, Junhui Li, Yanqing Zhao, Min Zhang, Daimeng Wei, Shimin Tao, Hao Yang, Min Zhang",
            "EMNLP Paper ID": "2649",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "7ac673e51c89a3658496bb0e22572f58650f23d5",
            "title": "Improving Minimum Bayes Risk Decoding with Multi-Prompt",
            "abstract": "While instruction fine-tuned LLMs are effective text generators, sensitivity to prompt construction makes performance unstable and sub-optimal in practice. Relying on a single\"best\"prompt cannot capture all differing approaches to a generation problem. Using this observation, we propose multi-prompt decoding, where many candidate generations are decoded from a prompt bank at inference-time. To ensemble candidates, we use Minimum Bayes Risk (MBR) decoding, which selects a final output using a trained value metric. We show multi-prompt improves MBR across a comprehensive set of conditional generation tasks, and show this is a result of estimating a more diverse and higher quality candidate space than that of a single prompt. Further experiments confirm multi-prompt improves generation across tasks, models and metrics.",
            "link": "https://www.semanticscholar.org/paper/7ac673e51c89a3658496bb0e22572f58650f23d5",
            "authors": "David Heineman, Yao Dou, Wei Xu",
            "EMNLP Paper ID": "3249",
            "Oral/Poster": "Oral",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "f889ab4903aaac6c485a505963bcd4c1b4cdf584",
            "title": "Instruction Fine-Tuning: Does Prompt Loss Matter?",
            "abstract": "We present a novel study analyzing the effects of various prompt loss token weights (PLW) for supervised instruction fine-tuning (SIFT). While prompt-masking (PLW = 0) is common for SIFT, some fine-tuning APIs support fractional PLWs and suggest that using a small non-zero PLW can help stabilize learning when fine-tuning on short-completion data. However, there has never been a study confirming this claim, and OpenAI, a major cloud-based SIFT provider, recently removed this parameter from their fine-tuning API. We found that performance of models fine-tuned on short-completion data had a statistically-significant negative quadratic relationship with PLW. Using small values (0.01 - 0.5) of PLW produced better results on multiple-choice and short-generation benchmarks (outperforming models fine-tuned on long-completion data) while large values (~ 1.0) of PLW produced better results on long-generation benchmarks. We explained this effect and verified its importance through additional experiments. This research serves as a warning to API providers about the importance of providing a PLW parameter for SIFT.",
            "link": "https://www.semanticscholar.org/paper/f889ab4903aaac6c485a505963bcd4c1b4cdf584",
            "authors": "Mathew Huerta-Enochian",
            "EMNLP Paper ID": "3302",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "76bc7cb435a1b3de79a4f3da7b06b275329baa45",
            "title": "Exploring Automated Keyword Mnemonics Generation with Large Language Models via Overgenerate-and-Rank",
            "abstract": "In this paper, we study an under-explored area of language and vocabulary learning: keyword mnemonics, a technique for memorizing vocabulary through memorable associations with a target word via a verbal cue. Typically, creating verbal cues requires extensive human effort and is quite time-consuming, necessitating an automated method that is more scalable. We propose a novel overgenerate-and-rank method via prompting large language models (LLMs) to generate verbal cues and then ranking them according to psycholinguistic measures and takeaways from a pilot user study. To assess cue quality, we conduct both an automated evaluation of imageability and coherence, as well as a human evaluation involving English teachers and learners. Results show that LLM-generated mnemonics are comparable to human-generated ones in terms of imageability, coherence, and perceived usefulness, but there remains plenty of room for improvement due to the diversity in background and preference among language learners.",
            "link": "https://www.semanticscholar.org/paper/76bc7cb435a1b3de79a4f3da7b06b275329baa45",
            "authors": "Jaewook Lee, Hunter McNichols, Andrew Lan",
            "matchScore": 301.77786,
            "original title": "Exploring Automated Keyword Mnemonics Generation with Large Language Models via Overgenerate-and-Rank",
            "original authors": "Jaewook Lee, Hunter McNichols, Andrew Lan",
            "EMNLP Paper ID": "1107",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "96f5ce59e3dd54fa508e13a499de7fd7b633b022",
            "title": "Symbolic Prompt Program Search: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization",
            "abstract": "In many modern LLM applications, such as retrieval augmented generation, prompts have become programs themselves. In these settings, prompt programs are repeatedly called with different user queries or data instances. A big practical challenge is optimizing such prompt programs. Recent work has mostly focused on either simple prompt programs or assumed that the general structure of a prompt program is fixed. We introduce SAMMO, a framework to perform symbolic prompt program search for compile-time optimizations of prompt programs. SAMMO represents prompt programs on a symbolic level which allows for a rich set of transformations that can be searched over during optimization. We show that SAMMO generalizes previous methods and improves the performance of complex prompts on (1) instruction tuning, (2) RAG pipeline tuning, and (3) prompt compression, across several different LLMs. We make all code available open-source at https://github.com/microsoft/sammo .",
            "link": "https://www.semanticscholar.org/paper/96f5ce59e3dd54fa508e13a499de7fd7b633b022",
            "authors": "Tobias Schnabel, Jennifer Neville",
            "matchScore": 323.32047,
            "original title": "Symbolic Prompt Program Search: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization",
            "original authors": "Tobias Schnabel, Jennifer Neville",
            "EMNLP Paper ID": "121",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "e2f8864c3e40298513ca320de0012818ce092bea",
            "title": "PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning",
            "abstract": "Recent advancements in large language models (LLMs) have raised concerns about inference costs, increasing the need for research into model compression. While knowledge distillation (KD) is a prominent method for this, research on KD for generative language models like LLMs is relatively sparse, and the approach of distilling student-friendly knowledge, which has shown promising performance in KD for classification models, remains unexplored in generative language models. To explore this approach, we propose PromptKD, a simple yet effective method that utilizes prompt tuning - for the first time in KD - to enable generative language models to transfer student-friendly knowledge. Unlike previous works in classification that require fine-tuning the entire teacher model for extracting student-friendly knowledge, PromptKD achieves similar effects by adding a small number of prompt tokens and tuning only the prompt with student guidance. Extensive experiments on instruction-following datasets show that PromptKD achieves state-of-the-art performance while adding only 0.0007% of the teacher's parameters as prompts. Further analysis suggests that distilling student-friendly knowledge alleviates exposure bias effectively throughout the entire training process, leading to performance enhancements.",
            "link": "https://www.semanticscholar.org/paper/e2f8864c3e40298513ca320de0012818ce092bea",
            "authors": "Gyeongman Kim, Doohyuk Jang, Eunho Yang",
            "matchScore": 287.59705,
            "original title": "PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning",
            "original authors": "Gyeongman Kim, Doohyuk Jang, Eunho Yang",
            "EMNLP Paper ID": "1285",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "bfa88a06c137b8152c72574ac89f7f3d2719bb29",
            "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
            "abstract": "In spite of the outstanding performance, Neural Architecture Search (NAS) is criticized for massive computation. Recently, Zero-shot NAS has emerged as a promising approach by exploiting Zero-cost (ZC) proxies, which markedly reduce computational demands. Despite this, existing ZC proxies heavily rely on expert knowledge and incur significant trial-and-error costs. Particularly in NLP tasks, most existing ZC proxies fail to surpass the performance of the naive baseline. To address these challenges, we introduce a novel framework, \\textbf{LPZero}, which is the first to automatically design ZC proxies for various tasks, achieving higher ranking consistency than human-designed proxies. Specifically, we model the ZC proxy as a symbolic equation and incorporate a unified proxy search space that encompasses existing ZC proxies, which are composed of a predefined set of mathematical symbols. To heuristically search for the best ZC proxy, LPZero incorporates genetic programming to find the optimal symbolic composition. We propose a \\textit{Rule-based Pruning Strategy (RPS),} which preemptively eliminates unpromising proxies, thereby mitigating the risk of proxy degradation. Extensive experiments on FlexiBERT, GPT-2, and LLaMA-7B demonstrate LPZero's superior ranking ability and performance on downstream tasks compared to current approaches.",
            "link": "https://www.semanticscholar.org/paper/bfa88a06c137b8152c72574ac89f7f3d2719bb29",
            "authors": "Peijie Dong, Lujun Li, Xiang Liu, Zhenheng Tang, Xuebo Liu, Qiang Wang, Xiaowen Chu",
            "matchScore": 285.7602,
            "original title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
            "original authors": "Peijie Dong, Lujun Li, Xiang Liu, Zhenheng Tang, Xuebo Liu, Qiang Wang, Xiaowen Chu",
            "EMNLP Paper ID": "1803",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "c57f7dae9cbae0c295be4b8ab5e696906bd4f2ef",
            "title": "StablePT: Towards Stable Prompting for Few-shot Learning via Input Separation",
            "abstract": "Large language models have shown their ability to become effective few-shot learners with prompting, revolutionizing the paradigm of learning with data scarcity. However, this approach largely depends on the quality of prompt initialization, and always exhibits large variability among different runs. Such property makes prompt tuning highly unreliable and vulnerable to poorly constructed prompts, which limits its extension to more real-world applications. To tackle this issue, we propose to treat the hard prompt and soft prompt as separate inputs to mitigate noise brought by the prompt initialization. Furthermore, we optimize soft prompts with contrastive learning for utilizing class-aware information in the training process to maintain model performance. Experimental results demonstrate that \\sysname outperforms state-of-the-art methods by 6.97% in accuracy and reduces the standard deviation by 1.92 on average. Furthermore, extensive experiments underscore its robustness and stability across 8 datasets covering various tasks. Codes are available at https://github.com/lccc0528/Stable/tree/main.",
            "link": "https://www.semanticscholar.org/paper/c57f7dae9cbae0c295be4b8ab5e696906bd4f2ef",
            "authors": "Xiaoming Liu, Chen Liu, Zhaohan Zhang, Chengzhengxu Li, Longtian Wang, Y. Lan, Chao Shen",
            "matchScore": 321.02112,
            "original title": "StablePT : Towards Stable Prompting for Few-shot Learning via Input Separation",
            "original authors": "Xiaoming Liu, Chen Liu, Zhaohan Zhang, Chengzhengxu Li, Longtian Wang, Yu Lan, Chao Shen",
            "EMNLP Paper ID": "1930",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "cf51da61a045aa5fe6382b265cf913975cd00dd3",
            "title": "Monotonic Paraphrasing Improves Generalization of Language Model Prompting",
            "abstract": "Performance of large language models (LLMs) may vary with different prompts or instructions of even the same task. One commonly recognized factor for this phenomenon is the model's familiarity with the given prompt or instruction, which is typically estimated by its perplexity. However, finding the prompt with the lowest perplexity is challenging, given the enormous space of possible prompting phrases. In this paper, we propose monotonic paraphrasing (MonoPara), an end-to-end decoding strategy that paraphrases given prompts or instructions into their lower perplexity counterparts based on an ensemble of a paraphrase LM for prompt (or instruction) rewriting, and a target LM (i.e. the prompt or instruction executor) that constrains the generation for lower perplexity. The ensemble decoding process can efficiently paraphrase the original prompt without altering its semantic meaning, while monotonically decreasing the perplexity of each generation as calculated by the target LM. We explore in detail both greedy and search-based decoding as two alternative decoding schemes of MonoPara. Notably, MonoPara does not require any training and can monotonically lower the perplexity of the paraphrased prompt or instruction, leading to improved performance of zero-shot LM prompting as evaluated on a wide selection of tasks. In addition, MonoPara is also shown to effectively improve LMs' generalization on perturbed and unseen task instructions.",
            "link": "https://www.semanticscholar.org/paper/cf51da61a045aa5fe6382b265cf913975cd00dd3",
            "authors": "Qin Liu, Fei Wang, Nan Xu, Tianyi Yan, Tao Meng, Muhao Chen",
            "matchScore": 244.54428,
            "original title": "Monotonic Paraphrasing Improves Generalization of Language Model Prompting",
            "original authors": "Qin Liu, Fei Wang, Nan Xu, Tianyi Lorena Yan, Tao Meng, Muhao Chen",
            "EMNLP Paper ID": "2027",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "564bffb42edb6a24be8b144f22eec97e0579028b",
            "title": "MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning for Discrete Prompt Optimization",
            "abstract": "RL-based techniques can be employed to search for prompts that, when fed into a target language model, maximize a set of user-specified reward functions. However, in many target applications, the natural reward functions are in tension with one another -- for example, content preservation vs. style matching in style transfer tasks. Current techniques focus on maximizing the average of reward functions, which does not necessarily lead to prompts that achieve balance across rewards -- an issue that has been well-studied in the multi-objective and robust optimization literature. In this paper, we conduct an empirical comparison of several existing multi-objective optimization techniques adapted to this new setting: RL-based discrete prompt optimization. We compare two methods optimizing the volume of the Pareto reward surface and one method that chooses an update direction that benefits all rewards simultaneously. We evaluate performance on two NLP tasks: style transfer and machine translation, each using three competing reward functions. Our experiments demonstrate that multi-objective methods that directly optimize the volume of the Pareto reward surface perform better and achieve a better balance of all rewards than those that attempt to find monotonic update directions.",
            "link": "https://www.semanticscholar.org/paper/564bffb42edb6a24be8b144f22eec97e0579028b",
            "authors": "Yasaman Jafari, Dheeraj Mekala, Rose Yu, Taylor Berg-Kirkpatrick",
            "matchScore": 276.6281,
            "original title": "MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning for Discrete Prompt Optimization",
            "original authors": "Yasaman Jafari, Dheeraj Mekala, Rose Yu, Taylor Berg-Kirkpatrick",
            "EMNLP Paper ID": "2028",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "ac7bab4962fca18ae1e62234b0275b9c19e77a44",
            "title": "StraGo: Harnessing Strategic Guidance for Prompt Optimization",
            "abstract": "Prompt engineering is pivotal for harnessing the capabilities of large language models (LLMs) across diverse applications. While existing prompt optimization methods improve prompt effectiveness, they often lead to prompt drifting, where newly generated prompts can adversely impact previously successful cases while addressing failures. Furthermore, these methods tend to rely heavily on LLMs' intrinsic capabilities for prompt optimization tasks. In this paper, we introduce StraGo (Strategic-Guided Optimization), a novel approach designed to mitigate prompt drifting by leveraging insights from both successful and failed cases to identify critical factors for achieving optimization objectives. StraGo employs a how-to-do methodology, integrating in-context learning to formulate specific, actionable strategies that provide detailed, step-by-step guidance for prompt optimization. Extensive experiments conducted across a range of tasks, including reasoning, natural language understanding, domain-specific knowledge, and industrial applications, demonstrate StraGo's superior performance. It establishes a new state-of-the-art in prompt optimization, showcasing its ability to deliver stable and effective prompt improvements.",
            "link": "https://www.semanticscholar.org/paper/ac7bab4962fca18ae1e62234b0275b9c19e77a44",
            "authors": "Yurong Wu, Yan Gao, Bin Benjamin Zhu, Zineng Zhou, Xiaodi Sun, Sheng Yang, Jian-Guang Lou, Zhiming Ding, Linjun Yang",
            "matchScore": 238.5051,
            "original title": "StraGo: Harnessing Strategic Guidance for Prompt Optimization",
            "original authors": "Yurong Wu, Yan Gao, Bin Benjamin Zhu, Zineng Zhou, Xiaodi Sun, Sheng Yang, Jian-Guang Lou, Zhiming Ding, Linjun Yang",
            "EMNLP Paper ID": "2066",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "e7e35bf7e359535b75344e9ba7fb9c6224ffd77b",
            "title": "PromptIntern: Saving Inference Costs by Internalizing Recurrent Prompt during Large Language Model Fine-tuning",
            "abstract": "Recent advances in fine-tuning large language models (LLMs) have greatly enhanced their usage in domain-specific tasks. Despite the success, fine-tuning continues to rely on repeated and lengthy prompts, which escalate computational expenses, require more resources, and lead to slower inference. In this paper, we present a novel approach, PromptIntern, which internalizes prompt knowledge during model fine-tuning to achieve efficient inference and save costs. Instead of compressing the prompts for a vanilla model, PromptIntern aims to embed the recurrent prompt directly into the model parameters. We design a fine-tuning pipeline that includes instruction template compression, few-shot example absorption, and a progressive internalization strategy, effectively diminishing the need for intricate prompts during inference. Comprehensive experiments on challenging NL2Code tasks demonstrate that our method reduces input tokens by more than 90%, accelerates inference by 4.2 times, and reduces monetary inference costs by 88.3%.",
            "link": "https://www.semanticscholar.org/paper/e7e35bf7e359535b75344e9ba7fb9c6224ffd77b",
            "authors": "Jiaru Zou, Meng Zhou, Tao Li, Shi Han, Dongmei Zhang",
            "matchScore": 351.0887,
            "original title": "PromptIntern: Saving Inference Costs by Internalizing Recurrent Prompt during Large Language Model Fine-tuning",
            "original authors": "Jiaru Zou, Mengyu Zhou, Tao Li, Shi Han, Dongmei Zhang",
            "EMNLP Paper ID": "2098",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "3f07bacdf8f345d9eeb320bb089fed4d61b4cec7",
            "title": "Selection-p: Self-Supervised Task-Agnostic Prompt Compression for Faithfulness and Transferability",
            "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in a wide range of natural language processing tasks when leveraging in-context learning. To mitigate the additional computational and financial costs associated with in-context learning, several prompt compression methods have been proposed to compress the in-context learning prompts. Despite their success, these methods face challenges with transferability due to model-specific compression, or rely on external training data, such as GPT-4. In this paper, we investigate the ability of LLMs to develop a unified compression method that discretizes uninformative tokens, utilizing a self-supervised pre-training technique. By introducing a small number of parameters during the continual pre-training, the proposed Selection-p produces a probability for each input token, indicating whether to preserve or discard it. Experiments show Selection-p achieves state-of-the-art performance across numerous classification tasks, achieving compression rates of up to 10 times while experiencing only a marginal 0.8% decrease in performance. Moreover, it exhibits superior transferability to different models compared to prior work. Additionally, we further analyze how Selection-p helps maintain performance on in-context learning with long contexts.",
            "link": "https://www.semanticscholar.org/paper/3f07bacdf8f345d9eeb320bb089fed4d61b4cec7",
            "authors": "Tsz Ting Chung, Leyang Cui, Lemao Liu, Xinting Huang, Shuming Shi, D. Yeung",
            "matchScore": 302.09265,
            "original title": "Selection-p: Self-Supervised Task-Agnostic Prompt Compression for Faithfulness and Transferability",
            "original authors": "Tsz Ting Chung, Leyang Cui, Lemao Liu, Xinting Huang, Shuming Shi, Dit-Yan Yeung",
            "EMNLP Paper ID": "2201",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "2ab9e5fa15dd79090f5d9bdce643d8fa83f9c1d1",
            "title": "FastMem: Fast Memorization of Prompt Improves Context Awareness of Large Language Models",
            "abstract": "Large language models (LLMs) excel in generating coherent text, but they often struggle with context awareness, leading to inaccuracies in tasks requiring faithful adherence to provided information. We introduce FastMem, a novel method designed to enhance instruction fine-tuned LLMs' context awareness through fast memorization of the prompt. FastMem maximizes the likelihood of the prompt before inference by updating only the last Feed-Forward Network (FFN) module. This targeted approach ensures efficient optimization without overfitting, significantly improving the model's ability to comprehend and accurately follow the context. Our experiments demonstrate substantial gains in reading comprehension, text summarization and adherence to output structures. For instance, FastMem improves the accuracy of Llama 3-8B-Inst on the NQ-SWAP dataset from 59.1% to 71.6%, and reduces the output structure failure rate of Qwen 1.5-4B-Chat from 34.9% to 25.5%. Extensive experimental results highlight FastMem's potential to offer a robust solution to enhance the reliability and accuracy of LLMs in various applications. Our code is available at: https://github.com/IAAR-Shanghai/FastMem",
            "link": "https://www.semanticscholar.org/paper/2ab9e5fa15dd79090f5d9bdce643d8fa83f9c1d1",
            "authors": "Junyi Zhu, Shuochen Liu, Yu Yu, Bo Tang, Yibo Yan, Zhiyu Li, Feiyu Xiong, Tong Xu, Matthew B. Blaschko",
            "matchScore": 272.65942,
            "original title": "FastMem: Fast Memorization of Prompt Improves Context Awareness of Large Language Models",
            "original authors": "Junyi Zhu, Shuochen Liu, Yu Yu, Bo Tang, Yibo Yan, Zhiyu li, Feiyu Xiong, Tong Xu, Matthew B. Blaschko",
            "EMNLP Paper ID": "2301",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "6f28e3cf3c0a532dd01afc1e74ab69ecab92fe75",
            "title": "Dual-Phase Accelerated Prompt Optimization",
            "abstract": "Gradient-free prompt optimization methods have made significant strides in enhancing the performance of closed-source Large Language Models (LLMs) across a wide range of tasks. However, existing approaches make light of the importance of high-quality prompt initialization and the identification of effective optimization directions, thus resulting in substantial optimization steps to obtain satisfactory performance. In this light, we aim to accelerate prompt optimization process to tackle the challenge of low convergence rate. We propose a dual-phase approach which starts with generating high-quality initial prompts by adopting a well-designed meta-instruction to delve into task-specific information, and iteratively optimize the prompts at the sentence level, leveraging previous tuning experience to expand prompt candidates and accept effective ones. Extensive experiments on eight datasets demonstrate the effectiveness of our proposed method, achieving a consistent accuracy gain over baselines with less than five optimization steps.",
            "link": "https://www.semanticscholar.org/paper/6f28e3cf3c0a532dd01afc1e74ab69ecab92fe75",
            "authors": "Muchen Yang, Moxin Li, Yongle Li, Zijun Chen, Chongming Gao, Junqi Zhang, Yangyang Li, Fuli Feng",
            "matchScore": 185.5464,
            "original title": "Dual-Phase Accelerated Prompt Optimization",
            "original authors": "Muchen Yang, Moxin Li, Yongle Li, Zijun Chen, Chongming Gao, Junqi Zhang, Yangyang Li, Fuli Feng",
            "EMNLP Paper ID": "2388",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Findings"
        },
        {
            "paperId": "8827b7b7e6dbba853c7e647fd06aa92e42b5f273",
            "title": "Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models",
            "abstract": "Prompt-based learning is susceptible to intrinsic bias present in pre-trained language models (LMs), leading to sub-optimal performance in prompt-based zero/few-shot settings. In this work, we propose a null-input prompting method to calibrate intrinsic bias encoded in pre-trained LMs. Different from prior efforts that address intrinsic bias primarily for social fairness and often involve excessive computational cost, our objective is to explore enhancing LMs' performance in downstream zero/few-shot learning while emphasizing the efficiency of intrinsic bias calibration. Specifically, we leverage a diverse set of auto-selected null-meaning inputs generated from GPT-4 to probe intrinsic bias of pre-trained LMs. Utilizing the bias-reflected probability distribution, we formulate a distribution disparity loss for bias calibration, where we exclusively update bias parameters ($0.1\\%$ of total parameters) of LMs towards equal probability distribution. Experimental results show that the calibration promotes an equitable starting point for LMs while preserving language modeling abilities. Across a wide range of datasets, including sentiment analysis and topic classification, our method significantly improves zero/few-shot learning performance of LMs for both in-context learning and prompt-based fine-tuning (on average $9\\%$ and $2\\%$, respectively).",
            "link": "https://www.semanticscholar.org/paper/8827b7b7e6dbba853c7e647fd06aa92e42b5f273",
            "authors": "Kang He, Yinghan Long, Kaushik Roy",
            "matchScore": 272.50146,
            "original title": "Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models",
            "original authors": "Kang He, Yinghan Long, Kaushik Roy",
            "EMNLP Paper ID": "2482",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "447bbdbeb5dfa9252b51a833eafe5e8f4d3b632e",
            "title": "Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models",
            "abstract": "We investigate how to elicit compositional generalization capabilities in large language models (LLMs). Compositional generalization empowers LLMs to solve complex problems by combining foundational skills, a critical reasoning ability akin to human intelligence. However, even the most advanced LLMs currently struggle with this form of reasoning. We examine this problem within the framework of in-context learning and find that demonstrating both foundational skills and compositional examples grounded in these skills within the same prompt context is crucial. We refer to this prompt structure as skills-in-context (SKiC). With as few as two exemplars, this in-context learning structure enables LLMs to tackle more challenging problems requiring innovative skill combinations, achieving near-perfect systematic generalization across a broad range of tasks. Intriguingly, SKiC also unlocks the latent potential of LLMs, allowing them to more actively utilize pre-existing internal skills acquired during earlier pretraining stages to solve complex reasoning problems. The SKiC structure is robust across different skill constructions and exemplar choices and demonstrates strong transferability to new tasks. Finally, inspired by our in-context learning study, we show that fine-tuning LLMs with SKiC-style data can elicit zero-shot weak-to-strong generalization, enabling the models to solve much harder problems directly with standard prompting.",
            "link": "https://www.semanticscholar.org/paper/447bbdbeb5dfa9252b51a833eafe5e8f4d3b632e",
            "authors": "Jiaao Chen, Xiaoman Pan, Dian Yu, Kaiqiang Song, Xiaoyang Wang, Dong Yu, Jianshu Chen",
            "matchScore": 161.2227,
            "original title": "Skills-in-Context: Unlocking Compositionality in Large Language Models",
            "original authors": "Jiaao Chen, Xiaoman Pan, Dian Yu, Kaiqiang Song, Xiaoyang Wang, Dong Yu, Jianshu Chen",
            "EMNLP Paper ID": "2697",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "a7f2987eb07d92033a7b1cadbfc94e6c1489f9bc",
            "title": "Style-Compress: An LLM-Based Prompt Compression Framework Considering Task-Specific Styles",
            "abstract": "Prompt compression condenses contexts while maintaining their informativeness for different usage scenarios. It not only shortens the inference time and reduces computational costs during the usage of large language models, but also lowers expenses when using closed-source models. In a preliminary study, we discover that when instructing language models to compress prompts, different compression styles (e.g., extractive or abstractive) impact performance of compressed prompts on downstream tasks. Building on this insight, we propose Style-Compress, a lightweight framework that adapts a smaller language model to compress prompts for a larger model on a new task without additional training. Our approach iteratively generates and selects effective compressed prompts as task-specific demonstrations through style variation and in-context learning, enabling smaller models to act as efficient compressors with task-specific examples. Style-Compress outperforms two baseline compression models in four tasks: original prompt reconstruction, text summarization, multi-hop QA, and CoT reasoning. In addition, with only 10 samples and 100 queries for adaptation, prompts compressed by Style-Compress achieve performance on par with or better than original prompts at a compression ratio of 0.25 or 0.5.",
            "link": "https://www.semanticscholar.org/paper/a7f2987eb07d92033a7b1cadbfc94e6c1489f9bc",
            "authors": "Xiao Pu, Tianxing He, Xiaojun Wan",
            "matchScore": 312.62665,
            "original title": "Style-Compress: An LLM-Based Prompt Compression Framework Considering Task-Specific Styles",
            "original authors": "Xiao Pu, Tianxing He, Xiaojun Wan",
            "EMNLP Paper ID": "2801",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "0973c19bc3612fadb52a445c621550eac644ec10",
            "title": "POSIX: A Prompt Sensitivity Index For Large Language Models",
            "abstract": "Despite their remarkable capabilities, Large Language Models (LLMs) are found to be surprisingly sensitive to minor variations in prompts, often generating significantly divergent outputs in response to minor variations in the prompts, such as spelling errors, alteration of wording or the prompt template. However, while assessing the quality of an LLM, the focus often tends to be solely on its performance on downstream tasks, while very little to no attention is paid to prompt sensitivity. To fill this gap, we propose POSIX - a novel PrOmpt Sensitivity IndeX as a reliable measure of prompt sensitivity, thereby offering a more comprehensive evaluation of LLM performance. The key idea behind POSIX is to capture the relative change in loglikelihood of a given response upon replacing the corresponding prompt with a different intent-preserving prompt. We provide thorough empirical evidence demonstrating the efficacy of POSIX in capturing prompt sensitivity and subsequently use it to measure and thereby compare prompt sensitivity of various open-source LLMs. We find that merely increasing the parameter count or instruction tuning does not necessarily reduce prompt sensitivity whereas adding some few-shot exemplars, even just one, almost always leads to significant decrease in prompt sensitivity. We also find that alterations to prompt template lead to the highest sensitivity in the case of MCQ type tasks, whereas paraphrasing results in the highest sensitivity in open-ended generation tasks. The code for reproducing our results is open-sourced at https://github.com/kowndinya-renduchintala/POSIX.",
            "link": "https://www.semanticscholar.org/paper/0973c19bc3612fadb52a445c621550eac644ec10",
            "authors": "Anwoy Chatterjee, S. Hsvn, Kowndinya Renduchintala, S. Bhatia, Tanmoy Chakraborty",
            "matchScore": 223.6537,
            "original title": "POSIX: A Prompt Sensitivity Index For Large Language Models",
            "original authors": "Anwoy Chatterjee, H S V N S Kowndinya Renduchintala, Sumit Bhatia, Tanmoy Chakraborty",
            "EMNLP Paper ID": "2802",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "e97fe11571ea59c566ad88742efaf58f4f177d02",
            "title": "From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression",
            "abstract": "Large language models (LLMs) have achieved significant performance gains using advanced prompting techniques over various tasks. However, the increasing length of prompts leads to high computational costs and often obscures crucial information. Prompt compression has been proposed to alleviate these issues, but it faces challenges in (i) capturing the global context and (ii) training the compressor effectively. To tackle these challenges, we introduce a novel prompt compression method, namely Reading To Compressing (R2C), utilizing the Fusion-in-Decoder (FiD) architecture to identify the important information in the prompt. Specifically, the cross-attention scores of the FiD are used to discern essential chunks and sentences from the prompt. R2C effectively captures the global context without compromising semantic consistency while detouring the necessity of pseudo-labels for training the compressor. Empirical results show that R2C retains key contexts, enhancing the LLM performance by 6% in out-of-domain evaluations while reducing the prompt length by 80%.",
            "link": "https://www.semanticscholar.org/paper/e97fe11571ea59c566ad88742efaf58f4f177d02",
            "authors": "Eunseong Choi, Sunkyung Lee, Minjin Choi, June Park, Jongwuk Lee",
            "matchScore": 255.84195,
            "original title": "From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression",
            "original authors": "Eunseong Choi, Sunkyung Lee, Minjin Choi, June Park, Jongwuk Lee",
            "EMNLP Paper ID": "2837",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Findings"
        },
        {
            "paperId": "d938ee9911fde45f75f9a05b05b8157a8c783718",
            "title": "ACCEPT: Adaptive Codebook for Composite and Efficient Prompt Tuning",
            "abstract": "Prompt Tuning has been a popular Parameter-Efficient Fine-Tuning method attributed to its remarkable performance with few updated parameters on various large-scale pretrained Language Models (PLMs). Traditionally, each prompt has been considered indivisible and updated independently, leading the parameters increase proportionally as prompt length grows. To address this issue, we propose Adaptive Codebook for Composite and Efficient Prompt Tuning (ACCEPT). In our method, we refer to the concept of product quantization (PQ), allowing all soft prompts to share a set of learnable codebook vectors in each subspace, with each prompt differentiated by a set of adaptive weights. We achieve the superior performance on 17 diverse natural language tasks including natural language understanding (NLU) and question answering (QA) tasks by tuning only 0.3% of parameters of the PLMs. Our approach also excels in few-shot and large model settings, highlighting its significant potential.",
            "link": "https://www.semanticscholar.org/paper/d938ee9911fde45f75f9a05b05b8157a8c783718",
            "authors": "Yu-Chen Lin, Wei-Hua Li, Jun-Cheng Chen, Chu-Song Chen",
            "matchScore": 247.35193,
            "original title": "ACCEPT: Adaptive Codebook for Composite and Efficient Prompt Tuning",
            "original authors": "Yu-Chen Lin, Wei-Hua Li, Jun-cheng Chen, Chu-Song Chen",
            "EMNLP Paper ID": "2942",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "04fd862b74f7161043e7b93e4fa1ca5acb484a06",
            "title": "ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs",
            "abstract": "Large language models (LLMs) have demonstrated impressive capabilities across various tasks, but their performance is highly sensitive to the prompts utilized. This variability poses challenges for accurate assessment and user satisfaction. Current research frequently overlooks instance-level prompt variations and their implications on subjective evaluations. To address these shortcomings, we introduce ProSA, a framework designed to evaluate and comprehend prompt sensitivity in LLMs. ProSA incorporates a novel sensitivity metric, PromptSensiScore, and leverages decoding confidence to elucidate underlying mechanisms. Our extensive study, spanning multiple tasks, uncovers that prompt sensitivity fluctuates across datasets and models, with larger models exhibiting enhanced robustness. We observe that few-shot examples can alleviate this sensitivity issue, and subjective evaluations are also susceptible to prompt sensitivities, particularly in complex, reasoning-oriented tasks. Furthermore, our findings indicate that higher model confidence correlates with increased prompt robustness. We believe this work will serve as a helpful tool in studying prompt sensitivity of LLMs. The project is released at: https://github.com/open-compass/ProSA .",
            "link": "https://www.semanticscholar.org/paper/04fd862b74f7161043e7b93e4fa1ca5acb484a06",
            "authors": "Jingming Zhuo, Songyang Zhang, Xinyu Fang, Haodong Duan, Dahua Lin, Kai Chen",
            "matchScore": 218.95706,
            "original title": "ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs",
            "original authors": "Jingming Zhuo, Songyang Zhang, Xinyu Fang, Haodong Duan, Dahua Lin, Kai Chen",
            "EMNLP Paper ID": "388",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "9788e5a2857769dd39bb98f570f9e2a7d482f066",
            "title": "AlignedCoT: Prompting Large Language Models via Native-Speaking Demonstrations",
            "abstract": "Large Language Models prompting, such as using in-context demonstrations, is a mainstream technique for invoking LLMs to perform high-performance and solid complex reasoning (e.g., mathematical reasoning, commonsense reasoning), and has the potential for further human-machine collaborative scientific findings. However, current LLMs are delicate and elusive in prompt words and styles. And there is an unseen gap between LLM understanding and human-written prompts. This paper introduces Alignedcot, an LLM-acquainted prompting technique that includes proficient ``native-speaking'' in in-context learning for the LLMs. Specifically, it achieves consistent and correct step-wise prompts in zero-shot scenarios by progressively probing, refining, and formatting the LLM chain of thoughts so that free from handcrafted few-shot demonstrations while maintaining the prompt quality. We conduct experiments on mathematical reasoning and commonsense reasoning. We find that LLMs with Alignedcot perform significantly superior to them with human-crafted demonstrations. We further apply Alignedcot for rewriting the GSM8K training set, resulting in a GSM8K-Align dataset. We observe its benefits for retrieval augmented generation. The code and data can be found at https://github.com/yangzhch6/AlignedCoT.",
            "link": "https://www.semanticscholar.org/paper/9788e5a2857769dd39bb98f570f9e2a7d482f066",
            "authors": "Zhicheng YANG, Yiwei Wang, Yinya Huang, Jing Xiong, Xiaodan Liang, Jing Tang",
            "matchScore": 271.8704,
            "original title": "AlignedCoT: Prompting Large Language Models via Native-Speaking Demonstrations",
            "original authors": "Zhicheng Yang, Yinya Huang, Jing Xiong, Liang Feng, Xiaodan Liang, Yiwei Wang, Jing Tang",
            "EMNLP Paper ID": "587",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        }
    ],
    "Bias and Fairness in Language Models": [
        {
            "paperId": "4ac4a5831ea0db86c5236a115e9f78c0e40e1b4e",
            "title": "Studying and Mitigating Biases in Sign Language Understanding Models",
            "abstract": "Ensuring that the benefits of sign language technologies are distributed equitably among all community members is crucial. Thus, it is important to address potential biases and inequities that may arise from the design or use of these resources. Crowd-sourced sign language datasets, such as the ASL Citizen dataset, are great resources for improving accessibility and preserving linguistic diversity, but they must be used thoughtfully to avoid reinforcing existing biases. In this work, we utilize the rich information about participant demographics and lexical features present in the ASL Citizen dataset to study and document the biases that may result from models trained on crowd-sourced sign datasets. Further, we apply several bias mitigation techniques during model training, and find that these techniques reduce performance disparities without decreasing accuracy. With the publication of this work, we release the demographic information about the participants in the ASL Citizen dataset to encourage future bias mitigation work in this space.",
            "link": "https://www.semanticscholar.org/paper/4ac4a5831ea0db86c5236a115e9f78c0e40e1b4e",
            "authors": "Katherine Atwell, Danielle Bragg, Malihe Alikhani",
            "EMNLP Paper ID": "27",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "255e98fd5f94c733c8d35e60335a5626a244518e",
            "title": "On the Influence of Gender and Race in Romantic Relationship Prediction from Large Language Models",
            "abstract": "We study the presence of heteronormative biases and prejudice against interracial romantic relationships in large language models by performing controlled name-replacement experiments for the task of relationship prediction. We show that models are less likely to predict romantic relationships for (a) same-gender character pairs than different-gender pairs; and (b) intra/inter-racial character pairs involving Asian names as compared to Black, Hispanic, or White names. We examine the contextualized embeddings of first names and find that gender for Asian names is less discernible than non-Asian names. We discuss the social implications of our findings, underlining the need to prioritize the development of inclusive and equitable technology.",
            "link": "https://www.semanticscholar.org/paper/255e98fd5f94c733c8d35e60335a5626a244518e",
            "authors": "Abhilasha Sancheti, Haozhe An, Rachel Rudinger",
            "EMNLP Paper ID": "67",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "72a37e8e156dea1692292fa20b93c63770fee5ee",
            "title": "A Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers",
            "abstract": "In this paper, we apply a method to quantify biases associated with named entities from various countries. We create counterfactual examples with small perturbations on target-domain data instead of relying on templates or specific datasets for bias detection. On widely used classifiers for subjectivity analysis, including sentiment, emotion, hate speech, and offensive text using Twitter data, our results demonstrate positive biases related to the language spoken in a country across all classifiers studied. Notably, the presence of certain country names in a sentence can strongly influence predictions, up to a 23\\% change in hate speech detection and up to a 60\\% change in the prediction of negative emotions such as anger. We hypothesize that these biases stem from the training data of pre-trained language models (PLMs) and find correlations between affect predictions and PLMs likelihood in English and unknown languages like Basque and Maori, revealing distinct patterns with exacerbate correlations. Further, we followed these correlations in-between counterfactual examples from a same sentence to remove the syntactical component, uncovering interesting results suggesting the impact of the pre-training data was more important for English-speaking-country names. Our anonymized code is [https://anonymous.4open.science/r/biases_ppl-576B/README.md](available here).",
            "link": "https://www.semanticscholar.org/paper/72a37e8e156dea1692292fa20b93c63770fee5ee",
            "authors": "Valentin Barriere, Sebastian Cifuentes",
            "EMNLP Paper ID": "77",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "aacf86efd2ebf0af622050ad41dda3d965e7214b",
            "title": "MiTTenS: A Dataset for Evaluating Gender Mistranslation",
            "abstract": "Translation systems, including foundation models capable of translation, can produce errors that result in gender mistranslation, and such errors can be especially harmful. To measure the extent of such potential harms when translating into and out of English, we introduce a dataset, MiTTenS, covering 26 languages from a variety of language families and scripts, including several traditionally under-represented in digital resources. The dataset is constructed with handcrafted passages that target known failure patterns, longer synthetically generated passages, and natural passages sourced from multiple domains. We demonstrate the usefulness of the dataset by evaluating both neural machine translation systems and foundation models, and show that all systems exhibit gender mistranslation and potential harm, even in high resource languages.",
            "link": "https://www.semanticscholar.org/paper/aacf86efd2ebf0af622050ad41dda3d965e7214b",
            "authors": "Kevin Robinson, Sneha Kudugunta, Romi Stella, Sunipa Dev, Jasmijn Bastings",
            "EMNLP Paper ID": "457",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "b3ff72674ce1c849a2cd9110f6d217927dcf7402",
            "title": "ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context",
            "abstract": "While the biases of language models in production are extensively documented, the biases of their guardrails have been neglected. This paper studies how contextual information about the user influences the likelihood of an LLM to refuse to execute a request. By generating user biographies that offer ideological and demographic information, we find a number of biases in guardrail sensitivity on GPT-3.5. Younger, female, and Asian-American personas are more likely to trigger a refusal guardrail when requesting censored or illegal information. Guardrails are also sycophantic, refusing to comply with requests for a political position the user is likely to disagree with. We find that certain identity groups and seemingly innocuous information, e.g., sports fandom, can elicit changes in guardrail sensitivity similar to direct statements of political ideology. For each demographic category and even for American football team fandom, we find that ChatGPT appears to infer a likely political ideology and modify guardrail behavior accordingly.",
            "link": "https://www.semanticscholar.org/paper/b3ff72674ce1c849a2cd9110f6d217927dcf7402",
            "authors": "Victoria R. Li, Yida Chen, Naomi Saphra",
            "EMNLP Paper ID": "712",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "b46f0ec1fb045aaf501a4252d4f80575f3cd4f08",
            "title": "Locating Information Gaps and Narrative Inconsistencies Across Languages: A Case Study of LGBT People Portrayals on Wikipedia",
            "abstract": "To explain social phenomena and identify systematic biases, much research in computational social science focuses on comparative text analyses. These studies often rely on coarse corpus-level statistics or local word-level analyses, mainly in English. We introduce the InfoGap method -- an efficient and reliable approach to locating information gaps and inconsistencies in articles at the fact level, across languages. We evaluate InfoGap by analyzing LGBT people's portrayals, across 2.7K biography pages on English, Russian, and French Wikipedias. We find large discrepancies in factual coverage across the languages. Moreover, our analysis reveals that biographical facts carrying negative connotations are more likely to be highlighted in Russian Wikipedia. Crucially, InfoGap both facilitates large scale analyses, and pinpoints local document- and fact-level information gaps, laying a new foundation for targeted and nuanced comparative language analysis at scale.",
            "link": "https://www.semanticscholar.org/paper/b46f0ec1fb045aaf501a4252d4f80575f3cd4f08",
            "authors": "Farhan Samir, Chan Young Park, Anjalie Field, Vered Shwartz, Yulia Tsvetkov",
            "EMNLP Paper ID": "751",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "4b743d624ede1b1cbdb22c570ef62be0aefe33e8",
            "title": "\"You Gotta be a Doctor, Lin\": An Investigation of Name-Based Bias of Large Language Models in Employment Recommendations",
            "abstract": "Social science research has shown that candidates with names indicative of certain races or genders often face discrimination in employment practices. Similarly, Large Language Models (LLMs) have demonstrated racial and gender biases in various applications. In this study, we utilize GPT-3.5-Turbo and Llama 3-70B-Instruct to simulate hiring decisions and salary recommendations for candidates with 320 first names that strongly signal their race and gender, across over 750,000 prompts. Our empirical results indicate a preference among these models for hiring candidates with White female-sounding names over other demographic groups across 40 occupations. Additionally, even among candidates with identical qualifications, salary recommendations vary by as much as 5% between different subgroups. A comparison with real-world labor data reveals inconsistent alignment with U.S. labor market characteristics, underscoring the necessity of risk investigation of LLM-powered systems.",
            "link": "https://www.semanticscholar.org/paper/4b743d624ede1b1cbdb22c570ef62be0aefe33e8",
            "authors": "H. Nghiem, John J. Prindle, Jieyu Zhao, Hal Daum'e",
            "EMNLP Paper ID": "816",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "eac8c19f0c7bcce7ff24dc70fd9dc8efdc73f0ce",
            "title": "Resampled Datasets Are Not Enough: Mitigating Societal Bias Beyond Single Attributes",
            "abstract": "We tackle societal bias in image-text datasets by removing spurious correlations between protected groups and image attributes. Traditional methods only target labeled attributes, ignoring biases from unlabeled ones. Using text-guided inpainting models, our approach ensures protected group independence from all attributes and mitigates inpainting biases through data filtering. Evaluations on multi-label image classification and image captioning tasks show our method effectively reduces bias without compromising performance across various models.",
            "link": "https://www.semanticscholar.org/paper/eac8c19f0c7bcce7ff24dc70fd9dc8efdc73f0ce",
            "authors": "Yusuke Hirota, Jerone T. A. Andrews, Dora Zhao, Orestis Papakyriakopoulos, Apostolos Modas, Yuta Nakashima, Alice Xiang",
            "EMNLP Paper ID": "956",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "a28071c63963cc59ba500cd00c140ac08eb5ccb0",
            "title": "Humans or LLMs as the Judge? A Study on Judgement Biases",
            "abstract": "Adopting human and large language models (LLM) as judges (a.k.a human- and LLM-as-a-judge) for evaluating the performance of LLMs has recently gained attention. Nonetheless, this approach concurrently introduces potential biases from human and LLMs, questioning the reliability of the evaluation results. In this paper, we propose a novel framework that is free from referencing groundtruth annotations for investigating Misinformation Oversight Bias, Gender Bias, Authority Bias and Beauty Bias on LLM and human judges. We curate a dataset referring to the revised Bloom's Taxonomy and conduct thousands of evaluations. Results show that human and LLM judges are vulnerable to perturbations to various degrees, and that even the cutting-edge judges possess considerable biases. We further exploit these biases to conduct attacks on LLM judges. We hope that our work can notify the community of the bias and vulnerability of human- and LLM-as-a-judge, as well as the urgency of developing robust evaluation systems.",
            "link": "https://www.semanticscholar.org/paper/a28071c63963cc59ba500cd00c140ac08eb5ccb0",
            "authors": "Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, Benyou Wang",
            "EMNLP Paper ID": "963",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "161c6d3851b73fc844c49a9921cfc6bb6da6904b",
            "title": "\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models",
            "abstract": "Model editing has emerged as a cost-effective strategy to update knowledge stored in language models. However, model editing can have unintended consequences after edits are applied: information unrelated to the edits can also be changed, and other general behaviors of the model can be wrongly altered. In this work, we investigate how model editing methods unexpectedly amplify model biases post-edit. We introduce a novel benchmark dataset, Seesaw-CF, for measuring bias-related harms of model editing and conduct the first in-depth investigation of how different weight-editing methods impact model bias. Specifically, we focus on biases with respect to demographic attributes such as race, geographic origin, and gender, as well as qualitative flaws in long-form texts generated by edited language models. We find that edited models exhibit, to various degrees, more biased behavior as they become less confident in attributes for Asian, African, and South American subjects. Furthermore, edited models amplify sexism and xenophobia in text generations while remaining seemingly coherent and logical. Finally, editing facts about place of birth, country of citizenship, or gender have particularly negative effects on the model's knowledge about unrelated features like field of work.",
            "link": "https://www.semanticscholar.org/paper/161c6d3851b73fc844c49a9921cfc6bb6da6904b",
            "authors": "Karina Halevy, Anna Sotnikova, Badr AlKhamissi, Syrielle Montariol, Antoine Bosselut",
            "EMNLP Paper ID": "1002",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 14): 13:00\u201314:00 (Afternoon)",
            "Type": "Main"
        },
        {
            "paperId": "031edeb613e65d4e0af357ddaef12a1d49478ba9",
            "title": "The Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention",
            "abstract": "Prompt-based\"diversity interventions\"are commonly adopted to improve the diversity of Text-to-Image (T2I) models depicting individuals with various racial or gender traits. However, will this strategy result in nonfactual demographic distribution, especially when generating real historical figures. In this work, we propose DemOgraphic FActualIty Representation (DoFaiR), a benchmark to systematically quantify the trade-off between using diversity interventions and preserving demographic factuality in T2I models. DoFaiR consists of 756 meticulously fact-checked test instances to reveal the factuality tax of various diversity prompts through an automated evidence-supported evaluation pipeline. Experiments on DoFaiR unveil that diversity-oriented instructions increase the number of different gender and racial groups in DALLE-3's generations at the cost of historically inaccurate demographic distributions. To resolve this issue, we propose Fact-Augmented Intervention (FAI), which instructs a Large Language Model (LLM) to reflect on verbalized or retrieved factual information about gender and racial compositions of generation subjects in history, and incorporate it into the generation context of T2I models. By orienting model generations using the reflected historical truths, FAI significantly improves the demographic factuality under diversity interventions while preserving diversity.",
            "link": "https://www.semanticscholar.org/paper/031edeb613e65d4e0af357ddaef12a1d49478ba9",
            "authors": "Yixin Wan, Di Wu, Haoran Wang, Kai-Wei Chang",
            "EMNLP Paper ID": "1036",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "7d4f2e0f9d79425674a97a94a4bd6cfd7636fc52",
            "title": "The Lou Dataset -- Exploring the Impact of Gender-Fair Language in German Text Classification",
            "abstract": "Gender-fair language, an evolving German linguistic variation, fosters inclusion by addressing all genders or using neutral forms. Nevertheless, there is a significant lack of resources to assess the impact of this linguistic shift on classification using language models (LMs), which are probably not trained on such variations. To address this gap, we present Lou, the first dataset featuring high-quality reformulations for German text classification covering seven tasks, like stance detection and toxicity classification. Evaluating 16 mono- and multi-lingual LMs on Lou shows that gender-fair language substantially impacts predictions by flipping labels, reducing certainty, and altering attention patterns. However, existing evaluations remain valid, as LM rankings of original and reformulated instances do not significantly differ. While we offer initial insights on the effect on German text classification, the findings likely apply to other languages, as consistent patterns were observed in multi-lingual and English LMs.",
            "link": "https://www.semanticscholar.org/paper/7d4f2e0f9d79425674a97a94a4bd6cfd7636fc52",
            "authors": "Andreas Waldis, Joel Birrer, Anne Lauscher, Iryna Gurevych",
            "EMNLP Paper ID": "1195",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "2044d10c08584acee2081db2363ffb679d687df7",
            "title": "Unlabeled Debiasing in Downstream Tasks via Class-wise Low Variance Regularization",
            "abstract": "Language models frequently inherit societal biases from their training data. Numerous techniques have been proposed to mitigate these biases during both the pre-training and fine-tuning stages. However, fine-tuning a pre-trained debiased language model on a downstream task can reintroduce biases into the model. Additionally, existing debiasing methods for downstream tasks either (i) require labels of protected attributes (e.g., age, race, or political views) that are often not available or (ii) rely on indicators of bias, which restricts their applicability to gender debiasing since they rely on gender-specific words. To address this, we introduce a novel debiasing regularization technique based on the class-wise variance of embeddings. Crucially, our method does not require attribute labels and targets any attribute, thus addressing the shortcomings of existing debiasing methods. Our experiments on encoder language models and three datasets demonstrate that our method outperforms existing strong debiasing baselines that rely on target attribute labels while maintaining performance on the target task.",
            "link": "https://www.semanticscholar.org/paper/2044d10c08584acee2081db2363ffb679d687df7",
            "authors": "Shahed Masoudian, Markus Frohman, Navid Rekabsaz, Markus Schedl",
            "EMNLP Paper ID": "1246",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "0abfe7622e0bc9299d22d9bfd5080cc080f9a77b",
            "title": "\"Global is Good, Local is Bad?\": Understanding Brand Bias in LLMs",
            "abstract": "Many recent studies have investigated social biases in LLMs but brand bias has received little attention. This research examines the biases exhibited by LLMs towards different brands, a significant concern given the widespread use of LLMs in affected use cases such as product recommendation and market analysis. Biased models may perpetuate societal inequalities, unfairly favoring established global brands while marginalizing local ones. Using a curated dataset across four brand categories, we probe the behavior of LLMs in this space. We find a consistent pattern of bias in this space -- both in terms of disproportionately associating global brands with positive attributes and disproportionately recommending luxury gifts for individuals in high-income countries. We also find LLMs are subject to country-of-origin effects which may boost local brand preference in LLM outputs in specific contexts.",
            "link": "https://www.semanticscholar.org/paper/0abfe7622e0bc9299d22d9bfd5080cc080f9a77b",
            "authors": "M. Kamruzzaman, H. Nguyen, Gene Louis Kim",
            "EMNLP Paper ID": "1476",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "642acdc9ee80b9d424d02affca0b6adaa5249052",
            "title": "$\\texttt{ModSCAN}$: Measuring Stereotypical Bias in Large Vision-Language Models from Vision and Language Modalities",
            "abstract": "Large vision-language models (LVLMs) have been rapidly developed and widely used in various fields, but the (potential) stereotypical bias in the model is largely unexplored. In this study, we present a pioneering measurement framework, $\\texttt{ModSCAN}$, to $\\underline{SCAN}$ the stereotypical bias within LVLMs from both vision and language $\\underline{Mod}$alities. $\\texttt{ModSCAN}$ examines stereotypical biases with respect to two typical stereotypical attributes (gender and race) across three kinds of scenarios: occupations, descriptors, and persona traits. Our findings suggest that 1) the currently popular LVLMs show significant stereotype biases, with CogVLM emerging as the most biased model; 2) these stereotypical biases may stem from the inherent biases in the training dataset and pre-trained models; 3) the utilization of specific prompt prefixes (from both vision and language modalities) performs well in reducing stereotypical biases. We believe our work can serve as the foundation for understanding and addressing stereotypical bias in LVLMs.",
            "link": "https://www.semanticscholar.org/paper/642acdc9ee80b9d424d02affca0b6adaa5249052",
            "authors": "Yukun Jiang, Zheng Li, Xinyue Shen, Yugeng Liu, Michael Backes, Yang Zhang",
            "EMNLP Paper ID": "1489",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "625a75ef994943b86be08da5d6b1d188528f5832",
            "title": "Linguistic Bias in ChatGPT: Language Models Reinforce Dialect Discrimination",
            "abstract": "We present a large-scale study of linguistic bias exhibited by ChatGPT covering ten dialects of English (Standard American English, Standard British English, and eight widely spoken non-\"standard\"varieties from around the world). We prompted GPT-3.5 Turbo and GPT-4 with text by native speakers of each variety and analyzed the responses via detailed linguistic feature annotation and native speaker evaluation. We find that the models default to\"standard\"varieties of English; based on evaluation by native speakers, we also find that model responses to non-\"standard\"varieties consistently exhibit a range of issues: stereotyping (19% worse than for\"standard\"varieties), demeaning content (25% worse), lack of comprehension (9% worse), and condescending responses (15% worse). We also find that if these models are asked to imitate the writing style of prompts in non-\"standard\"varieties, they produce text that exhibits lower comprehension of the input and is especially prone to stereotyping. GPT-4 improves on GPT-3.5 in terms of comprehension, warmth, and friendliness, but also exhibits a marked increase in stereotyping (+18%). The results indicate that GPT-3.5 Turbo and GPT-4 can perpetuate linguistic discrimination toward speakers of non-\"standard\"varieties.",
            "link": "https://www.semanticscholar.org/paper/625a75ef994943b86be08da5d6b1d188528f5832",
            "authors": "Eve Fleisig, G. Smith, Madeline Bossi, Ishita Rustagi, Xavier Yin, Dan Klein",
            "EMNLP Paper ID": "1562",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "468c1d2d8e384472f313ff0487839839727b8934",
            "title": "Social Bias Probing: Fairness Benchmarking for Language Models",
            "abstract": "While the impact of social biases in language models has been recognized, prior methods for bias evaluation have been limited to binary association tests on small datasets, limiting our understanding of bias complexities. This paper proposes a novel framework for probing language models for social biases by assessing disparate treatment, which involves treating individuals differently according to their affiliation with a sensitive demographic group. We curate SoFa, a large-scale benchmark designed to address the limitations of existing fairness collections. SoFa expands the analysis beyond the binary comparison of stereotypical versus anti-stereotypical identities to include a diverse range of identities and stereotypes. Comparing our methodology with existing benchmarks, we reveal that biases within language models are more nuanced than acknowledged, indicating a broader scope of encoded biases than previously recognized. Benchmarking LMs on SoFa, we expose how identities expressing different religions lead to the most pronounced disparate treatments across all models. Finally, our findings indicate that real-life adversities faced by various groups such as women and people with disabilities are mirrored in the behavior of these models.",
            "link": "https://www.semanticscholar.org/paper/468c1d2d8e384472f313ff0487839839727b8934",
            "authors": "Marta Marchiori Manerba, Karolina Sta'nczak, Riccardo Guidotti, Isabelle Augenstein",
            "EMNLP Paper ID": "1687",
            "Oral/Poster": "Oral",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "3b6d50e2f447b3f6b38c171686433b4d6e8702db",
            "title": "BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs",
            "abstract": "Evaluating the bias in Large Language Models (LLMs) becomes increasingly crucial with their rapid development. However, existing evaluation methods rely on fixed-form outputs and cannot adapt to the flexible open-text generation scenarios of LLMs (e.g., sentence completion and question answering). To address this, we introduce BiasAlert, a plug-and-play tool designed to detect social bias in open-text generations of LLMs. BiasAlert integrates external human knowledge with inherent reasoning capabilities to detect bias reliably. Extensive experiments demonstrate that BiasAlert significantly outperforms existing state-of-the-art methods like GPT4-as-A-Judge in detecting bias. Furthermore, through application studies, we demonstrate the utility of BiasAlert in reliable LLM bias evaluation and bias mitigation across various scenarios. Model and code will be publicly released.",
            "link": "https://www.semanticscholar.org/paper/3b6d50e2f447b3f6b38c171686433b4d6e8702db",
            "authors": "Zhiting Fan, Ruizhe Chen, Ruiling Xu, Zuozhu Liu",
            "EMNLP Paper ID": "1701",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "60a02dcd250f07a32ae4264fa58fcd7cd10ad07d",
            "title": "Applying Intrinsic Debiasing on Downstream Tasks: Challenges and Considerations for Machine Translation",
            "abstract": "Most works on gender bias focus on intrinsic bias -- removing traces of information about a protected group from the model's internal representation. However, these works are often disconnected from the impact of such debiasing on downstream applications, which is the main motivation for debiasing in the first place. In this work, we systematically test how methods for intrinsic debiasing affect neural machine translation models, by measuring the extrinsic bias of such systems under different design choices. We highlight three challenges and mismatches between the debiasing techniques and their end-goal usage, including the choice of embeddings to debias, the mismatch between words and sub-word tokens debiasing, and the effect on different target languages. We find that these considerations have a significant impact on downstream performance and the success of debiasing.",
            "link": "https://www.semanticscholar.org/paper/60a02dcd250f07a32ae4264fa58fcd7cd10ad07d",
            "authors": "Bar Iluz, Yanai Elazar, Asaf Yehudai, Gabriel Stanovsky",
            "EMNLP Paper ID": "1716",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "d7552c281caf7e2be5818e8f74119600644a41cb",
            "title": "Images Speak Louder than Words: Understanding and Mitigating Bias in Vision-Language Model from a Causal Mediation Perspective",
            "abstract": "Vision-language models (VLMs) pre-trained on extensive datasets can inadvertently learn biases by correlating gender information with specific objects or scenarios. Current methods, which focus on modifying inputs and monitoring changes in the model's output probability scores, often struggle to comprehensively understand bias from the perspective of model components. We propose a framework that incorporates causal mediation analysis to measure and map the pathways of bias generation and propagation within VLMs. This approach allows us to identify the direct effects of interventions on model bias and the indirect effects of interventions on bias mediated through different model components. Our results show that image features are the primary contributors to bias, with significantly higher impacts than text features, specifically accounting for 32.57% and 12.63% of the bias in the MSCOCO and PASCAL-SENTENCE datasets, respectively. Notably, the image encoder's contribution surpasses that of the text encoder and the deep fusion encoder. Further experimentation confirms that contributions from both language and vision modalities are aligned and non-conflicting. Consequently, focusing on blurring gender representations within the image encoder, which contributes most to the model bias, reduces bias efficiently by 22.03% and 9.04% in the MSCOCO and PASCAL-SENTENCE datasets, respectively, with minimal performance loss or increased computational demands.",
            "link": "https://www.semanticscholar.org/paper/d7552c281caf7e2be5818e8f74119600644a41cb",
            "authors": "Zhaotian Weng, Zijun Gao, Jerone Andrews, Jieyu Zhao",
            "EMNLP Paper ID": "1843",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "312a747b293aa165d18034f7d57fe29a1e979678",
            "title": "From Descriptive Richness to Bias: Unveiling the Dark Side of Generative Image Caption Enrichment",
            "abstract": "Large language models (LLMs) have enhanced the capacity of vision-language models to caption visual text. This generative approach to image caption enrichment further makes textual captions more descriptive, improving alignment with the visual context. However, while many studies focus on benefits of generative caption enrichment (GCE), are there any negative side effects? We compare standard-format captions and recent GCE processes from the perspectives of\"gender bias\"and\"hallucination\", showing that enriched captions suffer from increased gender bias and hallucination. Furthermore, models trained on these enriched captions amplify gender bias by an average of 30.9% and increase hallucination by 59.5%. This study serves as a caution against the trend of making captions more descriptive.",
            "link": "https://www.semanticscholar.org/paper/312a747b293aa165d18034f7d57fe29a1e979678",
            "authors": "Yusuke Hirota, Ryo Hachiuma, Chao-Han Huck Yang, Yuta Nakashima",
            "EMNLP Paper ID": "2150",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "7156051b1effa3ed5d072816cc94dbf6cb1252b3",
            "title": "What the Harm? Quantifying the Tangible Impact of Gender Bias in Machine Translation with a Human-centered Study",
            "abstract": "Gender bias in machine translation (MT) is recognized as an issue that can harm people and society. And yet, advancements in the field rarely involve people, the final MT users, or inform how they might be impacted by biased technologies. Current evaluations are often restricted to automatic methods, which offer an opaque estimate of what the downstream impact of gender disparities might be. We conduct an extensive human-centered study to examine if and to what extent bias in MT brings harms with tangible costs, such as quality of service gaps across women and men. To this aim, we collect behavioral data from 90 participants, who post-edited MT outputs to ensure correct gender translation. Across multiple datasets, languages, and types of users, our study shows that feminine post-editing demands significantly more technical and temporal effort, also corresponding to higher financial costs. Existing bias measurements, however, fail to reflect the found disparities. Our findings advocate for human-centered approaches that can inform the societal impact of bias.",
            "link": "https://www.semanticscholar.org/paper/7156051b1effa3ed5d072816cc94dbf6cb1252b3",
            "authors": "Beatrice Savoldi, Sara Papi, Matteo Negri, Ana Guerberof, L. Bentivogli",
            "EMNLP Paper ID": "2206",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "243dbf643ee5f7233ecc2bb315d00afd8e935e42",
            "title": "Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in Large Language Models",
            "abstract": "Large language models (LLMs) have been shown to propagate and amplify harmful stereotypes, particularly those that disproportionately affect marginalised communities. To understand the effect of these stereotypes more comprehensively, we introduce GlobalBias, a dataset of 876k sentences incorporating 40 distinct gender-by-ethnicity groups alongside descriptors typically used in bias literature, which enables us to study a broad set of stereotypes from around the world. We use GlobalBias to directly probe a suite of LMs via perplexity, which we use as a proxy to determine how certain stereotypes are represented in the model's internal representations. Following this, we generate character profiles based on given names and evaluate the prevalence of stereotypes in model outputs. We find that the demographic groups associated with various stereotypes remain consistent across model likelihoods and model outputs. Furthermore, larger models consistently display higher levels of stereotypical outputs, even when explicitly instructed not to.",
            "link": "https://www.semanticscholar.org/paper/243dbf643ee5f7233ecc2bb315d00afd8e935e42",
            "authors": "Zara Siddique, Liam D. Turner, Luis Espinosa Anke",
            "EMNLP Paper ID": "2319",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "2a157d33ef98319fd4e4daad487a760076d7b6ad",
            "title": "Evaluating Short-Term Temporal Fluctuations of Social Biases in Social Media Data and Masked Language Models",
            "abstract": "Social biases such as gender or racial biases have been reported in language models (LMs), including Masked Language Models (MLMs). Given that MLMs are continuously trained with increasing amounts of additional data collected over time, an important yet unanswered question is how the social biases encoded with MLMs vary over time. In particular, the number of social media users continues to grow at an exponential rate, and it is a valid concern for the MLMs trained specifically on social media data whether their social biases (if any) would also amplify over time. To empirically analyse this problem, we use a series of MLMs pretrained on chronologically ordered temporal snapshots of corpora. Our analysis reveals that, although social biases are present in all MLMs, most types of social bias remain relatively stable over time (with a few exceptions). To further understand the mechanisms that influence social biases in MLMs, we analyse the temporal corpora used to train the MLMs. Our findings show that some demographic groups, such as male, obtain higher preference over the other, such as female on the training corpora constantly.",
            "link": "https://www.semanticscholar.org/paper/2a157d33ef98319fd4e4daad487a760076d7b6ad",
            "authors": "Yi Zhou, D. Bollegala, Jos\u00e9 Camacho-Collados",
            "EMNLP Paper ID": "2546",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "a4d91bcd24c46a06f02ba8e66541092ac14b4968",
            "title": "Twists, Humps, and Pebbles: Multilingual Speech Recognition Models Exhibit Gender Performance Gaps",
            "abstract": "Current automatic speech recognition (ASR) models are designed to be used across many languages and tasks without substantial changes. However, this broad language coverage hides performance gaps within languages, for example, across genders. Our study systematically evaluates the performance of two widely used multilingual ASR models on three datasets, encompassing 19 languages from eight language families and two speaking conditions. Our findings reveal clear gender disparities, with the advantaged group varying across languages and models. Surprisingly, those gaps are not explained by acoustic or lexical properties. However, probing internal model states reveals a correlation with gendered performance gap. That is, the easier it is to distinguish speaker gender in a language using probes, the more the gap reduces, favoring female speakers. Our results show that gender disparities persist even in state-of-the-art models. Our findings have implications for the improvement of multilingual ASR systems, underscoring the importance of accessibility to training data and nuanced evaluation to predict and mitigate gender gaps. We release all code and artifacts at https://github.com/g8a9/multilingual-asr-gender-gap.",
            "link": "https://www.semanticscholar.org/paper/a4d91bcd24c46a06f02ba8e66541092ac14b4968",
            "authors": "Giuseppe Attanasio, Beatrice Savoldi, Dennis Fucci, Dirk Hovy",
            "EMNLP Paper ID": "2910",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "5997e3db4f58bc7f1c2b8bba604078301f88b23b",
            "title": "Local Contrastive Editing of Gender Stereotypes",
            "abstract": "Stereotypical bias encoded in language models (LMs) poses a threat to safe language technology, yet our understanding of how bias manifests in the parameters of LMs remains incomplete. We introduce local contrastive editing that enables the localization and editing of a subset of weights in a target model in relation to a reference model. We deploy this approach to identify and modify subsets of weights that are associated with gender stereotypes in LMs. Through a series of experiments, we demonstrate that local contrastive editing can precisely localize and control a small subset (<0.5%) of weights that encode gender bias. Our work (i) advances our understanding of how stereotypical biases can manifest in the parameter space of LMs and (ii) opens up new avenues for developing parameter-efficient strategies for controlling model properties in a contrastive manner.",
            "link": "https://www.semanticscholar.org/paper/5997e3db4f58bc7f1c2b8bba604078301f88b23b",
            "authors": "Marlene Lutz, Rochelle Choenni, Markus Strohmaier, Anne Lauscher",
            "EMNLP Paper ID": "2953",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "6560791590bd50bb73afe4b77dd0a8ede486905e",
            "title": "Gender Bias in Decision-Making with Large Language Models: A Study of Relationship Conflicts",
            "abstract": "Large language models (LLMs) acquire beliefs about gender from training data and can therefore generate text with stereotypical gender attitudes. Prior studies have demonstrated model generations favor one gender or exhibit stereotypes about gender, but have not investigated the complex dynamics that can influence model reasoning and decision-making involving gender. We study gender equity within LLMs through a decision-making lens with a new dataset, DeMET Prompts, containing scenarios related to intimate, romantic relationships. We explore nine relationship configurations through name pairs across three name lists (men, women, neutral). We investigate equity in the context of gender roles through numerous lenses: typical and gender-neutral names, with and without model safety enhancements, same and mixed-gender relationships, and egalitarian versus traditional scenarios across various topics. While all models exhibit the same biases (women favored, then those with gender-neutral names, and lastly men), safety guardrails reduce bias. In addition, models tend to circumvent traditional male dominance stereotypes and side with 'traditionally female' individuals more often, suggesting relationships are viewed as a female domain by the models.",
            "link": "https://www.semanticscholar.org/paper/6560791590bd50bb73afe4b77dd0a8ede486905e",
            "authors": "Sharon Levy, William D. Adler, T. Karver, Mark Dredze, Michelle R. Kaufman",
            "matchScore": 148.14691,
            "original title": "Gender Bias in Decision-Making with Large Language Models",
            "original authors": "Sharon Levy, William Adler, Tahilin Sanchez Karver, Mark Dredze, Michelle R Kaufman",
            "EMNLP Paper ID": "1172",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "f923bd2cd931c731c1b7f851caf0b1dcdc46dfe8",
            "title": "Evaluating Biases in Context-Dependent Health Questions",
            "abstract": "Chat-based large language models have the opportunity to empower individuals lacking high-quality healthcare access to receive personalized information across a variety of topics. However, users may ask underspecified questions that require additional context for a model to correctly answer. We study how large language model biases are exhibited through these contextual questions in the healthcare domain. To accomplish this, we curate a dataset of sexual and reproductive healthcare questions that are dependent on age, sex, and location attributes. We compare models' outputs with and without demographic context to determine group alignment among our contextual questions. Our experiments reveal biases in each of these attributes, where young adult female users are favored.",
            "link": "https://www.semanticscholar.org/paper/f923bd2cd931c731c1b7f851caf0b1dcdc46dfe8",
            "authors": "Sharon Levy, T. Karver, William D. Adler, Michelle R. Kaufman, Mark Dredze",
            "matchScore": 196.65065,
            "original title": "Evaluating Biases in Context-Dependent Health Questions",
            "original authors": "Sharon Levy, Tahilin Sanchez Karver, William Adler, Michelle R Kaufman, Mark Dredze",
            "EMNLP Paper ID": "1174",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "44e86f9d889c7309b1fecd273774b2703ae2fa8a",
            "title": "Irrelevant Alternatives Bias Large Language Model Hiring Decisions",
            "abstract": "We investigate whether LLMs display a well-known human cognitive bias, the attraction effect, in hiring decisions. The attraction effect occurs when the presence of an inferior candidate makes a superior candidate more appealing, increasing the likelihood of the superior candidate being chosen over a non-dominated competitor. Our study finds consistent and significant evidence of the attraction effect in GPT-3.5 and GPT-4 when they assume the role of a recruiter. Irrelevant attributes of the decoy, such as its gender, further amplify the observed bias. GPT-4 exhibits greater bias variation than GPT-3.5. Our findings remain robust even when warnings against the decoy effect are included and the recruiter role definition is varied.",
            "link": "https://www.semanticscholar.org/paper/44e86f9d889c7309b1fecd273774b2703ae2fa8a",
            "authors": "Kremena Valkanova, Pencho Yordanov",
            "matchScore": 263.1162,
            "original title": "Irrelevant Alternatives Bias Large Language Model Hiring Decisions",
            "original authors": "Kremena Valkanova, Pencho Yordanov",
            "EMNLP Paper ID": "1413",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "1ee8b053244d0bbb0674bd05c58ec61cc56b85b5",
            "title": "A Study of Implicit Ranking Unfairness in Large Language Models",
            "abstract": "Recently, Large Language Models (LLMs) have demonstrated a superior ability to serve as ranking models. However, concerns have arisen as LLMs will exhibit discriminatory ranking behaviors based on users' sensitive attributes (\\eg gender). Worse still, in this paper, we identify a subtler form of discrimination in LLMs, termed \\textit{implicit ranking unfairness}, where LLMs exhibit discriminatory ranking patterns based solely on non-sensitive user profiles, such as user names. Such implicit unfairness is more widespread but less noticeable, threatening the ethical foundation. To comprehensively explore such unfairness, our analysis will focus on three research aspects: (1) We propose an evaluation method to investigate the severity of implicit ranking unfairness. (2) We uncover the reasons for causing such unfairness. (3) To mitigate such unfairness effectively, we utilize a pair-wise regression method to conduct fair-aware data augmentation for LLM fine-tuning. The experiment demonstrates that our method outperforms existing approaches in ranking fairness, achieving this with only a small reduction in accuracy. Lastly, we emphasize the need for the community to identify and mitigate the implicit unfairness, aiming to avert the potential deterioration in the reinforced human-LLMs ecosystem deterioration.",
            "link": "https://www.semanticscholar.org/paper/1ee8b053244d0bbb0674bd05c58ec61cc56b85b5",
            "authors": "Chen Xu, Wenjie Wang, Yuxin Li, Liang Pang, Jun Xu, Tat-Seng Chua",
            "matchScore": 222.27814,
            "original title": "A Study of Implicit Ranking Unfairness in Large Language Models",
            "original authors": "Chen Xu, Wenjie Wang, Yuxin Li, Liang Pang, Jun Xu, Tat-Seng Chua",
            "EMNLP Paper ID": "1669",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 12): 17:45\u201318:45 (Evening)",
            "Type": "Findings"
        },
        {
            "paperId": "b92ec2ef54e4df2d08cbc66e4dda3e37b6362dbd",
            "title": "Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions",
            "abstract": "As Large Language Models (LLMs) continue to evolve, they are increasingly being employed in numerous studies to simulate societies and execute diverse social tasks. However, LLMs are susceptible to societal biases due to their exposure to human-generated data. Given that LLMs are being used to gain insights into various societal aspects, it is essential to mitigate these biases. To that end, our study investigates the presence of implicit gender biases in multi-agent LLM interactions and proposes two strategies to mitigate these biases. We begin by creating a dataset of scenarios where implicit gender biases might arise, and subsequently develop a metric to assess the presence of biases. Our empirical analysis reveals that LLMs generate outputs characterized by strong implicit bias associations (>= 50\\% of the time). Furthermore, these biases tend to escalate following multi-agent interactions. To mitigate them, we propose two strategies: self-reflection with in-context examples (ICE); and supervised fine-tuning. Our research demonstrates that both methods effectively mitigate implicit biases, with the ensemble of fine-tuning and self-reflection proving to be the most successful.",
            "link": "https://www.semanticscholar.org/paper/b92ec2ef54e4df2d08cbc66e4dda3e37b6362dbd",
            "authors": "Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-655, Chris Bamford, Devendra Singh, Diego Chaplot, laume Lample, L\u00e9lio Lucile Saulnier, Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Jerry Kang, Mark Bennett, Devon Carbado, Pam Casey, P. Liang, Chiyu Wu, Louis-Philippe Morency, Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, S. Welleck, Amir Yazdan Bakhsh, ing Bao, Mo Bavarian, Jeff Belgum, Ir-wan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brock-724 man, Tim Brooks, M. Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Su-Hong Chen, Ruby Chen, Jason Chen, Mark Chen, B. Chess, Chester Cho, Hyung Casey Chu, Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Tarun Goel, Gabriel Gogineni, Rapha Goh, Jonathan Gontijo-738 Lopes, Morgan Gordon, Scott Grafstein, Ryan Gray, Joshua Greene, Shixiang Shane Gross, Yufei Gu, Chris Guo, Jesse Hallacy, Jeff Han, Harris Yuchen, Mike He, Johannes Heaton, C. Heidecke, Alan Hesse, Wade Hickey, Peter Hickey, Hoeschele Brandon, Kenny Houghton, Shengli Hsu, Xin Hu, Joost Hu, Shantanu Huizinga, Shawn Jain, Jain Joanne, Angela Jang, Roger Jiang, Haozhun Jiang, Denny Jin, Shino Jin, Billie Jomoto, Hee-woo Jonn, Tomer Jun, \u0141ukasz Kaftan, Ali Kaiser, Ingmar Ka-748 mali, Kanitscheider, Nitish Shirish, Keskar Tabarak, Logan Khan, J. Kilpatrick, Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirch-751 ner, J. Kiros, Matthew Knight, Daniel Kokotajlo, \u0141ukasz Kondraciuk, A. Kondrich, Aris Kon-753 stantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Chak Daniel Levy, Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Ma-teusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, A. Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, S. McKinney, C. McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel P. Mossing, Tong Mu, Mira Murati, O. Murk, David M\u00e9ly, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Ouyang Long, Cullen O'Keefe, J. Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-779 der, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin D. Sokolowsky, Yang Song, Natalie Staudacher, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim-ing Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Tianhao Shengjia Zhao, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Sandhini Agarwal, Alex Gray, Jacob Hilton, Fraser Kelton, Luke Miller, Amanda Askell, P. Welinder, Paul F. Christiano, Joon Sung Park, Joseph O\u2019Brien, C. J. Cai, Ringel Morris, Percy Liang, Michael S. Bern-814, Alec Radford, Karthik Narasimhan, Tim Salimans, Rachel Rudinger, Jason Naradowsky, Brian Leonard, Nisan Stiennon, Ryan Ziegler, Chelsea Lowe, Alec Voss, Radford, Dario Amodei, Christiano. 2020. Learn-842, Tony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang, Mai ElSherief, Jie Zhao, Diba Mirza, Kai-Wei Belding, Chang William, Yang Wang, Yixin Wan, George Pu, Jiao Sun, Aparna Garimella, Kai-Wei Chang, Nanyun Peng, \u201ckelly",
            "matchScore": 273.2222,
            "original title": "Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions",
            "original authors": "Angana Borah, Rada Mihalcea",
            "EMNLP Paper ID": "1947",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "69f58648780d6de2fe79745790a5b974927735f3",
            "title": "BiasDora: Exploring Hidden Biased Associations in Vision-Language Models",
            "abstract": "Existing works examining Vision-Language Models (VLMs) for social biases predominantly focus on a limited set of documented bias associations, such as gender:profession or race:crime. This narrow scope often overlooks a vast range of unexamined implicit associations, restricting the identification and, hence, mitigation of such biases. We address this gap by probing VLMs to (1) uncover hidden, implicit associations across 9 bias dimensions. We systematically explore diverse input and output modalities and (2) demonstrate how biased associations vary in their negativity, toxicity, and extremity. Our work (3) identifies subtle and extreme biases that are typically not recognized by existing methodologies. We make the Dataset of retrieved associations, (Dora), publicly available here https://github.com/chahatraj/BiasDora.",
            "link": "https://www.semanticscholar.org/paper/69f58648780d6de2fe79745790a5b974927735f3",
            "authors": "Chahat Raj, A. Mukherjee, Aylin Caliskan, Antonios Anastasopoulos, Ziwei Zhu",
            "matchScore": 245.49683,
            "original title": "BiasDora: Exploring Hidden Biased Associations in Vision-Language Models",
            "original authors": "Chahat Raj, Anjishnu Mukherjee, Aylin Caliskan, Antonios Anastasopoulos, Ziwei Zhu",
            "EMNLP Paper ID": "2131",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "dc47abf03e644afa0bdc834bab11018d3804e3c8",
            "title": "OffsetBias: Leveraging Debiased Data for Tuning Evaluators",
            "abstract": "Employing Large Language Models (LLMs) to assess the quality of generated responses, such as prompting instruct-tuned models or fine-tuning judge models, has become a widely adopted evaluation method. It is also known that such evaluators are vulnerable to biases, such as favoring longer responses. While it is important to overcome this problem, the specifics of these biases remain under-explored. In this work, we qualitatively identify six types of biases inherent in various judge models. We propose EvalBiasBench as a meta-evaluation collection of hand-crafted test cases for each bias type. Additionally, we present de-biasing dataset construction methods and the associated preference dataset OffsetBias. Experimental results demonstrate that fine-tuning on our dataset significantly enhances the robustness of judge models against biases and improves performance across most evaluation scenarios. We release our datasets and the fine-tuned judge model to public.",
            "link": "https://www.semanticscholar.org/paper/dc47abf03e644afa0bdc834bab11018d3804e3c8",
            "authors": "Junsoo Park, Seungyeon Jwa, Meiying Ren, Daeyoung Kim, Sanghyuk Choi",
            "matchScore": 227.90596,
            "original title": "OffsetBias: Leveraging Debiased Data for Tuning Evaluators",
            "original authors": "Junsoo Park, Seungyeon Jwa, REN MEIYING, Daeyoung Kim, Sanghyuk Choi",
            "EMNLP Paper ID": "226",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "3abd61d29c47949b2445b9eed08512500ebb6380",
            "title": "Cognitive Bias in Decision-Making with LLMs",
            "abstract": "Large language models (LLMs) offer significant potential as tools to support an expanding range of decision-making tasks. Given their training on human (created) data, LLMs have been shown to inherit societal biases against protected groups, as well as be subject to bias functionally resembling cognitive bias. Human-like bias can impede fair and explainable decisions made with LLM assistance. Our work introduces BiasBuster, a framework designed to uncover, evaluate, and mitigate cognitive bias in LLMs, particularly in high-stakes decision-making tasks. Inspired by prior research in psychology and cognitive science, we develop a dataset containing 13,465 prompts to evaluate LLM decisions on different cognitive biases (e.g., prompt-induced, sequential, inherent). We test various bias mitigation strategies, while proposing a novel method utilizing LLMs to debias their own human-like cognitive bias within prompts. Our analysis provides a comprehensive picture of the presence and effects of cognitive bias across commercial and open-source models. We demonstrate that our selfhelp debiasing effectively mitigates model answers that display patterns akin to human cognitive bias without having to manually craft examples for each bias.",
            "link": "https://www.semanticscholar.org/paper/3abd61d29c47949b2445b9eed08512500ebb6380",
            "authors": "J. Echterhoff, Yao Liu, Abeer Alessa, Julian McAuley, Zexue He",
            "matchScore": 161.98242,
            "original title": "Cognitive Bias in Decision-Making with LLMs",
            "original authors": "Jessica Maria Echterhoff, Yao Liu, Abeer Alessa, Julian McAuley, Zexue He",
            "EMNLP Paper ID": "2469",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "b9e96c330c8e458c02c57c0b8ae38f8a667eb316",
            "title": "A Unified Framework and Dataset for Assessing Societal Bias in Vision-Language Models",
            "abstract": "Vision-language models (VLMs) have gained widespread adoption in both industry and academia. In this study, we propose a unified framework for systematically evaluating gender, race, and age biases in VLMs with respect to professions. Our evaluation encompasses all supported inference modes of the recent VLMs, including image-to-text, text-to-text, text-to-image, and image-to-image. Additionally, we propose an automated pipeline to generate high-quality synthetic datasets that intentionally conceal gender, race, and age information across different professional domains, both in generated text and images. The dataset includes action-based descriptions of each profession and serves as a benchmark for evaluating societal biases in vision-language models (VLMs). In our comparative analysis of widely used VLMs, we have identified that varying input-output modalities lead to discernible differences in bias magnitudes and directions. Additionally, we find that VLM models exhibit distinct biases across different bias attributes we investigated. We hope our work will help guide future progress in improving VLMs to learn socially unbiased representations. We will release our data and code.",
            "link": "https://www.semanticscholar.org/paper/b9e96c330c8e458c02c57c0b8ae38f8a667eb316",
            "authors": "Ashutosh Sathe, Prachi Jain, Sunayana Sitaram",
            "matchScore": 253.27496,
            "original title": "A Unified Framework and Dataset for Assessing Societal Bias in Vision-Language Models",
            "original authors": "Ashutosh Sathe, Prachi Jain, Sunayana Sitaram",
            "EMNLP Paper ID": "252",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "2cff7b52c90c9e2c7def0e7ea54a73e55d07d25f",
            "title": "Does Context Help Mitigate Gender Bias in Neural Machine Translation?",
            "abstract": "Neural Machine Translation models tend to perpetuate gender bias present in their training data distribution. Context-aware models have been previously suggested as a means to mitigate this type of bias. In this work, we examine this claim by analysing in detail the translation of stereotypical professions in English to German, and translation with non-informative context in Basque to Spanish. Our results show that, although context-aware models can significantly enhance translation accuracy for feminine terms, they can still maintain or even amplify gender bias. These results highlight the need for more fine-grained approaches to bias mitigation in Neural Machine Translation.",
            "link": "https://www.semanticscholar.org/paper/2cff7b52c90c9e2c7def0e7ea54a73e55d07d25f",
            "authors": "Harritxu Gete, Thierry Etchegoyhen",
            "matchScore": 266.50546,
            "original title": "Does Context Help Mitigate Gender Bias in Neural Machine Translation?",
            "original authors": "Harritxu Gete, Thierry Etchegoyhen",
            "EMNLP Paper ID": "2845",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "ff89469c3121e504259b67f0bc96deddc4187cbe",
            "title": "Evaluating Gender Bias of LLMs in Making Morality Judgements",
            "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in a multitude of Natural Language Processing (NLP) tasks. However, these models are still not immune to limitations such as social biases, especially gender bias. This work investigates whether current closed and open-source LLMs possess gender bias, especially when asked to give moral opinions. To evaluate these models, we curate and introduce a new dataset GenMO (Gender-bias in Morality Opinions) comprising parallel short stories featuring male and female characters respectively. Specifically, we test models from the GPT family (GPT-3.5-turbo, GPT-3.5-turbo-instruct, GPT-4-turbo), Llama 3 and 3.1 families (8B/70B), Mistral-7B and Claude 3 families (Sonnet and Opus). Surprisingly, despite employing safety checks, all production-standard models we tested display significant gender bias with GPT-3.5-turbo giving biased opinions in 24% of the samples. Additionally, all models consistently favour female characters, with GPT showing bias in 68-85% of cases and Llama 3 in around 81-85% instances. Additionally, our study investigates the impact of model parameters on gender bias and explores real-world situations where LLMs reveal biases in moral decision-making.",
            "link": "https://www.semanticscholar.org/paper/ff89469c3121e504259b67f0bc96deddc4187cbe",
            "authors": "Divij Bajaj, Yuanyuan Lei, Jonathan Tong, Ruihong Huang",
            "matchScore": 247.05716,
            "original title": "Evaluating Gender Bias of LLMs in Making Morality Judgements",
            "original authors": "Divij Bajaj, Yuanyuan Lei, Jonathan Tong, Ruihong Huang",
            "EMNLP Paper ID": "3041",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "27bb304d93aab9dd4d12f0d15a204df5674545ac",
            "title": "Women Are Beautiful, Men Are Leaders: Gender Stereotypes in Machine Translation and Language Modeling",
            "abstract": "We present GEST -- a new manually created dataset designed to measure gender-stereotypical reasoning in language models and machine translation systems. GEST contains samples for 16 gender stereotypes about men and women (e.g., Women are beautiful, Men are leaders) that are compatible with the English language and 9 Slavic languages. The definition of said stereotypes was informed by gender experts. We used GEST to evaluate English and Slavic masked LMs, English generative LMs, and machine translation systems. We discovered significant and consistent amounts of gender-stereotypical reasoning in almost all the evaluated models and languages. Our experiments confirm the previously postulated hypothesis that the larger the model, the more stereotypical it usually is.",
            "link": "https://www.semanticscholar.org/paper/27bb304d93aab9dd4d12f0d15a204df5674545ac",
            "authors": "Mat\u00fa\u0161 Pikuliak, Andrea Hrckova, Stefan Oresko, Mari\u00e1n Simko",
            "matchScore": 269.4903,
            "original title": "Women Are Beautiful, Men Are Leaders: Gender Stereotypes in Machine Translation and Language Modeling",
            "original authors": "Mat\u00fa\u0161 Pikuliak, Stefan Oresko, Andrea Hrckova, Marian Simko",
            "EMNLP Paper ID": "621",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 14): 13:00\u201314:00 (Afternoon)",
            "Type": "Findings"
        },
        {
            "paperId": "5b403776ac18823500865fb84c32f03bbdc4e8b2",
            "title": "JobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models",
            "abstract": "The use of Large Language Models (LLMs) in hiring has led to legislative actions to protect vulnerable demographic groups. This paper presents a novel framework for benchmarking hierarchical gender hiring bias in Large Language Models (LLMs) for resume scoring, revealing significant issues of reverse gender hiring bias and overdebiasing. Our contributions are fourfold: Firstly, we introduce a new construct grounded in labour economics, legal principles, and critiques of current bias benchmarks: hiring bias can be categorized into two types: Level bias (difference in the average outcomes between demographic counterfactual groups) and Spread bias (difference in the variance of outcomes between demographic counterfactual groups); Level bias can be further subdivided into statistical bias (i.e. changing with non-demographic content) and taste-based bias (i.e. consistent regardless of non-demographic content). Secondly, the framework includes rigorous statistical and computational hiring bias metrics, such as Rank After Scoring (RAS), Rank-based Impact Ratio, Permutation Test, and Fixed Effects Model. Thirdly, we analyze gender hiring biases in ten state-of-the-art LLMs. Seven out of ten LLMs show significant biases against males in at least one industry. An industry-effect regression reveals that the healthcare industry is the most biased against males. Moreover, we found that the bias performance remains invariant with resume content for eight out of ten LLMs. This indicates that the bias performance measured in this paper might apply to other resume datasets with different resume qualities. Fourthly, we provide a user-friendly demo and resume dataset to support the adoption and practical use of the framework, which can be generalized to other social traits and tasks.",
            "link": "https://www.semanticscholar.org/paper/5b403776ac18823500865fb84c32f03bbdc4e8b2",
            "authors": "Ze Wang, Zekun Wu, Xin Guan, Michael Thaler, A. Koshiyama, Skylar Lu, Sachin Beepath, Ediz Ertekin, Maria Perez-Ortiz",
            "matchScore": 256.3742,
            "original title": "JobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models",
            "original authors": "Ze Wang, Zekun Wu, Xin Guan, Michael Thaler, Adriano Koshiyama, Skylar Lu, Sachin Beepath, Ediz Ertekin, Maria Perez-Ortiz",
            "EMNLP Paper ID": "662",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "30993590c5dd8e5d2f37be1d565b7538250a7b5b",
            "title": "Divine LLaMAs: Bias, Stereotypes, Stigmatization, and Emotion Representation of Religion in Large Language Models",
            "abstract": "Emotions play important epistemological and cognitive roles in our lives, revealing our values and guiding our actions. Previous work has shown that LLMs display biases in emotion attribution along gender lines. However, unlike gender, which says little about our values, religion, as a socio-cultural system, prescribes a set of beliefs and values for its followers. Religions, therefore, cultivate certain emotions. Moreover, these rules are explicitly laid out and interpreted by religious leaders. Using emotion attribution, we explore how different religions are represented in LLMs. We find that: Major religions in the US and European countries are represented with more nuance, displaying a more shaded model of their beliefs. Eastern religions like Hinduism and Buddhism are strongly stereotyped. Judaism and Islam are stigmatized -- the models' refusal skyrocket. We ascribe these to cultural bias in LLMs and the scarcity of NLP literature on religion. In the rare instances where religion is discussed, it is often in the context of toxic language, perpetuating the perception of these religions as inherently toxic. This finding underscores the urgent need to address and rectify these biases. Our research underscores the crucial role emotions play in our lives and how our values influence them.",
            "link": "https://www.semanticscholar.org/paper/30993590c5dd8e5d2f37be1d565b7538250a7b5b",
            "authors": "Flor Miriam Plaza del Arco, A. C. Curry, Susanna Paoli, Alba Curry, Dirk Hovy",
            "matchScore": 306.83124,
            "original title": "Divine LLaMAs: Bias, Stereotypes, Stigmatization, and Emotion Representation of Religion in Large Language Models",
            "original authors": "Flor Miriam Plaza-del-Arco, Amanda Cercas Curry, Susanna Paoli, Alba Cercas Curry, Dirk Hovy",
            "EMNLP Paper ID": "866",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        }
    ],
    "Preference and Reinforcement Learning Optimization for Large Language Models": [
        {
            "paperId": "6f8c4311e65efebb9da5a542c0405684e82a77cc",
            "title": "Mitigating the Alignment Tax of RLHF",
            "abstract": "LLMs acquire a wide range of abilities during pre-training, but aligning LLMs under Reinforcement Learning with Human Feedback (RLHF) can lead to forgetting pretrained abilities, which is also known as the alignment tax. To investigate alignment tax, we conducted experiments with existing RLHF algorithms using OpenLLaMA-3B, which revealed a pronounced alignment tax in NLP tasks. Whereas, despite various techniques to mitigate forgetting, they are often at odds with the RLHF performance, leading to a trade-off between alignment performance and forgetting mitigation, leading to an alignment-forgetting trade-off. In this paper we show that model averaging, which simply interpolates between pre and post RLHF model weights, surprisingly achieves the most strongest alignment-forgetting Pareto front among a wide range of competing methods. To understand its effectiveness, we offer theoretical insights into model averaging, revealing that it enhances performance Pareto front by increasing feature diversity on the layers where tasks share overlapped feature spaces. Empirical evidence corroborates our analysis by showing the benefits of averaging low-level transformer layers. Building on the analysis and the observation that averaging different layers of the transformer leads to significantly different alignment-forgetting trade-offs, we propose Heterogeneous Model Averaging (HMA) to Heterogeneously find various combination ratios of model layers. HMA seeks to maximize the alignment performance while incurring minimal alignment tax. Moreover, we validate HMA's performance across a range of RLHF algorithms over OpenLLaMA-3B and further extend our findings to Mistral-7B which is evaluated by open-sourced preference model and GPT4. Code available here: https://github.com/avalonstrel/Mitigating-the-Alignment-Tax-of-RLHF.git.",
            "link": "https://www.semanticscholar.org/paper/6f8c4311e65efebb9da5a542c0405684e82a77cc",
            "authors": "Yong Lin, Lu Tan, Hangyu Lin, Wei Xiong, Zeming Zheng, Renjie Pi, Jipeng Zhang, Shizhe Diao, Haoxiang Wang, Hanze Dong, Han Zhao, Yuan Yao, T. Zhang",
            "EMNLP Paper ID": "80",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "7be0c002db797445ba4201ef56a32b51a203924f",
            "title": "Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence",
            "abstract": "Direct Preference Optimization (DPO) has emerged as a prominent algorithm for the direct and robust alignment of Large Language Models (LLMs) with human preferences, offering a more straightforward alternative to the complex Reinforcement Learning from Human Feedback (RLHF). Despite its promising efficacy, DPO faces a notable drawback:\"verbosity\", a common over-optimization phenomenon also observed in RLHF. While previous studies mainly attributed verbosity to biased labels within the data, we propose that the issue also stems from an inherent algorithmic length reliance in DPO. Specifically, we suggest that the discrepancy between sequence-level Kullback-Leibler (KL) divergences between chosen and rejected sequences, used in DPO, results in overestimated or underestimated rewards due to varying token lengths. Empirically, we utilize datasets with different label lengths to demonstrate the presence of biased rewards. We then introduce an effective downsampling approach, named SamPO, to eliminate potential length reliance. Our experimental evaluations, conducted across three LLMs of varying scales and a diverse array of conditional and open-ended benchmarks, highlight the efficacy of SamPO in mitigating verbosity, achieving improvements of 5% to 12% over DPO through debaised rewards. Our codes can be accessed at: https://github.com/LuJunru/SamPO/.",
            "link": "https://www.semanticscholar.org/paper/7be0c002db797445ba4201ef56a32b51a203924f",
            "authors": "Junru Lu, Jiazheng Li, Siyu An, Meng Zhao, Yulan He, Di Yin, Xing Sun",
            "EMNLP Paper ID": "132",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "f533bbe34eea2c3346c7d67149c62c4761fc5248",
            "title": "Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment",
            "abstract": "Aligning language models (LMs) based on human-annotated preference data is a crucial step in obtaining practical and performant LM-based systems. However, multilingual human preference data are difficult to obtain at scale, making it challenging to extend this framework to diverse languages. In this work, we evaluate a simple approach for zero-shot cross-lingual alignment, where a reward model is trained on preference data in one source language and directly applied to other target languages. On summarization and open-ended dialog generation, we show that this method is consistently successful under comprehensive evaluation settings, including human evaluation: cross-lingually aligned models are preferred by humans over unaligned models on up to>70% of evaluation instances. We moreover find that a different-language reward model sometimes yields better aligned models than a same-language reward model. We also identify best practices when there is no language-specific data for even supervised finetuning, another component in alignment.",
            "link": "https://www.semanticscholar.org/paper/f533bbe34eea2c3346c7d67149c62c4761fc5248",
            "authors": "Zhaofeng Wu, Ananth Balashankar, Yoon Kim, Jacob Eisenstein, Ahmad Beirami",
            "EMNLP Paper ID": "162",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "6fe7e6ce3cc0ebd038caa456d73fd7472e7d6c38",
            "title": "Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment",
            "abstract": "Alignment in artificial intelligence pursues the consistency between model responses and human preferences as well as values. In practice, the multifaceted nature of human preferences inadvertently introduces what is known as the\"alignment tax\"-a compromise where enhancements in alignment within one objective (e.g.,harmlessness) can diminish performance in others (e.g.,helpfulness). However, existing alignment techniques are mostly unidirectional, leading to suboptimal trade-offs and poor flexibility over various objectives. To navigate this challenge, we argue the prominence of grounding LLMs with evident preferences. We introduce controllable preference optimization (CPO), which explicitly specifies preference scores for different objectives, thereby guiding the model to generate responses that meet the requirements. Our experimental analysis reveals that the aligned models can provide responses that match various preferences among the\"3H\"(helpfulness, honesty, harmlessness) desiderata. Furthermore, by introducing diverse data and alignment goals, we surpass baseline methods in aligning with single objectives, hence mitigating the impact of the alignment tax and achieving improvements in multi-objective alignment.",
            "link": "https://www.semanticscholar.org/paper/6fe7e6ce3cc0ebd038caa456d73fd7472e7d6c38",
            "authors": "Yiju Guo, Ganqu Cui, Lifan Yuan, Ning Ding, Jiexin Wang, Huimin Chen, Bowen Sun, Ruobing Xie, Jie Zhou, Yankai Lin, Zhiyuan Liu, Maosong Sun",
            "EMNLP Paper ID": "171",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "3b5778b0aec88c1921fee7538400e49bed42a5fa",
            "title": "Direct Multi-Turn Preference Optimization for Language Agents",
            "abstract": "Adapting Large Language Models (LLMs) for agent tasks is critical in developing language agents. Direct Preference Optimization (DPO) is a promising technique for this adaptation with the alleviation of compounding errors, offering a means to directly optimize Reinforcement Learning (RL) objectives. However, applying DPO to multi-turn tasks presents challenges due to the inability to cancel the partition function. Overcoming this obstacle involves making the partition function independent of the current state and addressing length disparities between preferred and dis-preferred trajectories. In this light, we replace the policy constraint with the state-action occupancy measure constraint in the RL objective and add length normalization to the Bradley-Terry model, yielding a novel loss function named DMPO for multi-turn agent tasks with theoretical explanations. Extensive experiments on three multi-turn agent task datasets confirm the effectiveness and superiority of the DMPO loss.",
            "link": "https://www.semanticscholar.org/paper/3b5778b0aec88c1921fee7538400e49bed42a5fa",
            "authors": "Wentao Shi, Mengqi Yuan, Junkang Wu, Qifan Wang, Fuli Feng",
            "EMNLP Paper ID": "261",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "5f39f57b175a27930dfe26aafc224dab8f34f8fc",
            "title": "Word Alignment as Preference for Machine Translation",
            "abstract": "The problem of hallucination and omission, a long-standing problem in machine translation (MT), is more pronounced when a large language model (LLM) is used in MT because an LLM itself is susceptible to these phenomena. In this work, we mitigate the problem in an LLM-based MT model by guiding it to better word alignment. We first study the correlation between word alignment and the phenomena of hallucination and omission in MT. Then we propose to utilize word alignment as preference to optimize the LLM-based MT model. The preference data are constructed by selecting chosen and rejected translations from multiple MT tools. Subsequently, direct preference optimization is used to optimize the LLM-based model towards the preference signal. Given the absence of evaluators specifically designed for hallucination and omission in MT, we further propose selecting hard instances and utilizing GPT-4 to directly evaluate the performance of the models in mitigating these issues. We verify the rationality of these designed evaluation methods by experiments, followed by extensive results demonstrating the effectiveness of word alignment-based preference optimization to mitigate hallucination and omission.",
            "link": "https://www.semanticscholar.org/paper/5f39f57b175a27930dfe26aafc224dab8f34f8fc",
            "authors": "Qiyu Wu, Masaaki Nagata, Zhongtao Miao, Yoshimasa Tsuruoka",
            "EMNLP Paper ID": "359",
            "Oral/Poster": "Oral",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "c52e066ff7ad98e7a7db5194f10c14d5666fc5f3",
            "title": "Unsupervised Human Preference Learning",
            "abstract": "Large language models demonstrate impressive reasoning abilities but struggle to provide personalized content due to their lack of individual user preference information. Existing methods, such as in-context learning and parameter-efficient fine-tuning, fall short in capturing the complexity of human preferences, especially given the small, personal datasets individuals possess. In this paper, we propose a novel approach utilizing small parameter models as preference agents to generate natural language rules that guide a larger, pre-trained model, enabling efficient personalization. Our method involves a small, local\"steering wheel\"model that directs the outputs of a much larger foundation model, producing content tailored to an individual's preferences while leveraging the extensive knowledge and capabilities of the large model. Importantly, this personalization is achieved without the need to fine-tune the large model. Experimental results on email and article datasets, demonstrate that our technique significantly outperforms baseline personalization methods. By allowing foundation models to adapt to individual preferences in a data and compute-efficient manner, our approach paves the way for highly personalized language model applications.",
            "link": "https://www.semanticscholar.org/paper/c52e066ff7ad98e7a7db5194f10c14d5666fc5f3",
            "authors": "Sumuk Shashidhar, Abhinav Chinta, Vaibhav Sahai, Dilek Hakkani-Tur",
            "EMNLP Paper ID": "390",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "1ccdc5070da2ffc33dff7a6d6dfff7bb96501474",
            "title": "EPO: Hierarchical LLM Agents with Environment Preference Optimization",
            "abstract": "Long-horizon decision-making tasks present significant challenges for LLM-based agents due to the need for extensive planning over multiple steps. In this paper, we propose a hierarchical framework that decomposes complex tasks into manageable subgoals, utilizing separate LLMs for subgoal prediction and low-level action generation. To address the challenge of creating training signals for unannotated datasets, we develop a reward model that leverages multimodal environment feedback to automatically generate reward signals. We introduce Environment Preference Optimization (EPO), a novel method that generates preference signals from the environment's feedback and uses them to train LLM-based agents. Extensive experiments on ALFRED demonstrate the state-of-the-art performance of our framework, achieving first place on the ALFRED public leaderboard and showcasing its potential to improve long-horizon decision-making in diverse environments.",
            "link": "https://www.semanticscholar.org/paper/1ccdc5070da2ffc33dff7a6d6dfff7bb96501474",
            "authors": "Qi Zhao, Haotian Fu, Chen Sun, G. Konidaris",
            "EMNLP Paper ID": "720",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "561ba2b6bc20cf02e4a697cca48ce9d1aa2b59b6",
            "title": "Reinforcement Learning with Dynamic Multi-Reward Weighting for Multi-Style Controllable Generation",
            "abstract": "Textual style expresses a diverse set of information, including interpersonal dynamics (e.g., formality) and the author's emotions or attitudes (e.g., disgust). An open question is how language models can be explicitly controlled so that they weave together target styles when generating text: for example, to produce text that is both negative and non-toxic. One approach to such controlled generation is multi-objective reinforcement learning (RL), but how best to combine multiple objectives in a reward function is an open question. In this paper, we investigate various formulations of multi-style rewards, including calibrated outputs from discriminators and dynamic weighting by discriminator gradient magnitudes. We find that our proposed dynamic weighting outperforms static weighting approaches with respect to style control while maintaining linguistic quality, and we explore its effectiveness in 2- and 3-style control.",
            "link": "https://www.semanticscholar.org/paper/561ba2b6bc20cf02e4a697cca48ce9d1aa2b59b6",
            "authors": "Karin de Langis, Ryan Koo, Dongyeop Kang",
            "EMNLP Paper ID": "756",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "fd655992d1b0220e16004ca39774e9390fb28cee",
            "title": "mDPO: Conditional Preference Optimization for Multimodal Large Language Models",
            "abstract": "Direct preference optimization (DPO) has shown to be an effective method for large language model (LLM) alignment. Recent works have attempted to apply DPO to multimodal scenarios but have found it challenging to achieve consistent improvement. Through a comparative experiment, we identify the unconditional preference problem in multimodal preference optimization, where the model overlooks the image condition. To address this problem, we propose mDPO, a multimodal DPO objective that prevents the over-prioritization of language-only preferences by also optimizing image preference. Moreover, we introduce a reward anchor that forces the reward to be positive for chosen responses, thereby avoiding the decrease in their likelihood -- an intrinsic problem of relative preference optimization. Experiments on two multimodal LLMs of different sizes and three widely used benchmarks demonstrate that mDPO effectively addresses the unconditional preference problem in multimodal preference optimization and significantly improves model performance, particularly in reducing hallucination.",
            "link": "https://www.semanticscholar.org/paper/fd655992d1b0220e16004ca39774e9390fb28cee",
            "authors": "Fei Wang, Wenxuan Zhou, James Y. Huang, Nan Xu, Sheng Zhang, Hoifung Poon, Muhao Chen",
            "EMNLP Paper ID": "922",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "78a2943fd2424a5515d595d6bdc54b9a4dbb4389",
            "title": "WPO: Enhancing RLHF with Weighted Preference Optimization",
            "abstract": "Reinforcement learning from human feedback (RLHF) is a promising solution to align large language models (LLMs) more closely with human values. Off-policy preference optimization, where the preference data is obtained from other models, is widely adopted due to its cost efficiency and scalability. However, off-policy preference optimization often suffers from a distributional gap between the policy used for data collection and the target policy, leading to suboptimal optimization. In this paper, we propose a novel strategy to mitigate this problem by simulating on-policy learning with off-policy preference data. Our Weighted Preference Optimization (WPO) method adapts off-policy data to resemble on-policy data more closely by reweighting preference pairs according to their probability under the current policy. This method not only addresses the distributional gap problem but also enhances the optimization process without incurring additional costs. We validate our method on instruction following benchmarks including Alpaca Eval 2 and MT-bench. WPO not only outperforms Direct Preference Optimization (DPO) by up to 5.6% on Alpaca Eval 2 but also establishes a remarkable length-controlled winning rate against GPT-4-turbo of 76.7% based on Gemma-2-9b-it. We release the code and models at https://github.com/wzhouad/WPO.",
            "link": "https://www.semanticscholar.org/paper/78a2943fd2424a5515d595d6bdc54b9a4dbb4389",
            "authors": "Wenxuan Zhou, Ravi Agrawal, Shujian Zhang, Sathish Indurthi, Sanqiang Zhao, Kaiqiang Song, Silei Xu, Chenguang Zhu",
            "EMNLP Paper ID": "967",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "973814cd535facbf4f27c3de477b05bf19366030",
            "title": "ORPO: Monolithic Preference Optimization without Reference Model",
            "abstract": "While recent preference alignment algorithms for language models have demonstrated promising results, supervised fine-tuning (SFT) remains imperative for achieving successful convergence. In this paper, we study the crucial role of SFT within the context of preference alignment, emphasizing that a minor penalty for the disfavored generation style is sufficient for preference-aligned SFT. Building on this foundation, we introduce a straightforward and innovative reference model-free monolithic odds ratio preference optimization algorithm, ORPO, eliminating the necessity for an additional preference alignment phase. We demonstrate, both empirically and theoretically, that the odds ratio is a sensible choice for contrasting favored and disfavored styles during SFT across the diverse sizes from 125M to 7B. Specifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with ORPO on the UltraFeedback alone surpasses the performance of state-of-the-art language models with more than 7B and 13B parameters: achieving up to 12.20% on $\\text{AlpacaEval}_{2.0}$ (Figure 1), 66.19% on IFEval (instruction-level loose, Table 6), and 7.32 in MT-Bench (Figure 12). We release code and model checkpoints for Mistral-ORPO-$\\alpha$ (7B) and Mistral-ORPO-$\\beta$ (7B).",
            "link": "https://www.semanticscholar.org/paper/973814cd535facbf4f27c3de477b05bf19366030",
            "authors": "Jiwoo Hong, Noah Lee, James Thorne",
            "EMNLP Paper ID": "1294",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "3da5f21144fef19dd88f7dcc11a5d9f2edbfe417",
            "title": "RLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs",
            "abstract": "Preference optimization techniques have become a standard final stage for training state-of-art large language models (LLMs). However, despite widespread adoption, the vast majority of work to-date has focused on first-class citizen languages like English and Chinese. This captures a small fraction of the languages in the world, but also makes it unclear which aspects of current state-of-the-art research transfer to a multilingual setting. In this work, we perform an exhaustive study to achieve a new state-of-the-art in aligning multilingual LLMs. We introduce a novel, scalable method for generating high-quality multilingual feedback data to balance data coverage. We establish the benefits of cross-lingual transfer and increased dataset size in preference training. Our preference-trained model achieves a 54.4% win-rate against Aya 23 8B, the current state-of-the-art multilingual LLM in its parameter class, and a 69.5% win-rate or higher against widely used models like Gemma-1.1-7B-it, Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.3. As a result of our study, we expand the frontier of alignment techniques to 23 languages covering half of the world's population.",
            "link": "https://www.semanticscholar.org/paper/3da5f21144fef19dd88f7dcc11a5d9f2edbfe417",
            "authors": "John Dang, Arash Ahmadian, Kelly Marchisio, Julia Kreutzer, A. Ustun, Sara Hooker",
            "EMNLP Paper ID": "1519",
            "Oral/Poster": "Oral",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "b6f6fb54040fc9e667491c4468403af6f5743940",
            "title": "A Probability--Quality Trade-off in Aligned Language Models and its Relation to Sampling Adaptors",
            "abstract": "The relationship between the quality of a string, as judged by a human reader, and its probability, $p(\\boldsymbol{y})$ under a language model undergirds the development of better language models. For example, many popular algorithms for sampling from a language model have been conceived with the goal of manipulating $p(\\boldsymbol{y})$ to place higher probability on strings that humans deem of high quality. In this article, we examine the probability--quality relationship in language models explicitly aligned to human preferences, e.g., through reinforcement learning through human feedback. We show that, when sampling corpora from an aligned language model, there exists a trade-off between the strings' average reward and average log-likelihood under the prior language model, i.e., the same model before alignment with human preferences. We provide a formal treatment of this phenomenon and demonstrate how a choice of sampling adaptor allows for a selection of how much likelihood we exchange for the reward.",
            "link": "https://www.semanticscholar.org/paper/b6f6fb54040fc9e667491c4468403af6f5743940",
            "authors": "Naaman Tan, Josef Valvoda, Anej Svete, Tianyu Liu, Yanxia Qin, Kan Min-Yen, Ryan Cotterell",
            "EMNLP Paper ID": "1706",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "6f8d26bef8ac26f747a8bd9919109798f61eb0ed",
            "title": "DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging",
            "abstract": "Reinforcement learning from human feedback (RLHF) is a popular strategy for aligning large language models (LLMs) with desired behaviors. Reward modeling is a crucial step in RLHF. However, collecting paired preference data for training reward models is often costly and time-consuming, especially for domain-specific preferences requiring expert annotation. To address this challenge, we propose the \\textbf{Do}main knowled\\textbf{ge} merged \\textbf{R}eward \\textbf{M}odel (DogeRM), a novel framework that integrates domain-specific knowledge into a general reward model by model merging. The experiments demonstrate that DogeRM enhances performance across different benchmarks and provide a detailed analysis showcasing the effects of model merging, showing the great potential of facilitating model alignment.",
            "link": "https://www.semanticscholar.org/paper/6f8d26bef8ac26f747a8bd9919109798f61eb0ed",
            "authors": "Tzu-Han Lin, Chen-An Li, Hung-yi Lee, Yun-Nung Chen",
            "EMNLP Paper ID": "1817",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "0f1b1cd6309691ff5e5e7f90a8debef38c8c7117",
            "title": "GDPO: Learning to Directly Align Language Models with Diversity Using GFlowNets",
            "abstract": "A critical component of the current generation of language models is preference alignment, which aims to precisely control the model's behavior to meet human needs and values. The most notable among such methods is Reinforcement Learning with Human Feedback (RLHF) and its offline variant Direct Preference Optimization (DPO), both of which seek to maximize a reward model based on human preferences. In particular, DPO derives reward signals directly from the offline preference data, but in doing so overfits the reward signals and generates suboptimal responses that may contain human biases in the dataset. In this work, we propose a practical application of a diversity-seeking RL algorithm called GFlowNet-DPO (GDPO) in an offline preference alignment setting to curtail such challenges. Empirical results show GDPO can generate far more diverse responses than the baseline methods that are still relatively aligned with human values in dialog generation and summarization tasks.",
            "link": "https://www.semanticscholar.org/paper/0f1b1cd6309691ff5e5e7f90a8debef38c8c7117",
            "authors": "Oh Joon Kwon, Daiki E. Matsunaga, Kee-Eung Kim",
            "EMNLP Paper ID": "2023",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "0e3ffaf73eaf0e5579b0418118b1e7ffb1b91c92",
            "title": "FlipGuard: Defending Preference Alignment against Update Regression with Constrained Optimization",
            "abstract": "Recent breakthroughs in preference alignment have significantly improved Large Language Models' ability to generate texts that align with human preferences and values. However, current alignment metrics typically emphasize the post-hoc overall improvement, while overlooking a critical aspect: regression, which refers to the backsliding on previously correctly-handled data after updates. This potential pitfall may arise from excessive fine-tuning on already well-aligned data, which subsequently leads to over-alignment and degeneration. To address this challenge, we propose FlipGuard, a constrained optimization approach to detect and mitigate update regression with focal attention. Specifically, FlipGuard identifies performance degradation using a customized reward characterization and strategically enforces a constraint to encourage conditional congruence with the pre-aligned model during training. Comprehensive experiments demonstrate that FlipGuard effectively alleviates update regression while demonstrating excellent overall performance, with the added benefit of knowledge preservation while aligning preferences.",
            "link": "https://www.semanticscholar.org/paper/0e3ffaf73eaf0e5579b0418118b1e7ffb1b91c92",
            "authors": "Mingye Zhu, Yi Liu, Quan Wang, Junbo Guo, Zhendong Mao",
            "EMNLP Paper ID": "2058",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "8be81d531dfc4a1145474a1bb2f9c0cf15e19f45",
            "title": "Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration",
            "abstract": "While Reinforcement Learning from Human Feedback (RLHF) significantly enhances the generation quality of Large Language Models (LLMs), recent studies have raised concerns regarding the complexity and instability associated with the Proximal Policy Optimization (PPO) algorithm, proposing a series of order-based calibration methods as viable alternatives. This paper delves further into current order-based methods, examining their inefficiencies in utilizing reward values and addressing misalignment issues. Building upon these findings, we propose a novel \\textbf{V}alue-based \\textbf{C}ali\\textbf{B}ration (VCB) method to better align LLMs with human preferences. Experimental results demonstrate that VCB surpasses existing alignment methods on AI assistant and summarization datasets, providing impressive generalizability, robustness, and stability in diverse settings.",
            "link": "https://www.semanticscholar.org/paper/8be81d531dfc4a1145474a1bb2f9c0cf15e19f45",
            "authors": "Xin Mao, Fengming Li, Huimin Xu, Wei Zhang, A. Luu",
            "EMNLP Paper ID": "2110",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "2673597d9198133de20bfbdb47aee68abe5110a9",
            "title": "Model-based Preference Optimization in Abstractive Summarization without Human Feedback",
            "abstract": "In abstractive summarization, the challenge of producing concise and accurate summaries arises from the vast amount of information contained in the source document. Consequently, although Large Language Models (LLMs) can generate fluent text, they often introduce inaccuracies by hallucinating content not found in the original source. While supervised fine-tuning methods that maximize likelihood contribute to this issue, they do not consistently enhance the faithfulness of the summaries. Preference-based optimization methods, such as Direct Preference Optimization (DPO), can further refine the model to align with human preferences. However, these methods still heavily depend on costly human feedback. In this work, we introduce a novel and straightforward approach called Model-based Preference Optimization (MPO) to fine-tune LLMs for improved summarization abilities without any human feedback. By leveraging the model's inherent summarization capabilities, we create a preference dataset that is fully generated by the model using different decoding strategies. Our experiments on standard summarization datasets and various metrics demonstrate that our proposed MPO significantly enhances the quality of generated summaries without relying on human feedback.",
            "link": "https://www.semanticscholar.org/paper/2673597d9198133de20bfbdb47aee68abe5110a9",
            "authors": "Jaepill Choi, Kyubyung Chae, Jiwoo Song, Yohan Jo, Taesup Kim",
            "EMNLP Paper ID": "2349",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "370205dcfdab72c5551ea59b516097d5c3ee414a",
            "title": "Jump Starting Bandits with LLM-Generated Prior Knowledge",
            "abstract": "We present substantial evidence demonstrating the benefits of integrating Large Language Models (LLMs) with a Contextual Multi-Armed Bandit framework. Contextual bandits have been widely used in recommendation systems to generate personalized suggestions based on user-specific contexts. We show that LLMs, pre-trained on extensive corpora rich in human knowledge and preferences, can simulate human behaviours well enough to jump-start contextual multi-armed bandits to reduce online learning regret. We propose an initialization algorithm for contextual bandits by prompting LLMs to produce a pre-training dataset of approximate human preferences for the bandit. This significantly reduces online learning regret and data-gathering costs for training such models. Our approach is validated empirically through two sets of experiments with different bandit setups: one which utilizes LLMs to serve as an oracle and a real-world experiment utilizing data from a conjoint survey experiment.",
            "link": "https://www.semanticscholar.org/paper/370205dcfdab72c5551ea59b516097d5c3ee414a",
            "authors": "P. A. Alamdari, Yanshuai Cao, Kevin H. Wilson",
            "EMNLP Paper ID": "2578",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "bddd8187e5e07e8cfc9e430ddbafb851fb63457f",
            "title": "Rethinking the Role of Proxy Rewards in Language Model Alignment",
            "abstract": "Learning from human feedback via proxy reward modeling has been studied to align Large Language Models (LLMs) with human values. However, achieving reliable training through that proxy reward model (RM) is not a trivial problem, and its behavior remained as a black-box. In this paper, we study the role of proxy rewards in the LLM alignment via `reverse reward engineering' by composing interpretable features as a white-box reward function. We aim to replicate the ground truth (gold) reward signal by achieving a monotonic relationship between the proxy and gold reward signals after training the model using the proxy reward in reinforcement learning (RL). Our findings indicate that successfully emulating the gold reward requires generating responses that are relevant with enough length to open-ended questions, while also ensuring response consistency in closed-ended questions. Furthermore, resulting models optimizing our devised white-box reward show competitive performances with strong open-source RMs in alignment benchmarks. We highlight its potential usage as a simple but strong reward baseline for the LLM alignment, not requiring explicit human feedback dataset and RM training. Our code is available at https://github.com/naver-ai/rethinking-proxy-reward.",
            "link": "https://www.semanticscholar.org/paper/bddd8187e5e07e8cfc9e430ddbafb851fb63457f",
            "authors": "Sungdong Kim, Minjoon Seo",
            "EMNLP Paper ID": "2737",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "58fdc0be6dad25510ff7663b1cf8707c4496d08c",
            "title": "Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a supervised-friendly fashion",
            "abstract": "Reinforcement Learning (RL) has been used to finetune Large Language Models (LLMs) using a reward model trained from preference data, to better align with human judgment. The recently introduced direct alignment methods, which are often simpler, more stable, and computationally lighter, can more directly achieve this. However, these approaches cannot optimize arbitrary rewards, and the preference-based ones are not the only rewards of interest for LLMs (eg., unit tests for code generation or textual entailment for summarization, among others). RL-finetuning is usually done with a variation of policy gradient, which calls for on-policy or near-on-policy samples, requiring costly generations. We introduce Contrastive Policy Gradient, or CoPG, a simple and mathematically principled new RL algorithm that can estimate the optimal policy even from off-policy data. It can be seen as an off-policy policy gradient approach that does not rely on important sampling techniques and highlights the importance of using (the right) state baseline. We show this approach to generalize the direct alignment method IPO (identity preference optimization) and classic policy gradient. We experiment with the proposed CoPG on a toy bandit problem to illustrate its properties, as well as for finetuning LLMs on a summarization task, using a learned reward function considered as ground truth for the purpose of the experiments.",
            "link": "https://www.semanticscholar.org/paper/58fdc0be6dad25510ff7663b1cf8707c4496d08c",
            "authors": "Yannis Flet-Berliac, Nathan Grinsztajn, Florian Strub, Eugene Choi, Chris Cremer, Arash Ahmadian, Yash Chandak, M. G. Azar, O. Pietquin, Matthieu Geist",
            "EMNLP Paper ID": "2930",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "282058c7b1f07b878c154f3b4245992fd2bbb15e",
            "title": "Preference-Guided Reflective Sampling for Aligning Language Models",
            "abstract": "Iterative data generation and model re-training can effectively align large language models(LLMs) to human preferences. The process of data sampling is crucial, as it significantly influences the success of policy improvement. Repeated random sampling is a widely used method that independently queries the model multiple times to generate outputs. In this work, we propose a more effective sampling method, named Preference-Guided Reflective Sampling (PRS). Unlike random sampling, PRS employs a tree-based generation framework to enable more efficient sampling. It leverages adaptive self-refinement techniques to better explore the sampling space. By specifying user preferences in natural language, PRS can further optimize response generation according to these preferences. As a result, PRS can align models to diverse user preferences. Our experiments demonstrate that PRS generates higher-quality responses with significantly higher rewards. On AlpacaEval and Arena-Hard, PRS substantially outperforms repeated random sampling in best-of-$N$ sampling. Moreover, PRS shows strong performance when applied in iterative offline RL training.",
            "link": "https://www.semanticscholar.org/paper/282058c7b1f07b878c154f3b4245992fd2bbb15e",
            "authors": "Hai Ye, Hwee Tou Ng",
            "EMNLP Paper ID": "2976",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "315a5cb5dbbbe2be8468b2bb7c62ea72af8930da",
            "title": "Filtered Direct Preference Optimization",
            "abstract": "Reinforcement learning from human feedback (RLHF) plays a crucial role in aligning language models with human preferences. While the significance of dataset quality is generally recognized, explicit investigations into its impact within the RLHF framework, to our knowledge, have been limited. This paper addresses the issue of text quality within the preference dataset by focusing on direct preference optimization (DPO), an increasingly adopted reward-model-free RLHF method. We confirm that text quality significantly influences the performance of models optimized with DPO more than those optimized with reward-model-based RLHF. Building on this new insight, we propose an extension of DPO, termed filtered direct preference optimization (fDPO). fDPO uses a trained reward model to monitor the quality of texts within the preference dataset during DPO training. Samples of lower quality are discarded based on comparisons with texts generated by the model being optimized, resulting in a more accurate dataset. Experimental results demonstrate that fDPO enhances the final model performance. Our code is available at https://github.com/CyberAgentAILab/filtered-dpo.",
            "link": "https://www.semanticscholar.org/paper/315a5cb5dbbbe2be8468b2bb7c62ea72af8930da",
            "authors": "Tetsuro Morimura, Mitsuki Sakamoto, Yuu Jinnai, Kenshi Abe, Kaito Air",
            "EMNLP Paper ID": "3285",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "13c2d7d6f79e9cc05b96e5c7bf420d0dfac42eaa",
            "title": "Not All Preference Pairs Are Created Equal: A Recipe for Annotation-Efficient Iterative Preference Learning",
            "abstract": "Iterative preference learning, though yielding superior performances, requires online annotated preference labels. In this work, we study strategies to select worth-annotating response pairs for cost-efficient annotation while achieving competitive or even better performances compared with the random selection baseline for iterative preference learning. Built on assumptions regarding uncertainty and distribution shifts, we propose a comparative view to rank the implicit reward margins as predicted by DPO to select the response pairs that yield more benefits. Through extensive experiments, we show that annotating those response pairs with small margins is generally better than large or random, under both single- and multi-iteration scenarios. Besides, our empirical results suggest allocating more annotation budgets in the earlier iterations rather than later across multiple iterations.",
            "link": "https://www.semanticscholar.org/paper/13c2d7d6f79e9cc05b96e5c7bf420d0dfac42eaa",
            "authors": "Sen Yang, Leyang Cui, Deng Cai, Xinting Huang, Shuming Shi, Wai Lam",
            "matchScore": 336.3566,
            "original title": "Not All Preference Pairs Are Created Equal: A Recipe for Annotation-Efficient Iterative Preference Learning",
            "original authors": "Sen Yang, Leyang Cui, Deng Cai, Xinting Huang, Shuming Shi, Wai Lam",
            "EMNLP Paper ID": "1332",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "d1eeba0dfd9a7d60bd390ed5bcfbefe6ff452d17",
            "title": "BAPO: Base-Anchored Preference Optimization for Personalized Alignment in Large Language Models",
            "abstract": "While learning to align Large Language Models (LLMs) with human preferences has shown remarkable success, aligning these models to meet the diverse user preferences presents further challenges in preserving previous knowledge. This paper examines the impact of personalized preference optimization on LLMs, revealing that the extent of knowledge loss varies significantly with preference heterogeneity. Although previous approaches have utilized the KL constraint between the reference model and the policy model, we observe that they fail to maintain general knowledge and alignment when facing personalized preferences. To this end, we introduce Base-Anchored Preference Optimization (BAPO), a simple yet effective approach that utilizes the initial responses of reference model to mitigate forgetting while accommodating personalized alignment. BAPO effectively adapts to diverse user preferences while minimally affecting global knowledge or general alignment. Our experiments demonstrate the efficacy of BAPO in various setups.",
            "link": "https://www.semanticscholar.org/paper/d1eeba0dfd9a7d60bd390ed5bcfbefe6ff452d17",
            "authors": "Gihun Lee, Minchan Jeong, Yujin Kim, Hojung Jung, Jaehoon Oh, Sangmook Kim, Se-young Yun",
            "matchScore": 211.18707,
            "original title": "BAPO: Base-Anchored Preference Optimization for Personalized Alignment in LLMs",
            "original authors": "Gihun Lee, Minchan Jeong, Yujin Kim, Hojung Jung, Jaehoon Oh, SangMook Kim, Se-Young Yun",
            "EMNLP Paper ID": "1392",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "ad08f6474d005eb7382ef57ecdf5151b24f36285",
            "title": "Semi-Supervised Reward Modeling via Iterative Self-Training",
            "abstract": "Reward models (RM) capture the values and preferences of humans and play a central role in Reinforcement Learning with Human Feedback (RLHF) to align pretrained large language models (LLMs). Traditionally, training these models relies on extensive human-annotated preference data, which poses significant challenges in terms of scalability and cost. To overcome these limitations, we propose Semi-Supervised Reward Modeling (SSRM), an approach that enhances RM training using unlabeled data. Given an unlabeled dataset, SSRM involves three key iterative steps: pseudo-labeling unlabeled examples, selecting high-confidence examples through a confidence threshold, and supervised finetuning on the refined dataset. Across extensive experiments on various model configurations, we demonstrate that SSRM significantly improves reward models without incurring additional labeling costs. Notably, SSRM can achieve performance comparable to models trained entirely on labeled data of equivalent volumes. Overall, SSRM substantially reduces the dependency on large volumes of human-annotated data, thereby decreasing the overall cost and time involved in training effective reward models.",
            "link": "https://www.semanticscholar.org/paper/ad08f6474d005eb7382ef57ecdf5151b24f36285",
            "authors": "Yifei He, Haoxiang Wang, Ziyan Jiang, Alexandros Papangelis, Han Zhao",
            "matchScore": 249.72772,
            "original title": "Semi-Supervised Reward Modeling via Iterative Self-Training",
            "original authors": "Yifei He, Haoxiang Wang, Ziyan Jiang, Alexandros Papangelis, Han Zhao",
            "EMNLP Paper ID": "1528",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "830c277b2992f59ec2f21982e245bd1e17dd85ca",
            "title": "Step-level Value Preference Optimization for Mathematical Reasoning",
            "abstract": "Direct Preference Optimization (DPO) using an implicit reward model has proven to be an effective alternative to reinforcement learning from human feedback (RLHF) for fine-tuning preference aligned large language models (LLMs). However, the overall preference annotations of responses do not fully capture the fine-grained quality of model outputs in complex multi-step reasoning tasks, such as mathematical reasoning. To address this limitation, we introduce a novel algorithm called Step-level Value Preference Optimization (SVPO). Our approach employs Monte Carlo Tree Search (MCTS) to automatically annotate step-level preferences for multi-step reasoning. Furthermore, from the perspective of learning-to-rank, we train an explicit value model to replicate the behavior of the implicit reward model, complementing standard preference optimization. This value model enables the LLM to generate higher reward responses with minimal cost during inference. Experimental results demonstrate that our method achieves state-of-the-art performance on both in-domain and out-of-domain mathematical reasoning benchmarks. Our code is available at \\url{https://github.com/MARIO-Math-Reasoning/Super_MARIO}.",
            "link": "https://www.semanticscholar.org/paper/830c277b2992f59ec2f21982e245bd1e17dd85ca",
            "authors": "Guoxin Chen, Minpeng Liao, Chengxi Li, Kai Fan",
            "matchScore": 231.57138,
            "original title": "Step-level Value Preference Optimization for Mathematical Reasoning",
            "original authors": "Guoxin Chen, Minpeng Liao, Chengxi Li, Kai Fan",
            "EMNLP Paper ID": "1653",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "4b6b3e540f2c76382b9ea745ea2263d47bedcb6e",
            "title": "Rewarding What Matters: Step-by-Step Reinforcement Learning for Task-Oriented Dialogue",
            "abstract": "Reinforcement learning (RL) is a powerful approach to enhance task-oriented dialogue (TOD) systems. However, existing RL methods tend to mainly focus on generation tasks, such as dialogue policy learning (DPL) or response generation (RG), while neglecting dialogue state tracking (DST) for understanding. This narrow focus limits the systems to achieve globally optimal performance by overlooking the interdependence between understanding and generation. Additionally, RL methods face challenges with sparse and delayed rewards, which complicates training and optimization. To address these issues, we extend RL into both understanding and generation tasks by introducing step-by-step rewards throughout the token generation. The understanding reward increases as more slots are correctly filled in DST, while the generation reward grows with the accurate inclusion of user requests. Our approach provides a balanced optimization aligned with task completion. Experimental results demonstrate that our approach effectively enhances the performance of TOD systems and achieves new state-of-the-art results on three widely used datasets, including MultiWOZ2.0, MultiWOZ2.1, and In-Car. Our approach also shows superior few-shot ability in low-resource settings compared to current models.",
            "link": "https://www.semanticscholar.org/paper/4b6b3e540f2c76382b9ea745ea2263d47bedcb6e",
            "authors": "Huifang Du, Shuqin Li, Minghao Wu, Xuejing Feng, Yuanzi Li, Haofen Wang",
            "matchScore": 272.65936,
            "original title": "Rewarding What Matters: Step-by-Step Reinforcement Learning for Task-Oriented Dialogue",
            "original authors": "Huifang Du, Shuqin Li, Minghao Wu, Xuejing Feng, Yuan-Fang Li, Haofen Wang",
            "EMNLP Paper ID": "1692",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "ee3c57d53327c5f84a8f3988f592c6e2479c1924",
            "title": "Inverse-Q*: Token Level Reinforcement Learning for Aligning Large Language Models Without Preference Data",
            "abstract": "Reinforcement Learning from Human Feedback (RLHF) has proven effective in aligning large language models with human intentions, yet it often relies on complex methodologies like Proximal Policy Optimization (PPO) that require extensive hyper-parameter tuning and present challenges in sample efficiency and stability. In this paper, we introduce Inverse-Q*, an innovative framework that transcends traditional RL methods by optimizing token-level reinforcement learning without the need for additional reward or value models. Inverse-Q* leverages direct preference optimization techniques but extends them by estimating the conditionally optimal policy directly from the model's responses, facilitating more granular and flexible policy shaping. Our approach reduces reliance on human annotation and external supervision, making it especially suitable for low-resource settings. We present extensive experimental results demonstrating that Inverse-Q* not only matches but potentially exceeds the effectiveness of PPO in terms of convergence speed and the alignment of model responses with human preferences. Our findings suggest that Inverse-Q* offers a practical and robust alternative to conventional RLHF approaches, paving the way for more efficient and adaptable model training approaches.",
            "link": "https://www.semanticscholar.org/paper/ee3c57d53327c5f84a8f3988f592c6e2479c1924",
            "authors": "Han Xia, Songyang Gao, Qiming Ge, Zhiheng Xi, Qi Zhang, Xuanjing Huang",
            "matchScore": 318.70377,
            "original title": "Inverse-Q*: Token Level Reinforcement Learning for Aligning Large Language Models Without Preference Data",
            "original authors": "Han Xia, Songyang Gao, Qiming Ge, Zhiheng Xi, Qi Zhang, Xuanjing Huang",
            "EMNLP Paper ID": "1718",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "f218583fdd398a0841eb7767e7bf21e90fc60f81",
            "title": "On Diversified Preferences of Large Language Model Alignment",
            "abstract": "Aligning large language models (LLMs) with human preferences has been recognized as the key to improving LLMs' interaction quality. However, in this pluralistic world, human preferences can be diversified due to annotators' different tastes, which hinders the effectiveness of LLM alignment methods. This paper presents the first quantitative analysis of the experimental scaling law for reward models with varying sizes, from 1.3 billion to 7 billion parameters, trained with human feedback exhibiting diverse preferences. Our analysis reveals that the impact of diversified human preferences depends on both model size and data size. Larger models with sufficient capacity mitigate the negative effects of diverse preferences, while smaller models struggle to accommodate them. To mitigate the impact of diverse preferences, we introduce a new metric, Expected Calibration Error (ECE), to evaluate RMs and show their obvious positive correlation with the alignment performance of LLMs. Furthermore, we propose a Multi-Objective Reward learning method (MORE) to enhance the calibration performance of RMs on shared preferences. Through experiments on four models and five human preference datasets, we find the calibration error can be adopted as a key metric for evaluating RMs and MORE can obtain superior alignment performance.",
            "link": "https://www.semanticscholar.org/paper/f218583fdd398a0841eb7767e7bf21e90fc60f81",
            "authors": "Dun Zeng, Yong Dai, Pengyu Cheng, Tianhao Hu, Wanshun Chen, Nan Du, Zenglin Xu",
            "matchScore": 202.24896,
            "original title": "On Diversified Preferences of Large Language Model Alignment",
            "original authors": "Dun Zeng, Yong Dai, Pengyu Cheng, Longyue Wang, Tianhao Hu, Wanshun CHEN, nan du, Zenglin Xu",
            "EMNLP Paper ID": "1920",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
            "title": "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts",
            "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as the primary method for aligning large language models (LLMs) with human preferences. The RLHF process typically starts by training a reward model (RM) using human preference data. Conventional RMs are trained on pairwise responses to the same user request, with relative ratings indicating which response humans prefer. The trained RM serves as a proxy for human preferences. However, due to the black-box nature of RMs, their outputs lack interpretability, as humans cannot intuitively understand why an RM thinks a response is good or not. As RMs act as human preference proxies, we believe they should be human-interpretable to ensure that their internal decision processes are consistent with human preferences and to prevent reward hacking in LLM alignment. To build RMs with interpretable preferences, we propose a two-stage approach: i) train an Absolute-Rating Multi-Objective Reward Model (ArmoRM) with multi-dimensional absolute-rating data, each dimension corresponding to a human-interpretable objective (e.g., honesty, verbosity, safety); ii) employ a Mixture-of-Experts (MoE) strategy with a gating network that automatically selects the most suitable reward objectives based on the context. We efficiently trained an ArmoRM with Llama-3 8B and a gating network consisting of a shallow MLP on top of the ArmoRM. Our trained model, ArmoRM-Llama3-8B, obtains state-of-the-art performance on RewardBench, a benchmark evaluating RMs for language modeling. Notably, the performance of our model surpasses the LLM-as-a-judge method with GPT-4 judges by a margin, and approaches the performance of the much larger Nemotron-4 340B reward model.",
            "link": "https://www.semanticscholar.org/paper/adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
            "authors": "Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, Tong Zhang",
            "matchScore": 273.52542,
            "original title": "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts",
            "original authors": "Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, Tong Zhang",
            "EMNLP Paper ID": "2144",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "97f05d277bac590cdcfe512b8adf647bc958d966",
            "title": "Negating Negatives: Alignment with Human Negative Samples via Distributional Dispreference Optimization",
            "abstract": "Large language models (LLMs) have revolutionized the role of AI, yet pose potential social risks. To steer LLMs towards human preference, alignment technologies have been introduced and gained increasing attention. Nevertheless, existing methods heavily rely on high-quality positive-negative training pairs, suffering from noisy positive responses that are barely distinguishable from negative ones. Given recent LLMs' proficiency in generating helpful responses, this work pivots towards a new research question: can we achieve alignment using solely human-annotated negative samples, preserving helpfulness while reducing harmfulness? For this purpose, we propose Distributional Dispreference Optimization (D$^2$O), which maximizes the discrepancy between dispreferred responses and the generated non-negative ones. In this way, D$^2$O effectively eschews harmful information without incorporating noisy positive samples, while avoiding collapse using self-generated responses as anchors. We demonstrate that D$^2$O can be regarded as learning a distributional preference model reflecting human dispreference against negative responses, which is theoretically an upper bound of the instance-level DPO. Extensive experiments manifest that our method achieves comparable generation quality and surpasses the latest strong baselines in producing less harmful and more informative responses with better training stability and faster convergence.",
            "link": "https://www.semanticscholar.org/paper/97f05d277bac590cdcfe512b8adf647bc958d966",
            "authors": "Shitong Duan, Xiaoyuan Yi, Peng Zhang, Yan Liu, Zheng Liu, T. Lu, Xing Xie, Ning Gu",
            "matchScore": 333.51227,
            "original title": "Negating Negatives: Alignment with Human Negative Samples via Distributional Dispreference Optimization",
            "original authors": "Shitong Duan, Xiaoyuan Yi, Peng Zhang, Yan Liu, Zheng Liu, Tun Lu, Xing Xie, Ning Gu",
            "EMNLP Paper ID": "220",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "7709a9eefa9a67510111aef2877f2834a76c8829",
            "title": "Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked Preferences",
            "abstract": "Direct Preference Optimization (DPO) is an effective technique that leverages pairwise preference data (usually one chosen and rejected response pair per user prompt) to align LLMs to human preferences. In practice, multiple responses can exist for a given prompt with varying quality relative to each other. With availability of such quality ratings for multiple responses, we propose utilizing these responses to create multiple preference pairs for a given prompt. Our work focuses on systematically using the constructed multiple preference pair in DPO training via curriculum learning methodology. In particular, we order these multiple pairs of preference data from easy to hard (emulating curriculum training) according to various criteria. We show detailed comparisons of our proposed approach to the standard single-pair DPO setting. Our method, which we call Curry-DPO consistently shows increased performance gains on MTbench, Vicuna, WizardLM, and the UltraFeedback test set, highlighting its effectiveness. More specifically, Curry-DPO achieves a score of 7.43 on MT-bench with Zephy-7B model outperforming majority of existing LLMs with similar parameter size. Curry-DPO also achieves the highest adjusted win rates on Vicuna, WizardLM, and UltraFeedback test datasets (90.7%, 87.1%, and 87.9% respectively) in our experiments, with notable gains of upto 7.5% when compared to standard DPO technique.",
            "link": "https://www.semanticscholar.org/paper/7709a9eefa9a67510111aef2877f2834a76c8829",
            "authors": "Pulkit Pattnaik, Rishabh Maheshwary, Kelechi Ogueji, Vikas Yadav, Sathwik Tejaswi Madhusudhan",
            "matchScore": 132.3092,
            "original title": "Enhancing Alignment using Curriculum Learning & Ranked Preferences",
            "original authors": "Pulkit Pattnaik, Rishabh Maheshwary, Kelechi Ogueji, Vikas Yadav, Sathwik Tejaswi Madhusudhan",
            "EMNLP Paper ID": "2525",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "681c02ec8036ecf87499b1f8e5410a532eed0f57",
            "title": "Margin Matching Preference Optimization: Enhanced Model Alignment with Granular Feedback",
            "abstract": "Large language models (LLMs) fine-tuned with alignment techniques, such as reinforcement learning from human feedback, have been instrumental in developing some of the most capable AI systems to date. Despite their success, existing methods typically rely on simple binary labels, such as those indicating preferred outputs in pairwise preferences, which fail to capture the subtle differences in relative quality between pairs. To address this limitation, we introduce an approach called Margin Matching Preference Optimization (MMPO), which incorporates relative quality margins into optimization, leading to improved LLM policies and reward models. Specifically, given quality margins in pairwise preferences, we design soft target probabilities based on the Bradley-Terry model, which are then used to train models with the standard cross-entropy objective. Experiments with both human and AI feedback data demonstrate that MMPO consistently outperforms baseline methods, often by a substantial margin, on popular benchmarks including MT-bench and RewardBench. Notably, the 7B model trained with MMPO achieves state-of-the-art performance on RewardBench as of June 2024, outperforming other models of the same scale. Our analysis also shows that MMPO is more robust to overfitting, leading to better-calibrated models.",
            "link": "https://www.semanticscholar.org/paper/681c02ec8036ecf87499b1f8e5410a532eed0f57",
            "authors": "Kyuyoung Kim, Ah Jeong Seo, Hao Liu, Jinwoo Shin, Kimin Lee",
            "matchScore": 297.5629,
            "original title": "Margin Matching Preference Optimization: Enhanced Model Alignment with Granular Feedback",
            "original authors": "Kyuyoung Kim, Ah Jeong Seo, Hao Liu, Jinwoo Shin, Kimin Lee",
            "EMNLP Paper ID": "2631",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "52b64d2f80eb6fa42822790409368677157c13e6",
            "title": "Self-supervised Preference Optimization: Enhance Your Language Model with Preference Degree Awareness",
            "abstract": "Recently, there has been significant interest in replacing the reward model in Reinforcement Learning with Human Feedback (RLHF) methods for Large Language Models (LLMs), such as Direct Preference Optimization (DPO) and its variants. These approaches commonly use a binary cross-entropy mechanism on pairwise samples, i.e., minimizing and maximizing the loss based on preferred or dis-preferred responses, respectively. However, while this training strategy omits the reward model, it also overlooks the varying preference degrees within different responses. We hypothesize that this is a key factor hindering LLMs from sufficiently understanding human preferences. To address this problem, we propose a novel Self-supervised Preference Optimization (SPO) framework, which constructs a self-supervised preference degree loss combined with the alignment loss, thereby helping LLMs improve their ability to understand the degree of preference. Extensive experiments are conducted on two widely used datasets of different tasks. The results demonstrate that SPO can be seamlessly integrated with existing preference optimization methods and significantly boost their performance to achieve state-of-the-art performance. We also conduct detailed analyses to offer comprehensive insights into SPO, which verifies its effectiveness. The code is available at https://github.com/lijian16/SPO.",
            "link": "https://www.semanticscholar.org/paper/52b64d2f80eb6fa42822790409368677157c13e6",
            "authors": "Jian Li, Haojing Huang, Yujia Zhang, Pengfei Xu, Xi Chen, Rui Song, Lida Shi, Jingwen Wang, Hao Xu",
            "matchScore": 272.4303,
            "original title": "Self-supervised Preference Optimization: Enhance Your Language Model with Preference Degree Awareness",
            "original authors": "Jian Li, Haojing Huang, Yujia Zhang, Pengfei Xu, Xi Chen, Rui Song, Lida Shi, Jingwen Wang, Hao Xu",
            "EMNLP Paper ID": "2792",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "0111e51679678f81eafa0ce064b578f284eb8e59",
            "title": "Navigating Noisy Feedback: Enhancing Reinforcement Learning with Error-Prone Language Models",
            "abstract": "The correct specification of reward models is a well-known challenge in reinforcement learning. Hand-crafted reward functions often lead to inefficient or suboptimal policies and may not be aligned with user values. Reinforcement learning from human feedback is a successful technique that can mitigate such issues, however, the collection of human feedback can be laborious. Recent works have solicited feedback from pre-trained large language models rather than humans to reduce or eliminate human effort, however, these approaches yield poor performance in the presence of hallucination and other errors. This paper studies the advantages and limitations of reinforcement learning from large language model feedback and proposes a simple yet effective method for soliciting and applying feedback as a potential-based shaping function. We theoretically show that inconsistent rankings, which approximate ranking errors, lead to uninformative rewards with our approach. Our method empirically improves convergence speed and policy returns over commonly used baselines even with significant ranking errors, and eliminates the need for complex post-processing of reward functions.",
            "link": "https://www.semanticscholar.org/paper/0111e51679678f81eafa0ce064b578f284eb8e59",
            "authors": "Muhan Lin, Shuyang Shi, Yue (Sophie) Guo, Behdad Chalaki, Vaishnav Tadiparthi, Ehsan Moradi Pari, Simon Stepputtis, Joseph Campbell, Katia P. Sycara",
            "matchScore": 296.47363,
            "original title": "Navigating Noisy Feedback: Enhancing Reinforcement Learning with Error-Prone Language Models",
            "original authors": "Muhan Lin, Shuyang Shi, Yue Guo, Behdad Chalaki, Vaishnav Tadiparthi, Ehsan Moradi Pari, Simon Stepputtis, Joseph Campbell, Katia P. Sycara",
            "EMNLP Paper ID": "3066",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "711774dbe2d14045c4df37b57b324d18e0fe5572",
            "title": "On the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization",
            "abstract": "Reinforcement Learning from Human Feedback (RLHF) is an effective approach for aligning language models to human preferences. Central to RLHF is learning a reward function for scoring human preferences. Two main approaches for learning a reward model are 1) training an EXplicit Reward Model (EXRM) as in RLHF, and 2) using an implicit reward learned from preference data through methods such as Direct Preference Optimization (DPO). Prior work has shown that the implicit reward model of DPO (denoted as DPORM) can approximate an EXRM in the limit. DPORM's effectiveness directly implies the optimality of the learned policy, and also has practical implication for LLM alignment methods including iterative DPO. However, it is unclear how well DPORM empirically matches the performance of EXRM. This work studies the accuracy at distinguishing preferred and rejected answers for both DPORM and EXRM. Our findings indicate that even though DPORM fits the training dataset comparably, it generalizes less effectively than EXRM, especially when the validation datasets contain distribution shifts. Across five out-of-distribution settings, DPORM has a mean drop in accuracy of 3% and a maximum drop of 7%. These findings highlight that DPORM has limited generalization ability and substantiates the integration of an explicit reward model in iterative DPO approaches.",
            "link": "https://www.semanticscholar.org/paper/711774dbe2d14045c4df37b57b324d18e0fe5572",
            "authors": "Yong Lin, Skyler Seto, Maartje ter Hoeve, Katherine Metcalf, B. Theobald, Xuan Wang, Yizhe Zhang, Chen Huang, Tong Zhang",
            "matchScore": 289.44733,
            "original title": "On the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization",
            "original authors": "Yong Lin, Skyler Seto, Maartje Ter Hoeve, Katherine Metcalf, Barry-John Theobald, Xuan Wang, Yizhe Zhang, Chen Huang, Tong Zhang",
            "EMNLP Paper ID": "3067",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "c3703dda182472a8005c57053fc5207c725cfb75",
            "title": "Offline RLHF Methods Need More Accurate Supervision Signals",
            "abstract": "With the rapid advances in Large Language Models (LLMs), aligning LLMs with human preferences become increasingly important. Although Reinforcement Learning with Human Feedback (RLHF) proves effective, it is complicated and highly resource-intensive. As such, offline RLHF has been introduced as an alternative solution, which directly optimizes LLMs with ranking losses on a fixed preference dataset. Current offline RLHF only captures the ``ordinal relationship'' between responses, overlooking the crucial aspect of ``how much'' one is preferred over the others. To address this issue, we propose a simple yet effective solution called \\textbf{R}eward \\textbf{D}ifference \\textbf{O}ptimization, shorted as \\textbf{RDO}. Specifically, we introduce {\\it reward difference coefficients} to reweigh sample pairs in offline RLHF. We then develop a {\\it difference model} involving rich interactions between a pair of responses for predicting these difference coefficients. Experiments with 7B LLMs on the HH and TL;DR datasets substantiate the effectiveness of our method in both automatic metrics and human evaluation, thereby highlighting its potential for aligning LLMs with human intent and values.",
            "link": "https://www.semanticscholar.org/paper/c3703dda182472a8005c57053fc5207c725cfb75",
            "authors": "Shiqi Wang, Zhengze Zhang, Rui Zhao, Fei Tan, C. Nguyen",
            "matchScore": 285.7147,
            "original title": "Offline RLHF Methods Need More Accurate Supervision Signals",
            "original authors": "Shiqi Wang, Zhengze Zhang, Rui Zhao, Fei Tan, Nguyen Cam-Tu",
            "EMNLP Paper ID": "424",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "1ef297466d7250b26e47b3bd405d8a8662567731",
            "title": "Conditional Language Policy: A General Framework for Steerable Multi-Objective Finetuning",
            "abstract": "Reward-based finetuning is crucial for aligning language policies with intended behaviors (e.g., creativity and safety). A key challenge is to develop steerable language models that trade-off multiple (conflicting) objectives in a flexible and efficient manner. This paper presents Conditional Language Policy (CLP), a general framework for finetuning language models on multiple objectives. Building on techniques from multi-task training and parameter-efficient finetuning, CLP learn steerable models that effectively trade-off conflicting objectives at inference time. Notably, this does not require training or maintaining multiple models to achieve different trade-offs between the objectives. Through extensive experiments and ablations on two summarization datasets, we show that CLP learns steerable language models that outperform and Pareto-dominate the existing approaches for multi-objective finetuning.",
            "link": "https://www.semanticscholar.org/paper/1ef297466d7250b26e47b3bd405d8a8662567731",
            "authors": "Kaiwen Wang, Rahul Kidambi, Ryan Sullivan, Alekh Agarwal, Christoph Dann, Andrea Michi, Marco Gelmi, Yunxuan Li, Raghav Gupta, Kumar Avinava Dubey, Alexandre Ram'e, Johan Ferret, Geoffrey Cideron, Le Hou, Hongkun Yu, Amr Ahmed, Aranyak Mehta, L'eonard Hussenot, Olivier Bachem, Edouard Leurent",
            "matchScore": 237.38855,
            "original title": "Conditioned Language Policy: A General Framework For Steerable Multi-Objective Finetuning",
            "original authors": "Kaiwen Wang, Rahul Kidambi, Ryan Sullivan, Alekh Agarwal, Christoph Dann, Andrea Michi, Marco Gelmi, Yunxuan Li, Raghav Gupta, Kumar Avinava Dubey, Alexandre Rame, Johan Ferret, Geoffrey Cideron, Le Hou, Hongkun Yu, Amr Ahmed, Aranyak Mehta, Leonard Hussenot, Olivier Bachem, Edouard Leurent",
            "EMNLP Paper ID": "431",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        }
    ],
    "LLM Safety and Security Enhancements": [
        {
            "paperId": "19443d48399d4fe89a4b0a96917c50c6fd9c5af1",
            "title": "FLIRT: Feedback Loop In-context Red Teaming",
            "abstract": "Warning: this paper contains content that may be inappropriate or offensive. As generative models become available for public use in various applications, testing and analyzing vulnerabilities of these models has become a priority. Here we propose an automatic red teaming framework that evaluates a given model and exposes its vulnerabilities against unsafe and inappropriate content generation. Our framework uses in-context learning in a feedback loop to red team models and trigger them into unsafe content generation. We propose different in-context attack strategies to automatically learn effective and diverse adversarial prompts for text-to-image models. Our experiments demonstrate that compared to baseline approaches, our proposed strategy is significantly more effective in exposing vulnerabilities in Stable Diffusion (SD) model, even when the latter is enhanced with safety features. Furthermore, we demonstrate that the proposed framework is effective for red teaming text-to-text models, resulting in significantly higher toxic response generation rate compared to previously reported numbers.",
            "link": "https://www.semanticscholar.org/paper/19443d48399d4fe89a4b0a96917c50c6fd9c5af1",
            "authors": "Ninareh Mehrabi, Palash Goyal, Christophe Dupuy, Qian Hu, Shalini Ghosh, R. Zemel, Kai-Wei Chang, A. Galstyan, Rahul Gupta",
            "EMNLP Paper ID": "91",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "53184f9f4122eb4b762e7dcbaf14b90a42aa0053",
            "title": "SHIELD: Evaluation and Defense Strategies for Copyright Compliance in LLM Text Generation",
            "abstract": "Large Language Models (LLMs) have transformed machine learning but raised significant legal concerns due to their potential to produce text that infringes on copyrights, resulting in several high-profile lawsuits. The legal landscape is struggling to keep pace with these rapid advancements, with ongoing debates about whether generated text might plagiarize copyrighted materials. Current LLMs may infringe on copyrights or overly restrict non-copyrighted texts, leading to these challenges: (i) the need for a comprehensive evaluation benchmark to assess copyright compliance from multiple aspects; (ii) evaluating robustness against safeguard bypassing attacks; and (iii) developing effective defense targeted against the generation of copyrighted text. To tackle these challenges, we introduce a curated dataset to evaluate methods, test attack strategies, and propose lightweight, real-time defense to prevent the generation of copyrighted text, ensuring the safe and lawful use of LLMs. Our experiments demonstrate that current LLMs frequently output copyrighted text, and that jailbreaking attacks can significantly increase the volume of copyrighted output. Our proposed defense mechanism significantly reduces the volume of copyrighted text generated by LLMs by effectively refusing malicious requests. Code is publicly available at https://github.com/xz-liu/SHIELD",
            "link": "https://www.semanticscholar.org/paper/53184f9f4122eb4b762e7dcbaf14b90a42aa0053",
            "authors": "Xiaoze Liu, Ting Sun, Tianyang Xu, Feijie Wu, Cunxiang Wang, Xiaoqian Wang, Jing Gao",
            "EMNLP Paper ID": "191",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "c50ed7a76a73ae54b1e876de0ad82e608b0ac515",
            "title": "$DA^3$: A Distribution-Aware Adversarial Attack against Language Models",
            "abstract": "Language models can be manipulated by adversarial attacks, which introduce subtle perturbations to input data. While recent attack methods can achieve a relatively high attack success rate (ASR), we've observed that the generated adversarial examples have a different data distribution compared with the original examples. Specifically, these adversarial examples exhibit reduced confidence levels and greater divergence from the training data distribution. Consequently, they are easy to detect using straightforward detection methods, diminishing the efficacy of such attacks. To address this issue, we propose a Distribution-Aware Adversarial Attack ($DA^3$) method. $DA^3$ considers the distribution shifts of adversarial examples to improve attacks' effectiveness under detection methods. We further design a novel evaluation metric, the Non-detectable Attack Success Rate (NASR), which integrates both ASR and detectability for the attack task. We conduct experiments on four widely used datasets to validate the attack effectiveness and transferability of adversarial examples generated by $DA^3$ against both the white-box BERT-base and RoBERTa-base models and the black-box LLaMA2-7b model.",
            "link": "https://www.semanticscholar.org/paper/c50ed7a76a73ae54b1e876de0ad82e608b0ac515",
            "authors": "Yibo Wang, Xiangjue Dong, James Caverlee, Philip S. Yu",
            "EMNLP Paper ID": "203",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "854dd4e53498bf201e9cfaeb5e977a1f7612ef4c",
            "title": "ASETF: A Novel Method for Jailbreak Attack on LLMs through Translate Suffix Embeddings",
            "abstract": "The safety defense methods of Large language models(LLMs) stays limited because the dangerous prompts are manually curated to just few known attack types, which fails to keep pace with emerging varieties. Recent studies found that attaching suffixes to harmful instructions can hack the defense of LLMs and lead to dangerous outputs. However, similar to traditional text adversarial attacks, this approach, while effective, is limited by the challenge of the discrete tokens. This gradient based discrete optimization attack requires over 100,000 LLM calls, and due to the unreadable of adversarial suffixes, it can be relatively easily penetrated by common defense methods such as perplexity filters. To cope with this challenge, in this paper, we proposes an Adversarial Suffix Embedding Translation Framework (ASETF), aimed at transforming continuous adversarial suffix embeddings into coherent and understandable text. This method greatly reduces the computational overhead during the attack process and helps to automatically generate multiple adversarial samples, which can be used as data to strengthen LLMs security defense. Experimental evaluations were conducted on Llama2, Vicuna, and other prominent LLMs, employing harmful directives sourced from the Advbench dataset. The results indicate that our method significantly reduces the computation time of adversarial suffixes and achieves a much better attack success rate to existing techniques, while significantly enhancing the textual fluency of the prompts. In addition, our approach can be generalized into a broader method for generating transferable adversarial suffixes that can successfully attack multiple LLMs, even black-box LLMs, such as ChatGPT and Gemini.",
            "link": "https://www.semanticscholar.org/paper/854dd4e53498bf201e9cfaeb5e977a1f7612ef4c",
            "authors": "Hao Wang, Hao Li, Minlie Huang, Lei Sha",
            "EMNLP Paper ID": "300",
            "Oral/Poster": "Oral",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "1de5b3e45331d0902fe6496051121598df28d78f",
            "title": "Alignment-Enhanced Decoding:Defending via Token-Level Adaptive Refining of Probability Distributions",
            "abstract": "Large language models are susceptible to jailbreak attacks, which can result in the generation of harmful content. While prior defenses mitigate these risks by perturbing or inspecting inputs, they ignore competing objectives, the underlying cause of alignment failures. In this paper, we propose Alignment-Enhanced Decoding (AED), a novel defense that employs adaptive decoding to address the root causes of jailbreak issues. We first define the Competitive Index to quantify alignment failures and utilize feedback from self-evaluation to compute post-alignment logits. Then, AED adaptively combines AED and post-alignment logits with the original logits to obtain harmless and helpful distributions. Consequently, our method enhances safety alignment while maintaining helpfulness. We conduct experiments across five models and four common jailbreaks, with the results validating the effectiveness of our approach. Code is available at https://github.com/GIGABaozi/AED.git.",
            "link": "https://www.semanticscholar.org/paper/1de5b3e45331d0902fe6496051121598df28d78f",
            "authors": "Quan Liu, Zhenhong Zhou, Longzhu He, Yi Liu, Wei Zhang, Sen Su",
            "EMNLP Paper ID": "314",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "3c6893a23391a2922f070c5aec8b279064815ed3",
            "title": "Towards Understanding Jailbreak Attacks in LLMs: A Representation Space Analysis",
            "abstract": "Large language models (LLMs) are susceptible to a type of attack known as jailbreaking, which misleads LLMs to output harmful contents. Although there are diverse jailbreak attack strategies, there is no unified understanding on why some methods succeed and others fail. This paper explores the behavior of harmful and harmless prompts in the LLM's representation space to investigate the intrinsic properties of successful jailbreak attacks. We hypothesize that successful attacks share some similar properties: They are effective in moving the representation of the harmful prompt towards the direction to the harmless prompts. We leverage hidden representations into the objective of existing jailbreak attacks to move the attacks along the acceptance direction, and conduct experiments to validate the above hypothesis using the proposed objective. We hope this study provides new insights into understanding how LLMs understand harmfulness information.",
            "link": "https://www.semanticscholar.org/paper/3c6893a23391a2922f070c5aec8b279064815ed3",
            "authors": "Yuping Lin, Pengfei He, Han Xu, Yue Xing, Makoto Yamada, Hui Liu, Jiliang Tang",
            "EMNLP Paper ID": "795",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "7e75f61ffc99ef76e3bbd6a61176c414c84483e1",
            "title": "Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models",
            "abstract": "Language Language Models (LLMs) face safety concerns due to potential misuse by malicious users. Recent red-teaming efforts have identified adversarial suffixes capable of jailbreaking LLMs using the gradient-based search algorithm Greedy Coordinate Gradient (GCG). However, GCG struggles with computational inefficiency, limiting further investigations regarding suffix transferability and scalability across models and data. In this work, we bridge the connection between search efficiency and suffix transferability. We propose a two-stage transfer learning framework, DeGCG, which decouples the search process into behavior-agnostic pre-searching and behavior-relevant post-searching. Specifically, we employ direct first target token optimization in pre-searching to facilitate the search process. We apply our approach to cross-model, cross-data, and self-transfer scenarios. Furthermore, we introduce an interleaved variant of our approach, i-DeGCG, which iteratively leverages self-transferability to accelerate the search process. Experiments on HarmBench demonstrate the efficiency of our approach across various models and domains. Notably, our i-DeGCG outperforms the baseline on Llama2-chat-7b with ASRs of $43.9$ ($+22.2$) and $39.0$ ($+19.5$) on valid and test sets, respectively. Further analysis on cross-model transfer indicates the pivotal role of first target token optimization in leveraging suffix transferability for efficient searching.",
            "link": "https://www.semanticscholar.org/paper/7e75f61ffc99ef76e3bbd6a61176c414c84483e1",
            "authors": "Hongfu Liu, Yuxi Xie, Ye Wang, Michael Shieh",
            "EMNLP Paper ID": "806",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "09812e529903ff67c5fc5f1dcb2b3586eb3ffd23",
            "title": "Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment",
            "abstract": "Large Language Models (LLMs) are powerful zero-shot assessors used in real-world situations such as assessing written exams and benchmarking systems. Despite these critical applications, no existing work has analyzed the vulnerability of judge-LLMs to adversarial manipulation. This work presents the first study on the adversarial robustness of assessment LLMs, where we demonstrate that short universal adversarial phrases can be concatenated to deceive judge LLMs to predict inflated scores. Since adversaries may not know or have access to the judge-LLMs, we propose a simple surrogate attack where a surrogate model is first attacked, and the learned attack phrase then transferred to unknown judge-LLMs. We propose a practical algorithm to determine the short universal attack phrases and demonstrate that when transferred to unseen models, scores can be drastically inflated such that irrespective of the assessed text, maximum scores are predicted. It is found that judge-LLMs are significantly more susceptible to these adversarial attacks when used for absolute scoring, as opposed to comparative assessment. Our findings raise concerns on the reliability of LLM-as-a-judge methods, and emphasize the importance of addressing vulnerabilities in LLM assessment methods before deployment in high-stakes real-world scenarios.",
            "link": "https://www.semanticscholar.org/paper/09812e529903ff67c5fc5f1dcb2b3586eb3ffd23",
            "authors": "Vyas Raina, Adian Liusie, Mark J. F. Gales",
            "EMNLP Paper ID": "850",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "27577c6865955be2dc067ced388b8fa39622e738",
            "title": "RealVul: Can We Detect Vulnerabilities in Web Applications with LLM?",
            "abstract": "The latest advancements in large language models (LLMs) have sparked interest in their potential for software vulnerability detection. However, there is currently a lack of research specifically focused on vulnerabilities in the PHP language, and challenges in extracting samples and processing persist, hindering the model's ability to effectively capture the characteristics of specific vulnerabilities. In this paper, we present RealVul, the first LLM-based framework designed for PHP vulnerability detection, addressing these issues. By vulnerability candidate detection methods and employing techniques such as normalization, we can isolate potential vulnerability triggers while streamlining the code and eliminating unnecessary semantic information, enabling the model to better understand and learn from the generated vulnerability samples. We also address the issue of insufficient PHP vulnerability samples by improving data synthesis methods. To evaluate RealVul's performance, we conduct an extensive analysis using five distinct code LLMs on vulnerability data from 180 PHP projects. The results demonstrate a significant improvement in both effectiveness and generalization compared to existing methods, effectively boosting the vulnerability detection capabilities of these models.",
            "link": "https://www.semanticscholar.org/paper/27577c6865955be2dc067ced388b8fa39622e738",
            "authors": "Di Cao, Yong Liao, Xiuwei Shang",
            "EMNLP Paper ID": "959",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "32f4780a3e9e63e316d4ef23404139b7f2daa963",
            "title": "The Best Defense is Attack: Repairing Semantics in Textual Adversarial Examples",
            "abstract": "Recent studies have revealed the vulnerability of pre-trained language models to adversarial attacks. Existing adversarial defense techniques attempt to reconstruct adversarial examples within feature or text spaces. However, these methods struggle to effectively repair the semantics in adversarial examples, resulting in unsatisfactory performance and limiting their practical utility. To repair the semantics in adversarial examples, we introduce a novel approach named Reactive Perturbation Defocusing (Rapid). Rapid employs an adversarial detector to identify fake labels of adversarial examples and leverage adversarial attackers to repair the semantics in adversarial examples. Our extensive experimental results conducted on four public datasets, convincingly demonstrate the effectiveness of Rapid in various adversarial attack scenarios. To address the problem of defense performance validation in previous works, we provide a demonstration of adversarial detection and repair based on our work, which can be easily evaluated at https://tinyurl.com/22ercuf8.",
            "link": "https://www.semanticscholar.org/paper/32f4780a3e9e63e316d4ef23404139b7f2daa963",
            "authors": "Heng Yang, Ke Li",
            "EMNLP Paper ID": "977",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "c7fbab2b0df6ddc1c9e818e64ea953aa17a9ad4d",
            "title": "CleanGen: Mitigating Backdoor Attacks for Generation Tasks in Large Language Models",
            "abstract": "The remarkable performance of large language models (LLMs) in generation tasks has enabled practitioners to leverage publicly available models to power custom applications, such as chatbots and virtual assistants. However, the data used to train or fine-tune these LLMs is often undisclosed, allowing an attacker to compromise the data and inject backdoors into the models. In this paper, we develop a novel inference time defense, named CLEANGEN, to mitigate backdoor attacks for generation tasks in LLMs. CLEANGEN is a lightweight and effective decoding strategy that is compatible with the state-of-the-art (SOTA) LLMs. Our insight behind CLEANGEN is that compared to other LLMs, backdoored LLMs assign significantly higher probabilities to tokens representing the attacker-desired contents. These discrepancies in token probabilities enable CLEANGEN to identify suspicious tokens favored by the attacker and replace them with tokens generated by another LLM that is not compromised by the same attacker, thereby avoiding generation of attacker-desired content. We evaluate CLEANGEN against five SOTA backdoor attacks. Our results show that CLEANGEN achieves lower attack success rates (ASR) compared to five SOTA baseline defenses for all five backdoor attacks. Moreover, LLMs deploying CLEANGEN maintain helpfulness in their responses when serving benign user queries with minimal added computational overhead.",
            "link": "https://www.semanticscholar.org/paper/c7fbab2b0df6ddc1c9e818e64ea953aa17a9ad4d",
            "authors": "Yuetai Li, Zhangchen Xu, Fengqing Jiang, Luyao Niu, D. Sahabandu, Bhaskar Ramasubramanian, Radha Poovendran",
            "EMNLP Paper ID": "1037",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "f02fc3e2ab00d74f2397f70bab09a526e1910b56",
            "title": "Ranking Manipulation for Conversational Search Engines",
            "abstract": "Major search engine providers are rapidly incorporating Large Language Model (LLM)-generated content in response to user queries. These conversational search engines operate by loading retrieved website text into the LLM context for summarization and interpretation. Recent research demonstrates that LLMs are highly vulnerable to jailbreaking and prompt injection attacks, which disrupt the safety and quality goals of LLMs using adversarial strings. This work investigates the impact of prompt injections on the ranking order of sources referenced by conversational search engines. To this end, we introduce a focused dataset of real-world consumer product websites and formalize conversational search ranking as an adversarial problem. Experimentally, we analyze conversational search rankings in the absence of adversarial injections and show that different LLMs vary significantly in prioritizing product name, document content, and context position. We then present a tree-of-attacks-based jailbreaking technique which reliably promotes low-ranked products. Importantly, these attacks transfer effectively to state-of-the-art conversational search engines such as perplexity$.$ai. Given the strong financial incentive for website owners to boost their search ranking, we argue that our problem formulation is of critical importance for future robustness work.",
            "link": "https://www.semanticscholar.org/paper/f02fc3e2ab00d74f2397f70bab09a526e1910b56",
            "authors": "Samuel Pfrommer, Yatong Bai, Tanmay Gautam, S. Sojoudi",
            "EMNLP Paper ID": "1065",
            "Oral/Poster": "Oral",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "f21d0177e9374bb8579c1d9c71319f212f62b3d5",
            "title": "InferAligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance",
            "abstract": "With the rapid development of large language models (LLMs), they are not only used as general-purpose AI assistants but are also customized through further fine-tuning to meet the requirements of different applications. A pivotal factor in the success of current LLMs is the alignment process. Current alignment methods, such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), focus on training-time alignment and are often complex and cumbersome to implement. Therefore, we develop \\textbf{InferAligner}, a novel inference-time alignment method that utilizes cross-model guidance for harmlessness alignment. InferAligner utilizes safety steering vectors extracted from safety-aligned model to modify the activations of the target model when responding to harmful inputs, thereby guiding the target model to provide harmless responses. Experimental results show that our method can be very effectively applied to domain-specific models in finance, medicine, and mathematics, as well as to multimodal large language models (MLLMs) such as LLaVA. It significantly diminishes the Attack Success Rate (ASR) of both harmful instructions and jailbreak attacks, while maintaining almost unchanged performance in downstream tasks.",
            "link": "https://www.semanticscholar.org/paper/f21d0177e9374bb8579c1d9c71319f212f62b3d5",
            "authors": "Pengyu Wang, Dong Zhang, Linyang Li, Chenkun Tan, Xinghao Wang, Ke Ren, Botian Jiang, Xipeng Qiu",
            "EMNLP Paper ID": "1186",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "eb16eae728f54962992e6115c5dcd0df3be28c89",
            "title": "Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning",
            "abstract": "In-context learning, a paradigm bridging the gap between pre-training and fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in few-shot settings. Despite being widely applied, in-context learning is vulnerable to malicious attacks. In this work, we raise security concerns regarding this paradigm. Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model. Specifically, we design a new backdoor attack method, named ICLAttack, to target large language models based on in-context learning. Our method encompasses two types of attacks: poisoning demonstration examples and poisoning demonstration prompts, which can make models behave in alignment with predefined intentions. ICLAttack does not require additional fine-tuning to implant a backdoor, thus preserving the model's generality. Furthermore, the poisoned examples are correctly labeled, enhancing the natural stealth of our attack method. Extensive experimental results across several language models, ranging in size from 1.3B to 180B parameters, demonstrate the effectiveness of our attack method, exemplified by a high average attack success rate of 95.0% across the three datasets on OPT models.",
            "link": "https://www.semanticscholar.org/paper/eb16eae728f54962992e6115c5dcd0df3be28c89",
            "authors": "Shuai Zhao, Meihuizi Jia, Anh Tuan Luu, Fengjun Pan, Jinming Wen",
            "EMNLP Paper ID": "1340",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "b43eb24cfc1930fb9c7bace4b69cdf34fecfde3c",
            "title": "Defending Against Social Engineering Attacks in the Age of LLMs",
            "abstract": "The proliferation of Large Language Models (LLMs) poses challenges in detecting and mitigating digital deception, as these models can emulate human conversational patterns and facilitate chat-based social engineering (CSE) attacks. This study investigates the dual capabilities of LLMs as both facilitators and defenders against CSE threats. We develop a novel dataset, SEConvo, simulating CSE scenarios in academic and recruitment contexts, and designed to examine how LLMs can be exploited in these situations. Our findings reveal that, while off-the-shelf LLMs generate high-quality CSE content, their detection capabilities are suboptimal, leading to increased operational costs for defense. In response, we propose ConvoSentinel, a modular defense pipeline that improves detection at both the message and the conversation levels, offering enhanced adaptability and cost-effectiveness. The retrieval-augmented module in ConvoSentinel identifies malicious intent by comparing messages to a database of similar conversations, enhancing CSE detection at all stages. Our study highlights the need for advanced strategies to leverage LLMs in cybersecurity.",
            "link": "https://www.semanticscholar.org/paper/b43eb24cfc1930fb9c7bace4b69cdf34fecfde3c",
            "authors": "Lin Ai, Tharindu Kumarage, Amrita Bhattacharjee, Zizhou Liu, Zheng Hui, Michael Davinroy, James Cook, Laura Cassani, K. Trapeznikov, Matthias Kirchner, Arslan Basharat, A. Hoogs, Joshua Garland, Huan Liu, Julia Hirschberg",
            "EMNLP Paper ID": "1494",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "b15493107b8e3193aae28d818ef265f0e8790f49",
            "title": "BEEAR: Embedding-based Adversarial Removal of Safety Backdoors in Instruction-tuned Language Models",
            "abstract": "Safety backdoor attacks in large language models (LLMs) enable the stealthy triggering of unsafe behaviors while evading detection during normal interactions. The high dimensionality of potential triggers in the token space and the diverse range of malicious behaviors make this a critical challenge. We present BEEAR, a mitigation approach leveraging the insight that backdoor triggers induce relatively uniform drifts in the model's embedding space. Our bi-level optimization method identifies universal embedding perturbations that elicit unwanted behaviors and adjusts the model parameters to reinforce safe behaviors against these perturbations. Experiments show BEEAR reduces the success rate of RLHF time backdoor attacks from>95% to<1% and from 47% to 0% for instruction-tuning time backdoors targeting malicious code generation, without compromising model utility. Requiring only defender-defined safe and unwanted behaviors, BEEAR represents a step towards practical defenses against safety backdoors in LLMs, providing a foundation for further advancements in AI safety and security.",
            "link": "https://www.semanticscholar.org/paper/b15493107b8e3193aae28d818ef265f0e8790f49",
            "authors": "Yi Zeng, Weiyu Sun, Tran Ngoc Huynh, Dawn Song, Bo Li, Ruoxi Jia",
            "EMNLP Paper ID": "1530",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "e615f33365ea1d439507fc477588528ffb0764a8",
            "title": "Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks",
            "abstract": "We find that language models have difficulties generating fallacious and deceptive reasoning. When asked to generate deceptive outputs, language models tend to leak honest counterparts but believe them to be false. Exploiting this deficiency, we propose a jailbreak attack method that elicits an aligned language model for malicious output. Specifically, we query the model to generate a fallacious yet deceptively real procedure for the harmful behavior. Since a fallacious procedure is generally considered fake and thus harmless by LLMs, it helps bypass the safeguard mechanism. Yet the output is factually harmful since the LLM cannot fabricate fallacious solutions but proposes truthful ones. We evaluate our approach over five safety-aligned large language models, comparing four previous jailbreak methods, and show that our approach achieves competitive performance with more harmful outputs. We believe the findings could be extended beyond model safety, such as self-verification and hallucination.",
            "link": "https://www.semanticscholar.org/paper/e615f33365ea1d439507fc477588528ffb0764a8",
            "authors": "Yue Zhou, Henry Peng Zou, Barbara Di Eugenio, Yang Zhang",
            "EMNLP Paper ID": "1543",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "bc6f2d1b9a366967c89fb173795a59641021ad2c",
            "title": "Holistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction",
            "abstract": "Automated red teaming is an effective method for identifying misaligned behaviors in large language models (LLMs). Existing approaches, however, often focus primarily on improving attack success rates while overlooking the need for comprehensive test case coverage. Additionally, most of these methods are limited to single-turn red teaming, failing to capture the multi-turn dynamics of real-world human-machine interactions. To overcome these limitations, we propose HARM (Holistic Automated Red teaMing), which scales up the diversity of test cases using a top-down approach based on an extensible, fine-grained risk taxonomy. Our method also leverages a novel fine-tuning strategy and reinforcement learning techniques to facilitate multi-turn adversarial probing in a human-like manner. Experimental results demonstrate that our framework enables a more systematic understanding of model vulnerabilities and offers more targeted guidance for the alignment process.",
            "link": "https://www.semanticscholar.org/paper/bc6f2d1b9a366967c89fb173795a59641021ad2c",
            "authors": "Jinchuan Zhang, Yan Zhou, Yaxin Liu, Ziming Li, Songlin Hu",
            "EMNLP Paper ID": "1580",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "8d28d2ef602e8b518b7daecc39a0f2f8d2caaa09",
            "title": "MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance",
            "abstract": "The deployment of multimodal large language models (MLLMs) has brought forth a unique vulnerability: susceptibility to malicious attacks through visual inputs. This paper investigates the novel challenge of defending MLLMs against such attacks. Compared to large language models (LLMs), MLLMs include an additional image modality. We discover that images act as a ``foreign language\"that is not considered during safety alignment, making MLLMs more prone to producing harmful responses. Unfortunately, unlike the discrete tokens considered in text-based LLMs, the continuous nature of image signals presents significant alignment challenges, which poses difficulty to thoroughly cover all possible scenarios. This vulnerability is exacerbated by the fact that most state-of-the-art MLLMs are fine-tuned on limited image-text pairs that are much fewer than the extensive text-based pretraining corpus, which makes the MLLMs more prone to catastrophic forgetting of their original abilities during safety fine-tuning. To tackle these challenges, we introduce MLLM-Protector, a plug-and-play strategy that solves two subtasks: 1) identifying harmful responses via a lightweight harm detector, and 2) transforming harmful responses into harmless ones via a detoxifier. This approach effectively mitigates the risks posed by malicious visual inputs without compromising the original performance of MLLMs. Our results demonstrate that MLLM-Protector offers a robust solution to a previously unaddressed aspect of MLLM security.",
            "link": "https://www.semanticscholar.org/paper/8d28d2ef602e8b518b7daecc39a0f2f8d2caaa09",
            "authors": "Renjie Pi, Tianyang Han, Jianshu Zhang, Yueqi Xie, Rui Pan, Qing Lian, Hanze Dong, Jipeng Zhang, Tong Zhang",
            "EMNLP Paper ID": "1884",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "425d3c381dde8f576b6237a6d443b97880dd6bc2",
            "title": "Tastle: Distract Large Language Models for Automatic Jailbreak Attack",
            "abstract": "Extensive efforts have been made before the public release of Large language models (LLMs) to align their behaviors with human values. However, even meticulously aligned LLMs remain vulnerable to malicious manipulations such as jailbreaking, leading to unintended behaviors. In this work, we propose a novel black-box jailbreak framework for automated red teaming of LLMs. We designed malicious content concealing and memory reframing with an iterative optimization algorithm to jailbreak LLMs, motivated by the research about the distractibility and over-confidence phenomenon of LLMs. Extensive experiments of jailbreaking both open-source and proprietary LLMs demonstrate the superiority of our framework in terms of effectiveness, scalability and transferability. We also evaluate the effectiveness of existing jailbreak defense methods against our attack and highlight the crucial need to develop more effective and practical defense strategies.",
            "link": "https://www.semanticscholar.org/paper/425d3c381dde8f576b6237a6d443b97880dd6bc2",
            "authors": "Zeguan Xiao, Yan Yang, Guanhua Chen, Yun Chen",
            "EMNLP Paper ID": "1914",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "3f15fef394aa99f5c657b44b5275169d2a9dc21d",
            "title": "RAFT: Realistic Attacks to Fool Text Detectors",
            "abstract": "Large language models (LLMs) have exhibited remarkable fluency across various tasks. However, their unethical applications, such as disseminating disinformation, have become a growing concern. Although recent works have proposed a number of LLM detection methods, their robustness and reliability remain unclear. In this paper, we present RAFT: a grammar error-free black-box attack against existing LLM detectors. In contrast to previous attacks for language models, our method exploits the transferability of LLM embeddings at the word-level while preserving the original text quality. We leverage an auxiliary embedding to greedily select candidate words to perturb against the target detector. Experiments reveal that our attack effectively compromises all detectors in the study across various domains by up to 99%, and are transferable across source models. Manual human evaluation studies show our attacks are realistic and indistinguishable from original human-written text. We also show that examples generated by RAFT can be used to train adversarially robust detectors. Our work shows that current LLM detectors are not adversarially robust, underscoring the urgent need for more resilient detection mechanisms.",
            "link": "https://www.semanticscholar.org/paper/3f15fef394aa99f5c657b44b5275169d2a9dc21d",
            "authors": "James Wang, Ran Li, Junfeng Yang, Chengzhi Mao",
            "EMNLP Paper ID": "1988",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "aab95a2479ae7a9dd168ef32314cd654ba8f590c",
            "title": "From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking",
            "abstract": "The rapid development of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has exposed vulnerabilities to various adversarial attacks. This paper provides a comprehensive overview of jailbreaking research targeting both LLMs and MLLMs, highlighting recent advancements in evaluation benchmarks, attack techniques and defense strategies. Compared to the more advanced state of unimodal jailbreaking, multimodal domain remains underexplored. We summarize the limitations and potential research directions of multimodal jailbreaking, aiming to inspire future research and further enhance the robustness and security of MLLMs.",
            "link": "https://www.semanticscholar.org/paper/aab95a2479ae7a9dd168ef32314cd654ba8f590c",
            "authors": "Siyuan Wang, Zhuohan Long, Zhihao Fan, Zhongyu Wei",
            "EMNLP Paper ID": "2101",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "2fffb353658efbb4b8c6e4d336e41095ea8fbf90",
            "title": "Jailbreaking LLMs with Arabic Transliteration and Arabizi",
            "abstract": "This study identifies the potential vulnerabilities of Large Language Models (LLMs) to 'jailbreak' attacks, specifically focusing on the Arabic language and its various forms. While most research has concentrated on English-based prompt manipulation, our investigation broadens the scope to investigate the Arabic language. We initially tested the AdvBench benchmark in Standardized Arabic, finding that even with prompt manipulation techniques like prefix injection, it was insufficient to provoke LLMs into generating unsafe content. However, when using Arabic transliteration and chatspeak (or arabizi), we found that unsafe content could be produced on platforms like OpenAI GPT-4 and Anthropic Claude 3 Sonnet. Our findings suggest that using Arabic and its various forms could expose information that might remain hidden, potentially increasing the risk of jailbreak attacks. We hypothesize that this exposure could be due to the model's learned connection to specific words, highlighting the need for more comprehensive safety training across all language forms.",
            "link": "https://www.semanticscholar.org/paper/2fffb353658efbb4b8c6e4d336e41095ea8fbf90",
            "authors": "Mansour Al Ghanim, Saleh Almohaimeed, Meng Zheng, Yan Solihin, Qian Lou",
            "EMNLP Paper ID": "2317",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "50ceabc6aa41e08480fa5976342bfe04bb47bce3",
            "title": "Defending Jailbreak Prompts via In-Context Adversarial Game",
            "abstract": "Large Language Models (LLMs) demonstrate remarkable capabilities across diverse applications. However, concerns regarding their security, particularly the vulnerability to jailbreak attacks, persist. Drawing inspiration from adversarial training in deep learning and LLM agent learning processes, we introduce the In-Context Adversarial Game (ICAG) for defending against jailbreaks without the need for fine-tuning. ICAG leverages agent learning to conduct an adversarial game, aiming to dynamically extend knowledge to defend against jailbreaks. Unlike traditional methods that rely on static datasets, ICAG employs an iterative process to enhance both the defense and attack agents. This continuous improvement process strengthens defenses against newly generated jailbreak prompts. Our empirical studies affirm ICAG's efficacy, where LLMs safeguarded by ICAG exhibit significantly reduced jailbreak success rates across various attack scenarios. Moreover, ICAG demonstrates remarkable transferability to other LLMs, indicating its potential as a versatile defense mechanism.",
            "link": "https://www.semanticscholar.org/paper/50ceabc6aa41e08480fa5976342bfe04bb47bce3",
            "authors": "Yujun Zhou, Yufei Han, Haomin Zhuang, Taicheng Guo, Kehan Guo, Zhenwen Liang, Hongyan Bao, Xiangliang Zhang",
            "EMNLP Paper ID": "2620",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "6945b0f03c0f29caa288d435c12029b6f0e0cf06",
            "title": "STAR: SocioTechnical Approach to Red Teaming Language Models",
            "abstract": "This research introduces STAR, a sociotechnical framework that improves on current best practices for red teaming safety of large language models. STAR makes two key contributions: it enhances steerability by generating parameterised instructions for human red teamers, leading to improved coverage of the risk surface. Parameterised instructions also provide more detailed insights into model failures at no increased cost. Second, STAR improves signal quality by matching demographics to assess harms for specific groups, resulting in more sensitive annotations. STAR further employs a novel step of arbitration to leverage diverse viewpoints and improve label reliability, treating disagreement not as noise but as a valuable contribution to signal quality.",
            "link": "https://www.semanticscholar.org/paper/6945b0f03c0f29caa288d435c12029b6f0e0cf06",
            "authors": "Laura Weidinger, John F. J. Mellor, Bernat Guillen Pegueroles, Nahema Marchal, Ravin Kumar, Kristian Lum, Canfer Akbulut, Mark Diaz, Stevie Bergman, Mikel Rodriguez, Verena Rieser, William Isaac",
            "EMNLP Paper ID": "2962",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "4cfc877734987982e9cd2f16648c6a2e58b47042",
            "title": "Safety Arithmetic: A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations",
            "abstract": "Ensuring the safe alignment of large language models (LLMs) with human values is critical as they become integral to applications like translation and question answering. Current alignment methods struggle with dynamic user intentions and complex objectives, making models vulnerable to generating harmful content. We propose Safety Arithmetic, a training-free framework enhancing LLM safety across different scenarios: Base models, Supervised fine-tuned models (SFT), and Edited models. Safety Arithmetic involves Harm Direction Removal to avoid harmful content and Safety Alignment to promote safe responses. Additionally, we present NoIntentEdit, a dataset highlighting edit instances that could compromise model safety if used unintentionally. Our experiments show that Safety Arithmetic significantly improves safety measures, reduces over-safety, and maintains model utility, outperforming existing methods in ensuring safe content generation.",
            "link": "https://www.semanticscholar.org/paper/4cfc877734987982e9cd2f16648c6a2e58b47042",
            "authors": "Rima Hazra, Sayan Layek, Somnath Banerjee, Soujanya Poria",
            "EMNLP Paper ID": "3005",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "dcc997a1d370d9c6e5f4ece90156973efa6c0864",
            "title": "GPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation",
            "abstract": "Research on jailbreaking has been valuable for testing and understanding the safety and security issues of large language models (LLMs). In this paper, we introduce Iterative Refinement Induced Self-Jailbreak (IRIS), a novel approach that leverages the reflective capabilities of LLMs for jailbreaking with only black-box access. Unlike previous methods, IRIS simplifies the jailbreaking process by using a single model as both the attacker and target. This method first iteratively refines adversarial prompts through self-explanation, which is crucial for ensuring that even well-aligned LLMs obey adversarial instructions. IRIS then rates and enhances the output given the refined prompt to increase its harmfulness. We find that IRIS achieves jailbreak success rates of 98% on GPT-4, 92% on GPT-4 Turbo, and 94% on Llama-3.1-70B in under 7 queries. It significantly outperforms prior approaches in automatic, black-box, and interpretable jailbreaking, while requiring substantially fewer queries, thereby establishing a new standard for interpretable jailbreaking methods.",
            "link": "https://www.semanticscholar.org/paper/dcc997a1d370d9c6e5f4ece90156973efa6c0864",
            "authors": "Govind Ramesh, Yao Dou, Wei Xu",
            "EMNLP Paper ID": "3126",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "b22d2f4f2664c9115637396df4c2a5ad26b2b4da",
            "title": "Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing",
            "abstract": "Large language models (LLMs) are increasingly being adopted in a wide range of real-world applications. Despite their impressive performance, recent studies have shown that LLMs are vulnerable to deliberately crafted adversarial prompts even when aligned via Reinforcement Learning from Human Feedback or supervised fine-tuning. While existing defense methods focus on either detecting harmful prompts or reducing the likelihood of harmful responses through various means, defending LLMs against jailbreak attacks based on the inner mechanisms of LLMs remains largely unexplored. In this work, we investigate how LLMs response to harmful prompts and propose a novel defense method termed \\textbf{L}ayer-specific \\textbf{Ed}iting (LED) to enhance the resilience of LLMs against jailbreak attacks. Through LED, we reveal that several critical \\textit{safety layers} exist among the early layers of LLMs. We then show that realigning these safety layers (and some selected additional layers) with the decoded safe response from selected target layers can significantly improve the alignment of LLMs against jailbreak attacks. Extensive experiments across various LLMs (e.g., Llama2, Mistral) show the effectiveness of LED, which effectively defends against jailbreak attacks while maintaining performance on benign prompts. Our code is available at \\url{https://github.com/ledllm/ledllm}.",
            "link": "https://www.semanticscholar.org/paper/b22d2f4f2664c9115637396df4c2a5ad26b2b4da",
            "authors": "Wei Zhao, Zhe Li, Yige Li, Ye Zhang, Junfeng Sun",
            "matchScore": 312.3877,
            "original title": "Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing",
            "original authors": "Wei Zhao, Zhe Li, Yige Li, YE ZHANG, Jun Sun",
            "EMNLP Paper ID": "1008",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "1f69232d0e3ca10ee157d2bcdb029d4476144020",
            "title": "Immunization against harmful fine-tuning attacks",
            "abstract": "Large Language Models (LLMs) are often trained with safety guards intended to prevent harmful text generation. However, such safety training can be removed by fine-tuning the LLM on harmful datasets. While this emerging threat (harmful fine-tuning attacks) has been characterized by previous work, there is little understanding of how we should proceed in constructing and validating defenses against these attacks especially in the case where defenders would not have control of the fine-tuning process. We introduce a formal framework based on the training budget of an attacker which we call\"Immunization\"conditions. Using a formal characterisation of the harmful fine-tuning problem, we provide a thorough description of what a successful defense must comprise of and establish a set of guidelines on how rigorous defense research that gives us confidence should proceed.",
            "link": "https://www.semanticscholar.org/paper/1f69232d0e3ca10ee157d2bcdb029d4476144020",
            "authors": "Domenic Rosati, Jan Wehner, Kai Williams, Lukasz Bartoszcze, Jan Batzner, Hassan Sajjad, Frank Rudzicz",
            "matchScore": 210.81284,
            "original title": "Immunization against harmful fine-tuning attacks",
            "original authors": "Domenic Rosati, Jan Wehner, Kai Williams, Lukasz Bartoszcze, Hassan Sajjad, Frank Rudzicz",
            "EMNLP Paper ID": "1042",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "f36350a7ec3461ed06aafbbd27c63082109322ac",
            "title": "Light-weight Fine-tuning Method for Defending Adversarial Noise in Pre-trained Medical Vision-Language Models",
            "abstract": "Fine-tuning pre-trained Vision-Language Models (VLMs) has shown remarkable capabilities in medical image and textual depiction synergy. Nevertheless, many pre-training datasets are restricted by patient privacy concerns, potentially containing noise that can adversely affect downstream performance. Moreover, the growing reliance on multi-modal generation exacerbates this issue because of its susceptibility to adversarial attacks. To investigate how VLMs trained on adversarial noisy data perform on downstream medical tasks, we first craft noisy upstream datasets using multi-modal adversarial attacks. Through our comprehensive analysis, we unveil that moderate noise enhances model robustness and transferability, but increasing noise levels negatively impact downstream task performance. To mitigate this issue, we propose rectify adversarial noise (RAN) framework, a recipe designed to effectively defend adversarial attacks and rectify the influence of upstream noise during fine-tuning.",
            "link": "https://www.semanticscholar.org/paper/f36350a7ec3461ed06aafbbd27c63082109322ac",
            "authors": "Xu Han, Linghao Jin, Xuezhe Ma, Xiaofeng Liu",
            "matchScore": 342.46674,
            "original title": "Light-weight Fine-tuning Method for Defending Adversarial Noise in Pre-trained Medical Vision-Language Models",
            "original authors": "Xu Han, Linghao Jin, Xuezhe Ma, Xiaofeng Liu",
            "EMNLP Paper ID": "2169",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "d842c9a24f710370f7e1aa7724fdaa697ce12d9a",
            "title": "Virtual Context: Enhancing Jailbreak Attacks with Special Token Injection",
            "abstract": "Jailbreak attacks on large language models (LLMs) involve inducing these models to generate harmful content that violates ethics or laws, posing a significant threat to LLM security. Current jailbreak attacks face two main challenges: low success rates due to defensive measures and high resource requirements for crafting specific prompts. This paper introduces Virtual Context, which leverages special tokens, previously overlooked in LLM security, to improve jailbreak attacks. Virtual Context addresses these challenges by significantly increasing the success rates of existing jailbreak methods and requiring minimal background knowledge about the target model, thus enhancing effectiveness in black-box settings without additional overhead. Comprehensive evaluations show that Virtual Context-assisted jailbreak attacks can improve the success rates of four widely used jailbreak methods by approximately 40% across various LLMs. Additionally, applying Virtual Context to original malicious behaviors still achieves a notable jailbreak effect. In summary, our research highlights the potential of special tokens in jailbreak attacks and recommends including this threat in red-teaming testing to comprehensively enhance LLM security.",
            "link": "https://www.semanticscholar.org/paper/d842c9a24f710370f7e1aa7724fdaa697ce12d9a",
            "authors": "Yuqi Zhou, Lin Lu, Hanchi Sun, Pan Zhou, Lichao Sun",
            "matchScore": 261.28934,
            "original title": "Virtual Context Enhancing Jailbreak Attacks with Special Token Injection",
            "original authors": "YuqiZhou, Lin Lu, Hanchi Sun, Lichao Sun, Pan Zhou",
            "EMNLP Paper ID": "2332",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "1d75165a64dbcb98686e858f883ea2fc5393bd2b",
            "title": "Adversarial Attacks on Parts of Speech: An Empirical Study in Text-to-Image Generation",
            "abstract": "Recent studies show that text-to-image (T2I) models are vulnerable to adversarial attacks, especially with noun perturbations in text prompts. In this study, we investigate the impact of adversarial attacks on different POS tags within text prompts on the images generated by T2I models. We create a high-quality dataset for realistic POS tag token swapping and perform gradient-based attacks to find adversarial suffixes that mislead T2I models into generating images with altered tokens. Our empirical results show that the attack success rate (ASR) varies significantly among different POS tag categories, with nouns, proper nouns, and adjectives being the easiest to attack. We explore the mechanism behind the steering effect of adversarial suffixes, finding that the number of critical tokens and content fusion vary among POS tags, while features like suffix transferability are consistent across categories. We have made our implementation publicly available at - https://github.com/shahariar-shibli/Adversarial-Attack-on-POS-Tags.",
            "link": "https://www.semanticscholar.org/paper/1d75165a64dbcb98686e858f883ea2fc5393bd2b",
            "authors": "G. M. Shahariar, Jia Chen, Jiachen Li, Yue Dong",
            "matchScore": 230.41168,
            "original title": "Adversarial Attacks on Parts of Speech: An Empirical Study in Text-to-Image Generation",
            "original authors": "G M Shahariar, Jia Chen, Jiachen Li, Yue Dong",
            "EMNLP Paper ID": "2524",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "43ae563a561e6f8339902ccf062433fd19fe6027",
            "title": "Attacks against Abstractive Text Summarization Models through Lead Bias and Influence Functions",
            "abstract": "Large Language Models have introduced novel opportunities for text comprehension and generation. Yet, they are vulnerable to adversarial perturbations and data poisoning attacks, particularly in tasks like text classification and translation. However, the adversarial robustness of abstractive text summarization models remains less explored. In this work, we unveil a novel approach by exploiting the inherent lead bias in summarization models, to perform adversarial perturbations. Furthermore, we introduce an innovative application of influence functions, to execute data poisoning, which compromises the model's integrity. This approach not only shows a skew in the models behavior to produce desired outcomes but also shows a new behavioral change, where models under attack tend to generate extractive summaries rather than abstractive summaries.",
            "link": "https://www.semanticscholar.org/paper/43ae563a561e6f8339902ccf062433fd19fe6027",
            "authors": "Poojitha Thota, Shirin Nilizadeh",
            "matchScore": 273.6471,
            "original title": "Attacks against Abstractive Text Summarization Models through Lead Bias and Influence Functions",
            "original authors": "Poojitha Thota, Shirin Nilizadeh",
            "EMNLP Paper ID": "2679",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "09e2391ab4c266f96c71cc379c1a7c7ddd40ee33",
            "title": "DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers",
            "abstract": "The safety alignment of Large Language Models (LLMs) is vulnerable to both manual and automated jailbreak attacks, which adversarially trigger LLMs to output harmful content. However, current methods for jailbreaking LLMs, which nest entire harmful prompts, are not effective at concealing malicious intent and can be easily identified and rejected by well-aligned LLMs. This paper discovers that decomposing a malicious prompt into separated sub-prompts can effectively obscure its underlying malicious intent by presenting it in a fragmented, less detectable form, thereby addressing these limitations. We introduce an automatic prompt \\textbf{D}ecomposition and \\textbf{R}econstruction framework for jailbreak \\textbf{Attack} (DrAttack). DrAttack includes three key components: (a) `Decomposition' of the original prompt into sub-prompts, (b) `Reconstruction' of these sub-prompts implicitly by in-context learning with semantically similar but harmless reassembling demo, and (c) a `Synonym Search' of sub-prompts, aiming to find sub-prompts' synonyms that maintain the original intent while jailbreaking LLMs. An extensive empirical study across multiple open-source and closed-source LLMs demonstrates that, with a significantly reduced number of queries, DrAttack obtains a substantial gain of success rate over prior SOTA prompt-only attackers. Notably, the success rate of 78.0\\% on GPT-4 with merely 15 queries surpassed previous art by 33.1\\%. The project is available at https://github.com/xirui-li/DrAttack.",
            "link": "https://www.semanticscholar.org/paper/09e2391ab4c266f96c71cc379c1a7c7ddd40ee33",
            "authors": "Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, Cho-Jui Hsieh",
            "matchScore": 287.18237,
            "original title": "DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLMs Jailbreakers",
            "original authors": "Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, Cho-Jui Hsieh",
            "EMNLP Paper ID": "2698",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "4837da4279ff9b49ab7480bd780d3981d0b8094e",
            "title": "Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression",
            "abstract": "Increasingly, model compression techniques enable large language models (LLMs) to be deployed in real-world applications. As a result of this momentum towards local deployment, compressed LLMs will interact with a large population. Prior work on compression typically prioritize preserving perplexity, which is directly analogous to training loss. The impact of compression method on other critical aspects of model behavior\\, -- \\,particularly safety\\, -- \\,requires systematic assessment. To this end, we investigate the impact of model compression along four dimensions: (1) degeneration harm, i.e., bias and toxicity in generation; (2) representational harm, i.e., biases in discriminative tasks; (3) dialect bias; and(4) language modeling and downstream task performance. We examine a wide spectrum of LLM compression techniques, including unstructured pruning, semi-structured pruning, and quantization. Our analysis reveals that compression can lead to unexpected consequences. Although compression may unintentionally alleviate LLMs' degeneration harm, it can still exacerbate representational harm. Furthermore, increasing compression produces a divergent impact on different protected groups. Finally, different compression methods have drastically different safety impacts: for example, quantization mostly preserves bias while pruning degrades quickly. Our findings underscore the importance of integrating safety assessments into the development of compressed LLMs to ensure their reliability across real-world applications.\\footnote{Our implementation and results are available here: \\url{https://github.com/zhichaoxu-shufe/Beyond-Perplexity-Compression-Safety-Eval}}",
            "link": "https://www.semanticscholar.org/paper/4837da4279ff9b49ab7480bd780d3981d0b8094e",
            "authors": "Zhichao Xu, Ashim Gupta, Tao Li, Oliver Bentham, Vivek Srikumar",
            "matchScore": 251.86807,
            "original title": "Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression",
            "original authors": "Zhichao Xu, Ashim Gupta, Tao Li, Oliver Bentham, Vivek Srikumar",
            "EMNLP Paper ID": "2946",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "2b01cbe125ed13ccb3ef02e9536582825f2afd57",
            "title": "How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States",
            "abstract": "Large language models (LLMs) rely on safety alignment to avoid responding to malicious user inputs. Unfortunately, jailbreak can circumvent safety guardrails, resulting in LLMs generating harmful content and raising concerns about LLM safety. Due to language models with intensive parameters often regarded as black boxes, the mechanisms of alignment and jailbreak are challenging to elucidate. In this paper, we employ weak classifiers to explain LLM safety through the intermediate hidden states. We first confirm that LLMs learn ethical concepts during pre-training rather than alignment and can identify malicious and normal inputs in the early layers. Alignment actually associates the early concepts with emotion guesses in the middle layers and then refines them to the specific reject tokens for safe generations. Jailbreak disturbs the transformation of early unethical classification into negative emotions. We conduct experiments on models from 7B to 70B across various model families to prove our conclusion. Overall, our paper indicates the intrinsical mechanism of LLM safety and how jailbreaks circumvent safety guardrails, offering a new perspective on LLM safety and reducing concerns. Our code is available at https://github.com/ydyjya/LLM-IHS-Explanation.",
            "link": "https://www.semanticscholar.org/paper/2b01cbe125ed13ccb3ef02e9536582825f2afd57",
            "authors": "Zhenhong Zhou, Haiyang Yu, Xinghua Zhang, Rongwu Xu, Fei Huang, Yongbin Li",
            "matchScore": 308.73395,
            "original title": "How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States",
            "original authors": "Zhenhong Zhou, Haiyang Yu, Xinghua Zhang, Rongwu Xu, Fei Huang, Yongbin Li",
            "EMNLP Paper ID": "509",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Findings"
        }
    ],
    "Multilingual and Cross-Lingual Optimization of Large Language Models": [
        {
            "paperId": "a992a1ff27c1bd04a964bf7ed82c6db19fd7f671",
            "title": "Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?",
            "abstract": "Traditionally, success in multilingual machine translation can be attributed to three key factors in training data: large volume, diverse translation directions, and high quality. In the current practice of fine-tuning large language models (LLMs) for translation, we revisit the importance of these factors. We find that LLMs display strong translation capability after being fine-tuned on as few as 32 parallel sentences and that fine-tuning on a single translation direction enables translation in multiple directions. However, the choice of direction is critical: fine-tuning LLMs with only English on the target side can lead to task misinterpretation, which hinders translation into non-English languages. Problems also arise when noisy synthetic data is placed on the target side, especially when the target language is well-represented in LLM pre-training. Yet interestingly, synthesized data in an under-represented language has a less pronounced effect. Our findings suggest that when adapting LLMs to translation, the requirement on data quantity can be eased but careful considerations are still crucial to prevent an LLM from exploiting unintended data biases.",
            "link": "https://www.semanticscholar.org/paper/a992a1ff27c1bd04a964bf7ed82c6db19fd7f671",
            "authors": "D. Zhu, Pinzhen Chen, Miaoran Zhang, B. Haddow, Xiaoyu Shen, Dietrich Klakow",
            "EMNLP Paper ID": "43",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "70546742ecfeb46ff41be90d88a4d5c27c55ed2d",
            "title": "When Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages",
            "abstract": "Multilingual language models are widely used to extend NLP systems to low-resource languages. However, concrete evidence for the effects of multilinguality on language modeling performance in individual languages remains scarce. Here, we pre-train over 10,000 monolingual and multilingual language models for over 250 languages, including multiple language families that are under-studied in NLP. We assess how language modeling performance in each language varies as a function of (1) monolingual dataset size, (2) added multilingual dataset size, (3) linguistic similarity of the added languages, and (4) model size (up to 45M parameters). We find that in moderation, adding multilingual data improves low-resource language modeling performance, similar to increasing low-resource dataset sizes by up to 33%. Improvements depend on the syntactic similarity of the added multilingual data, with marginal additional effects of vocabulary overlap. However, high-resource languages consistently perform worse in multilingual pre-training scenarios. As dataset sizes increase, adding multilingual data begins to hurt performance for both low-resource and high-resource languages, likely due to limited model capacity (the\"curse of multilinguality\"). These results suggest that massively multilingual pre-training may not be optimal for any languages involved, but that more targeted models can significantly improve performance.",
            "link": "https://www.semanticscholar.org/paper/70546742ecfeb46ff41be90d88a4d5c27c55ed2d",
            "authors": "Tyler A. Chang, Catherine Arnett, Zhuowen Tu, Benjamin Bergen",
            "EMNLP Paper ID": "454",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "f2dd6ead79e5eab758993dd7f9b747194f5bf95e",
            "title": "Concept Space Alignment in Multilingual LLMs",
            "abstract": "Multilingual large language models (LLMs) seem to generalize somewhat across languages. We hypothesize this is a result of implicit vector space alignment. Evaluating such alignment, we see that larger models exhibit very high-quality linear alignments between corresponding concepts in different languages. Our experiments show that multilingual LLMs suffer from two familiar weaknesses: generalization works best for languages with similar typology, and for abstract concepts. For some models, e.g., the Llama-2 family of models, prompt-based embeddings align better than word embeddings, but the projections are less linear -- an observation that holds across almost all model families, indicating that some of the implicitly learned alignments are broken somewhat by prompt-based methods.",
            "link": "https://www.semanticscholar.org/paper/f2dd6ead79e5eab758993dd7f9b747194f5bf95e",
            "authors": "Qiwei Peng, Anders Sogaard",
            "EMNLP Paper ID": "614",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "ff27fd5059294c8356b871ab16afa624279d0ec1",
            "title": "Do Not Worry if You Do Not Have Data: Building Pretrained Language Models Using Translationese",
            "abstract": "In this paper, we explore the utility of Translationese as synthetic data created using machine translation for pre-training language models (LMs). Pre-training requires vast amounts of monolingual data, which is mostly unavailable for languages other than English. Recently, there has been a growing interest in using synthetic data to address this data scarcity. We take the case of English and Indic languages and translate web-crawled monolingual documents (clean) into the target language. Then, we train language models containing 28M and 85M parameters on this translationese data (synthetic). We show that their performance on downstream natural language understanding and generative tasks is only 3.56% poorer on NLU tasks and 1.51% on NLG tasks than LMs pre-trained on clean data. Further, we propose the use of lightweight TinyLMs pre-trained on clean data to filter synthetic data efficiently which significantly improves the performance of our models. We also find that LMs trained on synthetic data strongly benefit from extended pretraining on a tiny fraction (10%) of clean data. We release the data we collected and created as a part of this work, IndicMonoDoc, the largest collection of monolingual document-level corpora, which we hope will help bridge the gap between English and non-English performance for large language models.",
            "link": "https://www.semanticscholar.org/paper/ff27fd5059294c8356b871ab16afa624279d0ec1",
            "authors": "Meet Doshi, Raj Dabre, Pushpak Bhattacharyya",
            "EMNLP Paper ID": "652",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "cc2ef382261f63f5177ee11cd93cca1e5806cbf9",
            "title": "Mitigating Frequency Bias and Anisotropy in Language Model Pre-Training with Syntactic Smoothing",
            "abstract": "Language models strongly rely on frequency information because they maximize the likelihood of tokens during pre-training. As a consequence, language models tend to not generalize well to tokens that are seldom seen during training. Moreover, maximum likelihood training has been discovered to give rise to anisotropy: representations of tokens in a model tend to cluster tightly in a high-dimensional cone, rather than spreading out over their representational capacity. Our work introduces a method for quantifying the frequency bias of a language model by assessing sentence-level perplexity with respect to token-level frequency. We then present a method for reducing the frequency bias of a language model by inducing a syntactic prior over token representations during pre-training. Our Syntactic Smoothing method adjusts the maximum likelihood objective function to distribute the learning signal to syntactically similar tokens. This approach results in better performance on infrequent English tokens and a decrease in anisotropy. We empirically show that the degree of anisotropy in a model correlates with its frequency bias.",
            "link": "https://www.semanticscholar.org/paper/cc2ef382261f63f5177ee11cd93cca1e5806cbf9",
            "authors": "Richard Diehl Martinez, Z\u00e9bulon Goriely, Andrew Caines, P. Buttery, Lisa Beinborn",
            "EMNLP Paper ID": "670",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "07675d6362465471ee327910348c390de7fd57c1",
            "title": "Neuron Specialization: Leveraging intrinsic task modularity for multilingual machine translation",
            "abstract": "Training a unified multilingual model promotes knowledge transfer but inevitably introduces negative interference. Language-specific modeling methods show promise in reducing interference. However, they often rely on heuristics to distribute capacity and struggle to foster cross-lingual transfer via isolated modules. In this paper, we explore intrinsic task modularity within multilingual networks and leverage these observations to circumvent interference under multilingual translation. We show that neurons in the feed-forward layers tend to be activated in a language-specific manner. Meanwhile, these specialized neurons exhibit structural overlaps that reflect language proximity, which progress across layers. Based on these findings, we propose Neuron Specialization, an approach that identifies specialized neurons to modularize feed-forward layers and then continuously updates them through sparse networks. Extensive experiments show that our approach achieves consistent performance gains over strong baselines with additional analyses demonstrating reduced interference and increased knowledge transfer.",
            "link": "https://www.semanticscholar.org/paper/07675d6362465471ee327910348c390de7fd57c1",
            "authors": "Shaomu Tan, Di Wu, C. Monz",
            "EMNLP Paper ID": "729",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "48c94ddaecbb102ed200e0f08044aa271f9fb199",
            "title": "Revealing the Parallel Multilingual Learning within Large Language Models",
            "abstract": "In this study, we reveal an in-context learning (ICL) capability of multilingual large language models (LLMs): by translating the input to several languages, we provide Parallel Input in Multiple Languages (PiM) to LLMs, which significantly enhances their comprehension abilities. To test this capability, we design extensive experiments encompassing 8 typical datasets, 7 languages and 8 state-of-the-art multilingual LLMs. Experimental results show that (1) incorporating more languages help PiM surpass the conventional ICL further; (2) even combining with the translations that are inferior to baseline performance can also help. Moreover, by examining the activated neurons in LLMs, we discover a counterintuitive but interesting phenomenon. Contrary to the common thought that PiM would activate more neurons than monolingual input to leverage knowledge learned from diverse languages, PiM actually inhibits neurons and promotes more precise neuron activation especially when more languages are added. This phenomenon aligns with the neuroscience insight about synaptic pruning, which removes less used neural connections, strengthens remainders, and then enhances brain intelligence.",
            "link": "https://www.semanticscholar.org/paper/48c94ddaecbb102ed200e0f08044aa271f9fb199",
            "authors": "Yongyu Mu, Peinan Feng, Zhiquan Cao, Yuzhang Wu, Bei Li, Chenglong Wang, Tong Xiao, Kai Song, Tongran Liu, Chunliang Zhang, Jingbo Zhu",
            "EMNLP Paper ID": "781",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "b742c177af2dc80bb7b9d13fabf594662ff81862",
            "title": "Breaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale",
            "abstract": "In recent years, Large Language Models (LLMs) have made significant strides towards Artificial General Intelligence. However, training these models from scratch requires substantial computational resources and vast amounts of text data. In this paper, we explore an alternative approach to constructing an LLM for a new language by continually pretraining (CPT) from existing pretrained LLMs, instead of using randomly initialized parameters. Based on parallel experiments on 40 model sizes ranging from 40M to 5B parameters, we find that 1) CPT converges faster and saves significant resources in a scalable manner; 2) CPT adheres to an extended scaling law derived from Hoffmann et al. (2022) with a joint data-parameter scaling term; 3) The compute-optimal data-parameter allocation for CPT markedly differs based on our estimated scaling factors; 4) The effectiveness of transfer at scale is influenced by training duration and linguistic properties, while robust to data replaying, a method that effectively mitigates catastrophic forgetting in CPT. We hope our findings provide deeper insights into the transferability of LLMs at scale for the research community.",
            "link": "https://www.semanticscholar.org/paper/b742c177af2dc80bb7b9d13fabf594662ff81862",
            "authors": "Wenzhen Zheng, Wenbo Pan, Xu Xu, Libo Qin, Li Yue, Ming Zhou",
            "EMNLP Paper ID": "875",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "eb7467676e583aed2cca1b3aa8a998ba86b2af61",
            "title": "Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners",
            "abstract": "Recently, Large Language Models (LLMs) have shown impressive language capabilities. While most of the existing LLMs have very unbalanced performance across different languages, multilingual alignment based on translation parallel data is an effective method to enhance the LLMs' multilingual capabilities. In this work, we discover and comprehensively investigate the spontaneous multilingual alignment improvement of LLMs. We find that LLMs instruction-tuned on the question translation data (i.e. without annotated answers) are able to encourage the alignment between English and a wide range of languages, even including those unseen during instruction-tuning. Additionally, we utilize different settings and mechanistic interpretability methods to analyze the LLM's performance in the multilingual scenario comprehensively. Our work suggests that LLMs have enormous potential for improving multilingual alignment efficiently with great language and task generalization.",
            "link": "https://www.semanticscholar.org/paper/eb7467676e583aed2cca1b3aa8a998ba86b2af61",
            "authors": "Shimao Zhang, Changjiang Gao, Wenhao Zhu, Jiajun Chen, Xin Huang, Xue Han, Junlan Feng, Chao Deng, Shujian Huang",
            "EMNLP Paper ID": "914",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "a3ca19771db42fa926f61d79eac4958b8a2e2b66",
            "title": "PreAlign: Boosting Cross-Lingual Transfer by Early Establishment of Multilingual Alignment",
            "abstract": "Large language models demonstrate reasonable multilingual abilities, despite predominantly English-centric pretraining. However, the spontaneous multilingual alignment in these models is shown to be weak, leading to unsatisfactory cross-lingual transfer and knowledge sharing. Previous works attempt to address this issue by explicitly injecting multilingual alignment information during or after pretraining. Thus for the early stage in pretraining, the alignment is weak for sharing information or knowledge across languages. In this paper, we propose PreAlign, a framework that establishes multilingual alignment prior to language model pretraining. PreAlign injects multilingual alignment by initializing the model to generate similar representations of aligned words and preserves this alignment using a code-switching strategy during pretraining. Extensive experiments in a synthetic English to English-Clone setting demonstrate that PreAlign significantly outperforms standard multilingual joint training in language modeling, zero-shot cross-lingual transfer, and cross-lingual knowledge application. Further experiments in real-world scenarios further validate PreAlign's effectiveness across various model sizes.",
            "link": "https://www.semanticscholar.org/paper/a3ca19771db42fa926f61d79eac4958b8a2e2b66",
            "authors": "Jiahuan Li, Shujian Huang, Xinyu Dai, Jiajun Chen",
            "EMNLP Paper ID": "1145",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "e4b757235fdc51de0e67cce47e9b4c3a5cab551c",
            "title": "Breaking the Curse of Multilinguality with Cross-lingual Expert Language Models",
            "abstract": "Despite their popularity in non-English NLP, multilingual language models often underperform monolingual ones due to inter-language competition for model parameters. We propose Cross-lingual Expert Language Models (X-ELM), which mitigate this competition by independently training language models on subsets of the multilingual corpus. This process specializes X-ELMs to different languages while remaining effective as a multilingual ensemble. Our experiments show that when given the same compute budget, X-ELM outperforms jointly trained multilingual models across all considered languages and that these gains transfer to downstream tasks. X-ELM provides additional benefits over performance improvements: new experts can be iteratively added, adapting X-ELM to new languages without catastrophic forgetting. Furthermore, training is asynchronous, reducing the hardware requirements for multilingual training and democratizing multilingual modeling.",
            "link": "https://www.semanticscholar.org/paper/e4b757235fdc51de0e67cce47e9b4c3a5cab551c",
            "authors": "Terra Blevins, Tomasz Limisiewicz, Suchin Gururangan, Margaret Li, Hila Gonen, Noah A. Smith, Luke S. Zettlemoyer",
            "EMNLP Paper ID": "1233",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "f2bedc815eca8d5be46d88d27b44d1c86bbb8b90",
            "title": "Unraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications",
            "abstract": "Recently, large language models (LLMs) have achieved tremendous breakthroughs in the field of NLP, but still lack understanding of their internal neuron activities when processing different languages. We designed a method to convert dense LLMs into fine-grained MoE architectures, and then visually studied the multilingual activation patterns of LLMs through expert activation frequency heatmaps. Through comprehensive experiments on different model families, different model sizes, and different variants, we analyzed the similarities and differences in the internal neuron activation patterns of LLMs when processing different languages. Specifically, we investigated the distribution of high-frequency activated experts, multilingual shared experts, whether multilingual activation patterns are related to language families, and the impact of instruction tuning on activation patterns. We further explored leveraging the discovered differences in expert activation frequencies to guide sparse activation and pruning. Experimental results demonstrated that our method significantly outperformed random expert pruning and even exceeded the performance of unpruned models in some languages. Additionally, we found that configuring different pruning rates for different layers based on activation level differences could achieve better results. Our findings reveal the multilingual processing mechanisms within LLMs and utilize these insights to offer new perspectives for applications such as sparse activation and model pruning.",
            "link": "https://www.semanticscholar.org/paper/f2bedc815eca8d5be46d88d27b44d1c86bbb8b90",
            "authors": "Weize Liu, Yinlong Xu, Hongxia Xu, Jintai Chen, Xuming Hu, Jian Wu",
            "EMNLP Paper ID": "1381",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "a4027241c7b5f8ac6e7eb21eedeb21f19320d8b5",
            "title": "1+1>2: Can Large Language Models Serve as Cross-Lingual Knowledge Aggregators?",
            "abstract": "Large Language Models (LLMs) have garnered significant attention due to their remarkable ability to process information across various languages. Despite their capabilities, they exhibit inconsistencies in handling identical queries in different languages, presenting challenges for further advancement. This paper introduces a method to enhance the multilingual performance of LLMs by aggregating knowledge from diverse languages. This approach incorporates a low-resource knowledge detector specific to a language, a language selection process, and mechanisms for answer replacement and integration. Our experiments demonstrate notable performance improvements, particularly in reducing language performance disparity. An ablation study confirms that each component of our method significantly contributes to these enhancements. This research highlights the inherent potential of LLMs to harmonize multilingual capabilities and offers valuable insights for further exploration.",
            "link": "https://www.semanticscholar.org/paper/a4027241c7b5f8ac6e7eb21eedeb21f19320d8b5",
            "authors": "Yue Huang, Chenrui Fan, Yuan Li, Siyuan Wu, Tianyi Zhou, Xiangliang Zhang, Lichao Sun",
            "EMNLP Paper ID": "1552",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "4fab7f960aa833670a52c2fdbcd4d0943e2f3b67",
            "title": "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models",
            "abstract": "Fine-tuning pre-trained large language models (LLMs) on a diverse array of tasks has become a common approach for building models that can solve various natural language processing (NLP) tasks. However, where and to what extent these models retain task-specific knowledge remains largely unexplored. This study investigates the task-specific information encoded in pre-trained LLMs and the effects of instruction tuning on their representations across a diverse set of over 60 NLP tasks. We use a set of matrix analysis tools to examine the differences between the way pre-trained and instruction-tuned LLMs store task-specific information. Our findings reveal that while some tasks are already encoded within the pre-trained LLMs, others greatly benefit from instruction tuning. Additionally, we pinpointed the layers in which the model transitions from high-level general representations to more task-oriented representations. This finding extends our understanding of the governing mechanisms of LLMs and facilitates future research in the fields of parameter-efficient transfer learning and multi-task learning.",
            "link": "https://www.semanticscholar.org/paper/4fab7f960aa833670a52c2fdbcd4d0943e2f3b67",
            "authors": "Zheng Zhao, Yftah Ziser, Shay B. Cohen",
            "EMNLP Paper ID": "1769",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "6f5c24e67cae1c93ad31c17046a9f0f621b7cde9",
            "title": "Mitigating the Language Mismatch and Repetition Issues in LLM-based Machine Translation via Model Editing",
            "abstract": "Large Language Models (LLMs) have recently revolutionized the NLP field, while they still fall short in some specific down-stream tasks. In the work, we focus on utilizing LLMs to perform machine translation, where we observe that two patterns of errors frequently occur and drastically affect the translation quality: language mismatch and repetition. The work sets out to explore the potential for mitigating these two issues by leveraging model editing methods, e.g., by locating Feed-Forward Network (FFN) neurons or something that are responsible for the errors and deactivating them in the inference time. We find that directly applying such methods either limited effect on the targeted errors or has significant negative side-effect on the general translation quality, indicating that the located components may also be crucial for ensuring machine translation with LLMs on the rails. To this end, we propose to refine the located components by fetching the intersection of the locating results under different language settings, filtering out the aforementioned information that is irrelevant to targeted errors. The experiment results empirically demonstrate that our methods can effectively reduce the language mismatch and repetition ratios and meanwhile enhance or keep the general translation quality in most cases.",
            "link": "https://www.semanticscholar.org/paper/6f5c24e67cae1c93ad31c17046a9f0f621b7cde9",
            "authors": "Weichuan Wang, Zhaoyi Li, Defu Lian, Chen Ma, Linqi Song, Ying Wei",
            "EMNLP Paper ID": "1844",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "d970dbc0a43910f73fcdbdc2946f98aaa5d09590",
            "title": "A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives",
            "abstract": "Pretrained language models (PLMs) display impressive performances and have captured the attention of the NLP community. Establishing best practices in pretraining has, therefore, become a major focus of NLP research, especially since insights gained from monolingual English models may not necessarily apply to more complex multilingual models. One significant caveat of the current state of the art is that different works are rarely comparable: they often discuss different parameter counts, training data, and evaluation methodology. This paper proposes a comparison of multilingual pretraining objectives in a controlled methodological environment. We ensure that training data and model architectures are comparable, and discuss the downstream performances across 6 languages that we observe in probing and fine-tuning scenarios. We make two key observations: (1) the architecture dictates which pretraining objective is optimal; (2) multilingual translation is a very effective pretraining objective under the right conditions. We make our code, data, and model weights available at \\texttt{\\url{https://github.com/Helsinki-NLP/lm-vs-mt}}.",
            "link": "https://www.semanticscholar.org/paper/d970dbc0a43910f73fcdbdc2946f98aaa5d09590",
            "authors": "Zihao Li, Shaoxiong Ji, Timothee Mickus, Vincent Segonne, J\u00f6rg Tiedemann",
            "EMNLP Paper ID": "1865",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "1cc00055e622701ea414e2924b6dbda48cd2575d",
            "title": "The Zeno's Paradox of `Low-Resource' Languages",
            "abstract": "The disparity in the languages commonly studied in Natural Language Processing (NLP) is typically reflected by referring to languages as low vs high-resourced. However, there is limited consensus on what exactly qualifies as a `low-resource language.' To understand how NLP papers define and study `low resource' languages, we qualitatively analyzed 150 papers from the ACL Anthology and popular speech-processing conferences that mention the keyword `low-resource.' Based on our analysis, we show how several interacting axes contribute to `low-resourcedness' of a language and why that makes it difficult to track progress for each individual language. We hope our work (1) elicits explicit definitions of the terminology when it is used in papers and (2) provides grounding for the different axes to consider when connoting a language as low-resource.",
            "link": "https://www.semanticscholar.org/paper/1cc00055e622701ea414e2924b6dbda48cd2575d",
            "authors": "H. Nigatu, A. Tonja, Benjamin Rosman, T. Solorio, Monojit Choudhury",
            "EMNLP Paper ID": "2134",
            "Oral/Poster": "Oral",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "53f6119ca6e1489fb842982a00e4979364b84c18",
            "title": "Adaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes Fail to Improve?",
            "abstract": "In the last decade, the generalization and adaptation abilities of deep learning models were typically evaluated on fixed training and test distributions. Contrary to traditional deep learning, large language models (LLMs) are (i) even more overparameterized, (ii) trained on unlabeled text corpora curated from the Internet with minimal human intervention, and (iii) trained in an online fashion. These stark contrasts prevent researchers from transferring lessons learned on model generalization and adaptation in deep learning contexts to LLMs. To this end, our short paper introduces empirical observations that aim to shed light on further training of already pretrained language models. Specifically, we demonstrate that training a model on a text domain could degrade its perplexity on the test portion of the same domain. We observe with our subsequent analysis that the performance degradation is positively correlated with the similarity between the additional and the original pretraining dataset of the LLM. Our further token-level perplexity observations reveals that the perplexity degradation is due to a handful of tokens that are not informative about the domain. We hope these findings will guide us in determining when to adapt a model vs when to rely on its foundational capabilities.",
            "link": "https://www.semanticscholar.org/paper/53f6119ca6e1489fb842982a00e4979364b84c18",
            "authors": "Firat Oncel, Matthias Bethge, B. Ermi\u015f, M. Ravanelli, Cem Subakan, cCaugatay Yildiz",
            "EMNLP Paper ID": "2579",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "cce4e698001b8902d0f2945fabefccee680406cb",
            "title": "Can we teach language models to gloss endangered languages?",
            "abstract": "Interlinear glossed text (IGT) is a popular format in language documentation projects, where each morpheme is labeled with a descriptive annotation. Automating the creation of interlinear glossed text would be desirable to reduce annotator effort and maintain consistency across annotated corpora. Prior research has explored a number of statistical and neural methods for automatically producing IGT. As large language models (LLMs) have showed promising results across multilingual tasks, even for rare, endangered languages, it is natural to wonder whether they can be utilized for the task of generating IGT. We explore whether LLMs can be effective at the task of interlinear glossing with in-context learning, without any traditional training. We propose new approaches for selecting examples to provide in-context, observing that targeted selection can significantly improve performance. We find that LLM-based methods beat standard transformer baselines, despite requiring no training at all. These approaches still underperform state-of-the-art supervised systems for the task, but are highly practical for researchers outside of the NLP community, requiring minimal effort to use.",
            "link": "https://www.semanticscholar.org/paper/cce4e698001b8902d0f2945fabefccee680406cb",
            "authors": "Michael Ginn, Mans Hulden, Alexis Palmer",
            "matchScore": 233.08308,
            "original title": "Can we teach language models to gloss endangered languages?",
            "original authors": "Michael Ginn, Mans Hulden, Alexis Palmer",
            "EMNLP Paper ID": "1187",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "9f1b3fdf2af592c850b7e50c0022cb7cce2ae103",
            "title": "An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Language Model Inference",
            "abstract": "The development of state-of-the-art generative large language models (LLMs) disproportionately relies on English-centric tokenizers, vocabulary and pre-training data. Despite the fact that some LLMs have multilingual capabilities, recent studies have shown that their inference efficiency deteriorates when generating text in languages other than English. This results in increased inference time and costs. Cross-lingual vocabulary adaptation (CVA) methods have been proposed for adapting models to a target language aiming to improve downstream performance. However, the effectiveness of these methods on increasing inference efficiency of generative LLMs has yet to be explored. In this paper, we perform an empirical study of five CVA methods on four generative LLMs (including monolingual and multilingual models) across four typologically-diverse languages and four natural language understanding tasks. We find that CVA substantially contributes to LLM inference speedups of up to 271.5\\%. We also show that adapting LLMs that have been pre-trained on more balanced multilingual data results in downstream performance comparable to the original models.",
            "link": "https://www.semanticscholar.org/paper/9f1b3fdf2af592c850b7e50c0022cb7cce2ae103",
            "authors": "Atsuki Yamaguchi, Aline Villavicencio, Nikolaos Aletras",
            "matchScore": 254.1753,
            "original title": "An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Language Model Inference",
            "original authors": "Atsuki Yamaguchi, Aline Villavicencio, Nikolaos Aletras",
            "EMNLP Paper ID": "1380",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "d0d59e4e7fdea0897fed6d9b8f0598cd7aedc2bf",
            "title": "Should Cross-Lingual AMR Parsing go Meta? An Empirical Assessment of Meta-Learning and Joint Learning AMR Parsing",
            "abstract": "Cross-lingual AMR parsing is the task of predicting AMR graphs in a target language when training data is available only in a source language. Due to the small size of AMR training data and evaluation data, cross-lingual AMR parsing has only been explored in a small set of languages such as English, Spanish, German, Chinese, and Italian. Taking inspiration from Langedijk et al. (2022), who apply meta-learning to tackle cross-lingual syntactic parsing, we investigate the use of meta-learning for cross-lingual AMR parsing. We evaluate our models in $k$-shot scenarios (including 0-shot) and assess their effectiveness in Croatian, Farsi, Korean, Chinese, and French. Notably, Korean and Croatian test sets are developed as part of our work, based on the existing The Little Prince English AMR corpus, and made publicly available. We empirically study our method by comparing it to classical joint learning. Our findings suggest that while the meta-learning model performs slightly better in 0-shot evaluation for certain languages, the performance gain is minimal or absent when $k$ is higher than 0.",
            "link": "https://www.semanticscholar.org/paper/d0d59e4e7fdea0897fed6d9b8f0598cd7aedc2bf",
            "authors": "Jeongwoo Jay Kang, Maximin Coavoux, C'edric Lopez, Didier Schwab",
            "matchScore": 402.18115,
            "original title": "Should Cross-Lingual AMR Parsing go Meta? An Empirical Assessment of Meta-Learning and Joint Learning AMR Parsing",
            "original authors": "Jeongwoo Kang, Maximin Coavoux, C\u00e9dric Lopez, Didier Schwab",
            "EMNLP Paper ID": "16",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 14): 13:00\u201314:00 (Afternoon)",
            "Type": "Findings"
        },
        {
            "paperId": "e4ad68f51b6235d3b2c3824a00cb3ea27f49e30c",
            "title": "NeuroMax: Enhancing Neural Topic Modeling via Maximizing Mutual Information and Group Topic Regularization",
            "abstract": "Recent advances in neural topic models have concentrated on two primary directions: the integration of the inference network (encoder) with a pre-trained language model (PLM) and the modeling of the relationship between words and topics in the generative model (decoder). However, the use of large PLMs significantly increases inference costs, making them less practical for situations requiring low inference times. Furthermore, it is crucial to simultaneously model the relationships between topics and words as well as the interrelationships among topics themselves. In this work, we propose a novel framework called NeuroMax (Neural Topic Model with Maximizing Mutual Information with Pretrained Language Model and Group Topic Regularization) to address these challenges. NeuroMax maximizes the mutual information between the topic representation obtained from the encoder in neural topic models and the representation derived from the PLM. Additionally, NeuroMax employs optimal transport to learn the relationships between topics by analyzing how information is transported among them. Experimental results indicate that NeuroMax reduces inference time, generates more coherent topics and topic groups, and produces more representative document embeddings, thereby enhancing performance on downstream tasks.",
            "link": "https://www.semanticscholar.org/paper/e4ad68f51b6235d3b2c3824a00cb3ea27f49e30c",
            "authors": "Duy-Tung Pham, Thien Trang Nguyen Vu, T. Nguyen, L. Van, Duc Anh Nguyen, Thien Huu Nguyen",
            "matchScore": 316.19156,
            "original title": "NeuroMax: Enhancing Neural Topic Modeling via Maximizing Mutual Information and Group Topic Regularization",
            "original authors": "Duy-Tung Pham, Thien Trang Nguyen Vu, Tung Nguyen, Linh Van Ngo, Duc Anh Nguyen, Thien Huu Nguyen",
            "EMNLP Paper ID": "1627",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "e2047ddf1ec753c336f45ce9234ddf06f431b115",
            "title": "Unlocking the Potential of Model Merging for Low-Resource Languages",
            "abstract": "Adapting large language models (LLMs) to new languages typically involves continual pre-training (CT) followed by supervised fine-tuning (SFT). However, this CT-then-SFT approach struggles with limited data in the context of low-resource languages, failing to balance language modeling and task-solving capabilities. We thus propose model merging as an alternative for low-resource languages, combining models with distinct capabilities into a single model without additional training. We use model merging to develop task-solving LLMs for low-resource languages without SFT data in the target languages. Our experiments based on Llama-2-7B demonstrate that model merging effectively endows LLMs for low-resource languages with task-solving abilities, outperforming CT-then-SFT in scenarios with extremely scarce data. Observing performance saturation in model merging with more training tokens, we further analyze the merging process and introduce a slack variable to the model merging algorithm to mitigate the loss of important parameters, thereby enhancing performance. We hope that model merging can benefit more human languages suffering from data scarcity with its higher data efficiency.",
            "link": "https://www.semanticscholar.org/paper/e2047ddf1ec753c336f45ce9234ddf06f431b115",
            "authors": "Mingxu Tao, Chen Zhang, Quzhe Huang, Tianyao Ma, Songfang Huang, Dongyan Zhao, Yansong Feng",
            "matchScore": 211.30751,
            "original title": "Unlocking the Potential of Model Merging for Low-Resource Languages",
            "original authors": "Mingxu Tao, Chen Zhang, Quzhe Huang, Tianyao Ma, Songfang Huang, Dongyan Zhao, Yansong Feng",
            "EMNLP Paper ID": "1818",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "f9a1fa85ab5346fd5df0ee94d58f80e341f48f53",
            "title": "Low-Resource Machine Translation through the Lens of Personalized Federated Learning",
            "abstract": "We present a new approach based on the Personalized Federated Learning algorithm MeritFed that can be applied to Natural Language Tasks with heterogeneous data. We evaluate it on the Low-Resource Machine Translation task, using the dataset from the Large-Scale Multilingual Machine Translation Shared Task (Small Track #2) and the subset of Sami languages from the multilingual benchmark for Finno-Ugric languages. In addition to its effectiveness, MeritFed is also highly interpretable, as it can be applied to track the impact of each language used for training. Our analysis reveals that target dataset size affects weight distribution across auxiliary languages, that unrelated languages do not interfere with the training, and auxiliary optimizer parameters have minimal impact. Our approach is easy to apply with a few lines of code, and we provide scripts for reproducing the experiments at https://github.com/VityaVitalich/MeritFed",
            "link": "https://www.semanticscholar.org/paper/f9a1fa85ab5346fd5df0ee94d58f80e341f48f53",
            "authors": "Viktor Moskvoretskii, N. Tupitsa, Christian Biemann, Samuel Horv'ath, Eduard Gorbunov, Irina Nikishina",
            "matchScore": 263.02386,
            "original title": "Low-Resource Machine Translation through the Lens of Personalized Federated Learning",
            "original authors": "Viktor Moskvoretskii, Nazarii Tupitsa, Chris Biemann, Samuel Horv\u00e1th, Eduard Gorbunov, Irina Nikishina",
            "EMNLP Paper ID": "1836",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "032efa36fb05babc4a6ab1ce0558a282a3549e8b",
            "title": "Exploring Design Choices for Building Language-Specific LLMs",
            "abstract": "Despite rapid progress in large language models (LLMs), their performance on a vast majority of languages remain unsatisfactory. In this paper, we study building language-specific LLMs by adapting monolingual and multilingual LLMs. We conduct systematic experiments on how design choices (base model selection, vocabulary extension, and continued fine-tuning) impact the adapted LLM, both in terms of efficiency (how many tokens are needed to encode the same amount of information) and end task performance. We find that (1) the initial performance before the adaptation is not always indicative of the final performance. (2) Efficiency can easily improved with simple vocabulary extension and continued fine-tuning in most LLMs we study, and (3) The optimal adaptation method is highly language-dependent, and the simplest approach works well across various experimental settings. Adapting English-centric models can yield better results than adapting multilingual models despite their worse initial performance on low-resource languages. Together, our work lays foundations on efficiently building language-specific LLMs by adapting existing LLMs.",
            "link": "https://www.semanticscholar.org/paper/032efa36fb05babc4a6ab1ce0558a282a3549e8b",
            "authors": "Atula Tejaswi, Nilesh Gupta, Eunsol Choi",
            "matchScore": 232.57953,
            "original title": "Exploring Design Choices for Building Language-Specific LLMs",
            "original authors": "Atula Tejaswi, Nilesh Gupta, Eunsol Choi",
            "EMNLP Paper ID": "2138",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "0daaa8d653235333cb2225d01147ac1f436c4879",
            "title": "Together We Can: Multilingual Automatic Post-Editing for Low-Resource Languages",
            "abstract": "This exploratory study investigates the potential of multilingual Automatic Post-Editing (APE) systems to enhance the quality of machine translations for low-resource Indo-Aryan languages. Focusing on two closely related language pairs, English-Marathi and English-Hindi, we exploit the linguistic similarities to develop a robust multilingual APE model. To facilitate cross-linguistic transfer, we generate synthetic Hindi-Marathi and Marathi-Hindi APE triplets. Additionally, we incorporate a Quality Estimation (QE)-APE multi-task learning framework. While the experimental results underline the complementary nature of APE and QE, we also observe that QE-APE multitask learning facilitates effective domain adaptation. Our experiments demonstrate that the multilingual APE models outperform their corresponding English-Hindi and English-Marathi single-pair models by $2.5$ and $2.39$ TER points, respectively, with further notable improvements over the multilingual APE model observed through multi-task learning ($+1.29$ and $+1.44$ TER points), data augmentation ($+0.53$ and $+0.45$ TER points) and domain adaptation ($+0.35$ and $+0.45$ TER points). We release the synthetic data, code, and models accrued during this study publicly at https://github.com/cfiltnlp/Multilingual-APE.",
            "link": "https://www.semanticscholar.org/paper/0daaa8d653235333cb2225d01147ac1f436c4879",
            "authors": "S. Deoghare, Diptesh Kanojia, Pushpak Bhattacharyya",
            "matchScore": 225.49713,
            "original title": "Together We Can: Mulitlingual Automatic Post-Editing for Low-Resource Languages",
            "original authors": "Sourabh Dattatray Deoghare, Diptesh Kanojia, Pushpak Bhattacharyya",
            "EMNLP Paper ID": "2171",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "5d1c481fb7538f227352e72c59951e0fc26d2ac5",
            "title": "Breaking the Script Barrier in Multilingual Pre-Trained Language Models with Transliteration-Based Post-Training Alignment",
            "abstract": "Multilingual pre-trained models (mPLMs) have shown impressive performance on cross-lingual transfer tasks. However, the transfer performance is often hindered when a low-resource target language is written in a different script than the high-resource source language, even though the two languages may be related or share parts of their vocabularies. Inspired by recent work that uses transliteration to address this problem, our paper proposes a transliteration-based post-pretraining alignment (PPA) method aiming to improve the cross-lingual alignment between languages using diverse scripts. We select two areal language groups, $\\textbf{Mediterranean-Amharic-Farsi}$ and $\\textbf{South+East Asian Languages}$, wherein the languages are mutually influenced but use different scripts. We apply our method to these language groups and conduct extensive experiments on a spectrum of downstream tasks. The results show that after PPA, models consistently outperform the original model (up to 50% for some tasks) in English-centric transfer. In addition, when we use languages other than English as sources in transfer, our method obtains even larger improvements. We will make our code and models publicly available at \\url{https://github.com/cisnlp/Transliteration-PPA}.",
            "link": "https://www.semanticscholar.org/paper/5d1c481fb7538f227352e72c59951e0fc26d2ac5",
            "authors": "Orgest Xhelili, Yihong Liu, Hinrich Sch\u00fctze",
            "matchScore": 352.27545,
            "original title": "Breaking the Script Barrier in Multilingual Pre-Trained Language Models with Transliteration-Based Post-Training Alignment",
            "original authors": "Orgest Xhelili, Yihong Liu, Hinrich Schuetze",
            "EMNLP Paper ID": "2237",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "3bd76bd5d8e82738b49b186b345709ca282697e0",
            "title": "\"Vorbe\\c{s}ti Rom\\^ane\\c{s}te?\"A Recipe to Train Powerful Romanian LLMs with English Instructions",
            "abstract": "In recent years, Large Language Models (LLMs) have achieved almost human-like performance on various tasks. While some LLMs have been trained on multilingual data, most of the training data is in English; hence, their performance in English greatly exceeds other languages. To our knowledge, we are the first to collect and translate a large collection of texts, instructions, and benchmarks and train, evaluate, and release open-source LLMs tailored for Romanian. We evaluate our methods on four different categories, including academic benchmarks, MT-Bench (manually translated), and a professionally built historical, cultural, and social benchmark adapted to Romanian. We argue for the usefulness and high performance of RoLLMs by obtaining state-of-the-art results across the board. We publicly release all resources (i.e., data, training and evaluation code, models) to support and encourage research on Romanian LLMs while concurrently creating a generalizable recipe, adequate for other low or less-resourced languages.",
            "link": "https://www.semanticscholar.org/paper/3bd76bd5d8e82738b49b186b345709ca282697e0",
            "authors": "Mihai Masala, Denis C. Ilie-Ablachim, Alexandru Dima, D. Corlatescu, Miruna Zavelca, Ovio Olaru, Simina Terian-Dan, Andrei Terian-Dan, Marius Leordeanu, Horia Velicu, Marius Popescu, Mihai Dascalu, Traian Rebedea",
            "matchScore": 202.66306,
            "original title": "\u201cVorbe\u0219ti Rom\u00e2ne\u0219te?\u201d A Recipe to Train Powerful Romanian LLMs with English Instructions",
            "original authors": "Mihai Masala, Denis Ilie-Ablachim, Alexandru Dima, Dragos Georgian Corlatescu, Miruna-Andreea Zavelca, Ovio Olaru, Simina-Maria Terian, Andrei Terian, Marius Leordeanu, Horia Velicu, Marius Popescu, Mihai Dascalu, Traian Rebedea",
            "EMNLP Paper ID": "2288",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "5b1d02bfce832b8ab41b5209e637a9f42c26d912",
            "title": "Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed?",
            "abstract": "The vast majority of today's large language models (LLMs) are English-centric, having been pretrained predominantly on English text. Yet, in order to meet user expectations, models need to be able to respond appropriately in multiple languages once deployed in downstream applications. This requires strong cross-lingual transfer abilities. In this work, we investigate the minimal amount of multilinguality required during finetuning to elicit cross-lingual generalisation in English-centric LLMs. In experiments across four LLMs, we find that multilingual instruction tuning with as few as two to three languages is both necessary and sufficient to elicit effective cross-lingual generalisation, with the limiting factor being the degree to which a target language is seen during pretraining. Evaluations on five different tasks further reveal that multilingual instruction tuning is most beneficial for generative tasks that assume input/output language agreement, such as in chat settings, while being of less importance for highly structured classification-style tasks. Our code and data is available at https://github.com/ZurichNLP/multilingual-instruction-tuning.",
            "link": "https://www.semanticscholar.org/paper/5b1d02bfce832b8ab41b5209e637a9f42c26d912",
            "authors": "Tannon Kew, Florian Schottmann, Rico Sennrich",
            "matchScore": 328.62378,
            "original title": "Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed?",
            "original authors": "Tannon Kew, Florian Schottmann, Rico Sennrich",
            "EMNLP Paper ID": "2560",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "5563170442fc7a588a9af3a489813138c796bda9",
            "title": "One Model is All You Need: ByT5-Sanskrit, a Unified Model for Sanskrit NLP Tasks",
            "abstract": "Morphologically rich languages are notoriously challenging to process for downstream NLP applications. This paper presents a new pretrained language model, ByT5-Sanskrit, designed for NLP applications involving the morphologically rich language Sanskrit. We evaluate ByT5-Sanskrit on established Sanskrit word segmentation tasks, where it outperforms previous data-driven approaches by a considerable margin and matches the performance of the current best lexicon-based model. It is easier to deploy and more robust to data not covered by external linguistic resources. It also achieves new state-of-the-art results in Vedic Sanskrit dependency parsing and OCR post-correction tasks. Additionally, based on the Digital Corpus of Sanskrit, we introduce a novel multitask dataset for the joint training of Sanskrit word segmentation, lemmatization, and morphosyntactic tagging tasks. We fine-tune ByT5-Sanskrit on this dataset, creating a versatile multitask model for various downstream Sanskrit applications. We have used this model in Sanskrit linguistic annotation projects, in information retrieval setups, and as a preprocessing step in a Sanskrit machine translation pipeline. We also show that our approach yields new best scores for lemmatization and dependency parsing of other morphologically rich languages. We thus demonstrate that byte-level pretrained language models can achieve excellent performance for morphologically rich languages, outperforming tokenizer-based models and presenting an important vector of exploration when constructing NLP pipelines for such languages.",
            "link": "https://www.semanticscholar.org/paper/5563170442fc7a588a9af3a489813138c796bda9",
            "authors": "Sebastian Nehrdich, Oliver Hellwig, K. Keutzer",
            "matchScore": 347.8413,
            "original title": "One Model is All You Need: ByT5-Sanskrit, a Unified Model for Sanskrit NLP Tasks",
            "original authors": "Sebastian Nehrdich, Oliver Hellwig, Kurt Keutzer",
            "EMNLP Paper ID": "2681",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "fc20fd43f0c3ffaa7679ae1b29c25ee229758420",
            "title": "Targeted Multilingual Adaptation for Low-resource Language Families",
            "abstract": "The\"massively-multilingual\"training of multilingual models is known to limit their utility in any one language, and they perform particularly poorly on low-resource languages. However, there is evidence that low-resource languages can benefit from targeted multilinguality, where the model is trained on closely related languages. To test this approach more rigorously, we systematically study best practices for adapting a pre-trained model to a language family. Focusing on the Uralic family as a test case, we adapt XLM-R under various configurations to model 15 languages; we then evaluate the performance of each experimental setting on two downstream tasks and 11 evaluation languages. Our adapted models significantly outperform mono- and multilingual baselines. Furthermore, a regression analysis of hyperparameter effects reveals that adapted vocabulary size is relatively unimportant for low-resource languages, and that low-resource languages can be aggressively up-sampled during training at little detriment to performance in high-resource languages. These results introduce new best practices for performing language adaptation in a targeted setting.",
            "link": "https://www.semanticscholar.org/paper/fc20fd43f0c3ffaa7679ae1b29c25ee229758420",
            "authors": "C.M. Downey, Terra Blevins, Dhwani Serai, Dwija Parikh, Shane Steinert-Threlkeld",
            "matchScore": 226.01538,
            "original title": "Targeted Multilingual Adaptation for Low-resource Language Families",
            "original authors": "C. M. Downey, Terra Blevins, Dhwani Serai, Dwija Parikh, Shane Steinert-Threlkeld",
            "EMNLP Paper ID": "3006",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "0ce7f82d8576c0b642492a46ba9aad81f6de4910",
            "title": "Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMs",
            "abstract": "Large Language Models (LLMs) exhibit impressive zero/few-shot inference and generation quality for high-resource languages (HRLs). A few of them have been trained on low-resource languages (LRLs) and give decent performance. Owing to the prohibitive costs of training LLMs, they are usually used as a network service, with the client charged by the count of input and output tokens. The number of tokens strongly depends on the script and language, as well as the LLM's subword vocabulary. We show that LRLs are at a pricing disadvantage, because the well-known LLMs produce more tokens for LRLs than HRLs. This is because most currently popular LLMs are optimized for HRL vocabularies. Our objective is to level the playing field: reduce the cost of processing LRLs in contemporary LLMs while ensuring that predictive and generative qualities are not compromised. As means to reduce the number of tokens processed by the LLM, we consider code-mixing, translation, and transliteration of LRLs to HRLs. We perform an extensive study using the IndicXTREME classification and six generative tasks dataset, covering 15 Indic and 3 other languages, while using GPT-4 (one of the costliest LLM services released so far) as a commercial LLM. We observe and analyze interesting patterns involving token count, cost, and quality across a multitude of languages and tasks. We show that choosing the best policy to interact with the LLM can reduce cost by 90% while giving better or comparable performance compared to communicating with the LLM in the original LRL.",
            "link": "https://www.semanticscholar.org/paper/0ce7f82d8576c0b642492a46ba9aad81f6de4910",
            "authors": "Arijit Nag, Animesh Mukherjee, Niloy Ganguly, Soumen Chakrabarti",
            "matchScore": 252.78268,
            "original title": "Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMs",
            "original authors": "Arijit Nag, Animesh Mukherjee, Niloy Ganguly, Soumen Chakrabarti",
            "EMNLP Paper ID": "3008",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 14): 13:00\u201314:00 (Afternoon)",
            "Type": "Findings"
        },
        {
            "paperId": "d365d5637073f87acfa7df8255b44d4ab35be4f4",
            "title": "Contrastive Token Learning with Similarity Decay for Repetition Suppression in Machine Translation",
            "abstract": "For crosslingual conversation and trade, Neural Machine Translation (NMT) is pivotal yet faces persistent challenges with monotony and repetition in generated content. Traditional solutions that rely on penalizing text redundancy or token reoccurrence have shown limited efficacy, particularly for lengthy article and e-commerce descriptions with inherent redundancy, even with the advent of Large Language Models (LLMs). This paper investigates the underlying causes of textual repetition through the lens of information entropy, attributing the phenomenon to the elevated uncertainty within the input text. To address this, a novel algorithm named Contrastive Token Learning with Similarity Decay (CTSD) is introduced, which modulates the suppression of tokens dynamically, informed by varying attention weights and inter-token distances. Furthermore, an e-commerce dataset comprised of title texts of online real items is compiled and released susceptible to hallucination translations to benchmark the algorithm. Extensive evaluations demonstrate that CTSD significantly outperforms existing approaches in precision and generalizability. Additional online A/B testing underscores its practical value, showing marked improvements in user engagement and conversion. Notably, this method has been implemented with full traffic on eight multilingual sites of alibaba.com, the largest B2B e-commerce platform in the world.",
            "link": "https://www.semanticscholar.org/paper/d365d5637073f87acfa7df8255b44d4ab35be4f4",
            "authors": "Huangyu Dai, Ben Chen, Kaidi Chen, Ying Han, Zihan Liang, Wen Jiang",
            "matchScore": 253.8996,
            "original title": "Contrastive Token Learning with Similarity Decay for Repetition Suppression in Machine Translation",
            "original authors": "Huangyu Dai, Ben Chen, Kaidi Chen, Ying Han, Zihan Liang, Wen Jiang",
            "EMNLP Paper ID": "666",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "e397cac3f38aba116cc623bdf1a6d638537f5e29",
            "title": "LinguAlchemy: Fusing Typological and Geographical Elements for Unseen Language Generalization",
            "abstract": "Pretrained language models (PLMs) have become remarkably adept at task and language generalization. Nonetheless, they often fail when faced with unseen languages. In this work, we present LinguAlchemy, a regularization method that incorporates various linguistic information covering typological, geographical, and phylogenetic features to align PLMs representation to the corresponding linguistic information on each language. Our LinguAlchemy significantly improves the performance of mBERT and XLM-R on low-resource languages in multiple downstream tasks such as intent classification, news classification, and semantic relatedness compared to fully finetuned models and displaying a high degree of unseen language generalization. We further introduce AlchemyScale and AlchemyTune, extension of LinguAlchemy which adjusts the linguistic regularization weights automatically, alleviating the need for hyperparameter search.",
            "link": "https://www.semanticscholar.org/paper/e397cac3f38aba116cc623bdf1a6d638537f5e29",
            "authors": "Muhammad Farid Adilazuarda, Samuel Cahyawijaya, Alham Fikri Aji, Genta Indra Winata, Ayu Purwarianti",
            "matchScore": 303.20776,
            "original title": "LinguAlchemy: Fusing Typological and Geographical Elements for Unseen Language Generalization",
            "original authors": "Muhammad Farid Adilazuarda, Samuel Cahyawijaya, Genta Indra Winata, Ayu Purwarianti, Alham Fikri Aji",
            "EMNLP Paper ID": "782",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "95b8fea6b8342ec59f0652133184ce53ffae1fba",
            "title": "RoQLlama: A Lightweight Romanian Adapted Language Model",
            "abstract": "The remarkable achievements obtained by open-source large language models (LLMs) in recent years have predominantly been concentrated on tasks involving the English language. In this paper, we aim to advance the performance of Llama2 models on Romanian tasks. We tackle the problem of reduced computing resources by using QLoRA for training. We release RoQLlama-7b, a quantized LLM, which shows equal or improved results compared to its full-sized counterpart when tested on seven Romanian downstream tasks in the zero-shot setup. Also, it consistently achieves higher average scores across all few-shot prompts. Additionally, we introduce a novel Romanian dataset, namely RoMedQA, which contains single-choice medical questions in Romanian.",
            "link": "https://www.semanticscholar.org/paper/95b8fea6b8342ec59f0652133184ce53ffae1fba",
            "authors": "George-Andrei Dima, Andrei-Marius Avram, Cristian-George Cruaciun, Dumitru-Clementin Cercel",
            "matchScore": 231.13239,
            "original title": "RoQLlama: A Lightweight Romanian Adapted Language Model",
            "original authors": "George-Andrei Dima, Andrei-Marius Avram, Cristian-George Craciun, Dumitru-Clementin Cercel",
            "EMNLP Paper ID": "899",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        }
    ],
    "Logical Reasoning and Evaluation in Large Language Models": [
        {
            "paperId": "277d0bd64fd9ef929c66d3d94904d6a9e6840cf8",
            "title": "CoCoLoFa: A Dataset of News Comments with Common Logical Fallacies Written by LLM-Assisted Crowds",
            "abstract": "Detecting logical fallacies in texts can help users spot argument flaws, but automating this detection is not easy. Manually annotating fallacies in large-scale, real-world text data to create datasets for developing and validating detection models is costly. This paper introduces CoCoLoFa, the largest known logical fallacy dataset, containing 7,706 comments for 648 news articles, with each comment labeled for fallacy presence and type. We recruited 143 crowd workers to write comments embodying specific fallacy types (e.g., slippery slope) in response to news articles. Recognizing the complexity of this writing task, we built an LLM-powered assistant into the workers' interface to aid in drafting and refining their comments. Experts rated the writing quality and labeling validity of CoCoLoFa as high and reliable. BERT-based models fine-tuned using CoCoLoFa achieved the highest fallacy detection (F1=0.86) and classification (F1=0.87) performance on its test set, outperforming the state-of-the-art LLMs. Our work shows that combining crowdsourcing and LLMs enables us to more effectively construct datasets for complex linguistic phenomena that crowd workers find challenging to produce on their own.",
            "link": "https://www.semanticscholar.org/paper/277d0bd64fd9ef929c66d3d94904d6a9e6840cf8",
            "authors": "Min-Hsuan Yeh, Ruyuan Wan, Ting-Hao 'Kenneth' Huang",
            "EMNLP Paper ID": "89",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "28685ab4bb673cd7aac4f5711b0882d08d2fa7c7",
            "title": "In Search of the Long-Tail: Systematic Generation of Long-Tail Inferential Knowledge via Logical Rule Guided Search",
            "abstract": "To effectively use large language models (LLMs) for real-world queries, it is imperative that they generalize to the long-tail distribution, i.e. rare examples where models exhibit low confidence. In this work, we take the first step towards evaluating LLMs in the long-tail distribution of inferential knowledge. We exemplify long-tail evaluation on the Natural Language Inference task. First, we introduce Logic-Induced-Knowledge-Search (LINK), a systematic long-tail data generation framework, to obtain factually-correct yet long-tail inferential statements. LINK uses variable-wise prompting grounded on symbolic rules to seek low-confidence statements while ensuring factual correctness. We then use LINK to curate Logic-Induced-Long-Tail (LINT), a large-scale long-tail inferential knowledge dataset that contains 108K statements spanning four domains. We evaluate popular LLMs on LINT; we find that state-of-the-art LLMs show significant performance drop (21% relative drop for GPT4) on long-tail data as compared to on head distribution data, and smaller models show even more generalization weakness. These results further underscore the necessity of long-tail evaluation in developing generalizable LLMs.",
            "link": "https://www.semanticscholar.org/paper/28685ab4bb673cd7aac4f5711b0882d08d2fa7c7",
            "authors": "Huihan Li, Yuting Ning, Zeyi Liao, Siyuan Wang, Xiang Lorraine Li, Ximing Lu, Wenting Zhao, Faeze Brahman, Yejin Choi, Xiang Ren",
            "EMNLP Paper ID": "268",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "636b9e5bc46d2f249a5655e374aafe61a55412a7",
            "title": "Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?",
            "abstract": "State-of-the-art Large Language Models (LLMs) are accredited with an increasing number of different capabilities, ranging from reading comprehension, over advanced mathematical and reasoning skills to possessing scientific knowledge. In this paper we focus on their multi-hop reasoning capability: the ability to identify and integrate information from multiple textual sources. Given the concerns with the presence of simplifying cues in existing multi-hop reasoning benchmarks, which allow models to circumvent the reasoning requirement, we set out to investigate, whether LLMs are prone to exploiting such simplifying cues. We find evidence that they indeed circumvent the requirement to perform multi-hop reasoning, but they do so in more subtle ways than what was reported about their fine-tuned pre-trained language model (PLM) predecessors. Motivated by this finding, we propose a challenging multi-hop reasoning benchmark, by generating seemingly plausible multi-hop reasoning chains, which ultimately lead to incorrect answers. We evaluate multiple open and proprietary state-of-the-art LLMs, and find that their performance to perform multi-hop reasoning is affected, as indicated by up to 45% relative decrease in F1 score when presented with such seemingly plausible alternatives. We conduct a deeper analysis and find evidence that while LLMs tend to ignore misleading lexical cues, misleading reasoning paths indeed present a significant challenge.",
            "link": "https://www.semanticscholar.org/paper/636b9e5bc46d2f249a5655e374aafe61a55412a7",
            "authors": "Neeladri Bhuiya, Viktor Schlegel, Stefan Winkler",
            "EMNLP Paper ID": "279",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "6fede93e6b6388a0f4d766a3adf22367e751de93",
            "title": "Verification and Refinement of Natural Language Explanations through LLM-Symbolic Theorem Proving",
            "abstract": "Natural language explanations represent a proxy for evaluating explanation-based and multi-step Natural Language Inference (NLI) models. However, assessing the validity of explanations for NLI is challenging as it typically involves the crowd-sourcing of apposite datasets, a process that is time-consuming and prone to logical errors. To address existing limitations, this paper investigates the verification and refinement of natural language explanations through the integration of Large Language Models (LLMs) and Theorem Provers (TPs). Specifically, we present a neuro-symbolic framework, named Explanation-Refiner, that integrates TPs with LLMs to generate and formalise explanatory sentences and suggest potential inference strategies for NLI. In turn, the TP is employed to provide formal guarantees on the logical validity of the explanations and to generate feedback for subsequent improvements. We demonstrate how Explanation-Refiner can be jointly used to evaluate explanatory reasoning, autoformalisation, and error correction mechanisms of state-of-the-art LLMs as well as to automatically enhance the quality of explanations of variable complexity in different domains.",
            "link": "https://www.semanticscholar.org/paper/6fede93e6b6388a0f4d766a3adf22367e751de93",
            "authors": "Xin Quan, Marco Valentino, Louise A. Dennis, Andr\u00e9 Freitas",
            "EMNLP Paper ID": "329",
            "Oral/Poster": "Oral",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "f09904d113b33071385d39655e40b465c55e8784",
            "title": "Conditional and Modal Reasoning in Large Language Models",
            "abstract": "The reasoning abilities of large language models (LLMs) are the topic of a growing body of research in AI and cognitive science. In this paper, we probe the extent to which twenty-nine LLMs are able to distinguish logically correct inferences from logically fallacious ones. We focus on inference patterns involving conditionals (e.g., 'If Ann has a queen, then Bob has a jack') and epistemic modals (e.g., 'Ann might have an ace', 'Bob must have a king'). These inferences have been of special interest to logicians, philosophers, and linguists, since they play a central role in the fundamental human ability to reason about distal possibilities. Assessing LLMs on these inferences is thus highly relevant to the question of how much the reasoning abilities of LLMs match those of humans. All the LLMs we tested make some basic mistakes with conditionals or modals, though zero-shot chain-of-thought prompting helps them make fewer mistakes. Even the best performing LLMs make basic errors in modal reasoning, display logically inconsistent judgments across inference patterns involving epistemic modals and conditionals, and give answers about complex conditional inferences that do not match reported human judgments. These results highlight gaps in basic logical reasoning in today's LLMs.",
            "link": "https://www.semanticscholar.org/paper/f09904d113b33071385d39655e40b465c55e8784",
            "authors": "Wesley H. Holliday, M. Mandelkern",
            "EMNLP Paper ID": "433",
            "Oral/Poster": "Oral",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "f8c43f9defa73943062618280ec8f7bf60c89c54",
            "title": "When Reasoning Meets Information Aggregation: A Case Study with Sports Narratives",
            "abstract": "Reasoning is most powerful when an LLM accurately aggregates relevant information. We examine the critical role of information aggregation in reasoning by requiring the LLM to analyze sports narratives. To succeed at this task, an LLM must infer points from actions, identify related entities, attribute points accurately to players and teams, and compile key statistics to draw conclusions. We conduct comprehensive experiments with real NBA basketball data and present SportsGen, a new method to synthesize game narratives. By synthesizing data, we can rigorously evaluate LLMs' reasoning capabilities under complex scenarios with varying narrative lengths and density of information. Our findings show that most models, including GPT-4o, often fail to accurately aggregate basketball scores due to frequent scoring patterns. Open-source models like Llama-3 further suffer from significant score hallucinations. Finally, the effectiveness of reasoning is influenced by narrative complexity, information density, and domain-specific terms, highlighting the challenges in analytical reasoning tasks.",
            "link": "https://www.semanticscholar.org/paper/f8c43f9defa73943062618280ec8f7bf60c89c54",
            "authors": "Yebowen Hu, Kaiqiang Song, Sangwoo Cho, Xiaoyang Wang, Wenlin Yao, H. Foroosh, Dong Yu, Fei Liu",
            "EMNLP Paper ID": "472",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "00e88a0c006296e53bb7d4cfc90a134883ad34fd",
            "title": "A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners",
            "abstract": "This study introduces a hypothesis-testing framework to assess whether large language models (LLMs) possess genuine reasoning abilities or primarily depend on token bias. We go beyond evaluating LLMs on accuracy; rather, we aim to investigate their token bias in solving logical reasoning tasks. Specifically, we develop carefully controlled synthetic datasets, featuring conjunction fallacy and syllogistic problems. Our framework outlines a list of hypotheses where token biases are readily identifiable, with all null hypotheses assuming genuine reasoning capabilities of LLMs. The findings in this study suggest, with statistical guarantee, that most LLMs still struggle with logical reasoning. While they may perform well on classic problems, their success largely depends on recognizing superficial patterns with strong token bias, thereby raising concerns about their actual reasoning and generalization abilities. Codes and data are open-sourced at https://github.com/bowen-upenn/llm_token_bias.",
            "link": "https://www.semanticscholar.org/paper/00e88a0c006296e53bb7d4cfc90a134883ad34fd",
            "authors": "Bowen Jiang, Yangxinyu Xie, Zhuoqun Hao, Xiaomeng Wang, Tanwi Mallick, Weijie J. Su, Camillo J. Taylor, Dan Roth",
            "EMNLP Paper ID": "518",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "c80457a0579a0dcab914d01ae343c346210cf4e1",
            "title": "Scaling Synthetic Logical Reasoning Datasets with Context-Sensitive Declarative Grammars",
            "abstract": "Logical reasoning remains a challenge for natural language processing, but it can be improved by training language models to mimic theorem provers on procedurally generated problems. Previous work used domain-specific proof generation algorithms, which biases reasoning toward specific proof traces and limits auditability and extensibility. We present a simpler and more general declarative framework with flexible context-sensitive rules binding multiple languages (specifically, simplified English and the TPTP theorem-proving language). We construct first-order logic problems by selecting up to 32 premises and one hypothesis. We demonstrate that using semantic constraints during generation and careful English verbalization of predicates enhances logical reasoning without hurting natural English tasks. We use relatively small DeBERTa-v3 models to achieve state-of-the-art accuracy on the FOLIO human-authored logic dataset, surpassing GPT-4 in accuracy with or without an external solver by 12%.",
            "link": "https://www.semanticscholar.org/paper/c80457a0579a0dcab914d01ae343c346210cf4e1",
            "authors": "Damien Sileo",
            "EMNLP Paper ID": "580",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "a33815829ea243b226e17986fa1ffda97adecb76",
            "title": "Liar, Liar, Logical Mire: A Benchmark for Suppositional Reasoning in Large Language Models",
            "abstract": "Knights and knaves problems represent a classic genre of logical puzzles where characters either tell the truth or lie. The objective is to logically deduce each character's identity based on their statements. The challenge arises from the truth-telling or lying behavior, which influences the logical implications of each statement. Solving these puzzles requires not only direct deductions from individual statements, but the ability to assess the truthfulness of statements by reasoning through various hypothetical scenarios. As such, knights and knaves puzzles serve as compelling examples of suppositional reasoning. In this paper, we introduce $\\textit{TruthQuest}$, a benchmark for suppositional reasoning based on the principles of knights and knaves puzzles. Our benchmark presents problems of varying complexity, considering both the number of characters and the types of logical statements involved. Evaluations on $\\textit{TruthQuest}$ show that large language models like Llama 3 and Mixtral-8x7B exhibit significant difficulties solving these tasks. A detailed error analysis of the models' output reveals that lower-performing models exhibit a diverse range of reasoning errors, frequently failing to grasp the concept of truth and lies. In comparison, more proficient models primarily struggle with accurately inferring the logical implications of potentially false statements.",
            "link": "https://www.semanticscholar.org/paper/a33815829ea243b226e17986fa1ffda97adecb76",
            "authors": "Philipp Mondorf, Barbara Plank",
            "EMNLP Paper ID": "798",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "3e5e0e1841e23fc41f4e3243c9091bf6e7e7d199",
            "title": "Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic",
            "abstract": "Recent language models enable new opportunities for structured reasoning with text, such as the construction of intuitive, proof-like textual entailment trees without relying on brittle formal logic. However, progress in this direction has been hampered by a long-standing lack of a clear protocol for determining what valid compositional entailment is. This absence causes noisy datasets and limited performance gains by modern neuro-symbolic engines. To address these problems, we formulate a consistent and theoretically grounded approach to annotating decompositional entailment and evaluate its impact on LLM-based textual inference. We find that our new dataset, RDTE (Recognizing Decompositional Textual Entailment), has a substantially higher internal consistency (+9%) than prior decompositional entailment datasets. We also find that training an RDTE-oriented entailment classifier via knowledge distillation and employing it in an entailment tree reasoning engine significantly improves both accuracy and proof quality, illustrating the practical benefit of this advance for textual inference.",
            "link": "https://www.semanticscholar.org/paper/3e5e0e1841e23fc41f4e3243c9091bf6e7e7d199",
            "authors": "Nathaniel Weir, Kate Sanders, Orion Weller, Shreya Sharma, Dongwei Jiang, Zhengping Jiang, Bhavana Dalvi, Oyvind Tafjord, Peter Alexander Jansen, Peter Clark, Benjamin Van Durme",
            "EMNLP Paper ID": "1062",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "64e50707a5398ad0d0cb08cf4b372000f3e14562",
            "title": "Belief Revision: The Adaptability of Large Language Models Reasoning",
            "abstract": "The capability to reason from text is crucial for real-world NLP applications. Real-world scenarios often involve incomplete or evolving data. In response, individuals update their beliefs and understandings accordingly. However, most existing evaluations assume that language models (LMs) operate with consistent information. We introduce Belief-R, a new dataset designed to test LMs' belief revision ability when presented with new evidence. Inspired by how humans suppress prior inferences, this task assesses LMs within the newly proposed delta reasoning ($\\Delta R$) framework. Belief-R features sequences of premises designed to simulate scenarios where additional information could necessitate prior conclusions drawn by LMs. We evaluate $\\sim$30 LMs across diverse prompting strategies and found that LMs generally struggle to appropriately revise their beliefs in response to new information. Further, models adept at updating often underperformed in scenarios without necessary updates, highlighting a critical trade-off. These insights underscore the importance of improving LMs' adaptiveness to changing information, a step toward more reliable AI systems.",
            "link": "https://www.semanticscholar.org/paper/64e50707a5398ad0d0cb08cf4b372000f3e14562",
            "authors": "Bryan Wilie, Samuel Cahyawijaya, Etsuko Ishii, Junxian He, Pascale Fung",
            "EMNLP Paper ID": "1188",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "396ea10d3ab89da41d02693d7165c4b98ecbb5f3",
            "title": "Puzzle Solving using Reasoning of Large Language Models: A Survey",
            "abstract": "Exploring the capabilities of Large Language Models (LLMs) in puzzle solving unveils critical insights into their potential and challenges in AI, marking a significant step towards understanding their applicability in complex reasoning tasks. This survey leverages a unique taxonomy -- dividing puzzles into rule-based and rule-less categories -- to critically assess LLMs through various methodologies, including prompting techniques, neuro-symbolic approaches, and fine-tuning. Through a critical review of relevant datasets and benchmarks, we assess LLMs' performance, identifying significant challenges in complex puzzle scenarios. Our findings highlight the disparity between LLM capabilities and human-like reasoning, particularly in those requiring advanced logical inference. The survey underscores the necessity for novel strategies and richer datasets to advance LLMs' puzzle-solving proficiency and contribute to AI's logical reasoning and creative problem-solving advancements.",
            "link": "https://www.semanticscholar.org/paper/396ea10d3ab89da41d02693d7165c4b98ecbb5f3",
            "authors": "Panagiotis Giadikiaroglou, Maria Lymperaiou, Giorgos Filandrianos, G. Stamou",
            "EMNLP Paper ID": "1348",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "8910a19837457ee16da0003745a64ab100b3c33c",
            "title": "Reasoning or a Semblance of it? A Diagnostic Study of Transitive Reasoning in LLMs",
            "abstract": "Evaluating Large Language Models (LLMs) on reasoning benchmarks demonstrates their ability to solve compositional questions. However, little is known of whether these models engage in genuine logical reasoning or simply rely on implicit cues to generate answers. In this paper, we investigate the transitive reasoning capabilities of two distinct LLM architectures, LLaMA 2 and Flan-T5, by manipulating facts within two compositional datasets: QASC and Bamboogle. We controlled for potential cues that might influence the models' performance, including (a) word/phrase overlaps across sections of test input; (b) models' inherent knowledge during pre-training or fine-tuning; and (c) Named Entities. Our findings reveal that while both models leverage (a), Flan-T5 shows more resilience to experiments (b and c), having less variance than LLaMA 2. This suggests that models may develop an understanding of transitivity through fine-tuning on knowingly relevant datasets, a hypothesis we leave to future work.",
            "link": "https://www.semanticscholar.org/paper/8910a19837457ee16da0003745a64ab100b3c33c",
            "authors": "Houman Mehrafarin, Arash Eshghi, Ioannis Konstas",
            "EMNLP Paper ID": "1355",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "6dd9a63109f1b5722c9e7c8a2458b4c64731b475",
            "title": "A Systematic Analysis of Large Language Models as Soft Reasoners: The Case of Syllogistic Inferences",
            "abstract": "The reasoning abilities of Large Language Models (LLMs) are becoming a central focus of study in NLP. In this paper, we consider the case of syllogistic reasoning, an area of deductive reasoning studied extensively in logic and cognitive psychology. Previous research has shown that pre-trained LLMs exhibit reasoning biases, such as $\\textit{content effects}$, avoid answering that $\\textit{no conclusion follows}$, display human-like difficulties, and struggle with multi-step reasoning. We contribute to this research line by systematically investigating the effects of chain-of-thought reasoning, in-context learning (ICL), and supervised fine-tuning (SFT) on syllogistic reasoning, considering syllogisms with conclusions that support or violate world knowledge, as well as ones with multiple premises. Crucially, we go beyond the standard focus on accuracy, with an in-depth analysis of the conclusions generated by the models. Our results suggest that the behavior of pre-trained LLMs can be explained by heuristics studied in cognitive science and that both ICL and SFT improve model performance on valid inferences, although only the latter mitigates most reasoning biases without harming model consistency.",
            "link": "https://www.semanticscholar.org/paper/6dd9a63109f1b5722c9e7c8a2458b4c64731b475",
            "authors": "Leonardo Bertolazzi, Albert Gatt, Raffaella Bernardi",
            "EMNLP Paper ID": "1600",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "6d45b1276ef97a34543ee8d2bb317f7990f1feef",
            "title": "Are LLMs Good Zero-Shot Fallacy Classifiers?",
            "abstract": "Fallacies are defective arguments with faulty reasoning. Detecting and classifying them is a crucial NLP task to prevent misinformation, manipulative claims, and biased decisions. However, existing fallacy classifiers are limited by the requirement for sufficient labeled data for training, which hinders their out-of-distribution (OOD) generalization abilities. In this paper, we focus on leveraging Large Language Models (LLMs) for zero-shot fallacy classification. To elicit fallacy-related knowledge and reasoning abilities of LLMs, we propose diverse single-round and multi-round prompting schemes, applying different task-specific instructions such as extraction, summarization, and Chain-of-Thought reasoning. With comprehensive experiments on benchmark datasets, we suggest that LLMs could be potential zero-shot fallacy classifiers. In general, LLMs under single-round prompting schemes have achieved acceptable zero-shot performances compared to the best full-shot baselines and can outperform them in all OOD inference scenarios and some open-domain tasks. Our novel multi-round prompting schemes can effectively bring about more improvements, especially for small LLMs. Our analysis further underlines the future research on zero-shot fallacy classification. Codes and data are available at: https://github.com/panFJCharlotte98/Fallacy_Detection.",
            "link": "https://www.semanticscholar.org/paper/6d45b1276ef97a34543ee8d2bb317f7990f1feef",
            "authors": "Fengjun Pan, Xiaobao Wu, Zongrui Li, A. Luu",
            "EMNLP Paper ID": "1657",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 14): 13:00\u201314:00 (Afternoon)",
            "Type": "Main"
        },
        {
            "paperId": "714de40734ceacd062e2f0a14e581e7c86f09224",
            "title": "Exploring the Role of Reasoning Structures for Constructing Proofs in Multi-Step Natural Language Reasoning with Large Language Models",
            "abstract": "When performing complex multi-step reasoning tasks, the ability of Large Language Models (LLMs) to derive structured intermediate proof steps is important for ensuring that the models truly perform the desired reasoning and for improving models' explainability. This paper is centred around a focused study: whether the current state-of-the-art generalist LLMs can leverage the structures in a few examples to better construct the proof structures with \\textit{in-context learning}. Our study specifically focuses on structure-aware demonstration and structure-aware pruning. We demonstrate that they both help improve performance. A detailed analysis is provided to help understand the results.",
            "link": "https://www.semanticscholar.org/paper/714de40734ceacd062e2f0a14e581e7c86f09224",
            "authors": "Zi'ou Zheng, Christopher Malon, Martin Renqiang Min, Xiaodan Zhu",
            "EMNLP Paper ID": "1781",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "e0b3b35d92aa0a1e03687446767203e5af6d94d4",
            "title": "Symbolic Working Memory Enhances Language Models for Complex Rule Application",
            "abstract": "Large Language Models (LLMs) have shown remarkable reasoning performance but struggle with multi-step deductive reasoning involving a series of rule application steps, especially when rules are presented non-sequentially. Our preliminary analysis shows that while LLMs excel in single-step rule application, their performance drops significantly in multi-step scenarios due to the challenge in rule grounding. It requires anchoring the applicable rule and supporting facts at each step, amidst multiple input rules, facts, and inferred facts. To address this, we propose augmenting LLMs with external working memory and introduce a neurosymbolic framework for rule application. The memory stores facts and rules in both natural language and symbolic forms, enabling precise tracking. Utilizing this memory, our framework iteratively performs symbolic rule grounding and LLM-based rule implementation. The former matches predicates and variables of symbolic rules and facts to ground applicable rules at each step. Experiments indicate our framework's effectiveness in rule application and its robustness across various steps and settings~\\footnote{Code and data are available at \\url{https://github.com/SiyuanWangw/RuleApplication}.}.",
            "link": "https://www.semanticscholar.org/paper/e0b3b35d92aa0a1e03687446767203e5af6d94d4",
            "authors": "Siyuan Wang, Zhongyu Wei, Yejin Choi, Xiang Ren",
            "EMNLP Paper ID": "2103",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "0c1782b5cb2898d2e226c66bc7146bce802051bd",
            "title": "Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?",
            "abstract": "Recent large language models (LLMs) have demonstrated remarkable generalization abilities in mathematics and logical reasoning tasks. Prior research indicates that LLMs pre-trained with programming language data exhibit high mathematical and reasoning abilities; however, this causal relationship has not been rigorously tested. Our research aims to verify which programming languages and features during pre-training affect logical inference performance. Specifically, we pre-trained decoder-based language models from scratch using datasets from ten programming languages (e.g., Python, C, Java) and three natural language datasets (Wikipedia, Fineweb, C4) under identical conditions. Thereafter, we evaluated the trained models in a few-shot in-context learning setting on logical reasoning tasks: FLD and bAbi, which do not require commonsense or world knowledge. The results demonstrate that nearly all models trained with programming languages consistently outperform those trained with natural languages, indicating that programming languages contain factors that elicit logic inference performance. In addition, we found that models trained with programming languages exhibit a better ability to follow instructions compared to those trained with natural languages. Further analysis reveals that the depth of Abstract Syntax Trees representing parsed results of programs also affects logical reasoning performance. These findings will offer insights into the essential elements of pre-training for acquiring the foundational abilities of LLMs.",
            "link": "https://www.semanticscholar.org/paper/0c1782b5cb2898d2e226c66bc7146bce802051bd",
            "authors": "Fumiya Uchiyama, Takeshi Kojima, Andrew Gambardella, Qi Cao, Yusuke Iwasawa, Yutaka Matsuo",
            "EMNLP Paper ID": "2230",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "8da430da1e4e4b4b9e1432aa8b47937dd0d590c9",
            "title": "Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?",
            "abstract": "Solving grid puzzles involves a significant amount of logical reasoning. Hence, it is a good domain to evaluate the reasoning capability of a model which can then guide us to improve the reasoning ability of models. However, most existing works evaluate only the final predicted answer of a puzzle, without delving into an in-depth analysis of the LLMs' reasoning chains (such as where they falter) or providing any finer metrics to evaluate them. Since LLMs may rely on simple heuristics or artifacts to predict the final answer, it is crucial to evaluate the generated reasoning chain beyond overall correctness measures, for accurately evaluating the reasoning abilities of LLMs. To this end, we first develop GridPuzzle, an evaluation dataset comprising 274 grid-based puzzles with different complexities. Second, we propose a new error taxonomy derived from manual analysis of reasoning chains from LLMs including GPT-4, Claude-3, Gemini, Mistral, and Llama-2. Then, we develop an LLM-based framework for large-scale subjective evaluation (i.e., identifying errors) and an objective metric, PuzzleEval, to evaluate the correctness of reasoning chains. Evaluating reasoning chains from LLMs leads to several interesting findings. We further show that existing prompting methods used for enhancing models' reasoning abilities do not improve performance on GridPuzzle. This highlights the importance of understanding fine-grained errors and presents a challenge for future research to enhance LLMs' puzzle-solving abilities by developing methods that address these errors. Data and source code are available at https://github.com/Mihir3009/GridPuzzle.",
            "link": "https://www.semanticscholar.org/paper/8da430da1e4e4b4b9e1432aa8b47937dd0d590c9",
            "authors": "Nemika Tyagi, Mihir Parmar, Mohith Kulkarni, Aswin Rrv, Nisarg Patel, Mutsumi Nakamura, Arindam Mitra, Chitta Baral",
            "EMNLP Paper ID": "2584",
            "Oral/Poster": "Oral",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "f04c25fcf3247ff4d8eca72d862b22090b884b75",
            "title": "Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies",
            "abstract": "A diverse array of reasoning strategies has been proposed to elicit the capabilities of large language models. However, in this paper, we point out that traditional evaluations which focus solely on performance metrics miss a key factor: the increased effectiveness due to additional compute. By overlooking this aspect, a skewed view of strategy efficiency is often presented. This paper introduces a framework that incorporates the compute budget into the evaluation, providing a more informative comparison that takes into account both performance metrics and computational cost. In this budget-aware perspective, we find that complex reasoning strategies often don't surpass simpler baselines purely due to algorithmic ingenuity, but rather due to the larger computational resources allocated. When we provide a simple baseline like chain-of-thought self-consistency with comparable compute resources, it frequently outperforms reasoning strategies proposed in the literature. In this scale-aware perspective, we find that unlike self-consistency, certain strategies such as multi-agent debate or Reflexion can become worse if more compute budget is utilized.",
            "link": "https://www.semanticscholar.org/paper/f04c25fcf3247ff4d8eca72d862b22090b884b75",
            "authors": "Junlin Wang, Siddharth Jain, Dejiao Zhang, Baishakhi Ray, Varun Kumar, Ben Athiwaratkun",
            "EMNLP Paper ID": "2586",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "4e5ea6c8463418b4c28b02c8aad041e4c1ba5165",
            "title": "Flee the Flaw: Annotating the Underlying Logic of Fallacious Arguments Through Templates and Slot-filling",
            "abstract": "Prior research in computational argumentation has mainly focused on scoring the quality of arguments, with less attention on explicating logical errors. In this work, we introduce four sets of explainable templates for common informal logical fallacies designed to explicate a fallacy's implicit logic. Using our templates, we conduct an annotation study on top of 400 fallacious arguments taken from LOGIC dataset and achieve a high agreement score (Krippendorf's alpha of 0.54) and reasonable coverage (0.83). Finally, we conduct an experiment for detecting the structure of fallacies and discover that state-of-the-art language models struggle with detecting fallacy templates (0.47 accuracy). To facilitate research on fallacies, we make our dataset and guidelines publicly available.",
            "link": "https://www.semanticscholar.org/paper/4e5ea6c8463418b4c28b02c8aad041e4c1ba5165",
            "authors": "Irfan Robbani, Paul Reisert, Naoya Inoue, Surawat Pothong, Cam\u00e9lia Guerraoui, Wenzhi Wang, Shoichi Naito, Jungmin Choi, Kentaro Inui",
            "EMNLP Paper ID": "2686",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "12aea189b9d108cac5475a81f73f0187c05bd730",
            "title": "Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models",
            "abstract": "As Large Language Models (LLMs) continue to exhibit remarkable performance in natural language understanding tasks, there is a crucial need to measure their ability for human-like multi-step logical reasoning. Existing logical reasoning evaluation benchmarks often focus primarily on simplistic single-step or multi-step reasoning with a limited set of inference rules. Furthermore, the lack of datasets for evaluating non-monotonic reasoning represents a crucial gap since it aligns more closely with human-like reasoning. To address these limitations, we propose Multi-LogiEval, a comprehensive evaluation dataset encompassing multi-step logical reasoning with various inference rules and depths. Multi-LogiEval covers three logic types--propositional, first-order, and non-monotonic--consisting of more than 30 inference rules and more than 60 of their combinations with various depths. Leveraging this dataset, we conduct evaluations on a range of LLMs including GPT-4, ChatGPT, Gemini-Pro, Yi, Orca, and Mistral, employing a zero-shot chain-of-thought. Experimental results show that there is a significant drop in the performance of LLMs as the reasoning steps/depth increases (average accuracy of ~68% at depth-1 to ~43% at depth-5). We further conduct a thorough investigation of reasoning chains generated by LLMs which reveals several important findings. We believe that Multi-LogiEval facilitates future research for evaluating and enhancing the logical reasoning ability of LLMs. Data is available at https://github.com/Mihir3009/Multi-LogiEval.",
            "link": "https://www.semanticscholar.org/paper/12aea189b9d108cac5475a81f73f0187c05bd730",
            "authors": "Nisarg Patel, Mohith Kulkarni, Mihir Parmar, Aashna Budhiraja, Mutsumi Nakamura, Neeraj Varshney, Chitta Baral",
            "EMNLP Paper ID": "2768",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "5581bf85386737bd3378eec68189759a05280bea",
            "title": "FOLIO: Natural Language Reasoning with First-Order Logic",
            "abstract": "Large language models (LLMs) have achieved remarkable performance on a variety of natural language understanding tasks. However, existing benchmarks are inadequate in measuring the complex logical reasoning capabilities of a model. We present FOLIO, a human-annotated, logically complex and diverse dataset for reasoning in natural language (NL), equipped with first-order logic (FOL) annotations. FOLIO consists of 1,430 examples (unique conclusions), each paired with one of 487 sets of premises used to deductively reason for the validity of each conclusion. The logical correctness of the premises and conclusions is ensured by their FOL annotations, which are automatically verified by an FOL inference engine. In addition to the main NL reasoning task, NL-FOL pairs in FOLIO constitute a new NL-FOL translation dataset. Our experiments on FOLIO systematically evaluate the FOL reasoning ability of supervised fine-tuning on medium-sized language models. For both NL reasoning and NL-FOL translation, we benchmark multiple state-of-the-art language models. Our results show that a subset of FOLIO presents a challenge for one of the most capable {Large Language Model (LLM)} publicly available, GPT-4.",
            "link": "https://www.semanticscholar.org/paper/5581bf85386737bd3378eec68189759a05280bea",
            "authors": "Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, E. Zubova, Yujie Qiao, Matthew Burtell, David Peng, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Shafiq R. Joty, Alexander R. Fabbri, Wojciech Kryscinski, Xi Victoria Lin, Caiming Xiong, Dragomir R. Radev",
            "EMNLP Paper ID": "3070",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "21b0e9cf7966f3641f0fed59bae6e11e3cc85930",
            "title": "In-Context Learning May Not Elicit Trustworthy Reasoning: A-Not-B Errors in Pretrained Language Models",
            "abstract": "Recent advancements in artificial intelligence have led to the creation of highly capable large language models (LLMs) that can perform tasks in a human-like manner. However, LLMs exhibit only infant-level cognitive abilities in certain areas. One such area is the A-Not-B error, a phenomenon seen in infants where they repeat a previously rewarded behavior despite well-observed changed conditions. This highlights their lack of inhibitory control -- the ability to stop a habitual or impulsive response. In our work, we design a text-based multi-choice QA scenario similar to the A-Not-B experimental settings to systematically test the inhibitory control abilities of LLMs. We found that state-of-the-art LLMs (like Llama3-8b) perform consistently well with in-context learning (ICL) but make errors and show a significant drop of as many as 83.3% in reasoning tasks when the context changes trivially. This suggests that LLMs only have inhibitory control abilities on par with human infants in this regard, often failing to suppress the previously established response pattern during ICL.",
            "link": "https://www.semanticscholar.org/paper/21b0e9cf7966f3641f0fed59bae6e11e3cc85930",
            "authors": "Pengrui Han, Peiyang Song, Haofei Yu, Jiaxuan You",
            "matchScore": 313.2461,
            "original title": "In-Context Learning May Not Elicit Trustworthy Reasoning: A-Not-B Errors in Pretrained Language Models",
            "original authors": "Pengrui Han, Peiyang Song, Haofei Yu, Jiaxuan You",
            "EMNLP Paper ID": "1147",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "7c1f1598a2926e1ab384d48db5411183f2c548d5",
            "title": "Exploring the Limits of Fine-grained LLM-based Physics Inference via Premise Removal Interventions",
            "abstract": "Language models (LMs) can hallucinate when performing complex mathematical reasoning. Physics provides a rich domain for assessing their mathematical capabilities, where physical context requires that any symbolic manipulation satisfies complex semantics (\\textit{e.g.,} units, tensorial order). In this work, we systematically remove crucial context from prompts to force instances where model inference may be algebraically coherent, yet unphysical. We assess LM capabilities in this domain using a curated dataset encompassing multiple notations and Physics subdomains. Further, we improve zero-shot scores using synthetic in-context examples, and demonstrate non-linear degradation of derivation quality with perturbation strength via the progressive omission of supporting premises. We find that the models' mathematical reasoning is not physics-informed in this setting, where physical context is predominantly ignored in favour of reverse-engineering solutions.",
            "link": "https://www.semanticscholar.org/paper/7c1f1598a2926e1ab384d48db5411183f2c548d5",
            "authors": "Jordan Meadows, Tamsin James, Andre Freitas",
            "matchScore": 325.45282,
            "original title": "Exploring the Limits of Fine-grained LLM-based Physics Inference via Premise Removal Interventions",
            "original authors": "Jordan Meadows, Tamsin Emily James, Andre Freitas",
            "EMNLP Paper ID": "1323",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "3ef2ddc2ec699e52899a26ac3ba6d9a069c30ead",
            "title": "Learning Semantic Structure through First-Order-Logic Translation",
            "abstract": "In this paper, we study whether transformer-based language models can extract predicate argument structure from simple sentences. We firstly show that language models sometimes confuse which predicates apply to which objects. To mitigate this, we explore two tasks: question answering (Q/A), and first order logic (FOL) translation, and two regimes, prompting and finetuning. In FOL translation, we finetune several large language models on synthetic datasets designed to gauge their generalization abilities. For Q/A, we finetune encoder models like BERT and RoBERTa and use prompting for LLMs. The results show that FOL translation for LLMs is better suited to learn predicate argument structure.",
            "link": "https://www.semanticscholar.org/paper/3ef2ddc2ec699e52899a26ac3ba6d9a069c30ead",
            "authors": "Akshay Chaturvedi, Nicholas Asher",
            "matchScore": 240.42801,
            "original title": "Learning Semantic Structure through First-Order-Logic Translation",
            "original authors": "Akshay Chaturvedi, Nicholas Asher",
            "EMNLP Paper ID": "1354",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "e2c237a7c3d9a811901030296e65bc5206eb78e8",
            "title": "Traffic Light or Light Traffic? Investigating Phrasal Semantics in Large Language Models",
            "abstract": "Phrases are fundamental linguistic units through which humans convey semantics. This study critically examines the capacity of API-based large language models (LLMs) to comprehend phrase semantics, utilizing three human-annotated datasets. We assess the performance of LLMs in executing phrase semantic reasoning tasks guided by natural language instructions and explore the impact of common prompting techniques, including few-shot demonstrations and Chain-of-Thought reasoning. Our findings reveal that LLMs greatly outperform traditional embedding methods across the datasets; however, they do not show a significant advantage over fine-tuned methods. The effectiveness of advanced prompting strategies shows variability. We conduct detailed error analyses to interpret the limitations faced by LLMs in comprehending phrase semantics. Code and data can be found at https://github.com/memray/llm_phrase_semantics.",
            "link": "https://www.semanticscholar.org/paper/e2c237a7c3d9a811901030296e65bc5206eb78e8",
            "authors": "Rui Meng, Ye Liu, Lifu Tu, Daqing He, Yingbo Zhou, Semih Yavuz",
            "matchScore": 271.93356,
            "original title": "Traffic Light or Light Traffic? Investigating Phrasal Semantics in Large Language Models",
            "original authors": "Rui Meng, Ye Liu, Lifu Tu, Daqing He, Yingbo Zhou, Semih Yavuz",
            "EMNLP Paper ID": "1805",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "df79b3522b3d199c6a24b40f4afd682f1045a2e9",
            "title": "Can LLMs Reason in the Wild with Programs?",
            "abstract": "Large Language Models (LLMs) have shown superior capability to solve reasoning problems with programs. While being a promising direction, most of such frameworks are trained and evaluated in settings with a prior knowledge of task requirements. However, as LLMs become more capable, it is necessary to assess their reasoning abilities in more realistic scenarios where many real-world problems are open-ended with ambiguous scope, and often require multiple formalisms to solve. To investigate this, we introduce the task of reasoning in the wild, where an LLM is tasked to solve a reasoning problem of unknown type by identifying the subproblems and their corresponding formalisms, and writing a program to solve each subproblem, guided by a tactic. We create a large tactic-guided trajectory dataset containing detailed solutions to a diverse set of reasoning problems, ranging from well-defined single-form reasoning (e.g., math, logic), to ambiguous and hybrid ones (e.g., commonsense, combined math and logic). This allows us to test various aspects of LLMs reasoning at the fine-grained level such as the selection and execution of tactics, and the tendency to take undesired shortcuts. In experiments, we highlight that existing LLMs fail significantly on problems with ambiguous and mixed scope, revealing critical limitations and overfitting issues (e.g. accuracy on GSM8K drops by at least 50\\%). We further show the potential of finetuning a local LLM on the tactic-guided trajectories in achieving better performance. Project repo is available at github.com/gblackout/Reason-in-the-Wild",
            "link": "https://www.semanticscholar.org/paper/df79b3522b3d199c6a24b40f4afd682f1045a2e9",
            "authors": "Yuan Yang, Siheng Xiong, Ali Payani, Ehsan Shareghi, F. Fekri",
            "matchScore": 183.99881,
            "original title": "Can LLMs Reason in the Wild with Programs?",
            "original authors": "Yuan Yang, Siheng Xiong, Ali Payani, Ehsan Shareghi, Faramarz Fekri",
            "EMNLP Paper ID": "2017",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "bf4f5af90bbd3ec2195a8b3bd9198b5f6318100b",
            "title": "Unveiling Narrative Reasoning Limits of Large Language Models with Trope in Movie Synopses",
            "abstract": "Large language models (LLMs) equipped with chain-of-thoughts (CoT) prompting have shown significant multi-step reasoning capabilities in factual content like mathematics, commonsense, and logic. However, their performance in narrative reasoning, which demands greater abstraction capabilities, remains unexplored. This study utilizes tropes in movie synopses to assess the abstract reasoning abilities of state-of-the-art LLMs and uncovers their low performance. We introduce a trope-wise querying approach to address these challenges and boost the F1 score by 11.8 points. Moreover, while prior studies suggest that CoT enhances multi-step reasoning, this study shows CoT can cause hallucinations in narrative content, reducing GPT-4's performance. We also introduce an Adversarial Injection method to embed trope-related text tokens into movie synopses without explicit tropes, revealing CoT's heightened sensitivity to such injections. Our comprehensive analysis provides insights for future research directions.",
            "link": "https://www.semanticscholar.org/paper/bf4f5af90bbd3ec2195a8b3bd9198b5f6318100b",
            "authors": "Hung-Ting Su, Ya-Ching Hsu, Xudong Lin, X. Shi, Yulei Niu, Han-Yuan Hsu, Hung-yi Lee, Winston H. Hsu",
            "matchScore": 296.98,
            "original title": "Unveiling Narrative Reasoning Limits of Large Language Models with Trope in Movie Synopses",
            "original authors": "Hung-Ting Su, Ya-Ching Hsu, Xudong Lin, Xiang-Qian Shi, Yulei Niu, Han-Yuan Hsu, Hung-yi Lee, Winston H. Hsu",
            "EMNLP Paper ID": "2852",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "1c69025669b05866c710d1bf35a9c73cabb992e1",
            "title": "Explicit Inductive Inference using Large Language Models",
            "abstract": "Large Language Models (LLMs) are reported to hold undesirable attestation bias on inference tasks: when asked to predict if a premise P entails a hypothesis H, instead of considering H's conditional truthfulness entailed by P, LLMs tend to use the out-of-context truth label of H as a fragile proxy. In this paper, we propose a pipeline that exploits this bias to do explicit inductive inference. Our pipeline uses an LLM to transform a premise into a set of attested alternatives, and then aggregate answers of the derived new entailment inquiries to support the original inference prediction. On a directional predicate entailment benchmark, we demonstrate that by applying this simple pipeline, we can improve the overall performance of LLMs on inference and substantially alleviate the impact of their attestation bias.",
            "link": "https://www.semanticscholar.org/paper/1c69025669b05866c710d1bf35a9c73cabb992e1",
            "authors": "Tianyang Liu, Tianyi Li, Liang Cheng, Mark Steedman",
            "matchScore": 203.55827,
            "original title": "Explicit Inductive Inference using Large Language Models",
            "original authors": "Tianyang Liu, Tianyi Li, Liang Cheng, Mark Steedman",
            "EMNLP Paper ID": "3023",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "3b0acd330d789d2cc71e5b76a2595770b1ef4795",
            "title": "P-FOLIO: Evaluating and Improving Logical Reasoning with Abundant Human-Written Reasoning Chains",
            "abstract": "Existing methods on understanding the capabilities of LLMs in logical reasoning rely on binary entailment classification or synthetically derived rationales, which are not sufficient for proper investigation of model's capabilities. We present P-FOLIO, a human-annotated dataset consisting of diverse and complex reasoning chains for a set of realistic logical reasoning stories also written by humans. P-FOLIO is collected with an annotation protocol that facilitates humans to annotate well-structured natural language proofs for first-order logic reasoning problems in a step-by-step manner. The number of reasoning steps in P-FOLIO span from 0 to 20. We further use P-FOLIO to evaluate and improve large-language-model (LLM) reasoning capabilities. We evaluate LLM reasoning capabilities at a fine granularity via single-step inference rule classification, with more diverse inference rules of more diverse and higher levels of complexities than previous works. Given that a single model-generated reasoning chain could take a completely different path than the human-annotated one, we sample multiple reasoning chains from a model and use pass@k metrics for evaluating the quality of model-generated reasoning chains. We show that human-written reasoning chains significantly boost the logical reasoning capabilities of LLMs via many-shot prompting and fine-tuning. Furthermore, fine-tuning Llama3-7B on P-FOLIO improves the model performance by 10% or more on three other out-of-domain logical reasoning datasets. We also conduct detailed analysis to show where most powerful LLMs fall short in reasoning. We will release the dataset and code publicly.",
            "link": "https://www.semanticscholar.org/paper/3b0acd330d789d2cc71e5b76a2595770b1ef4795",
            "authors": "Simeng Han, Aaron Yu, Rui Shen, Zhenting Qi, Martin Riddell, Wenfei Zhou, Yujie Qiao, Yilun Zhao, Semih Yavuz, Ye Liu, Shafiq Joty, Yingbo Zhou, Caiming Xiong, Dragomir R. Radev, Rex Ying, Arman Cohan",
            "matchScore": 317.02423,
            "original title": "P-FOLIO: Evaluating and Improving Logical Reasoning with Abundant Human-Written Reasoning Chains",
            "original authors": "SIMENG HAN, Aaron Yu, Rui Shen, Zhenting Qi, Martin Riddell, Wenfei Zhou, Yujie Qiao, Yilun Zhao, Semih Yavuz, Ye Liu, Shafiq Joty, Yingbo Zhou, Caiming Xiong, Rex Ying, Arman Cohan, Dragomir Radev",
            "EMNLP Paper ID": "3182",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "4917d60c82ce041afad1b63887d49d0062031f66",
            "title": "Zero-Shot Fact Verification via Natural Logic and Large Language Models",
            "abstract": "The recent development of fact verification systems with natural logic has enhanced their explainability by aligning claims with evidence through set-theoretic operators, providing faithful justifications. Despite these advancements, such systems often rely on a large amount of training data annotated with natural logic. To address this issue, we propose a zero-shot method that utilizes the generalization capabilities of instruction-tuned large language models. To comprehensively assess the zero-shot capabilities of our method and other fact verification systems, we evaluate all models on both artificial and real-world claims, including multilingual datasets. We also compare our method against other fact verification systems in two setups. First, in the zero-shot generalization setup, we demonstrate that our approach outperforms other systems that were not specifically trained on natural logic data, achieving an average accuracy improvement of 8.96 points over the best-performing baseline. Second, in the zero-shot transfer setup, we show that current systems trained on natural logic data do not generalize well to other domains, and our method outperforms these systems across all datasets with real-world claims.",
            "link": "https://www.semanticscholar.org/paper/4917d60c82ce041afad1b63887d49d0062031f66",
            "authors": "Marek Strong, Rami Aly, Andreas Vlachos",
            "matchScore": 286.2199,
            "original title": "Zero-Shot Fact Verification via Natural Logic and Large Language Models",
            "original authors": "Marek Strong, Rami Aly, Andreas Vlachos",
            "EMNLP Paper ID": "3262",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "16f537d11776c970aebeb3a8c6c896b97ea00820",
            "title": "Can Large Language Models Understand DL-Lite Ontologies? An Empirical Study",
            "abstract": "Large language models (LLMs) have shown significant achievements in solving a wide range of tasks. Recently, LLMs' capability to store, retrieve and infer with symbolic knowledge has drawn a great deal of attention, showing their potential to understand structured information. However, it is not yet known whether LLMs can understand Description Logic (DL) ontologies. In this work, we empirically analyze the LLMs' capability of understanding DL-Lite ontologies covering 6 representative tasks from syntactic and semantic aspects. With extensive experiments, we demonstrate both the effectiveness and limitations of LLMs in understanding DL-Lite ontologies. We find that LLMs can understand formal syntax and model-theoretic semantics of concepts and roles. However, LLMs struggle with understanding TBox NI transitivity and handling ontologies with large ABoxes. We hope that our experiments and analyses provide more insights into LLMs and inspire to build more faithful knowledge engineering solutions.",
            "link": "https://www.semanticscholar.org/paper/16f537d11776c970aebeb3a8c6c896b97ea00820",
            "authors": "Keyu Wang, Guilin Qi, Jiaqi Li, Songlin Zhai",
            "matchScore": 298.25302,
            "original title": "Can Large Language Models Understand DL-Lite Ontologies? An Empirical Study",
            "original authors": "Keyu Wang, Guilin Qi, Jiaqi Li, Songlin Zhai",
            "EMNLP Paper ID": "519",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "d46c1b5f457ab2f37f834ff07cdd1ae15667a648",
            "title": "Plausibly Problematic Questions in Multiple-Choice Benchmarks for Commonsense Reasoning",
            "abstract": "Questions involving commonsense reasoning about everyday situations often admit many $\\textit{possible}$ or $\\textit{plausible}$ answers. In contrast, multiple-choice question (MCQ) benchmarks for commonsense reasoning require a hard selection of a single correct answer, which, in principle, should represent the $\\textit{most}$ plausible answer choice. On $250$ MCQ items sampled from two commonsense reasoning benchmarks, we collect $5,000$ independent plausibility judgments on answer choices. We find that for over 20% of the sampled MCQs, the answer choice rated most plausible does not match the benchmark gold answers; upon manual inspection, we confirm that this subset exhibits higher rates of problems like ambiguity or semantic mismatch between question and answer choices. Experiments with LLMs reveal low accuracy and high variation in performance on the subset, suggesting our plausibility criterion may be helpful in identifying more reliable benchmark items for commonsense evaluation.",
            "link": "https://www.semanticscholar.org/paper/d46c1b5f457ab2f37f834ff07cdd1ae15667a648",
            "authors": "Shramay Palta, Nishant Balepur, Peter Rankel, Sarah Wiegreffe, Marine Carpuat, Rachel Rudinger",
            "matchScore": 274.57263,
            "original title": "Plausibly Problematic Questions in Multiple-Choice Benchmarks for Commonsense Reasoning",
            "original authors": "Shramay Palta, Nishant Balepur, Peter A. Rankel, Sarah Wiegreffe, Marine Carpuat, Rachel Rudinger",
            "EMNLP Paper ID": "706",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "aafc487e8a71c32daa151ce56f9c656a86631cab",
            "title": "Self-Contradictory Reasoning Evaluation and Detection",
            "abstract": "In a plethora of recent work, large language models (LLMs) demonstrated impressive reasoning ability, but many proposed downstream reasoning tasks only focus on final answers. Two fundamental questions persist: 1) how consistent is the reasoning, and 2) can models detect unreliable reasoning? In this paper, we investigate self-contradictory (Self-Contra) reasoning, where the model reasoning does not support its answers. To answer 1), we define and assess the Self-Contra rate across three datasets and delve into finer-grained categories of Self-Contra reasoning. We find that LLMs often contradict themselves in reasoning tasks involving contextual information understanding or commonsense. The model may generate correct answers by taking shortcuts in reasoning or overlooking contextual evidence, leading to compromised reasoning. For 2), we task the state-of-the-art model GPT-4 with identifying Self-Contra reasoning and finer-grained fallacies. We find that finer-grained categories enhanced detection can improve GPT-4's ability to detect Self-Contra. However, it is only able to detect Self-Contra with a 52.2% F1 score, much lower compared to 66.7% for humans. Our results indicate that current LLMs lack the robustness necessary for reliable reasoning and we emphasize the urgent need for establishing best practices in comprehensive reasoning evaluations beyond pure performance-based metrics.",
            "link": "https://www.semanticscholar.org/paper/aafc487e8a71c32daa151ce56f9c656a86631cab",
            "authors": "Ziyi Liu, Isabelle G. Lee, Yongkang Du, Soumya Sanyal, Jieyu Zhao",
            "matchScore": 174.32732,
            "original title": "Self-contradictory reasoning evaluation and detection",
            "original authors": "Ziyi Liu, Soumya Sanyal, Isabelle Lee, Yongkang Du, Rahul Gupta, Yang Liu, Jieyu Zhao",
            "EMNLP Paper ID": "752",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        }
    ],
    "Optimization and Scalability Strategies for Large Language Models": [
        {
            "paperId": "db49aa6ff9f329f72fb0071a2d2312092d2fa877",
            "title": "Rethinking Pruning Large Language Models: Benefits and Pitfalls of Reconstruction Error Minimization",
            "abstract": "This work suggests fundamentally rethinking the current practice of pruning large language models (LLMs). The way it is done is by divide and conquer: split the model into submodels, sequentially prune them, and reconstruct predictions of the dense counterparts on small calibration data one at a time; the final model is obtained simply by putting the resulting sparse submodels together. While this approach enables pruning under memory constraints, it generates high reconstruction errors. In this work, we first present an array of reconstruction techniques that can significantly reduce this error by more than $90\\%$. Unwittingly, however, we discover that minimizing reconstruction error is not always ideal and can overfit the given calibration data, resulting in rather increased language perplexity and poor performance at downstream tasks. We find out that a strategy of self-generating calibration data can mitigate this trade-off between reconstruction and generalization, suggesting new directions in the presence of both benefits and pitfalls of reconstruction for pruning LLMs.",
            "link": "https://www.semanticscholar.org/paper/db49aa6ff9f329f72fb0071a2d2312092d2fa877",
            "authors": "Sungbin Shin, Wonpyo Park, Jaeho Lee, Namhoon Lee",
            "EMNLP Paper ID": "143",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "a7498196b05db7627a9763d581bf2782ae430a9e",
            "title": "Model Balancing Helps Low-data Training and Fine-tuning",
            "abstract": "Recent advances in foundation models have emphasized the need to align pre-trained models with specialized domains using small, curated datasets. Studies on these foundation models underscore the importance of low-data training and fine-tuning. This topic, well-known in natural language processing (NLP), has also gained increasing attention in the emerging field of scientific machine learning (SciML). To address the limitations of low-data training and fine-tuning, we draw inspiration from Heavy-Tailed Self-Regularization (HT-SR) theory, analyzing the shape of empirical spectral densities (ESDs) and revealing an imbalance in training quality across different model layers. To mitigate this issue, we adapt a recently proposed layer-wise learning rate scheduler, TempBalance, which effectively balances training quality across layers and enhances low-data training and fine-tuning for both NLP and SciML tasks. Notably, TempBalance demonstrates increasing performance gains as the amount of available tuning data decreases. Comparative analyses further highlight the effectiveness of TempBalance and its adaptability as an\"add-on\"method for improving model performance.",
            "link": "https://www.semanticscholar.org/paper/a7498196b05db7627a9763d581bf2782ae430a9e",
            "authors": "Zihang Liu, Yuanzhe Hu, Tianyu Pang, Yefan Zhou, Pu Ren, Yaoqing Yang",
            "EMNLP Paper ID": "160",
            "Oral/Poster": "Oral",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "652344ac5269e90105d6af9e7bd72665577fe8e6",
            "title": "MetaGPT: Merging Large Language Models Using Model Exclusive Task Arithmetic",
            "abstract": "The advent of large language models (LLMs) like GPT-4 has catalyzed the exploration of multi-task learning (MTL), in which a single model demonstrates proficiency across diverse tasks. Task arithmetic has emerged as a cost-effective approach for MTL. It enables performance enhancement across multiple tasks by adding their corresponding task vectors to a pre-trained model. However, the current lack of a method that can simultaneously achieve optimal performance, computational efficiency, and data privacy limits their application to LLMs. In this paper, we propose \\textbf{M}odel \\textbf{E}xclusive \\textbf{T}ask \\textbf{A}rithmetic for merging \\textbf{GPT}-scale models, which formalizes the objective of model merging into a multi-task learning framework, aiming to minimize the average loss difference between the merged model and each individual task model. Since data privacy limits the use of multi-task training data, we leverage LLMs' local linearity and task vectors' orthogonality to separate the data term and scaling coefficients term and derive a model-exclusive task arithmetic method. Our proposed MetaGPT is data-agnostic and bypasses the heavy search process, making it cost-effective and easy to implement for LLMs.Extensive experiments demonstrate that MetaGPT leads to improvements in task arithmetic and achieves state-of-the-art performance on multiple tasks.",
            "link": "https://www.semanticscholar.org/paper/652344ac5269e90105d6af9e7bd72665577fe8e6",
            "authors": "Yuyan Zhou, Liang Song, Bingning Wang, Weipeng Chen",
            "EMNLP Paper ID": "197",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "4c51430adfe11ff9f56399b710178d8b12411430",
            "title": "Collaborative Performance Prediction for Large Language Models",
            "abstract": "Comprehensively understanding and accurately predicting the performance of large language models across diverse downstream tasks has emerged as a pivotal challenge in NLP research. The pioneering scaling law on downstream works demonstrated intrinsic similarities within model families and utilized such similarities for performance prediction. However, they tend to overlook the similarities between model families and only consider design factors listed in the original scaling law. To overcome these limitations, we introduce a novel framework, Collaborative Performance Prediction (CPP), which significantly enhances prediction accuracy by leveraging the historical performance of various models on downstream tasks and other design factors for both model and task. We also collect a collaborative data sourced from online platforms containing both historical performance and additional design factors. With the support of the collaborative data, CPP not only surpasses traditional scaling laws in predicting the performance of scaled LLMs but also facilitates a detailed analysis of factor importance, an area previously overlooked.",
            "link": "https://www.semanticscholar.org/paper/4c51430adfe11ff9f56399b710178d8b12411430",
            "authors": "Qiyuan Zhang, Fuyuan Lyu, Xue Liu, Chen Ma",
            "EMNLP Paper ID": "288",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "6fdea305b054201a840531ba1f39bb08307a7200",
            "title": "Scaling Laws Across Model Architectures: A Comparative Analysis of Dense and MoE Models in Large Language Models",
            "abstract": "The scaling of large language models (LLMs) is a critical research area for the efficiency and effectiveness of model training and deployment. Our work investigates the transferability and discrepancies of scaling laws between Dense Models and Mixture of Experts (MoE) models. Through a combination of theoretical analysis and extensive experiments, including consistent loss scaling, optimal batch size and learning rate scaling, and resource allocation strategies scaling, our findings reveal that the power-law scaling framework also applies to MoE Models, indicating that the fundamental principles governing the scaling behavior of these models are preserved, even though the architecture differs. Additionally, MoE Models demonstrate superior generalization, resulting in lower testing losses with the same training compute budget compared to Dense Models. These findings indicate the scaling consistency and transfer generalization capabilities of MoE Models, providing new insights for optimizing MoE Model training and deployment strategies.",
            "link": "https://www.semanticscholar.org/paper/6fdea305b054201a840531ba1f39bb08307a7200",
            "authors": "Siqi Wang, Zhengyu Chen, Bei Li, Keqing He, Min Zhang, Jingang Wang",
            "EMNLP Paper ID": "622",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "64dbe50f4bf7b37f395437f13028a06624aa70e8",
            "title": "Personalized Pieces: Efficient Personalized Large Language Models through Collaborative Efforts",
            "abstract": "Personalized large language models (LLMs) aim to tailor interactions, content, and recommendations to individual user preferences. While parameter-efficient fine-tuning (PEFT) methods excel in performance and generalization, they are costly and limit communal benefits when used individually. To this end, we introduce Personalized Pieces (Per-Pcs), a framework that allows users to safely share and assemble personalized PEFT efficiently with collaborative efforts. Per-Pcs involves selecting sharers, breaking their PEFT into pieces, and training gates for each piece. These pieces are added to a pool, from which target users can select and assemble personalized PEFT using their history data. This approach preserves privacy and enables fine-grained user modeling without excessive storage and computation demands. Experimental results show Per-Pcs outperforms non-personalized and PEFT retrieval baselines, offering performance comparable to OPPU with significantly lower resource use across six tasks. Further analysis highlights Per-Pcs's robustness concerning sharer count and selection strategy, pieces sharing ratio, and scalability in computation time and storage space. Per-Pcs's modularity promotes safe sharing, making LLM personalization more efficient, effective, and widely accessible through collaborative efforts.",
            "link": "https://www.semanticscholar.org/paper/64dbe50f4bf7b37f395437f13028a06624aa70e8",
            "authors": "Zhaoxuan Tan, Zheyuan Liu, Meng Jiang",
            "EMNLP Paper ID": "725",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "d32b1539610db4bd5e02b547132e70af7c399f64",
            "title": "Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning",
            "abstract": "Personalization in large language models (LLMs) is increasingly important, aiming to align the LLMs' interactions, content, and recommendations with individual user preferences. Recent advances have highlighted effective prompt design by enriching user queries with non-parametric knowledge through behavior history retrieval and textual profiles. However, these methods faced limitations due to a lack of model ownership, resulting in constrained customization and privacy issues, and often failed to capture complex, dynamic user behavior patterns. To address these shortcomings, we introduce One PEFT Per User (OPPU), employing personalized parameter-efficient fine-tuning (PEFT) modules to store user-specific behavior patterns and preferences. By plugging in personal PEFT parameters, users can own and use their LLMs individually. OPPU integrates parametric user knowledge in the personal PEFT parameters with non-parametric knowledge from retrieval and profiles, adapting LLMs to user behavior shifts. Experimental results demonstrate that OPPU significantly outperforms existing prompt-based methods across seven diverse tasks in the LaMP benchmark. Further studies reveal OPPU's enhanced capabilities in handling user behavior shifts, modeling users at different activity levels, maintaining robustness across various user history formats, and displaying versatility with different PEFT methods.",
            "link": "https://www.semanticscholar.org/paper/d32b1539610db4bd5e02b547132e70af7c399f64",
            "authors": "Zhaoxuan Tan, Qingkai Zeng, Yijun Tian, Zheyuan Liu, Bing Yin, Meng Jiang",
            "EMNLP Paper ID": "727",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "9ad6ab78eadff118fad2afd6804cfba6bfa89223",
            "title": "CoBa: Convergence Balancer for Multitask Finetuning of Large Language Models",
            "abstract": "Multi-task learning (MTL) benefits the fine-tuning of large language models (LLMs) by providing a single model with improved performance and generalization ability across tasks, presenting a resource-efficient alternative to developing separate models for each task. Yet, existing MTL strategies for LLMs often fall short by either being computationally intensive or failing to ensure simultaneous task convergence. This paper presents CoBa, a new MTL approach designed to effectively manage task convergence balance with minimal computational overhead. Utilizing Relative Convergence Scores (RCS), Absolute Convergence Scores (ACS), and a Divergence Factor (DF), CoBa dynamically adjusts task weights during the training process, ensuring that the validation loss of all tasks progress towards convergence at an even pace while mitigating the issue of individual task divergence. The results of our experiments involving three disparate datasets underscore that this approach not only fosters equilibrium in task convergence but enhances the LLMs' performance by up to 13% relative to the second-best baselines. Code is open-sourced at https://github.com/codefuse-ai/MFTCoder.",
            "link": "https://www.semanticscholar.org/paper/9ad6ab78eadff118fad2afd6804cfba6bfa89223",
            "authors": "Zi Gong, Hang Yu, Cong Liao, Bingchang Liu, Chaoyu Chen, Jianguo Li",
            "EMNLP Paper ID": "921",
            "Oral/Poster": "Oral",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "b0c7bd210c8ae00bba5759de1d5dd0a09f8eba8d",
            "title": "MOSEL: Inference Serving Using Dynamic Modality Selection",
            "abstract": "Rapid advancements over the years have helped machine learning models reach previously hard-to-achieve goals, sometimes even exceeding human capabilities. However, to attain the desired accuracy, the model sizes and in turn their computational requirements have increased drastically. Thus, serving predictions from these models to meet any target latency and cost requirements of applications remains a key challenge, despite recent work in building inference-serving systems as well as algorithmic approaches that dynamically adapt models based on inputs. In this paper, we introduce a form of dynamism, modality selection, where we adaptively choose modalities from inference inputs while maintaining the model quality. We introduce MOSEL, an automated inference serving system for multi-modal ML models that carefully picks input modalities per request based on user-defined performance and accuracy requirements. MOSEL exploits modality configurations extensively, improving system throughput by 3.6$\\times$ with an accuracy guarantee and shortening job completion times by 11$\\times$.",
            "link": "https://www.semanticscholar.org/paper/b0c7bd210c8ae00bba5759de1d5dd0a09f8eba8d",
            "authors": "Bodun Hu, Le Xu, Jeongyoon Moon, N. Yadwadkar, Aditya Akella",
            "EMNLP Paper ID": "1013",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "c0df8b59bc5fd4145800360e1532eac06d45ca65",
            "title": "Stable Language Model Pre-training by Reducing Embedding Variability",
            "abstract": "Stable pre-training is essential for achieving better-performing language models. However, tracking pre-training stability by calculating gradient variance at every step is impractical due to the significant computational costs. We explore Token Embedding Variability (TEV) as a simple and efficient proxy for assessing pre-training stability in language models with pre-layer normalization, given that shallower layers are more prone to gradient explosion (section 2.2). Moreover, we propose Multi-head Low-Rank Attention (MLRA) as an architecture to alleviate such instability by limiting the exponential growth of output embedding variance, thereby preventing the gradient explosion (section 3.2). Empirical results on GPT-2 with MLRA demonstrate increased stability and lower perplexity, particularly in deeper models.",
            "link": "https://www.semanticscholar.org/paper/c0df8b59bc5fd4145800360e1532eac06d45ca65",
            "authors": "Woojin Chung, Jiwoo Hong, Na Min An, James Thorne, Se-Young Yun",
            "EMNLP Paper ID": "1235",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "9cf34c447ba9d82960bf7509edbd38e97cd5d445",
            "title": "TroL: Traversal of Layers for Large Language and Vision Models",
            "abstract": "Large language and vision models (LLVMs) have been driven by the generalization power of large language models (LLMs) and the advent of visual instruction tuning. Along with scaling them up directly, these models enable LLVMs to showcase powerful vision language (VL) performances by covering diverse tasks via natural language instructions. However, existing open-source LLVMs that perform comparably to closed-source LLVMs such as GPT-4V are often considered too large (e.g., 26B, 34B, and 110B parameters), having a larger number of layers. These large models demand costly, high-end resources for both training and inference. To address this issue, we present a new efficient LLVM family with 1.8B, 3.8B, and 7B LLM model sizes, Traversal of Layers (TroL), which enables the reuse of layers in a token-wise manner. This layer traversing technique simulates the effect of looking back and retracing the answering stream while increasing the number of forward propagation layers without physically adding more layers. We demonstrate that TroL employs a simple layer traversing approach yet efficiently outperforms the open-source LLVMs with larger model sizes and rivals the performances of the closed-source LLVMs with substantial sizes.",
            "link": "https://www.semanticscholar.org/paper/9cf34c447ba9d82960bf7509edbd38e97cd5d445",
            "authors": "Byung-Kwan Lee, Sangyun Chung, Chae Won Kim, Beomchan Park, Yonghyun Ro",
            "EMNLP Paper ID": "1319",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "77928cb6067c3d2e0bb304d4966660cd2b9a2af3",
            "title": "A Learning Rate Path Switching Training Paradigm for Version Updates of Large Language Models",
            "abstract": "Due to the continuous emergence of new data, version updates have become an indispensable requirement for Large Language Models (LLMs). The training paradigms for version updates of LLMs include pre-training from scratch (PTFS) and continual pre-training (CPT). Preliminary experiments demonstrate that PTFS achieves better pre-training performance, while CPT has lower training cost. Moreover, their performance and training cost gaps widen progressively with version updates. To investigate the underlying reasons for this phenomenon, we analyze the effect of learning rate adjustments during the two stages of CPT: preparing an initialization checkpoint and continual pre-training based on this checkpoint. We find that a large learning rate in the first stage and a complete learning rate decay process in the second stage are crucial for version updates of LLMs. Hence, we propose a learning rate path switching training paradigm. Our paradigm comprises one main path, where we pre-train a LLM with the maximal learning rate, and multiple branching paths, each of which corresponds to an update of the LLM with newly-added training data. Extensive experiments demonstrate the effectiveness and generalization of our paradigm. Particularly, when training four versions of LLMs, our paradigm reduces the total training cost to 58% compared to PTFS, while maintaining comparable pre-training performance.",
            "link": "https://www.semanticscholar.org/paper/77928cb6067c3d2e0bb304d4966660cd2b9a2af3",
            "authors": "Zhihao Wang, Shiyu Liu, Jianheng Huang, Zheng Wang, Yixuan Liao, Xiaoxin Chen, Junfeng Yao, Jinsong Su",
            "EMNLP Paper ID": "1567",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "7b158dad9f1c992b9500eae5c1f06b894ec4cf00",
            "title": "Mixture-of-Skills: Learning to Optimize Data Usage for Fine-Tuning Large Language Models",
            "abstract": "Large language models (LLMs) are typically fine-tuned on diverse and extensive datasets sourced from various origins to develop a comprehensive range of skills, such as writing, reasoning, chatting, coding, and more. Each skill has unique characteristics, and these datasets are often heterogeneous and imbalanced, making the fine-tuning process highly challenging. Balancing the development of each skill while ensuring the model maintains its overall performance requires sophisticated techniques and careful dataset curation. In this work, we propose a general, model-agnostic, reinforcement learning framework, Mixture-of-Skills (MoS), that learns to optimize data usage automatically during the fine-tuning process. This framework ensures the optimal comprehensive skill development of LLMs by dynamically adjusting the focus on different datasets based on their current learning state. To validate the effectiveness of MoS, we conduct extensive experiments using three diverse LLM backbones on two widely used benchmarks and demonstrate that MoS substantially enhances model performance. Building on the success of MoS, we propose MoSpec, an adaptation for task-specific fine-tuning, which harnesses the utilities of various datasets for a specific purpose. Our work underlines the significance of dataset rebalancing and present MoS as a powerful, general solution for optimizing data usage in the fine-tuning of LLMs for various purposes.",
            "link": "https://www.semanticscholar.org/paper/7b158dad9f1c992b9500eae5c1f06b894ec4cf00",
            "authors": "Minghao Wu, Thuy-Trang Vu, Lizhen Qu, Gholamreza Haffari",
            "EMNLP Paper ID": "1640",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "48d2324d4b936d4ebfbd07c2aaa5f0684c63fff0",
            "title": "Mitigating Training Imbalance in LLM Fine-Tuning via Selective Parameter Merging",
            "abstract": "Supervised fine-tuning (SFT) is crucial for adapting Large Language Models (LLMs) to specific tasks. In this work, we demonstrate that the order of training data can lead to significant training imbalances, potentially resulting in performance degradation. Consequently, we propose to mitigate this imbalance by merging SFT models fine-tuned with different data orders, thereby enhancing the overall effectiveness of SFT. Additionally, we introduce a novel technique,\"parameter-selection merging,\"which outperforms traditional weighted-average methods on five datasets. Further, through analysis and ablation studies, we validate the effectiveness of our method and identify the sources of performance improvements.",
            "link": "https://www.semanticscholar.org/paper/48d2324d4b936d4ebfbd07c2aaa5f0684c63fff0",
            "authors": "Yiming Ju, Ziyi Ni, Xingrun Xing, Zhixiong Zeng, Hanyu Zhao, Siqi Fan, Zheng Zhang",
            "EMNLP Paper ID": "1872",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 12): 17:45\u201318:45 (Evening)",
            "Type": "Main"
        },
        {
            "paperId": "fce434329773bdec3deb7e5985c1780492f8cd6f",
            "title": "CMR Scaling Law: Predicting Critical Mixture Ratios for Continual Pre-training of Language Models",
            "abstract": "Large Language Models (LLMs) excel in diverse tasks but often underperform in specialized fields due to limited domain-specific or proprietary corpus. Continual pre-training (CPT) enhances LLM capabilities by imbuing new domain-specific or proprietary knowledge while replaying general corpus to prevent catastrophic forgetting. The data mixture ratio of general corpus and domain-specific corpus, however, has been chosen heuristically, leading to sub-optimal training efficiency in practice. In this context, we attempt to re-visit the scaling behavior of LLMs under the hood of CPT, and discover a power-law relationship between loss, mixture ratio, and training tokens scale. We formalize the trade-off between general and domain-specific capabilities, leading to a well-defined Critical Mixture Ratio (CMR) of general and domain data. By striking the balance, CMR maintains the model's general ability and achieves the desired domain transfer, ensuring the highest utilization of available resources. Considering the balance between efficiency and effectiveness, CMR can be regarded as the optimal mixture ratio. Through extensive experiments, we ascertain the predictability of CMR, propose CMR scaling law and have substantiated its generalization. These findings offer practical guidelines for optimizing LLM training in specialized domains, ensuring both general and domain-specific performance while efficiently managing training resources.",
            "link": "https://www.semanticscholar.org/paper/fce434329773bdec3deb7e5985c1780492f8cd6f",
            "authors": "Jiawei Gu, Zacc Yang, Chuanghao Ding, Rui Zhao, Fei Tan",
            "EMNLP Paper ID": "1900",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "fc4cf5c9c24c1fe05e0ee3c33490252bb2e5e8d5",
            "title": "Exploring space efficiency in a tree-based linear model for extreme multi-label classification",
            "abstract": "Extreme multi-label classification (XMC) aims to identify relevant subsets from numerous labels. Among the various approaches for XMC, tree-based linear models are effective due to their superior efficiency and simplicity. However, the space complexity of tree-based methods is not well-studied. Many past works assume that storing the model is not affordable and apply techniques such as pruning to save space, which may lead to performance loss. In this work, we conduct both theoretical and empirical analyses on the space to store a tree model under the assumption of sparse data, a condition frequently met in text data. We found that, some features may be unused when training binary classifiers in a tree method, resulting in zero values in the weight vectors. Hence, storing only non-zero elements can greatly save space. Our experimental results indicate that tree models can achieve up to a 95% reduction in storage space compared to the standard one-vs-rest method for multi-label text classification. Our research provides a simple procedure to estimate the size of a tree model before training any classifier in the tree nodes. Then, if the model size is already acceptable, this approach can help avoid modifying the model through weight pruning or other techniques.",
            "link": "https://www.semanticscholar.org/paper/fc4cf5c9c24c1fe05e0ee3c33490252bb2e5e8d5",
            "authors": "He-Zhe Lin, Cheng-Hung Liu, Chih-Jen Lin",
            "EMNLP Paper ID": "1915",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "685283f452856d3d70902860e5b0634e695d9a42",
            "title": "Scaling Laws for Linear Complexity Language Models",
            "abstract": "The interest in linear complexity models for large language models is on the rise, although their scaling capacity remains uncertain. In this study, we present the scaling laws for linear complexity language models to establish a foundation for their scalability. Specifically, we examine the scaling behaviors of three efficient linear architectures. These include TNL, a linear attention model with data-independent decay; HGRN2, a linear RNN with data-dependent decay; and cosFormer2, a linear attention model without decay. We also include LLaMA as a baseline architecture for softmax attention for comparison. These models were trained with six variants, ranging from 70M to 7B parameters on a 300B-token corpus, and evaluated with a total of 1,376 intermediate checkpoints on various downstream tasks. These tasks include validation loss, commonsense reasoning, and information retrieval and generation. The study reveals that existing linear complexity language models exhibit similar scaling capabilities as conventional transformer-based models while also demonstrating superior linguistic proficiency and knowledge retention.",
            "link": "https://www.semanticscholar.org/paper/685283f452856d3d70902860e5b0634e695d9a42",
            "authors": "Xuyang Shen, Dong Li, Ruitao Leng, Zhen Qin, Weigao Sun, Yiran Zhong",
            "EMNLP Paper ID": "1931",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "fc46b1c65bdc89d89ddf91a5f59137c696fb6101",
            "title": "Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging",
            "abstract": "While large language models (LLMs) excel in many domains, their complexity and scale challenge deployment in resource-limited environments. Current compression techniques, such as parameter pruning, often fail to effectively utilize the knowledge from pruned parameters. To address these challenges, we propose Manifold-Based Knowledge Alignment and Layer Merging Compression (MKA), a novel approach that uses manifold learning and the Normalized Pairwise Information Bottleneck (NPIB) measure to merge similar layers, reducing model size while preserving essential performance. We evaluate MKA on multiple benchmark datasets and various LLMs. Our findings show that MKA not only preserves model performance but also achieves substantial compression ratios, outperforming traditional pruning methods. Moreover, when coupled with quantization, MKA delivers even greater compression. Specifically, on the MMLU dataset using the Llama3-8B model, MKA achieves a compression ratio of 43.75% with a minimal performance decrease of only 2.82\\%. The proposed MKA method offers a resource-efficient and performance-preserving model compression technique for LLMs.",
            "link": "https://www.semanticscholar.org/paper/fc46b1c65bdc89d89ddf91a5f59137c696fb6101",
            "authors": "Deyuan Liu, Zhanyue Qin, Hairu Wang, Zhao Yang, Zecheng Wang, Fangying Rong, Qingbin Liu, Yanchao Hao, Xi Chen, Cunhang Fan, Zhao Lv, Zhiying Tu, Dianhui Chu, Dianbo Sui",
            "EMNLP Paper ID": "2151",
            "Oral/Poster": "Oral",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "c0a9ba57687132dfe3cb5f0975a35f896b1bcdf2",
            "title": "Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning",
            "abstract": "Network pruning has emerged as a potential solution to make LLMs cheaper to deploy. However, existing LLM pruning approaches universally rely on the C4 dataset as the calibration data for calculating pruning scores, leaving its optimality unexplored. In this study, we evaluate the choice of calibration data on LLM pruning, across a wide range of datasets that are most commonly used in LLM training and evaluation, including four pertaining datasets as well as three categories of downstream tasks encompassing nine datasets. Each downstream dataset is prompted with In-Context Learning (ICL) and Chain-of-Thought (CoT), respectively. Besides the already intriguing observation that the choice of calibration data significantly impacts the performance of pruned LLMs, our results also uncover several subtle and often unexpected findings, summarized as follows: (1) C4 is not the optimal choice for LLM pruning, even among commonly used pre-training datasets; (2) arithmetic datasets, when used as calibration data, performs on par or even better than pre-training datasets; (3) pruning with downstream datasets does not necessarily help the corresponding downstream task, compared to pre-training data; (4) ICL is widely beneficial to all data categories, whereas CoT is only useful on certain tasks. Our findings shed light on the importance of carefully selecting calibration data for LLM pruning and pave the way for more efficient deployment of these powerful models in real-world applications. We release our code at: https://github.com/abx393/llm-pruning-calibration-data.",
            "link": "https://www.semanticscholar.org/paper/c0a9ba57687132dfe3cb5f0975a35f896b1bcdf2",
            "authors": "Abhinav Bandari, Lu Yin, Cheng-Yu Hsieh, Ajay Kumar Jaiswal, Tianlong Chen, Li Shen, Ranjay Krishna, Shiwei Liu",
            "EMNLP Paper ID": "2215",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "4a20c3d273537272fde9cc9a0d937f914fc2a0dd",
            "title": "DEM: Distribution Edited Model for Training with Mixed Data Distributions",
            "abstract": "Training with mixed data distributions is a common and important part of creating multi-task and instruction-following models. The diversity of the data distributions and cost of joint training makes the optimization procedure extremely challenging. Data mixing methods partially address this problem, albeit having a sub-optimal performance across data sources and require multiple expensive training runs. In this paper, we propose a simple and efficient alternative for better optimization of the data sources by combining models individually trained on each data source with the base model using basic element-wise vector operations. The resulting model, namely Distribution Edited Model (DEM), is 11x cheaper than standard data mixing and outperforms strong baselines on a variety of benchmarks, yielding up to 6.2% improvement on MMLU, 11.5% on BBH, 16.1% on DROP, and 9.3% on HELM with models of size 3B to 13B. Notably, DEM does not require full re-training when modifying a single data-source, thus making it very flexible and scalable for training with diverse data sources.",
            "link": "https://www.semanticscholar.org/paper/4a20c3d273537272fde9cc9a0d937f914fc2a0dd",
            "authors": "Dhananjay Ram, Aditya Rawal, Momchil Hardalov, Nikolaos Pappas, Sheng Zha",
            "EMNLP Paper ID": "2445",
            "Oral/Poster": "Oral",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "5c72a3d6c3e3245b7c2fee751f92e834bc1344d3",
            "title": "DEFT: Data Efficient Fine-Tuning for Pre-Trained Language Models via Unsupervised Core-Set Selection",
            "abstract": "Recent advances have led to the availability of many pre-trained language models (PLMs); however, a question that remains is how much data is truly needed to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT-UCS, a data-efficient fine-tuning framework that leverages unsupervised core-set selection to identify a smaller, representative dataset that reduces the amount of data needed to fine-tune PLMs for downstream tasks. We examine the efficacy of DEFT-UCS in the context of text-editing LMs, and compare to the state-of-the art text-editing model, CoEDIT. Our results demonstrate that DEFT-UCS models are just as accurate as CoEDIT, across eight different datasets consisting of six different editing tasks, while finetuned on 70% less data.",
            "link": "https://www.semanticscholar.org/paper/5c72a3d6c3e3245b7c2fee751f92e834bc1344d3",
            "authors": "Devleena Das, Vivek Khetan",
            "EMNLP Paper ID": "2650",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "0f20baeae798c1638716585a7b896d5885389e62",
            "title": "Scalable Data Ablation Approximations for Language Models through Modular Training and Merging",
            "abstract": "Training data compositions for Large Language Models (LLMs) can significantly affect their downstream performance. However, a thorough data ablation study exploring large sets of candidate data mixtures is typically prohibitively expensive since the full effect is seen only after training the models; this can lead practitioners to settle for sub-optimal data mixtures. We propose an efficient method for approximating data ablations which trains individual models on subsets of a training corpus and reuses them across evaluations of combinations of subsets. In continued pre-training experiments, we find that, given an arbitrary evaluation set, the perplexity score of a single model trained on a candidate set of data is strongly correlated with perplexity scores of parameter averages of models trained on distinct partitions of that data. From this finding, we posit that researchers and practitioners can conduct inexpensive simulations of data ablations by maintaining a pool of models that were each trained on partitions of a large training corpus, and assessing candidate data mixtures by evaluating parameter averages of combinations of these models. This approach allows for substantial improvements in amortized training efficiency -- scaling only linearly with respect to new data -- by enabling reuse of previous training computation, opening new avenues for improving model performance through rigorous, incremental data assessment and mixing.",
            "link": "https://www.semanticscholar.org/paper/0f20baeae798c1638716585a7b896d5885389e62",
            "authors": "Clara Na, Ian Magnusson, A. Jha, Tom Sherborne, Emma Strubell, Jesse Dodge, Pradeep Dasigi",
            "EMNLP Paper ID": "2843",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "e2128b8dd70cbeeb39e1b899cf8304d8b3967189",
            "title": "Exploring Intrinsic Language-specific Subspaces in Fine-tuning Multilingual Neural Machine Translation",
            "abstract": "Multilingual neural machine translation models support fine-tuning hundreds of languages simultaneously. However, fine-tuning on full parameters solely is inefficient potentially leading to negative interactions among languages. In this work, we demonstrate that the fine-tuning for a language occurs in its intrinsic language-specific subspace with a tiny fraction of entire parameters. Thus, we propose language-specific LoRA to isolate intrinsic language-specific subspaces. Furthermore, we propose architecture learning techniques and introduce a gradual pruning schedule during fine-tuning to exhaustively explore the optimal setting and the minimal intrinsic subspaces for each language, resulting in a lightweight yet effective fine-tuning procedure. The experimental results on a 12-language subset and a 30-language subset of FLORES-101 show that our methods not only outperform full-parameter fine-tuning up to 2.25 spBLEU scores but also reduce trainable parameters to $0.4\\%$ for high and medium-resource languages and $1.6\\%$ for low-resource ones.",
            "link": "https://www.semanticscholar.org/paper/e2128b8dd70cbeeb39e1b899cf8304d8b3967189",
            "authors": "Zhe Cao, Zhi Qu, Hidetaka Kamigaito, Taro Watanabe",
            "EMNLP Paper ID": "2847",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "94f2713308576194bad749442657951aa725d5ad",
            "title": "RevMUX: Data Multiplexing with Reversible Adapters for Efficient LLM Batch Inference",
            "abstract": "Large language models (LLMs) have brought a great breakthrough to the natural language processing (NLP) community, while leading the challenge of handling concurrent customer queries due to their high throughput demands. Data multiplexing addresses this by merging multiple inputs into a single composite input, allowing more efficient inference through a shared forward pass. However, as distinguishing individuals from a composite input is challenging, conventional methods typically require training the entire backbone, yet still suffer from performance degradation. In this paper, we introduce RevMUX, a parameter-efficient data multiplexing framework that incorporates a reversible design in the multiplexer, which can be reused by the demultiplexer to perform reverse operations and restore individual samples for classification. Extensive experiments on four datasets and three types of LLM backbones demonstrate the effectiveness of RevMUX for enhancing LLM inference efficiency while retaining a satisfactory classification performance.",
            "link": "https://www.semanticscholar.org/paper/94f2713308576194bad749442657951aa725d5ad",
            "authors": "Yige Xu, Xu Guo, Zhiwei Zeng, Chunyan Miao",
            "EMNLP Paper ID": "3105",
            "Oral/Poster": "Oral",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "89a6e57f59fdfae4e8ae16e0753cc29b6b996001",
            "title": "Initialization of Large Language Models via Reparameterization to Mitigate Loss Spikes",
            "abstract": "Loss spikes, a phenomenon in which the loss value diverges suddenly, is a fundamental issue in the pre-training of large language models. This paper supposes that the non-uniformity of the norm of the parameters is one of the causes of loss spikes. Here, in training of neural networks, the scale of the gradients is required to be kept constant throughout the layers to avoid the vanishing and exploding gradients problem. However, to meet these requirements in the Transformer model, the norm of the model parameters must be non-uniform, and thus, parameters whose norm is smaller are more sensitive to the parameter update. To address this issue, we propose a novel technique, weight scaling as reparameterization (WeSaR). WeSaR introduces a gate parameter per parameter matrix and adjusts it to the value satisfying the requirements. Because of the gate parameter, WeSaR sets the norm of the original parameters uniformly, which results in stable training. Experimental results with the Transformer decoders consisting of 130 million, 1.3 billion, and 13 billion parameters showed that WeSaR stabilizes and accelerates training and that it outperformed compared methods including popular initialization methods.",
            "link": "https://www.semanticscholar.org/paper/89a6e57f59fdfae4e8ae16e0753cc29b6b996001",
            "authors": "Kosuke Nishida, Kyosuke Nishida, Kuniko Saito",
            "EMNLP Paper ID": "3277",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "7d00e7337fbbccd0208361b4c204c75239a96521",
            "title": "LaCo: Large Language Model Pruning via Layer Collapse",
            "abstract": "Large language models (LLMs) based on transformer are witnessing a notable trend of size expansion, which brings considerable costs to both model training and inference. However, existing methods such as model quantization, knowledge distillation, and model pruning are constrained by various issues, including hardware support limitations, the need for extensive training, and alterations to the model internal structure. In this paper, we propose a concise layer-wise structured pruner called \\textit{Layer Collapse (LaCo)}, in which rear model layers collapse into a prior layer, enabling a rapid reduction in model size while preserving the model structure. Comprehensive experiments show that our method maintains an average task performance of over 80\\% at pruning ratios of 25-30\\%, significantly outperforming existing state-of-the-art structured pruning methods. We also conduct post-training experiments to confirm that the \\textit{LaCo} effectively inherits the parameters of the original model. Additionally, we perform ablation studies on various settings of \\textit{LaCo}. Finally, we discuss our motivation from the perspective of layer-wise similarity and evaluate the performance of the pruned LLMs across various pruning ratios\\footnote{\\url{https://github.com/yangyifei729/LaCo}}.",
            "link": "https://www.semanticscholar.org/paper/7d00e7337fbbccd0208361b4c204c75239a96521",
            "authors": "Yifei Yang, Zouying Cao, Hai Zhao",
            "matchScore": 254.99615,
            "original title": "LaCo: Large Language Model Pruning via Layer Collapse",
            "original authors": "Yifei Yang, zouying cao, hai zhao",
            "EMNLP Paper ID": "1309",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "3d8701dbf0414de8085e5f02a8d8f2026efd400a",
            "title": "MUSCLE: A Model Update Strategy for Compatible LLM Evolution",
            "abstract": "Large Language Models (LLMs) are regularly updated to enhance performance, typically through changes in data or architecture. Within the update process, developers often prioritize improving overall performance metrics, paying less attention to maintaining compatibility with earlier model versions. Instance-level degradation (instance regression) of performance from one model version to the next can interfere with a user's mental model of the capabilities of a particular language model. Users having to adapt their mental model with every update can lead to dissatisfaction, especially when the new model has degraded compared to a prior version for a known use case (model update regression). We find that when pretrained LLM base models are updated, fine-tuned user-facing downstream task adapters experience negative flips -- previously correct instances are now predicted incorrectly. We observe model update regression between different model versions on a diverse set of tasks and models, even when the downstream task training procedures remain identical. We argue for the importance of maintaining model update compatibility during updates, and present evaluation metrics designed specifically for generative tasks, while also being applicable to discriminative tasks. We propose a training strategy to minimize the extent of instance regression in model updates, involving training of a compatibility adapter that can enhance task fine-tuned language models. We show negative flips reduce by up to 40% e.g. when updating Llama 1 to Llama 2 with our proposed method.",
            "link": "https://www.semanticscholar.org/paper/3d8701dbf0414de8085e5f02a8d8f2026efd400a",
            "authors": "Jessica Echterhoff, Fartash Faghri, Raviteja Vemulapalli, Ting-Yao Hu, Chun-Liang Li, Oncel Tuzel, Hadi Pouransari",
            "matchScore": 222.04037,
            "original title": "MUSCLE: A Model Update Strategy for Compatible LLM Evolution",
            "original authors": "Jessica Maria Echterhoff, Fartash Faghri, Raviteja Vemulapalli, Ting-Yao Hu, Chun-Liang Li, Oncel Tuzel, Hadi Pouransari",
            "EMNLP Paper ID": "1498",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "52d921d7167fe3a4de6468bffabb01529c3fcc43",
            "title": "Pruning Foundation Models for High Accuracy without Retraining",
            "abstract": "Despite the superior performance, it is challenging to deploy foundation models or large language models (LLMs) due to their massive parameters and computations. While pruning is a promising technique to reduce model size and accelerate the inference, the traditional pruning techniques can hardly be applied for LLMs as they need to finetune the model on the full dataset with multiple epochs consuming massive data and hardware resources. To deal with this problem, post-training pruning methods are proposed to prune LLMs in one-shot without retraining. However, their accuracy after pruning may suffer from certain performance degradation due to the lack of retraining with massive data. To address this issue, in this paper, we first formulate the post-training problem for layer-wise LLM compression to simultaneously prune multiple weights in LLMs. Next, we provide an optimal solution for this problem and design our post-training pruning algorithm for both unstructured and semi-structured sparsity. Our extensive experiments demonstrate the superior performance of the proposed methods in comparison to SOTA baselines across various LLM families including transformer-based LLMs and Mamba-based LLMs. Code link: https://github.com/piuzha/APT",
            "link": "https://www.semanticscholar.org/paper/52d921d7167fe3a4de6468bffabb01529c3fcc43",
            "authors": "Pu Zhao, Fei Sun, Xuan Shen, Pinrui Yu, Zhenglun Kong, Yanzhi Wang, Xue Lin",
            "matchScore": 227.987,
            "original title": "Pruning Foundation Models for High Accuracy without Retraining",
            "original authors": "Pu Zhao, Fei Sun, Xuan Shen, Pinrui Yu, Zhenglun Kong, Xue Lin, Yanzhi Wang",
            "EMNLP Paper ID": "2003",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "34f043fd41d68b3482cb3f55f2d440248540deca",
            "title": "Pruning Multilingual Large Language Models for Multilingual Inference",
            "abstract": "Multilingual large language models (MLLMs), trained on multilingual balanced data, demonstrate better zero-shot learning performance in non-English languages compared to large language models trained on English-dominant data. However, the disparity in performance between English and non-English languages remains a challenge yet to be fully addressed. A distinctive characteristic of MLLMs is their high-quality translation capabilities, indicating an acquired proficiency in aligning between languages. This study explores how to enhance the zero-shot performance of MLLMs in non-English languages by leveraging their alignment capability between English and non-English languages. To achieve this, we first analyze the behavior of MLLMs when performing translation and reveal that there are large magnitude features that play a critical role in the translation process. Inspired by these findings, we retain the weights associated with operations involving the large magnitude features and prune other weights to force MLLMs to rely on these features for tasks beyond translation. We empirically demonstrate that this pruning strategy can enhance the MLLMs' performance in non-English language.",
            "link": "https://www.semanticscholar.org/paper/34f043fd41d68b3482cb3f55f2d440248540deca",
            "authors": "Hwichan Kim, Jun Suzuki, Tosho Hirasawa, Mamoru Komachi",
            "matchScore": 239.24683,
            "original title": "Pruning Multilingual Large Language Models for Multilingual Inference",
            "original authors": "Hwichan Kim, Jun Suzuki, Tosho Hirasawa, Mamoru Komachi",
            "EMNLP Paper ID": "2044",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "5064ba52891df8ebb9b0f242dba52a7909e53878",
            "title": "In2Core: Leveraging Influence Functions for Coreset Selection in Instruction Finetuning of Large Language Models",
            "abstract": "Despite advancements, fine-tuning Large Language Models (LLMs) remains costly due to the extensive parameter count and substantial data requirements for model generalization. Accessibility to computing resources remains a barrier for the open-source community. To address this challenge, we propose the In2Core algorithm, which selects a coreset by analyzing the correlation between training and evaluation samples with a trained model. Notably, we assess the model's internal gradients to estimate this relationship, aiming to rank the contribution of each training point. To enhance efficiency, we propose an optimization to compute influence functions with a reduced number of layers while achieving similar accuracy. By applying our algorithm to instruction fine-tuning data of LLMs, we can achieve similar performance with just 50% of the training data. Meantime, using influence functions to analyze model coverage to certain testing samples could provide a reliable and interpretable signal on the training set's coverage of those test points.",
            "link": "https://www.semanticscholar.org/paper/5064ba52891df8ebb9b0f242dba52a7909e53878",
            "authors": "Ayrton San Joaquin, Bin Wang, Zhengyuan Liu, Nicholas Asher, Brian Lim, Philippe Muller, Nancy F. Chen",
            "matchScore": 307.57574,
            "original title": "In2Core: Leveraging Influence Functions for Coreset Selection in Instruction Finetuning of Large Language Models",
            "original authors": "Ayrton San Joaquin, Bin Wang, Zhengyuan Liu, Philippe Muller, Nicholas Asher, Brian Lim, Nancy F. Chen",
            "EMNLP Paper ID": "2105",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "548881f1e934ea87c47f2b0facf4665ac306a7c0",
            "title": "Model Merging and Safety Alignment: One Bad Model Spoils the Bunch",
            "abstract": "Merging Large Language Models (LLMs) is a cost-effective technique for combining multiple expert LLMs into a single versatile model, retaining the expertise of the original ones. However, current approaches often overlook the importance of safety alignment during merging, leading to highly misaligned models. This work investigates the effects of model merging on alignment. We evaluate several popular model merging techniques, demonstrating that existing methods do not only transfer domain expertise but also propagate misalignment. We propose a simple two-step approach to address this problem: (i) generating synthetic safety and domain-specific data, and (ii) incorporating these generated data into the optimization process of existing data-aware model merging techniques. This allows us to treat alignment as a skill that can be maximized in the resulting merged LLM. Our experiments illustrate the effectiveness of integrating alignment-related data during merging, resulting in models that excel in both domain expertise and alignment.",
            "link": "https://www.semanticscholar.org/paper/548881f1e934ea87c47f2b0facf4665ac306a7c0",
            "authors": "Hasan Hammoud, Umberto Michieli, Fabio Pizzati, Philip H. S. Torr, Adel Bibi, Bernard Ghanem, Mete Ozay",
            "matchScore": 321.48224,
            "original title": "Model Merging and Safety Alignment: One Bad Model Spoils the Bunch",
            "original authors": "Hasan Abed Al Kader Hammoud, Umberto Michieli, Fabio Pizzati, Philip Torr, Adel Bibi, Bernard Ghanem, Mete Ozay",
            "EMNLP Paper ID": "2551",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "911045e6713f865ebcef6b95563a92e32b76bcb6",
            "title": "Merge to Learn: Efficiently Adding Skills to Language Models with Model Merging",
            "abstract": "Adapting general-purpose language models to new skills is currently an expensive process that must be repeated as new instruction datasets targeting new skills are created, or can cause the models to forget older skills. In this work, we investigate the effectiveness of adding new skills to preexisting models by training on the new skills in isolation and later merging with the general model (e.g. using task vectors). In experiments focusing on scientific literature understanding, safety, and coding, we find that the parallel-train-then-merge procedure, which is significantly cheaper than retraining the models on updated data mixtures, is often comparably effective. Our experiments also show that parallel training is especially well-suited for enabling safety features in LMs relative to continued finetuning and retraining, as it dramatically improves model compliance with safe prompts while preserving its ability to refuse dangerous or harmful prompts.",
            "link": "https://www.semanticscholar.org/paper/911045e6713f865ebcef6b95563a92e32b76bcb6",
            "authors": "Jacob Daniel Morrison, Noah A. Smith, Hanna Hajishirzi, Pang Wei Koh, Jesse Dodge, Pradeep Dasigi",
            "matchScore": 252.6593,
            "original title": "Merge to Learn: Efficiently Adding Skills to Language Models with Model Merging",
            "original authors": "Jacob Morrison, Noah A. Smith, Hannaneh Hajishirzi, Pang Wei Koh, Jesse Dodge, Pradeep Dasigi",
            "EMNLP Paper ID": "3001",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "2018a64911680af735172e3bab2719a80927279f",
            "title": "Mitigating Catastrophic Forgetting in Language Transfer via Model Merging",
            "abstract": "As open-weight large language models (LLMs) achieve ever more impressive performances across a wide range of tasks in English, practitioners aim to adapt these models to different languages. However, such language adaptation is often accompanied by catastrophic forgetting of the base model's capabilities, severely limiting the usefulness of the resulting model. We address this issue by proposing Branch-and-Merge (BaM), a new adaptation method based on iteratively merging multiple models, fine-tuned on a subset of the available training data. BaM is based on the insight that this yields lower magnitude but higher quality weight changes, reducing forgetting of the source domain while maintaining learning on the target domain. We demonstrate in an extensive empirical study on Bulgarian and German that BaM can significantly reduce forgetting while matching or even improving target domain performance compared to both standard continued pretraining and instruction finetuning across different model architectures.",
            "link": "https://www.semanticscholar.org/paper/2018a64911680af735172e3bab2719a80927279f",
            "authors": "Anton Alexandrov, Veselin Raychev, Mark Niklas M\u00fcller, Ce Zhang, Martin T. Vechev, Kristina Toutanova",
            "matchScore": 245.18019,
            "original title": "Mitigating Catastrophic Forgetting in Language Transfer via Model Merging",
            "original authors": "Anton Alexandrov, Veselin Raychev, Mark Niklas Mueller, Ce Zhang, Martin Vechev, Kristina Toutanova",
            "EMNLP Paper ID": "3301",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "deb50983a161533b5e097c019380aac7b87dbce7",
            "title": "Tending Towards Stability: Convergence Challenges in Small Language Models",
            "abstract": "Increasing the number of parameters in language models is a common strategy to enhance their performance. However, smaller language models remain valuable due to their lower operational costs. Despite their advantages, smaller models frequently underperform compared to their larger counterparts, even when provided with equivalent data and computational resources. Specifically, their performance tends to degrade in the late pretraining phase. This is anecdotally attributed to their reduced representational capacity. Yet, the exact causes of this performance degradation remain unclear. We use the Pythia model suite to analyse the training dynamics that underlie this phenomenon. Across different model sizes, we investigate the convergence of the Attention and MLP activations to their final state and examine how the effective rank of their parameters influences this process. We find that nearly all layers in larger models stabilise early in training - within the first 20% - whereas layers in smaller models exhibit slower and less stable convergence, especially when their parameters have lower effective rank. By linking the convergence of layers' activations to their parameters' effective rank, our analyses can guide future work to address inefficiencies in the learning dynamics of small models.",
            "link": "https://www.semanticscholar.org/paper/deb50983a161533b5e097c019380aac7b87dbce7",
            "authors": "Richard Diehl Martinez, Pietro Lesci, P. Buttery",
            "matchScore": 235.75885,
            "original title": "Tending Towards Stability: Convergence Challenges in Small Language Models",
            "original authors": "Richard Diehl Martinez, Pietro Lesci, Paula Buttery",
            "EMNLP Paper ID": "669",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "ca600e28021bcd4a679e78711bc0a7ebba74b6c5",
            "title": "Scaling Behavior for Large Language Models regarding Numeral Systems: An Example using Pythia",
            "abstract": "Though Large Language Models (LLMs) have shown remarkable abilities in mathematics reasoning, they are still struggling with performing numeric operations accurately, such as addition and multiplication. Numbers can be tokenized into tokens in various ways by different LLMs and affect the numeric operations performance. Currently, there are two representatives: 1) Tokenize into $1$-digit, and 2) Tokenize into $1\\sim 3$ digit. The difference is roughly equivalent to using different numeral systems (namely base $10$ or base $10^{3}$). In light of this, we study the scaling behavior of different numeral systems in the context of transformer-based large language models. We empirically show that a base $10$ system is consistently more data-efficient than a base $10^{2}$ or $10^{3}$ system across training data scale, model sizes under from-scratch training settings, while different number systems have very similar fine-tuning performances. We attribute this to higher token frequencies of a base $10$ system. Additionally, we reveal extrapolation behavior patterns on addition and multiplication. We identify that base $100$ and base $1000$ systems struggle on token-level discernment and token-level operations. We also sheds light on the mechanism learnt by the models.",
            "link": "https://www.semanticscholar.org/paper/ca600e28021bcd4a679e78711bc0a7ebba74b6c5",
            "authors": "Zhejian Zhou, Jiayu Wang, Dahua Lin, Kai Chen",
            "matchScore": 262.3985,
            "original title": "Scaling Behavior for Large Language Models regarding Numeral Systems: An Example using Pythia",
            "original authors": "Zhejian Zhou, JIayu Wang, Dahua Lin, Kai Chen",
            "EMNLP Paper ID": "763",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        }
    ],
    "Machine Translation with Large Language Models": [
        {
            "paperId": "97992c13baa6185c03d9e672f53185bc59822596",
            "title": "Chain-of-Dictionary Prompting Elicits Translation in Large Language Models",
            "abstract": "Large language models (LLMs) have shown surprisingly good performance in multilingual neural machine translation (MNMT) even when trained without parallel data. Yet, despite the fact that the amount of training data is gigantic, they still struggle with translating rare words, particularly for low-resource languages. Even worse, it is usually unrealistic to retrieve relevant demonstrations for in-context learning with low-resource languages on LLMs, which restricts the practical use of LLMs for translation -- how should we mitigate this problem? To this end, we present a novel method, CoD, which augments LLMs with prior knowledge with the chains of multilingual dictionaries for a subset of input words to elicit translation abilities for LLMs. Extensive experiments indicate that augmenting ChatGPT with CoD elicits large gains by up to 13x chrF++ points for MNMT (3.08 to 42.63 for English to Serbian written in Cyrillic script) on FLORES-200 full devtest set. We further demonstrate the importance of chaining the multilingual dictionaries, as well as the superiority of CoD to few-shot demonstration for low-resource languages.",
            "link": "https://www.semanticscholar.org/paper/97992c13baa6185c03d9e672f53185bc59822596",
            "authors": "Hongyuan Lu, Haoyang Huang, Dongdong Zhang, Haoran Yang, Wai Lam, Furu Wei",
            "EMNLP Paper ID": "125",
            "Oral/Poster": "Oral",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "30c8f876c03e48d4536dc56534c2a35454f3391d",
            "title": "LLMs Are Zero-Shot Context-Aware Simultaneous Translators",
            "abstract": "The advent of transformers has fueled progress in machine translation. More recently large language models (LLMs) have come to the spotlight thanks to their generality and strong performance in a wide range of language tasks, including translation. Here we show that open-source LLMs perform on par with or better than some state-of-the-art baselines in simultaneous machine translation (SiMT) tasks, zero-shot. We also demonstrate that injection of minimal background information, which is easy with an LLM, brings further performance gains, especially on challenging technical subject-matter. This highlights LLMs' potential for building next generation of massively multilingual, context-aware and terminologically accurate SiMT systems that require no resource-intensive training or fine-tuning.",
            "link": "https://www.semanticscholar.org/paper/30c8f876c03e48d4536dc56534c2a35454f3391d",
            "authors": "Roman Koshkin, Katsuhito Sudoh, Satoshi Nakamura",
            "EMNLP Paper ID": "144",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "16c790cccf95f3bdab15f380423a95ccafaff5d4",
            "title": "PsFuture: A Pseudo-Future-based Zero-Shot Adaptive Policy for Simultaneous Machine Translation",
            "abstract": "Simultaneous Machine Translation (SiMT) requires target tokens to be generated in real-time as streaming source tokens are consumed. Traditional approaches to SiMT typically require sophisticated architectures and extensive parameter configurations for training adaptive read/write policies, which in turn demand considerable computational power and memory. We propose PsFuture, the first zero-shot adaptive read/write policy for SiMT, enabling the translation model to independently determine read/write actions without the necessity for additional training. Furthermore, we introduce a novel training strategy, Prefix-to-Full (P2F), specifically tailored to adjust offline translation models for SiMT applications, exploiting the advantages of the bidirectional attention mechanism inherent in offline models. Experiments across multiple benchmarks demonstrate that our zero-shot policy attains performance on par with strong baselines and the P2F method can further enhance performance, achieving an outstanding trade-off between translation quality and latency.",
            "link": "https://www.semanticscholar.org/paper/16c790cccf95f3bdab15f380423a95ccafaff5d4",
            "authors": "Libo Zhao, Jing Li, Ziqian Zeng",
            "EMNLP Paper ID": "210",
            "Oral/Poster": "Oral",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "4a9ac49dc374e1f873ff7d993e3afe50097195fc",
            "title": "What do Large Language Models Need for Machine Translation Evaluation?",
            "abstract": "Leveraging large language models (LLMs) for various natural language processing tasks has led to superlative claims about their performance. For the evaluation of machine translation (MT), existing research shows that LLMs are able to achieve results comparable to fine-tuned multilingual pre-trained language models. In this paper, we explore what translation information, such as the source, reference, translation errors and annotation guidelines, is needed for LLMs to evaluate MT quality. In addition, we investigate prompting techniques such as zero-shot, Chain of Thought (CoT) and few-shot prompting for eight language pairs covering high-, medium- and low-resource languages, leveraging varying LLM variants. Our findings indicate the importance of reference translations for an LLM-based evaluation. While larger models do not necessarily fare better, they tend to benefit more from CoT prompting, than smaller models. We also observe that LLMs do not always provide a numerical score when generating evaluations, which poses a question on their reliability for the task. Our work presents a comprehensive analysis for resource-constrained and training-less LLM-based evaluation of machine translation. We release the accrued prompt templates, code and data publicly for reproducibility.",
            "link": "https://www.semanticscholar.org/paper/4a9ac49dc374e1f873ff7d993e3afe50097195fc",
            "authors": "Shenbin Qian, Archchana Sindhujan, Minnie Kabra, Diptesh Kanojia, Constantin Oruasan, Tharindu Ranasinghe, Fr\u00e9d\u00e9ric Blain",
            "EMNLP Paper ID": "414",
            "Oral/Poster": "Oral",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "4991cbd4fe71769ce0a74b27715488124560b5b5",
            "title": "Multiple Sources are Better Than One: Incorporating External Knowledge in Low-Resource Glossing",
            "abstract": "In this paper, we address the data scarcity problem in automatic data-driven glossing for low-resource languages by coordinating multiple sources of linguistic expertise. We supplement models with translations at both the token and sentence level as well as leverage the extensive linguistic capability of modern LLMs. Our enhancements lead to an average absolute improvement of 5%-points in word-level accuracy over the previous state of the art on a typologically diverse dataset spanning six low-resource languages. The improvements are particularly noticeable for the lowest-resourced language Gitksan, where we achieve a 10%-point improvement. Furthermore, in a simulated ultra-low resource setting for the same six languages, training on fewer than 100 glossed sentences, we establish an average 10%-point improvement in word-level accuracy over the previous state-of-the-art system.",
            "link": "https://www.semanticscholar.org/paper/4991cbd4fe71769ce0a74b27715488124560b5b5",
            "authors": "Changbing Yang, Garrett Nicolai, Miikka Silfverberg",
            "EMNLP Paper ID": "497",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "a23a89855e3af2e6cec7fd4a01e12cacdf6c727f",
            "title": "Aligning Translation-Specific Understanding to General Understanding in Large Language Models",
            "abstract": "Large Language models (LLMs) have exhibited remarkable abilities in understanding complex texts, offering a promising path towards human-like translation performance. However, this study reveals the misalignment between the translation-specific understanding and the general understanding inside LLMs. This understanding misalignment leads to LLMs mistakenly or literally translating some complicated concepts that they accurately comprehend in the general scenarios (e.g., QA). To align the translation-specific understanding to the general one, we propose a novel translation process, DUAT (Difficult words Understanding Aligned Translation), explicitly incorporating the general understanding on the complicated content incurring inconsistent understanding to guide the translation. Specifically, DUAT performs cross-lingual interpretation for the difficult-to-translate words and enhances the translation with the generated interpretations. Furthermore, we reframe the external tools to improve DUAT in detecting difficult words and generating helpful interpretations. We conduct experiments on the self-constructed benchmark Challenge-WMT, consisting of samples that are prone to mistranslation. Human evaluation results on high-resource and low-resource language pairs indicate that DUAT significantly facilitates the understanding alignment, which improves the translation quality (up to +3.85 COMET) and reduces the literality of the translation by -25% to -51%.",
            "link": "https://www.semanticscholar.org/paper/a23a89855e3af2e6cec7fd4a01e12cacdf6c727f",
            "authors": "Yi-Chong Huang, Xiaocheng Feng, Baohang Li, Chengpeng Fu, Wenshuai Huo, Ting Liu, Bing Qin",
            "EMNLP Paper ID": "551",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "4a02d2fef5b94a4da8346f1fa88ae5f9294f886b",
            "title": "An image speaks a thousand words, but can everyone listen? On image transcreation for cultural relevance",
            "abstract": "Given the rise of multimedia content, human translators increasingly focus on culturally adapting not only words but also other modalities such as images to convey the same meaning. While several applications stand to benefit from this, machine translation systems remain confined to dealing with language in speech and text. In this work, we take a first step towards translating images to make them culturally relevant. First, we build three pipelines comprising state-of-the-art generative models to do the task. Next, we build a two-part evaluation dataset: i) concept: comprising 600 images that are cross-culturally coherent, focusing on a single concept per image, and ii) application: comprising 100 images curated from real-world applications. We conduct a multi-faceted human evaluation of translated images to assess for cultural relevance and meaning preservation. We find that as of today, image-editing models fail at this task, but can be improved by leveraging LLMs and retrievers in the loop. Best pipelines can only translate 5% of images for some countries in the easier concept dataset and no translation is successful for some countries in the application dataset, highlighting the challenging nature of the task. Our code and data is released here: https://github.com/simran-khanuja/image-transcreation.",
            "link": "https://www.semanticscholar.org/paper/4a02d2fef5b94a4da8346f1fa88ae5f9294f886b",
            "authors": "Simran Khanuja, Sathyanarayanan Ramamoorthy, Yueqi Song, Graham Neubig",
            "EMNLP Paper ID": "1150",
            "Oral/Poster": "Oral",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "2ba117de088a6e85f3855dc3acad982cd0e9ed83",
            "title": "MMTE: Corpus and Metrics for Evaluating Machine Translation Quality of Metaphorical Language",
            "abstract": "Machine Translation (MT) has developed rapidly since the release of Large Language Models and current MT evaluation is performed through comparison with reference human translations or by predicting quality scores from human-labeled data. However, these mainstream evaluation methods mainly focus on fluency and factual reliability, whilst paying little attention to figurative quality. In this paper, we investigate the figurative quality of MT and propose a set of human evaluation metrics focused on the translation of figurative language. We additionally present a multilingual parallel metaphor corpus generated by post-editing. Our evaluation protocol is designed to estimate four aspects of MT: Metaphorical Equivalence, Emotion, Authenticity, and Quality. In doing so, we observe that translations of figurative expressions display different traits from literal ones.",
            "link": "https://www.semanticscholar.org/paper/2ba117de088a6e85f3855dc3acad982cd0e9ed83",
            "authors": "Shunyu Wang, Ge Zhang, Han Wu, Tyler Loakman, Wenhao Huang, Chenghua Lin",
            "EMNLP Paper ID": "1322",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "de1c034f8ee77c9f23f4e0d26d7e2520b67f6017",
            "title": "Can Automatic Metrics Assess High-Quality Translations?",
            "abstract": "Automatic metrics for evaluating translation quality are typically validated by measuring how well they correlate with human assessments. However, correlation methods tend to capture only the ability of metrics to differentiate between good and bad source-translation pairs, overlooking their reliability in distinguishing alternative translations for the same source. In this paper, we confirm that this is indeed the case by showing that current metrics are insensitive to nuanced differences in translation quality. This effect is most pronounced when the quality is high and the variance among alternatives is low. Given this finding, we shift towards detecting high-quality correct translations, an important problem in practical decision-making scenarios where a binary check of correctness is prioritized over a nuanced evaluation of quality. Using the MQM framework as the gold standard, we systematically stress-test the ability of current metrics to identify translations with no errors as marked by humans. Our findings reveal that current metrics often over or underestimate translation quality, indicating significant room for improvement in automatic evaluation methods.",
            "link": "https://www.semanticscholar.org/paper/de1c034f8ee77c9f23f4e0d26d7e2520b67f6017",
            "authors": "Sweta Agrawal, Ant\u00f3nio Farinhas, Ricardo Rei, Andr'e F. T. Martins",
            "EMNLP Paper ID": "1670",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "20c3bb12929f323b5d9b2a8b7d8b3e6bdfa0fa1d",
            "title": "Modeling User Preferences with Automatic Metrics: Creating a High-Quality Preference Dataset for Machine Translation",
            "abstract": "Alignment with human preferences is an important step in developing accurate and safe large language models. This is no exception in machine translation (MT), where better handling of language nuances and context-specific variations leads to improved quality. However, preference data based on human feedback can be very expensive to obtain and curate at a large scale. Automatic metrics, on the other hand, can induce preferences, but they might not match human expectations perfectly. In this paper, we propose an approach that leverages the best of both worlds. We first collect sentence-level quality assessments from professional linguists on translations generated by multiple high-quality MT systems and evaluate the ability of current automatic metrics to recover these preferences. We then use this analysis to curate a new dataset, MT-Pref (metric induced translation preference) dataset, which comprises 18k instances covering 18 language directions, using texts sourced from multiple domains post-2022. We show that aligning TOWER models on MT-Pref significantly improves translation quality on WMT23 and FLORES benchmarks.",
            "link": "https://www.semanticscholar.org/paper/20c3bb12929f323b5d9b2a8b7d8b3e6bdfa0fa1d",
            "authors": "Sweta Agrawal, Jos'e G. C. de Souza, Ricardo Rei, Ant\u00f3nio Farinhas, Gonccalo R. A. Faria, Patrick Fernandes, Nuno M. Guerreiro, Andr'e F. T. Martins",
            "EMNLP Paper ID": "1673",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "8450f3b0d77c4657a6827530fb094054b7060e75",
            "title": "Ladder: A Model-Agnostic Framework Boosting LLM-based Machine Translation to the Next Level",
            "abstract": "General-purpose Large Language Models (LLMs) like GPT-4 have achieved remarkable advancements in machine translation (MT) by leveraging extensive web content. On the other hand, translation-specific LLMs are built by pre-training on domain-specific monolingual corpora and fine-tuning with human-annotated translation data. Despite the superior performance, these methods either demand an unprecedented scale of computing and data or substantial human editing and annotation efforts. In this paper, we develop MT-Ladder, a novel model-agnostic and cost-effective tool to refine the performance of general LLMs for MT. MT-Ladder is trained on pseudo-refinement triplets which can be easily obtained from existing LLMs without additional human cost. During training, we propose a hierarchical fine-tuning strategy with an easy-to-hard schema, improving MT-Ladder's refining performance progressively. The trained MT-Ladder can be seamlessly integrated with any general-purpose LLMs to boost their translation performance. By utilizing Gemma-2B/7B as the backbone, MT-Ladder-2B can elevate raw translations to the level of top-tier open-source models (e.g., refining BigTranslate-13B with +6.91 BLEU and +3.52 COMET for XX-En), and MT-Ladder-7B can further enhance model performance to be on par with the state-of-the-art GPT-4. Extensive ablation and analysis corroborate the effectiveness of MT-Ladder in diverse settings. Our code is available at https://github.com/fzp0424/Ladder",
            "link": "https://www.semanticscholar.org/paper/8450f3b0d77c4657a6827530fb094054b7060e75",
            "authors": "Zhaopeng Feng, Ruizhe Chen, Yan Zhang, Zijie Meng, Zuozhu Liu",
            "EMNLP Paper ID": "1790",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "8772b7deb7f4c250d6a49a10f77f1d976440ee9b",
            "title": "Towards Cross-Cultural Machine Translation with Retrieval-Augmented Generation from Multilingual Knowledge Graphs",
            "abstract": "Translating text that contains entity names is a challenging task, as cultural-related references can vary significantly across languages. These variations may also be caused by transcreation, an adaptation process that entails more than transliteration and word-for-word translation. In this paper, we address the problem of cross-cultural translation on two fronts: (i) we introduce XC-Translate, the first large-scale, manually-created benchmark for machine translation that focuses on text that contains potentially culturally-nuanced entity names, and (ii) we propose KG-MT, a novel end-to-end method to integrate information from a multilingual knowledge graph into a neural machine translation model by leveraging a dense retrieval mechanism. Our experiments and analyses show that current machine translation systems and large language models still struggle to translate texts containing entity names, whereas KG-MT outperforms state-of-the-art approaches by a large margin, obtaining a 129% and 62% relative improvement compared to NLLB-200 and GPT-4, respectively.",
            "link": "https://www.semanticscholar.org/paper/8772b7deb7f4c250d6a49a10f77f1d976440ee9b",
            "authors": "Simone Conia, Daniel Lee, Min Li, U. F. Minhas, Saloni Potdar, Yunyao Li",
            "EMNLP Paper ID": "1926",
            "Oral/Poster": "Oral",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "af4328bc1eb2d80e4fc035b143cecba5580d39ef",
            "title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation",
            "abstract": "Large language models (LLMs) have achieved state-of-the-art performance in various language processing tasks, motivating their adoption in simultaneous translation. Current fine-tuning methods to adapt LLMs for simultaneous translation focus on prompting optimization strategies using either data augmentation or prompt structure modifications. However, these methods suffer from several issues, such as unnecessarily expanded training sets, computational inefficiency from dumping the key and value cache, increased prompt sizes, or restriction to a single decision policy. To eliminate these issues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs for simultaneous translation. It utilizes a novel attention mask approach that models simultaneous translation during fine-tuning by masking attention for a desired decision policy. Applying the proposed SimulMask on a Falcon LLM for the IWSLT 2017 dataset, we have observed a significant translation quality improvement compared to state-of-the-art prompting optimization strategies on five language pairs while reducing the computational cost.",
            "link": "https://www.semanticscholar.org/paper/af4328bc1eb2d80e4fc035b143cecba5580d39ef",
            "authors": "Matthew Raffel, Victor Agostinelli, Lizhong Chen",
            "EMNLP Paper ID": "2267",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "525988a0bba936ac21a3210a5e91948741116bd9",
            "title": "Back to School: Translation Using Grammar Books",
            "abstract": "Machine translation systems for high resource languages perform exceptionally well and produce high quality translations. Unfortunately, the vast majority of languages are not considered high resource and lack the quantity of parallel sentences needed to train such systems. These under-represented languages are not without resources, however, and bilingual dictionaries and grammar books are available as linguistic reference material. With current large language models (LLMs) supporting near book-length contexts, we can begin to use the available material to ensure advancements are shared among all of the world's languages. In this paper, we demonstrate incorporating grammar books in the prompt of GPT-4 to improve machine translation and evaluate the performance on 16 topologically diverse low-resource languages, using a combination of reference material to show that the machine translation performance of LLMs can be improved using this method.",
            "link": "https://www.semanticscholar.org/paper/525988a0bba936ac21a3210a5e91948741116bd9",
            "authors": "Jonathan Hus, Antonios Anastasopoulos",
            "EMNLP Paper ID": "2638",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "f0520b991c1a16449a95ff98771d3cf86ed71428",
            "title": "GottBERT: a pure German Language Model",
            "abstract": "Lately, pre-trained language models advanced the field of natural language processing (NLP). The introduction of Bidirectional Encoders for Transformers (BERT) and its optimized version RoBERTa have had significant impact and increased the relevance of pre-trained models. First, research in this field mainly started on English data followed by models trained with multilingual text corpora. However, current research shows that multilingual models are inferior to monolingual models. Currently, no German single language RoBERTa model is yet published, which we introduce in this work (GottBERT). The German portion of the OSCAR data set was used as text corpus. In an evaluation we compare its performance on the two Named Entity Recognition (NER) tasks Conll 2003 and GermEval 2014 as well as on the text classification tasks GermEval 2018 (fine and coarse) and GNAD with existing German single language BERT models and two multilingual ones. GottBERT was pre-trained related to the original RoBERTa model using fairseq. All downstream tasks were trained using hyperparameter presets taken from the benchmark of German BERT. The experiments were setup utilizing FARM. Performance was measured by the $F_{1}$ score. GottBERT was successfully pre-trained on a 256 core TPU pod using the RoBERTa BASE architecture. Even without extensive hyper-parameter optimization, in all NER and one text classification task, GottBERT already outperformed all other tested German and multilingual models. In order to support the German NLP field, we publish GottBERT under the AGPLv3 license.",
            "link": "https://www.semanticscholar.org/paper/f0520b991c1a16449a95ff98771d3cf86ed71428",
            "authors": "Raphael Scheible, Fabian Thomczyk, P. Tippmann, V. Jaravine, M. Boeker",
            "EMNLP Paper ID": "2870",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "a1917a9f429a65522e872a523fb9e0d2e8cf08d1",
            "title": "xCOMET-lite: Bridging the Gap Between Efficiency and Quality in Learned MT Evaluation Metrics",
            "abstract": "State-of-the-art trainable machine translation evaluation metrics like xCOMET achieve high correlation with human judgment but rely on large encoders (up to 10.7B parameters), making them computationally expensive and inaccessible to researchers with limited resources. To address this issue, we investigate whether the knowledge stored in these large encoders can be compressed while maintaining quality. We employ distillation, quantization, and pruning techniques to create efficient xCOMET alternatives and introduce a novel data collection pipeline for efficient black-box distillation. Our experiments show that, using quantization, xCOMET can be compressed up to three times with no quality degradation. Additionally, through distillation, we create an xCOMET-lite metric, which has only 2.6% of xCOMET-XXL parameters, but retains 92.1% of its quality. Besides, it surpasses strong small-scale metrics like COMET-22 and BLEURT-20 on the WMT22 metrics challenge dataset by 6.4%, despite using 50% fewer parameters. All code, dataset, and models are available online.",
            "link": "https://www.semanticscholar.org/paper/a1917a9f429a65522e872a523fb9e0d2e8cf08d1",
            "authors": "Daniil Larionov, Mikhail Seleznyov, Vasiliy Viskov, Alexander Panchenko, Steffen Eger",
            "EMNLP Paper ID": "3051",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 14): 13:00\u201314:00 (Afternoon)",
            "Type": "Main"
        },
        {
            "paperId": "1be2056a2fb91c84e5429bce41636a78e76e7c99",
            "title": "Simultaneous Interpretation Corpus Construction by Large Language Models in Distant Language Pair",
            "abstract": "In Simultaneous Machine Translation (SiMT) systems, training with a simultaneous interpretation (SI) corpus is an effective method for achieving high-quality yet low-latency systems. However, it is very challenging to curate such a corpus due to limitations in the abilities of annotators, and hence, existing SI corpora are limited. Therefore, we propose a method to convert existing speech translation corpora into interpretation-style data, maintaining the original word order and preserving the entire source content using Large Language Models (LLM-SI-Corpus). We demonstrate that fine-tuning SiMT models in text-to-text and speech-to-text settings with the LLM-SI-Corpus reduces latencies while maintaining the same level of quality as the models trained with offline datasets. The LLM-SI-Corpus is available at \\url{https://github.com/yusuke1997/LLM-SI-Corpus}.",
            "link": "https://www.semanticscholar.org/paper/1be2056a2fb91c84e5429bce41636a78e76e7c99",
            "authors": "Yusuke Sakai, Mana Makinae, Hidetaka Kamigaito, Taro Watanabe",
            "EMNLP Paper ID": "3183",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "4cedcf7fe5ca624c744a74204ed32d2541ba72ba",
            "title": "Unsupervised Multimodal Machine Translation for Low-resource Distant Language Pairs",
            "abstract": "Unsupervised machine translation (UMT) has recently attracted more attention from researchers, enabling models to translate when languages lack parallel corpora. However, the current works mainly consider close language pairs (e.g., English-German and English-French), and the effectiveness of visual content for distant language pairs has yet to be investigated. This article proposes an unsupervised multimodal machine translation model for low-resource distant language pairs. Specifically, we first employ adequate measures such as transliteration and re-ordering to bring distant language pairs closer together. We then use visual content to extend masked language modeling and generate visual masked language modeling for UMT. Finally, empirical experiments are conducted on our distant language pair dataset and the public Multi30k dataset. Experimental results demonstrate the superior performance of our model, with BLEU score improvements of 2.5 and 2.6 on translation for distant language pairs English-Uyghur and Chinese-Uyghur. Moreover, our model also brings remarkable results for close language pairs, improving 2.3 BLEU compared with the existing models in English-German.",
            "link": "https://www.semanticscholar.org/paper/4cedcf7fe5ca624c744a74204ed32d2541ba72ba",
            "authors": "Turghun Tayir, Lin Li",
            "matchScore": 270.35046,
            "original title": "Visual Pivoting Unsupervised Multimodal Machine Translation in Low-Resource Distant Language Pairs",
            "original authors": "Turghun Tayir, Lin Li, Xiaohui Tao, Mieradilijiang Maimaiti, Ming Li, Jianquan Liu",
            "EMNLP Paper ID": "1124",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "9d23568fce5806937592a16f0dc598f5872f89e8",
            "title": "Large Language Model for Multi-Domain Translation: Benchmarking and Domain CoT Fine-tuning",
            "abstract": "Achieving consistent high-quality machine translation (MT) across diverse domains remains a significant challenge, primarily due to the limited and imbalanced parallel training data available in various domains. While large language models (LLMs) have demonstrated impressive general understanding and generation abilities, their potential in multi-domain MT is under-explored. We establish a comprehensive benchmark for multi-domain translation, featuring 25 German$\\Leftrightarrow$English and 22 Chinese$\\Leftrightarrow$English test sets respectively covering 15 domains. Our evaluation of prominent LLMs reveals a discernible performance gap against traditional MT systems, highlighting domain overfitting and catastrophic forgetting issues after fine-tuning on domain-limited corpora. To mitigate this, we propose a domain Chain of Thought (CoT) fine-tuning technique that utilizes the intrinsic multi-domain intelligence of LLMs to improve translation performance. This method inspires the LLM to perceive domain information from the source text, which then serves as a helpful hint to guide the translation process. Despite being trained on a small dataset of four domains, our CoT fine-tune approach achieves notable enhancements in translation accuracy and domain robustness than traditional fine-tuning, as evidenced by an average 1.53 BLEU score increase in over 20 German$\\rightarrow$English distinct out-of-domain tests.",
            "link": "https://www.semanticscholar.org/paper/9d23568fce5806937592a16f0dc598f5872f89e8",
            "authors": "Tianxiang Hu, Pei Zhang, Baosong Yang, Jun Xie, Derek F. Wong, Rui Wang",
            "matchScore": 271.04422,
            "original title": "Large Language Model for Multi-Domain Translation: Benchmarking and Domain CoT Fine-tuning",
            "original authors": "Tianxiang Hu, Pei Zhang, Baosong Yang, Jun Xie, Derek F. Wong, Rui Wang",
            "EMNLP Paper ID": "1155",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "52b72a1b31fdbf4dcd40db3cb4dbef9407e00759",
            "title": "Self-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+ Languages",
            "abstract": "LLMs have become a go-to solution not just for text generation, but also for natural language understanding (NLU) tasks. Acquiring extensive knowledge through language modeling on web-scale corpora, they excel on English NLU, yet struggle to extend their NLU capabilities to underrepresented languages. In contrast, machine translation models (MT) produce excellent multilingual representations, resulting in strong translation performance even for low-resource languages. MT encoders, however, lack the knowledge necessary for comprehensive NLU that LLMs obtain through language modeling training on immense corpora. In this work, we get the best both worlds by integrating MT encoders directly into LLM backbones via sample-efficient self-distillation. The resulting MT-LLMs preserve the inherent multilingual representational alignment from the MT encoder, allowing lower-resource languages to tap into the rich knowledge embedded in English-centric LLMs. Merging the MT encoder and LLM in a single model, we mitigate the propagation of translation errors and inference overhead of MT decoding inherent to discrete translation-based cross-lingual transfer (e.g., translate-test). Evaluation spanning three prominent NLU tasks and 127 predominantly low-resource languages renders MT-LLMs highly effective in cross-lingual transfer. MT-LLMs substantially and consistently outperform translate-test based on the same MT model, showing that we truly unlock multilingual language understanding for LLMs.",
            "link": "https://www.semanticscholar.org/paper/52b72a1b31fdbf4dcd40db3cb4dbef9407e00759",
            "authors": "Fabian David Schmidt, Philipp Borchert, Ivan Vuli'c, Goran Glavavs",
            "matchScore": 306.66458,
            "original title": "Self-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+ Languages",
            "original authors": "Fabian David Schmidt, Philipp Borchert, Ivan Vuli\u0107, Goran Glava\u0161",
            "EMNLP Paper ID": "1371",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "05f9c55a2354e0533916d5fbffe470ed3d280854",
            "title": "Creative and Context-Aware Translation of East Asian Idioms with GPT-4",
            "abstract": "As a type of figurative language, an East Asian idiom condenses rich cultural background into only a few characters. Translating such idioms is challenging for human translators, who often resort to choosing a context-aware translation from an existing list of candidates. However, compiling a dictionary of candidate translations demands much time and creativity even for expert translators. To alleviate such burden, we evaluate if GPT-4 can help generate high-quality translations. Based on automatic evaluations of faithfulness and creativity, we first identify Pareto-optimal prompting strategies that can outperform translation engines from Google and DeepL. Then, at a low cost, our context-aware translations can achieve far more high-quality translations per idiom than the human baseline. We open-source all code and data to facilitate further research.",
            "link": "https://www.semanticscholar.org/paper/05f9c55a2354e0533916d5fbffe470ed3d280854",
            "authors": "Kenan Tang, Peiyang Song, Yao Qin, Xifeng Yan",
            "matchScore": 257.5741,
            "original title": "Creative and Context-Aware Translation of East Asian Idioms with GPT-4",
            "original authors": "Kenan Tang, Peiyang Song, Yao Qin, Xifeng Yan",
            "EMNLP Paper ID": "1938",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "d3ad0931e5c6e0e7152485eb103a7301836b57cf",
            "title": "LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages",
            "abstract": "Large Language Models (LLMs) demonstrate remarkable translation capabilities in high-resource language tasks, yet their performance in low-resource languages is hindered by insufficient multilingual data during pre-training. To address this, we conduct extensive multilingual continual pre-training on the LLaMA series models, enabling translation support across more than 100 languages. Through a comprehensive analysis of training strategies, such as vocabulary expansion and data augmentation, we develop LLaMAX. Remarkably, without sacrificing its generalization ability, LLaMAX achieves significantly higher translation performance compared to existing open-source LLMs (by more than 10 spBLEU points) and performs on-par with specialized translation model (M2M-100-12B) on the Flores-101 benchmark. Extensive experiments indicate that LLaMAX can serve as a robust multilingual foundation model. The code \\footnote{\\url{https://github.com/CONE-MT/LLaMAX/.}} and the models \\footnote{\\url{https://huggingface.co/LLaMAX/.}} are publicly available.",
            "link": "https://www.semanticscholar.org/paper/d3ad0931e5c6e0e7152485eb103a7301836b57cf",
            "authors": "Yinquan Lu, Wenhao Zhu, Lei Li, Yu Qiao, Fei Yuan",
            "matchScore": 291.01056,
            "original title": "XLLaMA2: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages",
            "original authors": "Yinquan Lu, Wenhao Zhu, Lei Li, Yu Qiao, Fei Yuan",
            "EMNLP Paper ID": "2165",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "ee945326e2ceccac919b3e1226afa6ff3772ae7f",
            "title": "DICTDIS: Dictionary Constrained Disambiguation for Improved NMT",
            "abstract": "Domain-specific neural machine translation (NMT) systems (e.g., in educational applications) are socially significant with the potential to help make information accessible to a diverse set of users in multilingual societies. It is desirable that such NMT systems be lexically constrained and draw from domain-specific dictionaries. Dictionaries could present multiple candidate translations for a source word/phrase due to the polysemous nature of words. The onus is then on the NMT model to choose the contextually most appropriate candidate. Prior work has largely ignored this problem and focused on the single candidate constraint setting wherein the target word or phrase is replaced by a single constraint. In this work we present DictDis, a lexically constrained NMT system that disambiguates between multiple candidate translations derived from dictionaries. We achieve this by augmenting training data with multiple dictionary candidates to actively encourage disambiguation during training by implicitly aligning multiple candidate constraints. We demonstrate the utility of DictDis via extensive experiments on English-Hindi and English-German sentences in a variety of domains including regulatory, finance, engineering. We also present comparisons on standard benchmark test datasets. In comparison with existing approaches for lexically constrained and unconstrained NMT, we demonstrate superior performance with respect to constraint copy and disambiguation related measures on all domains while also obtaining improved fluency of up to 2-3 BLEU points on some domains.",
            "link": "https://www.semanticscholar.org/paper/ee945326e2ceccac919b3e1226afa6ff3772ae7f",
            "authors": "Ayush Maheshwari, Piyush Sharma, P. Jyothi, Ganesh Ramakrishnan",
            "matchScore": 241.87817,
            "original title": "DictDis: Dictionary Constrained Disambiguation for Improved NMT",
            "original authors": "Ayush Maheshwari, Preethi Jyothi, Ganesh Ramakrishnan",
            "EMNLP Paper ID": "2191",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "fb3d4e53fd3ac818f86d1cd32cb7423a64fc6cfe",
            "title": "Benchmarking Machine Translation with Cultural Awareness",
            "abstract": "Translating culture-related content is vital for effective cross-cultural communication. However, many culture-specific items (CSIs) often lack viable translations across languages, making it challenging to collect high-quality, diverse parallel corpora with CSI annotations. This difficulty hinders the analysis of cultural awareness of machine translation (MT) systems, including traditional neural MT and the emerging MT paradigm using large language models (LLM). To address this gap, we introduce a novel parallel corpus, enriched with CSI annotations in 6 language pairs for investigating Culturally-Aware Machine Translation--CAMT. Furthermore, we design two evaluation metrics to assess CSI translations, focusing on their pragmatic translation quality. Our findings show the superior ability of LLMs over neural MTs in leveraging external cultural knowledge for translating CSIs, especially those lacking translations in the target culture.",
            "link": "https://www.semanticscholar.org/paper/fb3d4e53fd3ac818f86d1cd32cb7423a64fc6cfe",
            "authors": "Binwei Yao, Ming Jiang, Diyi Yang, Junjie Hu",
            "matchScore": 168.56764,
            "original title": "Benchmarking Machine Translation with Cultural Awareness",
            "original authors": "Binwei Yao, Ming Jiang, Tara Bobinac, Diyi Yang, Junjie Hu",
            "EMNLP Paper ID": "2557",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "49672006290b125aea958d1bae5d07c8e48ce8bb",
            "title": "LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons",
            "abstract": "Data scarcity in low-resource languages can be addressed with word-to-word translations from labeled task data in high-resource languages using bilingual lexicons. However, bilingual lexicons often have limited lexical overlap with task data, which results in poor translation coverage and lexicon utilization. We propose lexicon-conditioned data generation LexC-Gen, a method that generates low-resource-language classification task data at scale. Specifically, LexC-Gen first uses high-resource-language words from bilingual lexicons to generate lexicon-compatible task data, and then it translates them into low-resource languages with bilingual lexicons via word translation. Across 17 extremely low-resource languages, LexC-Gen generated data is competitive with expert-translated gold data, and yields on average 5.6 and 8.9 points improvement over existing lexicon-based word translation methods on sentiment analysis and topic classification tasks respectively. Through ablation study, we show that conditioning on bilingual lexicons is the key component of LexC-Gen. LexC-Gen serves as a potential solution to close the performance gap between open-source multilingual models, such as BLOOMZ and Aya-101, and state-of-the-art commercial models like GPT-4o on low-resource-language tasks.",
            "link": "https://www.semanticscholar.org/paper/49672006290b125aea958d1bae5d07c8e48ce8bb",
            "authors": "Zheng-Xin Yong, Cristina Menghini, Stephen H. Bach",
            "matchScore": 322.01212,
            "original title": "LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons",
            "original authors": "Zheng Xin Yong, Cristina Menghini, Stephen Bach",
            "EMNLP Paper ID": "2707",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Findings"
        },
        {
            "paperId": "b5c70c2f9ae398b2458e1cf79cbf75d6131cfe6c",
            "title": "LexMatcher: Dictionary-centric Data Collection for LLM-based Machine Translation",
            "abstract": "The fine-tuning of open-source large language models (LLMs) for machine translation has recently received considerable attention, marking a shift towards data-centric research from traditional neural machine translation. However, the area of data collection for instruction fine-tuning in machine translation remains relatively underexplored. In this paper, we present LexMatcher, a simple yet effective method for data curation, the design of which is driven by the coverage of senses found in bilingual dictionaries. The construction process comprises data retrieval from an existing corpus and data augmentation that supplements the infrequent senses of polysemous words. Utilizing LLaMA2 as our base model, our approach outperforms the established baselines on the WMT2022 test sets and also exhibits remarkable performance in tasks related to word sense disambiguation and specialized terminology translation. These results underscore the effectiveness of LexMatcher in enhancing LLM-based machine translation. The code, data, and models are available at https://github.com/ARIES-LM/Lexmatcher-MT.git.",
            "link": "https://www.semanticscholar.org/paper/b5c70c2f9ae398b2458e1cf79cbf75d6131cfe6c",
            "authors": "Yongjing Yin, Jiali Zeng, Yafu Li, Fandong Meng, Yue Zhang",
            "matchScore": 234.6408,
            "original title": "LexMatcher: Dictionary-centric Data Curation for LLM-based Machine Translation",
            "original authors": "Yongjing Yin, Jiali Zeng, Yafu Li, Fandong Meng, Yue Zhang",
            "EMNLP Paper ID": "2841",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "228ac408943b38cb732ac87023f415100110c9e6",
            "title": "xTower: A Multilingual LLM for Explaining and Correcting Translation Errors",
            "abstract": "While machine translation (MT) systems are achieving increasingly strong performance on benchmarks, they often produce translations with errors and anomalies. Understanding these errors can potentially help improve the translation quality and user experience. This paper introduces xTower, an open large language model (LLM) built on top of TowerBase designed to provide free-text explanations for translation errors in order to guide the generation of a corrected translation. The quality of the generated explanations by xTower are assessed via both intrinsic and extrinsic evaluation. We ask expert translators to evaluate the quality of the explanations across two dimensions: relatedness towards the error span being explained and helpfulness in error understanding and improving translation quality. Extrinsically, we test xTower across various experimental setups in generating translation corrections, demonstrating significant improvements in translation quality. Our findings highlight xTower's potential towards not only producing plausible and helpful explanations of automatic translations, but also leveraging them to suggest corrected translations.",
            "link": "https://www.semanticscholar.org/paper/228ac408943b38cb732ac87023f415100110c9e6",
            "authors": "Marcos Treviso, Nuno M. Guerreiro, Sweta Agrawal, Ricardo Rei, Jos\u00e9 P. Pombal, T\u00e2nia Vaz, Helena Wu, Beatriz Silva, Daan van Stigt, Andr'e F. T. Martins",
            "matchScore": 281.73547,
            "original title": "xTower: A Multilingual LLM for Explaining and Correcting Translation Errors",
            "original authors": "Marcos V Treviso, Nuno M Guerreiro, Sweta Agrawal, Ricardo Rei, Jos\u00e9 Pombal, Tania Vaz, Helena Wu, Beatriz Silva, Daan van Stigt, Andre Martins",
            "EMNLP Paper ID": "2916",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "54580034de5cad609fa0a6085cdb07fb12d26d36",
            "title": "Locally Measuring Cross-lingual Lexical Alignment: A Domain and Word Level Perspective",
            "abstract": "NLP research on aligning lexical representation spaces to one another has so far focused on aligning language spaces in their entirety. However, cognitive science has long focused on a local perspective, investigating whether translation equivalents truly share the same meaning or the extent that cultural and regional influences result in meaning variations. With recent technological advances and the increasing amounts of available data, the longstanding question of cross-lingual lexical alignment can now be approached in a more data-driven manner. However, developing metrics for the task requires some methodology for comparing metric efficacy. We address this gap and present a methodology for analyzing both synthetic validations and a novel naturalistic validation using lexical gaps in the kinship domain. We further propose new metrics, hitherto unexplored on this task, based on contextualized embeddings. Our analysis spans 16 diverse languages, demonstrating that there is substantial room for improvement with the use of newer language models. Our research paves the way for more accurate and nuanced cross-lingual lexical alignment methodologies and evaluation.",
            "link": "https://www.semanticscholar.org/paper/54580034de5cad609fa0a6085cdb07fb12d26d36",
            "authors": "Taelin Karidi, Eitan Grossman, Omri Abend",
            "matchScore": 278.198,
            "original title": "Locally Measuring Cross-lingual Lexical Alignment: A Domain and Word Level Perspective",
            "original authors": "Taelin Karidi, Eitan Grossman, Omri Abend",
            "EMNLP Paper ID": "3048",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Findings"
        },
        {
            "paperId": "ca2326284831b5cc4adaebcf8a91a7eebb2da5f8",
            "title": "Translation of Multifaceted Data without Re-Training of Machine Translation Systems",
            "abstract": "Translating major language resources to build minor language resources becomes a widely-used approach. Particularly in translating complex data points composed of multiple components, it is common to translate each component separately. However, we argue that this practice often overlooks the interrelation between components within the same data point. To address this limitation, we propose a novel MT pipeline that considers the intra-data relation in implementing MT for training data. In our MT pipeline, all the components in a data point are concatenated to form a single translation sequence and subsequently reconstructed to the data components after translation. We introduce a Catalyst Statement (CS) to enhance the intra-data relation, and Indicator Token (IT) to assist the decomposition of a translated sequence into its respective data components. Through our approach, we have achieved a considerable improvement in translation quality itself, along with its effectiveness as training data. Compared with the conventional approach that translates each data component separately, our method yields better training data that enhances the performance of the trained model by 2.690 points for the web page ranking (WPR) task, and 0.845 for the question generation (QG) task in the XGLUE benchmark.",
            "link": "https://www.semanticscholar.org/paper/ca2326284831b5cc4adaebcf8a91a7eebb2da5f8",
            "authors": "Hyeonseok Moon, Seungyoon Lee, Seongtae Hong, Seungjun Lee, Chanjun Park, Heu-Jeoung Lim",
            "matchScore": 275.99078,
            "original title": "Translation of Multifaceted Data without Re-Training of Machine Translation Systems",
            "original authors": "Hyeonseok Moon, Seungyoon Lee, SeongTae Hong, Seungjun Lee, Chanjun Park, Heuiseok Lim",
            "EMNLP Paper ID": "423",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "0db468a7f7d52b1edba7a2ff78a8cf0eb2e36f85",
            "title": "AnyTrans: Translate AnyText in the Image with Large Scale Models",
            "abstract": "This paper introduces AnyTrans, an all-encompassing framework for the task-Translate AnyText in the Image (TATI), which includes multilingual text translation and text fusion within images. Our framework leverages the strengths of large-scale models, such as Large Language Models (LLMs) and text-guided diffusion models, to incorporate contextual cues from both textual and visual elements during translation. The few-shot learning capability of LLMs allows for the translation of fragmented texts by considering the overall context. Meanwhile, the advanced inpainting and editing abilities of diffusion models make it possible to fuse translated text seamlessly into the original image while preserving its style and realism. Additionally, our framework can be constructed entirely using open-source models and requires no training, making it highly accessible and easily expandable. To encourage advancement in the TATI task, we have meticulously compiled a test dataset called MTIT6, which consists of multilingual text image translation data from six language pairs.",
            "link": "https://www.semanticscholar.org/paper/0db468a7f7d52b1edba7a2ff78a8cf0eb2e36f85",
            "authors": "Zhipeng Qian, Pei Zhang, Baosong Yang, Kai Fan, Yiwei Ma, Derek F. Wong, Xiaoshuai Sun, Rongrong Ji",
            "matchScore": 228.23792,
            "original title": "AnyTrans: Translate AnyText in the Image with Large Scale Models",
            "original authors": "Zhipeng Qian, Pei Zhang, Baosong Yang, Kai Fan, Yiwei Ma, Derek F. Wong, Xiaoshuai Sun, Rongrong Ji",
            "EMNLP Paper ID": "503",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "4425962f05871828d91cad596a3c54c26208b12a",
            "title": "Evaluating Automatic Metrics with Incremental Machine Translation Systems",
            "abstract": "We introduce a dataset comprising commercial machine translations, gathered weekly over six years across 12 translation directions. Since human A/B testing is commonly used, we assume commercial systems improve over time, which enables us to evaluate machine translation (MT) metrics based on their preference for more recent translations. Our study not only confirms several prior findings, such as the advantage of neural metrics over non-neural ones, but also explores the debated issue of how MT quality affects metric reliability--an investigation that smaller datasets in previous research could not sufficiently explore. Overall, our research demonstrates the dataset's value as a testbed for metric evaluation. We release our code at https://github.com/gjwubyron/Evo",
            "link": "https://www.semanticscholar.org/paper/4425962f05871828d91cad596a3c54c26208b12a",
            "authors": "Guojun Wu, Shay B. Cohen, Rico Sennrich",
            "matchScore": 233.71674,
            "original title": "Evaluating Automatic Metrics with Incremental Machine Translation Systems",
            "original authors": "Guojun Wu, Shay B Cohen, Rico Sennrich",
            "EMNLP Paper ID": "598",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "95166d7b62c864dd5f10eda5f1ad0a32fa12f004",
            "title": "TransLLaMa: LLM-based Simultaneous Translation System",
            "abstract": "Decoder-only large language models (LLMs) have recently demonstrated impressive capabilities in text generation and reasoning. Nonetheless, they have limited applications in simultaneous machine translation (SiMT), currently dominated by encoder-decoder transformers. This study demonstrates that, after fine-tuning on a small dataset comprising causally aligned source and target sentence pairs, a pre-trained open-source LLM can control input segmentation directly by generating a special\"wait\"token. This obviates the need for a separate policy and enables the LLM to perform English-German and English-Russian SiMT tasks with BLEU scores that are comparable to those of specific state-of-the-art baselines. We also evaluated closed-source models such as GPT-4, which displayed encouraging results in performing the SiMT task without prior training (zero-shot), indicating a promising avenue for enhancing future SiMT systems.",
            "link": "https://www.semanticscholar.org/paper/95166d7b62c864dd5f10eda5f1ad0a32fa12f004",
            "authors": "Roman Koshkin, Katsuhito Sudoh, Satoshi Nakamura",
            "matchScore": 212.78307,
            "original title": "TransLLaMa: LLM-based Simultaneous Translation System",
            "original authors": "Roman Koshkin, Katsuhito Sudoh, Satoshi Nakamura",
            "EMNLP Paper ID": "83",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        }
    ],
    "Detection and Mitigation of Hallucinations in Large Language and Vision-Language Models": [
        {
            "paperId": "bf54792cf01761a2c51ac3410287797fff665cd4",
            "title": "EFUF: Efficient Fine-grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models",
            "abstract": "Multimodal large language models (MLLMs) have attracted increasing attention in the past few years, but they may still generate descriptions that include objects not present in the corresponding images, a phenomenon known as object hallucination. To eliminate hallucinations, existing methods manually annotate paired responses with and without hallucinations, and then employ various alignment algorithms to improve the alignment capability between images and text. However, they not only demand considerable computation resources during the finetuning stage but also require expensive human annotation to construct paired data needed by the alignment algorithms. To address these issues, we borrow the idea of unlearning and propose an efficient fine-grained unlearning framework (EFUF), which can eliminate hallucinations without the need for paired data. Extensive experiments show that our method consistently reduces hallucinations while preserving the generation quality with modest computational overhead. Our code and datasets will be publicly available.",
            "link": "https://www.semanticscholar.org/paper/bf54792cf01761a2c51ac3410287797fff665cd4",
            "authors": "Shangyu Xing, Fei Zhao, Zhen Wu, Tuo An, Weihao Chen, Chunhui Li, Jianbing Zhang, Xinyu Dai",
            "EMNLP Paper ID": "142",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "143a05fb36be8198d7675b594c0656b5652da3cb",
            "title": "Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps",
            "abstract": "When asked to summarize articles or answer questions given a passage, large language models (LLMs) can hallucinate details and respond with unsubstantiated answers that are inaccurate with respect to the input context. This paper describes a simple approach for detecting such contextual hallucinations. We hypothesize that contextual hallucinations are related to the extent to which an LLM attends to information in the provided context versus its own generations. Based on this intuition, we propose a simple hallucination detection model whose input features are given by the ratio of attention weights on the context versus newly generated tokens (for each attention head). We find that a linear classifier based on these lookback ratio features is as effective as a richer detector that utilizes the entire hidden states of an LLM or a text-based entailment model. The lookback ratio-based detector -- Lookback Lens -- is found to transfer across tasks and even models, allowing a detector that is trained on a 7B model to be applied (without retraining) to a larger 13B model. We further apply this detector to mitigate contextual hallucinations, and find that a simple classifier-guided decoding approach is able to reduce the amount of hallucination, for example by 9.6% in the XSum summarization task.",
            "link": "https://www.semanticscholar.org/paper/143a05fb36be8198d7675b594c0656b5652da3cb",
            "authors": "Yung-Sung Chuang, Linlu Qiu, Cheng-Yu Hsieh, Ranjay Krishna, Yoon Kim, James Glass",
            "EMNLP Paper ID": "169",
            "Oral/Poster": "Oral",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "61bc39b1da0363759edf82445d5ebe4e3e9f92ba",
            "title": "HELPD: Mitigating Hallucination of LVLMs by Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding",
            "abstract": "Large Vision-Language Models (LVLMs) have shown remarkable performance on many visual-language tasks. However, these models still suffer from multimodal hallucination, which means the generation of objects or content that violates the images. Many existing work detects hallucination by directly judging whether an object exists in an image, overlooking the association between the object and semantics. To address this issue, we propose Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding (HELPD). This framework incorporates hallucination feedback at both object and sentence semantic levels. Remarkably, even with a marginal degree of training, this approach can alleviate over 15% of hallucination. Simultaneously, HELPD penalizes the output logits according to the image attention window to avoid being overly affected by generated text. HELPD can be seamlessly integrated with any LVLMs. Our experiments demonstrate that the proposed framework yields favorable results across multiple hallucination benchmarks. It effectively mitigates hallucination for different LVLMs and concurrently improves their text generation quality.",
            "link": "https://www.semanticscholar.org/paper/61bc39b1da0363759edf82445d5ebe4e3e9f92ba",
            "authors": "Fan Yuan, Chi Qin, Xiaogang Xu, Piji Li",
            "EMNLP Paper ID": "200",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "fcee1c19e12f3b7e3595aeba702416d055bdbc3f",
            "title": "Knowledge Verification to Nip Hallucination in the Bud",
            "abstract": "While large language models (LLMs) have demonstrated exceptional performance across various tasks following human alignment, they may still generate responses that sound plausible but contradict factual knowledge, a phenomenon known as hallucination. In this paper, we demonstrate the feasibility of mitigating hallucinations by verifying and minimizing the inconsistency between external knowledge present in the alignment data and the intrinsic knowledge embedded within foundation LLMs. Specifically, we propose a novel approach called Knowledge Consistent Alignment (KCA), which employs a well-aligned LLM to automatically formulate assessments based on external knowledge to evaluate the knowledge boundaries of foundation LLMs. To address knowledge inconsistencies in the alignment data, KCA implements several specific strategies to deal with these data instances. We demonstrate the superior efficacy of KCA in reducing hallucinations across six benchmarks, utilizing foundation LLMs of varying backbones and scales. This confirms the effectiveness of mitigating hallucinations by reducing knowledge inconsistency. Our code, model weights, and data are openly accessible at \\url{https://github.com/fanqiwan/KCA}.",
            "link": "https://www.semanticscholar.org/paper/fcee1c19e12f3b7e3595aeba702416d055bdbc3f",
            "authors": "Fanqi Wan, Xinting Huang, Leyang Cui, Xiaojun Quan, Wei Bi, Shuming Shi",
            "EMNLP Paper ID": "291",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "f0a79fe7765ab253480a0be6d29c889eac19eb3c",
            "title": "Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models",
            "abstract": "Large Language Models (LLMs) have shown impressive capabilities but still suffer from the issue of hallucinations. A significant type of this issue is the false premise hallucination, which we define as the phenomenon when LLMs generate hallucinated text when confronted with false premise questions. In this paper, we perform a comprehensive analysis of the false premise hallucination and elucidate its internal working mechanism: a small subset of attention heads (which we designate as false premise heads) disturb the knowledge extraction process, leading to the occurrence of false premise hallucination. Based on our analysis, we propose \\textbf{FAITH} (\\textbf{F}alse premise \\textbf{A}ttention head constra\\textbf{I}ining for mi\\textbf{T}igating \\textbf{H}allucinations), a novel and effective method to mitigate false premise hallucinations. It constrains the false premise attention heads during the model inference process. Impressively, extensive experiments demonstrate that constraining only approximately $1\\%$ of the attention heads in the model yields a notable increase of nearly $20\\%$ of model performance.",
            "link": "https://www.semanticscholar.org/paper/f0a79fe7765ab253480a0be6d29c889eac19eb3c",
            "authors": "Hongbang Yuan, Pengfei Cao, Zhuoran Jin, Yubo Chen, Daojian Zeng, Kang Liu, Jun Zhao",
            "EMNLP Paper ID": "294",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "fa5a8e7cbbbb8ee47610733c363bb96bf31e049b",
            "title": "Does Object Grounding Really Reduce Hallucination of Large Vision-Language Models?",
            "abstract": "Large vision-language models (LVLMs) have recently dramatically pushed the state of the art in image captioning and many image understanding tasks (e.g., visual question answering). LVLMs, however, often \\textit{hallucinate} and produce captions that mention concepts that cannot be found in the image. These hallucinations erode the trustworthiness of LVLMs and are arguably among the main obstacles to their ubiquitous adoption. Recent work suggests that addition of grounding objectives -- those that explicitly align image regions or objects to text spans -- reduces the amount of LVLM hallucination. Although intuitive, this claim is not empirically justified as the reduction effects have been established, we argue, with flawed evaluation protocols that (i) rely on data (i.e., MSCOCO) that has been extensively used in LVLM training and (ii) measure hallucination via question answering rather than open-ended caption generation. In this work, in contrast, we offer the first systematic analysis of the effect of fine-grained object grounding on LVLM hallucination under an evaluation protocol that more realistically captures LVLM hallucination in open generation. Our extensive experiments over three backbone LLMs reveal that grounding objectives have little to no effect on object hallucination in open caption generation.",
            "link": "https://www.semanticscholar.org/paper/fa5a8e7cbbbb8ee47610733c363bb96bf31e049b",
            "authors": "Gregor Geigle, R. Timofte, Goran Glavas",
            "EMNLP Paper ID": "304",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "7a16e034525bdab6e5abc78bed488a565ed874be",
            "title": "An Audit on the Perspectives and Challenges of Hallucinations in NLP",
            "abstract": "We audit how hallucination in large language models (LLMs) is characterized in peer-reviewed literature, using a critical examination of 103 publications across NLP research. Through the examination of the literature, we identify a lack of agreement with the term `hallucination' in the field of NLP. Additionally, to compliment our audit, we conduct a survey with 171 practitioners from the field of NLP and AI to capture varying perspectives on hallucination. Our analysis calls for the necessity of explicit definitions and frameworks outlining hallucination within NLP, highlighting potential challenges, and our survey inputs provide a thematic understanding of the influence and ramifications of hallucination in society.",
            "link": "https://www.semanticscholar.org/paper/7a16e034525bdab6e5abc78bed488a565ed874be",
            "authors": "Pranav Narayanan Venkit, Tatiana Chakravorti, Vipul Gupta, Heidi Biggs, Mukund Srinath, Koustava Goswami, Sarah Rajtmajer, Shomir Wilson",
            "EMNLP Paper ID": "730",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "a354c2b5f567470aea44828a98c0cb6b32fe1a9e",
            "title": "DAMRO: Dive into the Attention Mechanism of LVLM to Reduce Object Hallucination",
            "abstract": "Despite the great success of Large Vision-Language Models (LVLMs), they inevitably suffer from hallucination. As we know, both the visual encoder and the Large Language Model (LLM) decoder in LVLMs are Transformer-based, allowing the model to extract visual information and generate text outputs via attention mechanisms. We find that the attention distribution of LLM decoder on image tokens is highly consistent with the visual encoder and both distributions tend to focus on particular background tokens rather than the referred objects in the image. We attribute to the unexpected attention distribution to an inherent flaw in the visual encoder itself, which misguides LLMs to over emphasize the redundant information and generate object hallucination. To address the issue, we propose DAMRO, a novel training-free strategy that $D$ive into $A$ttention $M$echanism of LVLM to $R$educe $O$bject Hallucination. Specifically, our approach employs classification token (CLS) of ViT to filter out high-attention outlier tokens scattered in the background and then eliminate their influence during decoding stage. We evaluate our method on LVLMs including LLaVA-1.5, LLaVA-NeXT and InstructBLIP, using various benchmarks such as POPE, CHAIR, MME and GPT-4V Aided Evaluation. The results demonstrate that our approach significantly reduces the impact of these outlier tokens, thus effectively alleviating the hallucination of LVLMs. The code of our method will be released soon.",
            "link": "https://www.semanticscholar.org/paper/a354c2b5f567470aea44828a98c0cb6b32fe1a9e",
            "authors": "Xuan Gong, Tianshi Ming, Xinpeng Wang, Zhihua Wei",
            "EMNLP Paper ID": "873",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "54972b2e4304d2164a61036ae947df2503c07009",
            "title": "Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?",
            "abstract": "When large language models are aligned via supervised fine-tuning, they may encounter new factual information that was not acquired through pre-training. It is often conjectured that this can teach the model the behavior of hallucinating factually incorrect responses, as the model is trained to generate facts that are not grounded in its pre-existing knowledge. In this work, we study the impact of such exposure to new knowledge on the capability of the fine-tuned model to utilize its pre-existing knowledge. To this end, we design a controlled setup, focused on closed-book QA, where we vary the proportion of the fine-tuning examples that introduce new knowledge. We demonstrate that large language models struggle to acquire new factual knowledge through fine-tuning, as fine-tuning examples that introduce new knowledge are learned significantly slower than those consistent with the model's knowledge. However, we also find that as the examples with new knowledge are eventually learned, they linearly increase the model's tendency to hallucinate. Taken together, our results highlight the risk in introducing new factual knowledge through fine-tuning, and support the view that large language models mostly acquire factual knowledge through pre-training, whereas fine-tuning teaches them to use it more efficiently.",
            "link": "https://www.semanticscholar.org/paper/54972b2e4304d2164a61036ae947df2503c07009",
            "authors": "Zorik Gekhman, G. Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, Jonathan Herzig",
            "EMNLP Paper ID": "887",
            "Oral/Poster": "Oral",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "a3a723d149c6f5d7a17616029e6491570abaa9f1",
            "title": "Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification",
            "abstract": "Large Visual Language Models (LVLMs) struggle with hallucinations in visual instruction following task(s), limiting their trustworthiness and real-world applicability. We propose Pelican -- a novel framework designed to detect and mitigate hallucinations through claim verification. Pelican first decomposes the visual claim into a chain of sub-claims based on first-order predicates. These sub-claims consist of (predicate, question) pairs and can be conceptualized as nodes of a computational graph. We then use Program-of-Thought prompting to generate Python code for answering these questions through flexible composition of external tools. Pelican improves over prior work by introducing (1) intermediate variables for precise grounding of object instances, and (2) shared computation for answering the sub-question to enable adaptive corrections and inconsistency identification. We finally use reasoning abilities of LLM to verify the correctness of the the claim by considering the consistency and confidence of the (question, answer) pairs from each sub-claim. Our experiments reveal a drop in hallucination rate by $\\sim$8%-32% across various baseline LVLMs and a 27% drop compared to approaches proposed for hallucination mitigation on MMHal-Bench. Results on two other benchmarks further corroborate our results.",
            "link": "https://www.semanticscholar.org/paper/a3a723d149c6f5d7a17616029e6491570abaa9f1",
            "authors": "Pritish Sahu, Karan Sikka, Ajay Divakaran",
            "EMNLP Paper ID": "955",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "4c4a3328153e85749c690e68acc13b42a7225e50",
            "title": "ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models",
            "abstract": "Tool-augmented large language models (LLMs) are rapidly being integrated into real-world applications. Due to the lack of benchmarks, the community has yet to fully understand the hallucination issues within these models. To address this challenge, we introduce a comprehensive diagnostic benchmark, ToolBH. Specifically, we assess the LLM's hallucinations through two perspectives: depth and breadth. In terms of depth, we propose a multi-level diagnostic process, including (1) solvability detection, (2) solution planning, and (3) missing-tool analysis. For breadth, we consider three scenarios based on the characteristics of the toolset: missing necessary tools, potential tools, and limited functionality tools. Furthermore, we developed seven tasks and collected 700 evaluation samples through multiple rounds of manual annotation. The results show the significant challenges presented by the ToolBH benchmark. The current advanced models Gemini-1.5-Pro and GPT-4o only achieve total scores of 45.3 and 37.0, respectively, on a scale of 100. In this benchmark, larger model parameters do not guarantee better performance; the training data and response strategies also play crucial roles in tool-enhanced LLM scenarios. Our diagnostic analysis indicates that the primary reason for model errors lies in assessing task solvability. Additionally, open-weight models suffer from performance drops with verbose replies, whereas proprietary models excel with longer reasoning.",
            "link": "https://www.semanticscholar.org/paper/4c4a3328153e85749c690e68acc13b42a7225e50",
            "authors": "Yuxiang Zhang, Jing Chen, Junjie Wang, Yaxin Liu, Cheng Yang, Chufan Shi, Xinyu Zhu, Zihao Lin, Hanwen Wan, Yujiu Yang, Tetsuya Sakai, Tian Feng, Hayato Yamana",
            "EMNLP Paper ID": "1333",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "4e148a693753ecb1cb55cc6458629c9fba0f93dd",
            "title": "Analysis of Plan-based Retrieval for Grounded Text Generation",
            "abstract": "In text generation, hallucinations refer to the generation of seemingly coherent text that contradicts established knowledge. One compelling hypothesis is that hallucinations occur when a language model is given a generation task outside its parametric knowledge (due to rarity, recency, domain, etc.). A common strategy to address this limitation is to infuse the language models with retrieval mechanisms, providing the model with relevant knowledge for the task. In this paper, we leverage the planning capabilities of instruction-tuned LLMs and analyze how planning can be used to guide retrieval to further reduce the frequency of hallucinations. We empirically evaluate several variations of our proposed approach on long-form text generation tasks. By improving the coverage of relevant facts, plan-guided retrieval and generation can produce more informative responses while providing a higher rate of attribution to source documents.",
            "link": "https://www.semanticscholar.org/paper/4e148a693753ecb1cb55cc6458629c9fba0f93dd",
            "authors": "Ameya Godbole, Nicholas Monath, Seungyeon Kim, A. Rawat, Andrew McCallum, M. Zaheer",
            "EMNLP Paper ID": "1514",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "cbaf69a654ccbd76dad8049dc22ed831913a1d77",
            "title": "Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector",
            "abstract": "Hallucination detection is a challenging task for large language models (LLMs), and existing studies heavily rely on powerful closed-source LLMs such as GPT-4. In this paper, we propose an autonomous LLM-based agent framework, called HaluAgent, which enables relatively smaller LLMs (e.g. Baichuan2-Chat 7B) to actively select suitable tools for detecting multiple hallucination types such as text, code, and mathematical expression. In HaluAgent, we integrate the LLM, multi-functional toolbox, and design a fine-grained three-stage detection framework along with memory mechanism. To facilitate the effectiveness of HaluAgent, we leverage existing Chinese and English datasets to synthesize detection trajectories for fine-tuning, which endows HaluAgent with the capability for bilingual hallucination detection. Extensive experiments demonstrate that only using 2K samples for tuning LLMs, HaluAgent can perform hallucination detection on various types of tasks and datasets, achieving performance comparable to or even higher than GPT-4 without tool enhancements on both in-domain and out-of-domain datasets. We release our dataset and code at https://github.com/RUCAIBox/HaluAgent.",
            "link": "https://www.semanticscholar.org/paper/cbaf69a654ccbd76dad8049dc22ed831913a1d77",
            "authors": "Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, Hongzhi Zhang, Fuzheng Zhang, Di Zhang, Kun Gai, Ji-Rong Wen",
            "EMNLP Paper ID": "1683",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "0370f0e8459bc687c6adaaff2e34de35bb480d81",
            "title": "The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs",
            "abstract": "Large language models (LLMs) have recently experienced remarkable progress, where the advent of multi-modal large language models (MLLMs) has endowed LLMs with visual capabilities, leading to impressive performances in various multi-modal tasks. However, those powerful MLLMs such as GPT-4V still fail spectacularly when presented with certain image and text inputs. In this paper, we identify a typical class of inputs that baffles MLLMs, which consist of images that are highly relevant but inconsistent with answers, causing MLLMs to suffer from hallucination. To quantify the effect, we propose CorrelationQA, the first benchmark that assesses the hallucination level given spurious images. This benchmark contains 7,308 text-image pairs across 13 categories. Based on the proposed CorrelationQA, we conduct a thorough analysis on 9 mainstream MLLMs, illustrating that they universally suffer from this instinctive bias to varying degrees. We hope that our curated benchmark and evaluation results aid in better assessments of the MLLMs\u2019 robustness in the presence of misleading images. The resource is available in https://github.com/MasaiahHan/CorrelationQA.",
            "link": "https://www.semanticscholar.org/paper/0370f0e8459bc687c6adaaff2e34de35bb480d81",
            "authors": "Tianyang Han, Qing Lian, Rui Pan, Renjie Pi, Jipeng Zhang, Shizhe Diao, Yong Lin, Tong Zhang",
            "EMNLP Paper ID": "1901",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "18f418588ef33a4f2c35c6591da09c75c30fdff0",
            "title": "Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models",
            "abstract": "Large Vision-Language Models (LVLMs) have achieved impressive performance, yet research has pointed out a serious issue with object hallucinations within these models. However, there is no clear conclusion as to which part of the model these hallucinations originate from. In this paper, we present an in-depth investigation into the object hallucination problem specifically within the CLIP model, which serves as the backbone for many state-of-the-art vision-language systems. We unveil that even in isolation, the CLIP model is prone to object hallucinations, suggesting that the hallucination problem is not solely due to the interaction between vision and language modalities. To address this, we propose a counterfactual data augmentation method by creating negative samples with a variety of hallucination issues. We demonstrate that our method can effectively mitigate object hallucinations for CLIP model, and we show the the enhanced model can be employed as a visual encoder, effectively alleviating the object hallucination issue in LVLMs.",
            "link": "https://www.semanticscholar.org/paper/18f418588ef33a4f2c35c6591da09c75c30fdff0",
            "authors": "Yufang Liu, Tao Ji, Changzhi Sun, Yuanbin Wu, Aimin Zhou",
            "EMNLP Paper ID": "2265",
            "Oral/Poster": "Oral",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "130c2241efd4f455f55c570a4f8ba07dd4207e9d",
            "title": "Enhanced Hallucination Detection in Neural Machine Translation through Simple Detector Aggregation",
            "abstract": "Hallucinated translations pose significant threats and safety concerns when it comes to the practical deployment of machine translation systems. Previous research works have identified that detectors exhibit complementary performance different detectors excel at detecting different types of hallucinations. In this paper, we propose to address the limitations of individual detectors by combining them and introducing a straightforward method for aggregating multiple detectors. Our results demonstrate the efficacy of our aggregated detector, providing a promising step towards evermore reliable machine translation systems.",
            "link": "https://www.semanticscholar.org/paper/130c2241efd4f455f55c570a4f8ba07dd4207e9d",
            "authors": "Anas Himmi, Guillaume Staerman, Marine Picot, Pierre Colombo, Nuno M. Guerreiro",
            "EMNLP Paper ID": "2313",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "9a3db70e4154ce6031006ddb0a83e6c0d112ffbb",
            "title": "Mitigating Open-Vocabulary Caption Hallucinations",
            "abstract": "While recent years have seen rapid progress in image-conditioned text generation, image captioning still suffers from the fundamental issue of hallucinations, namely, the generation of spurious details that cannot be inferred from the given image. Existing methods largely use closed-vocabulary object lists to mitigate or evaluate hallucinations in image captioning, ignoring the long-tailed nature of hallucinations that occur in practice. To this end, we propose a framework for addressing hallucinations in image captioning in the open-vocabulary setting. Our framework includes a new benchmark, OpenCHAIR, that leverages generative foundation models to evaluate open-vocabulary object hallucinations for image captioning, surpassing the popular and similarly-sized CHAIR benchmark in both diversity and accuracy. Furthermore, to mitigate open-vocabulary hallucinations without using a closed object list, we propose MOCHa, an approach harnessing advancements in reinforcement learning. Our multi-objective reward function explicitly targets the trade-off between fidelity and adequacy in generations without requiring any strong supervision. MOCHa improves a large variety of image captioning models, as captured by our OpenCHAIR benchmark and other existing metrics. Code and models can be found at: https://github.com/assafbk/mocha_code",
            "link": "https://www.semanticscholar.org/paper/9a3db70e4154ce6031006ddb0a83e6c0d112ffbb",
            "authors": "Assaf Ben-Kish, Moran Yanuka, Morris Alper, Raja Giryes, Hadar Averbuch-Elor",
            "EMNLP Paper ID": "3271",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "849b3727dbb41c37f92a338ac5860b764a5b94f4",
            "title": "Mitigating Hallucinations of Large Language Models in Medical Information Extraction via Contrastive Decoding",
            "abstract": "The impressive capabilities of large language models (LLMs) have attracted extensive interests of applying LLMs to medical field. However, the complex nature of clinical environments presents significant hallucination challenges for LLMs, hindering their widespread adoption. In this paper, we address these hallucination issues in the context of Medical Information Extraction (MIE) tasks by introducing ALternate Contrastive Decoding (ALCD). We begin by redefining MIE tasks as an identify-and-classify process. We then separate the identification and classification functions of LLMs by selectively masking the optimization of tokens during fine-tuning. During the inference stage, we alternately contrast output distributions derived from sub-task models. This approach aims to selectively enhance the identification and classification capabilities while minimizing the influence of other inherent abilities in LLMs. Additionally, we propose an alternate adaptive constraint strategy to more effectively adjust the scale and scope of contrastive tokens. Through comprehensive experiments on two different backbones and six diverse medical information extraction tasks, ALCD demonstrates significant improvements in resolving hallucination issues compared to conventional decoding methods.",
            "link": "https://www.semanticscholar.org/paper/849b3727dbb41c37f92a338ac5860b764a5b94f4",
            "authors": "Derong Xu, Ziheng Zhang, Zhihong Zhu, Zhenxi Lin, Qidong Liu, Xian Wu, Tong Xu, Xiangyu Zhao, Yefeng Zheng, Enhong Chen",
            "matchScore": 195.12595,
            "original title": "Mitigating Hallucinations of Large Language Models in Medical Domain via Contrastive Decoding",
            "original authors": "Derong Xu, Ziheng Zhang, Zhihong Zhu, Zhenxi Lin, Qidong Liu, Xian Wu, Tong Xu, Xiangyu Zhao, Yefeng Zheng, Enhong Chen",
            "EMNLP Paper ID": "1622",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "7d494fbf4534ef1c2ffab9b711f05ac95154d81b",
            "title": "Multilingual Fine-Grained News Headline Hallucination Detection",
            "abstract": "The popularity of automated news headline generation has surged with advancements in pre-trained language models. However, these models often suffer from the ``hallucination'' problem, where the generated headline is not fully supported by its source article. Efforts to address this issue have predominantly focused on English, using over-simplistic classification schemes that overlook nuanced hallucination types. In this study, we introduce the first multilingual, fine-grained news headline hallucination detection dataset that contains over 11 thousand pairs in 5 languages, each annotated with detailed hallucination types by experts. We conduct extensive experiments on this dataset under two settings. First, we implement several supervised fine-tuning approaches as preparatory solutions and demonstrate this dataset's challenges and utilities. Second, we test various large language models' in-context learning abilities and propose two novel techniques, language-dependent demonstration selection and coarse-to-fine prompting, to boost the few-shot hallucination detection performance in terms of the example-F1 metric. We release this dataset to foster further research in multilingual, fine-grained headline hallucination detection.",
            "link": "https://www.semanticscholar.org/paper/7d494fbf4534ef1c2ffab9b711f05ac95154d81b",
            "authors": "Jiaming Shen, Tianqi Liu, Jialu Liu, Zhen Qin, Jay Pavagadhi, Simon Baumgartner, Michael Bendersky",
            "matchScore": 252.73529,
            "original title": "Multilingual Fine-Grained News Headline Hallucination Detection",
            "original authors": "Jiaming Shen, Tianqi Liu, Jialu Liu, Zhen Qin, Jay Pavagadhi, Simon Baumgartner, Michael Bendersky",
            "EMNLP Paper ID": "1647",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "01b4977937966694c00f6c7b55e712eef50603a4",
            "title": "Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations",
            "abstract": "State-of-the-art language models (LMs) sometimes generate non-factual hallucinations that misalign with world knowledge. To explore the mechanistic causes of these hallucinations, we create diagnostic datasets with subject-relation queries and adapt interpretability methods to trace hallucinations through internal model representations. We discover two general and distinct mechanistic causes of hallucinations shared across LMs (Llama-2, Pythia, GPT-J): 1) knowledge enrichment hallucinations: insufficient subject attribute knowledge in lower layer MLPs, and 2) answer extraction hallucinations: failure to select the correct object attribute in upper layer attention heads. We also found these two internal mechanistic causes of hallucinations are reflected in external manifestations. Based on insights from our mechanistic analysis, we propose a novel hallucination mitigation method through targeted restoration of the LM's internal fact recall pipeline, demonstrating superior performance compared to baselines.",
            "link": "https://www.semanticscholar.org/paper/01b4977937966694c00f6c7b55e712eef50603a4",
            "authors": "Lei Yu, Meng Cao, Jackie C. K. Cheung, Yue Dong",
            "matchScore": 243.81067,
            "original title": "Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations",
            "original authors": "Lei Yu, Meng Cao, Jackie CK Cheung, Yue Dong",
            "EMNLP Paper ID": "1663",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "a649db6921c327b75df38d4b81e9c8b4173fb175",
            "title": "AUTOHALLUSION: Automatic Generation of Hallucination Benchmarks for Vision-Language Models",
            "abstract": "Large vision-language models (LVLMs) are prone to hallucinations, where certain contextual cues in an image can trigger the language module to produce overconfident and incorrect reasoning about abnormal or hypothetical objects. While some benchmarks have been developed to investigate LVLM hallucinations, they often rely on hand-crafted corner cases whose failure patterns may not generalize well. Additionally, fine-tuning on these examples could undermine their validity. To address this, we aim to scale up the number of cases through an automated approach, reducing human bias in crafting such corner cases. This motivates the development of AutoHallusion, the first automated benchmark generation approach that employs several key strategies to create a diverse range of hallucination examples. Our generated visual-question pairs pose significant challenges to LVLMs, requiring them to overcome contextual biases and distractions to arrive at correct answers. AutoHallusion enables us to create new benchmarks at the minimum cost and thus overcomes the fragility of hand-crafted benchmarks. It also reveals common failure patterns and reasons, providing key insights to detect, avoid, or control hallucinations. Comprehensive evaluations of top-tier LVLMs, e.g., GPT-4V(ision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, show a 97.7% and 98.7% success rate of hallucination induction on synthetic and real-world datasets of AutoHallusion, paving the way for a long battle against hallucinations. The codebase and data can be accessed at https://github.com/wuxiyang1996/AutoHallusion.",
            "link": "https://www.semanticscholar.org/paper/a649db6921c327b75df38d4b81e9c8b4173fb175",
            "authors": "Xiyang Wu, Tianrui Guan, Dianqi Li, Shuaiyi Huang, Xiaoyu Liu, Xijun Wang, Ruiqi Xian, Abhinav Shrivastava, Furong Huang, Jordan L. Boyd-Graber, Tianyi Zhou, Dinesh Manocha",
            "matchScore": 252.20839,
            "original title": "AUTOHALLUSION: Automatic Generation of Hallucination Benchmarks for Vision-Language Models",
            "original authors": "Xiyang Wu, Tianrui Guan, Dianqi Li, Shuaiyi Huang, Xiaoyu Liu, Xijun Wang, Ruiqi Xian, Abhinav Shrivastava, Furong Huang, Jordan Lee Boyd-Graber, Tianyi Zhou, Dinesh Manocha",
            "EMNLP Paper ID": "1774",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "83d81e31f5c32f6989d98be1133adfc08db094ce",
            "title": "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models",
            "abstract": "Since large language models (LLMs) achieve significant success in recent years, the hallucination issue remains a challenge, numerous benchmarks are proposed to detect the hallucination. Nevertheless, some of these benchmarks are not naturally generated by LLMs but are intentionally induced. Also, many merely focus on the factuality hallucination while ignoring the faithfulness hallucination. Additionally, although dialogue pattern is more widely utilized in the era of LLMs, current benchmarks only concentrate on sentence-level and passage-level hallucination. In this study, we propose DiaHalu, the first dialogue-level hallucination evaluation benchmark to our knowledge. Initially, we integrate the collected topics into system prompts and facilitate a dialogue between two ChatGPT3.5. Subsequently, we manually modify the contents that do not adhere to human language conventions and then have LLMs re-generate, simulating authentic human-machine interaction scenarios. Finally, professional scholars annotate all the samples in the dataset. DiaHalu covers four common multi-turn dialogue domains and five hallucination subtypes, extended from factuality and faithfulness hallucination. Experiments through some well-known LLMs and detection methods on the dataset show that DiaHalu is a challenging benchmark, holding significant value for further research.",
            "link": "https://www.semanticscholar.org/paper/83d81e31f5c32f6989d98be1133adfc08db094ce",
            "authors": "Kedi Chen, Qin Chen, Jie Zhou, Yishen He, Liang He",
            "matchScore": 297.89883,
            "original title": "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models",
            "original authors": "KediChen, Qin Chen, Jie Zhou, He Yishen, Liang He",
            "EMNLP Paper ID": "1894",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "1541bc9e588bfcd4bf365c868fa2f11461896980",
            "title": "Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models",
            "abstract": "Recent advancements in massively multilingual machine translation systems have significantly enhanced translation accuracy; however, even the best performing systems still generate hallucinations, severely impacting user trust. Detecting hallucinations in Machine Translation (MT) remains a critical challenge, particularly since existing methods excel with High-Resource Languages (HRLs) but exhibit substantial limitations when applied to Low-Resource Languages (LRLs). This paper evaluates sentence-level hallucination detection approaches using Large Language Models (LLMs) and semantic similarity within massively multilingual embeddings. Our study spans 16 language directions, covering HRLs, LRLs, with diverse scripts. We find that the choice of model is essential for performance. On average, for HRLs, Llama3-70B outperforms the previous state of the art by as much as 0.16 MCC (Matthews Correlation Coefficient). However, for LRLs we observe that Claude Sonnet outperforms other LLMs on average by 0.03 MCC. The key takeaway from our study is that LLMs can achieve performance comparable or even better than previously proposed models, despite not being explicitly trained for any machine translation task. However, their advantage is less significant for LRLs.",
            "link": "https://www.semanticscholar.org/paper/1541bc9e588bfcd4bf365c868fa2f11461896980",
            "authors": "Kenza Benkirane, Laura Gongas, Shahar Pelles, Naomi Fuchs, Joshua Darmon, Pontus Stenetorp, David Ifeoluwa Adelani, Eduardo S\u00e1nchez, Meta",
            "matchScore": 270.1861,
            "original title": "Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models",
            "original authors": "Kenza Benkirane, Laura Gongas, Shahar Pelles, Naomi Fuchs, Joshua Darmon, Pontus Stenetorp, David Ifeoluwa Adelani, Eduardo S\u00e1nchez",
            "EMNLP Paper ID": "2000",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "f12b2e22ed2530e2b87054e5735208449616bf3c",
            "title": "Navigating Hallucinations for Reasoning of Unintentional Activities",
            "abstract": "In this work we present a novel task of understanding unintentional human activities in videos. We formalize this problem as a reasoning task under zero-shot scenario, where given a video of an unintentional activity we want to know why it transitioned from intentional to unintentional. We first evaluate the effectiveness of current state-of-the-art Large Multimodal Models on this reasoning task and observe that they suffer from hallucination. We further propose a novel prompting technique,termed as Dream of Thoughts (DoT), which allows the model to navigate through hallucinated thoughts to achieve better reasoning. To evaluate the performance on this task, we also introduce three different specialized metrics designed to quantify the models reasoning capability. We perform our experiments on two different datasets, OOPs and UCF-Crimes, and our findings show that DOT prompting technique is able to outperform standard prompting, while minimizing hallucinations.",
            "link": "https://www.semanticscholar.org/paper/f12b2e22ed2530e2b87054e5735208449616bf3c",
            "authors": "Shresth Grover, Vibhav Vineet, Y. S. Rawat",
            "matchScore": 212.74667,
            "original title": "Navigating Hallucinations for Reasoning of Unintentional Activities",
            "original authors": "Shresth Grover, Vibhav Vineet, Yogesh S Rawat",
            "EMNLP Paper ID": "2002",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "15aaf20d02a1e26be9106e66d065fd1ca5600e29",
            "title": "What if...?: Thinking Counterfactual Keywords Helps to Mitigate Hallucination in Large Multi-modal Models",
            "abstract": "This paper presents a way of enhancing the reliability of Large Multi-modal Models (LMMs) in addressing hallucination, where the models generate cross-modal inconsistent responses. Without additional training, we propose Counterfactual Inception, a novel method that implants counterfactual thinking into LMMs using self-generated counterfactual keywords. Our method is grounded in the concept of counterfactual thinking, a cognitive process where human considers alternative realities, enabling more extensive context exploration. Bridging the human cognition mechanism into LMMs, we aim for the models to engage with and generate responses that span a wider contextual scene understanding, mitigating hallucinatory outputs. We further introduce Plausibility Verification Process (PVP), a simple yet robust keyword constraint that effectively filters out sub-optimal keywords to enable the consistent triggering of counterfactual thinking in the model responses. Comprehensive analyses across various LMMs, including both open-source and proprietary models, corroborate that counterfactual thinking significantly reduces hallucination and helps to broaden contextual understanding based on true visual clues.",
            "link": "https://www.semanticscholar.org/paper/15aaf20d02a1e26be9106e66d065fd1ca5600e29",
            "authors": "Junho Kim, Yeonju Kim, Yonghyun Ro",
            "matchScore": 305.21722,
            "original title": "What if\u2026?: Thinking Counterfactual Keywords Helps to Mitigate Hallucination in Large Multi-modal Models",
            "original authors": "Junho Kim, KIM YEONJU, Yong Man Ro",
            "EMNLP Paper ID": "2159",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Findings"
        },
        {
            "paperId": "c98349977aff4bdeabaaad6358420ae30d6188d0",
            "title": "Pre-trained Language Models Return Distinguishable Probability Distributions to Unfaithfully Hallucinated Texts",
            "abstract": "In this work, we show the pre-trained language models return distinguishable generation probability and uncertainty distribution to unfaithfully hallucinated texts, regardless of their size and structure. By examining 24 models on 6 data sets, we find out that 88-98% of cases return statistically significantly distinguishable generation probability and uncertainty distributions. Using this general phenomenon, we showcase a hallucination-reducing training algorithm. Our algorithm outperforms other baselines by achieving higher faithfulness metrics while maintaining sound general text quality measures.",
            "link": "https://www.semanticscholar.org/paper/c98349977aff4bdeabaaad6358420ae30d6188d0",
            "authors": "Taehun Cha, Donghun Lee",
            "matchScore": 299.49768,
            "original title": "Pre-trained Language Models Return Distinguishable Probability Distributions to Unfaithfully Hallucinated Texts",
            "original authors": "Taehun Cha, Donghun Lee",
            "EMNLP Paper ID": "2468",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "c1a71387908ec16de970f250f7bce4b9e4306ac7",
            "title": "Mitigating Hallucination in Fictional Character Role-Play",
            "abstract": "Role-playing has wide-ranging applications in customer support, embodied agents, computational social science, etc. The influence of parametric world knowledge of large language models (LLMs) often causes role-playing characters to act out of character and hallucinate about things outside the scope of their knowledge. In this work, we focus on the evaluation and mitigation of hallucination in fictional character role-play. We introduce a dataset with more than 2,000 characters and 72,000 interviews, including 18,000 adversarial questions. We propose RoleFact, a role-playing method that mitigates hallucination by modulating the influence of parametric knowledge using a pre-calibrated confidence threshold. Experiments show that the proposed method improves the factual precision of generated responses by 18% for adversarial questions with a 44% reduction in temporal hallucination for time-sensitive interviews. The code and the dataset will be available at https://github.com/NafisSadeq/rolefact.git.",
            "link": "https://www.semanticscholar.org/paper/c1a71387908ec16de970f250f7bce4b9e4306ac7",
            "authors": "Nafis Sadeq, Zhouhang Xie, Byungkyu Kang, Prarit Lamba, Xiang Gao, Julian McAuley",
            "matchScore": 222.05904,
            "original title": "Mitigating Hallucination in Fictional Character Role-Play",
            "original authors": "Nafis Sadeq, Zhouhang Xie, Byungkyu Kang, Prarit Lamba, Xiang Gao, Julian McAuley",
            "EMNLP Paper ID": "2793",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 14): 13:00\u201314:00 (Afternoon)",
            "Type": "Findings"
        },
        {
            "paperId": "705ffeccfde95c3b0723f197c4565f7d3f0451a1",
            "title": "Zero-Resource Hallucination Prevention for Large Language Models",
            "abstract": "The prevalent use of large language models (LLMs) in various domains has drawn attention to the issue of\"hallucination,\"which refers to instances where LLMs generate factually inaccurate or ungrounded information. Existing techniques for hallucination detection in language assistants rely on intricate fuzzy, specific free-language-based chain of thought (CoT) techniques or parameter-based methods that suffer from interpretability issues. Additionally, the methods that identify hallucinations post-generation could not prevent their occurrence and suffer from inconsistent performance due to the influence of the instruction format and model style. In this paper, we introduce a novel pre-detection self-evaluation technique, referred to as SELF-FAMILIARITY, which focuses on evaluating the model's familiarity with the concepts present in the input instruction and withholding the generation of response in case of unfamiliar concepts. This approach emulates the human ability to refrain from responding to unfamiliar topics, thus reducing hallucinations. We validate SELF-FAMILIARITY across four different large language models, demonstrating consistently superior performance compared to existing techniques. Our findings propose a significant shift towards preemptive strategies for hallucination mitigation in LLM assistants, promising improvements in reliability, applicability, and interpretability.",
            "link": "https://www.semanticscholar.org/paper/705ffeccfde95c3b0723f197c4565f7d3f0451a1",
            "authors": "Junyu Luo, Cao Xiao, Fenglong Ma",
            "matchScore": 218.40918,
            "original title": "Zero-Resource Hallucination Prevention for Large Language Models",
            "original authors": "Junyu Luo, Cao Xiao, Fenglong Ma",
            "EMNLP Paper ID": "724",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 14): 13:00\u201314:00 (Afternoon)",
            "Type": "Findings"
        },
        {
            "paperId": "a27529194bd61c6013fb0f48797f08e4998a1be8",
            "title": "SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully",
            "abstract": "Large language models (LLMs) demonstrate great performance in text generation. However, LLMs are still suffering from hallucinations. In this work, we propose an inference-time method, Self-Highlighted Hesitation (SH2), to help LLMs decode more truthfully. SH2 is based on a simple fact rooted in information theory that for an LLM, the tokens predicted with lower probabilities are prone to be more informative than others. Our analysis shows that the tokens assigned with lower probabilities by an LLM are more likely to be closely related to factual information, such as nouns, proper nouns, and adjectives. Therefore, we propose to ''highlight'' the factual information by selecting the tokens with the lowest probabilities and concatenating them to the original context, thus forcing the model to repeatedly read and hesitate on these tokens before generation. During decoding, we also adopt contrastive decoding to emphasize the difference in the output probabilities brought by the hesitation. Experimental results demonstrate that our SH2, requiring no additional data or models, can effectively help LLMs elicit factual knowledge and distinguish hallucinated contexts. Significant and consistent improvements are achieved by SH2 for LLaMA-7b, LLaMA2-7b and Mistral-7b on multiple hallucination tasks.",
            "link": "https://www.semanticscholar.org/paper/a27529194bd61c6013fb0f48797f08e4998a1be8",
            "authors": "Jushi Kai, Tianhang Zhang, Hai Hu, Zhouhan Lin",
            "matchScore": 301.71844,
            "original title": "SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully",
            "original authors": "Jushi Kai, Tianhang Zhang, Hai Hu, Zhouhan Lin",
            "EMNLP Paper ID": "898",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "0ba76fbb7a4a2e6a221b4c31321e9846eca2fe92",
            "title": "Reference-free Hallucination Detection for Large Vision-Language Models",
            "abstract": "Large vision-language models (LVLMs) have made significant progress in recent years. While LVLMs exhibit excellent ability in language understanding, question answering, and conversations of visual inputs, they are prone to producing hallucinations. While several methods are proposed to evaluate the hallucinations in LVLMs, most are reference-based and depend on external tools, which complicates their practical application. To assess the viability of alternative methods, it is critical to understand whether the reference-free approaches, which do not rely on any external tools, can efficiently detect hallucinations. Therefore, we initiate an exploratory study to demonstrate the effectiveness of different reference-free solutions in detecting hallucinations in LVLMs. In particular, we conduct an extensive study on three kinds of techniques: uncertainty-based, consistency-based, and supervised uncertainty quantification methods on four representative LVLMs across two different tasks. The empirical results show that the reference-free approaches are capable of effectively detecting non-factual responses in LVLMs, with the supervised uncertainty quantification method outperforming the others, achieving the best performance across different settings.",
            "link": "https://www.semanticscholar.org/paper/0ba76fbb7a4a2e6a221b4c31321e9846eca2fe92",
            "authors": "Qing Li, Chenyang Lyu, Jiahui Geng, Derui Zhu, Maxim Panov, Fakhri Karray",
            "matchScore": 223.48123,
            "original title": "Reference-free Hallucination Detection for Large Vision-Language Models",
            "original authors": "Qing Li, Jiahui Geng, Chenyang Lyu, Derui Zhu, Maxim Panov, Fakhri Karray",
            "EMNLP Paper ID": "901",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Findings"
        },
        {
            "paperId": "592c14f2a0c0bb1ac7affae3cb0dfac2ccc15bcc",
            "title": "FaithScore: Fine-grained Evaluations of Hallucinations in Large Vision-Language Models",
            "abstract": "We introduce FaithScore (Faithfulness to Atomic Image Facts Score), a reference-free and fine-grained evaluation metric that measures the faithfulness of the generated free-form answers from large vision-language models (LVLMs). The FaithScore evaluation first identifies sub-sentences containing descriptive statements that need to be verified, then extracts a comprehensive list of atomic facts from these sub-sentences, and finally conducts consistency verification between fine-grained atomic facts and the input image. Meta-evaluation demonstrates that our metric highly correlates with human judgments of faithfulness. We collect two benchmark datasets (i.e. LLaVA-1k and MSCOCO-Cap) for evaluating LVLMs instruction-following hallucinations. We measure hallucinations in state-of-the-art LVLMs with FaithScore on the datasets. Results reveal that current systems are prone to generate hallucinated content unfaithful to the image, which leaves room for future improvements. We hope our metric FaithScore can help evaluate future LVLMs in terms of faithfulness and provide insightful advice for enhancing LVLMs' faithfulness.",
            "link": "https://www.semanticscholar.org/paper/592c14f2a0c0bb1ac7affae3cb0dfac2ccc15bcc",
            "authors": "Liqiang Jing, Ruosen Li, Yunmo Chen, Xinya Du",
            "matchScore": 275.99448,
            "original title": "FaithScore: Fine-grained Evaluations of Hallucinations in Large Vision-Language Models",
            "original authors": "Liqiang Jing, Ruosen Li, Yunmo Chen, Xinya Du",
            "EMNLP Paper ID": "989",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        }
    ],
    "Enhancing Vision-Language Models: Alignment, Generalization, and Multilingual Capabilities": [
        {
            "paperId": "00a3703fd076bddfa1ac14786f66e91ccd043140",
            "title": "ImageInWords: Unlocking Hyper-Detailed Image Descriptions",
            "abstract": "Despite the longstanding adage\"an image is worth a thousand words,\"creating accurate and hyper-detailed image descriptions for training Vision-Language models remains challenging. Current datasets typically have web-scraped descriptions that are short, low-granularity, and often contain details unrelated to the visual content. As a result, models trained on such data generate descriptions replete with missing information, visual inconsistencies, and hallucinations. To address these issues, we introduce ImageInWords (IIW), a carefully designed human-in-the-loop annotation framework for curating hyper-detailed image descriptions and a new dataset resulting from this process. We validate the framework through evaluations focused on the quality of the dataset and its utility for fine-tuning with considerations for readability, comprehensiveness, specificity, hallucinations, and human-likeness. Our dataset significantly improves across these dimensions compared to recently released datasets (+66%) and GPT-4V outputs (+48%). Furthermore, models fine-tuned with IIW data excel by +31% against prior work along the same human evaluation dimensions. Given our fine-tuned models, we also evaluate text-to-image generation and vision-language reasoning. Our model's descriptions can generate images closest to the original, as judged by both automated and human metrics. We also find our model produces more compositionally rich descriptions, outperforming the best baseline by up to 6% on ARO, SVO-Probes, and Winoground datasets.",
            "link": "https://www.semanticscholar.org/paper/00a3703fd076bddfa1ac14786f66e91ccd043140",
            "authors": "Roopal Garg, Andrea Burns, Burcu Karagol Ayan, Yonatan Bitton, Ceslee Montgomery, Yasumasa Onoe, Andrew Bunner, Ranjay Krishna, Jason Baldridge, Radu Soricut",
            "EMNLP Paper ID": "7",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "e36584cf5c53d19a2d2b888ee05cc2f7afd52693",
            "title": "Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation",
            "abstract": "Recent advances in image tokenizers, such as VQ-VAE, have enabled text-to-image generation using auto-regressive methods, similar to language modeling. However, these methods have yet to leverage pre-trained language models, despite their adaptability to various downstream tasks. In this work, we explore this gap by adapting a pre-trained language model for auto-regressive text-to-image generation, and find that pre-trained language models offer limited help. We provide a two-fold explanation by analyzing tokens from each modality. First, we demonstrate that image tokens possess significantly different semantics compared to text tokens, rendering pre-trained language models no more effective in modeling them than randomly initialized ones. Second, the text tokens in the image-text datasets are too simple compared to normal language model pre-training data, which causes the catastrophic degradation of language models' capability.",
            "link": "https://www.semanticscholar.org/paper/e36584cf5c53d19a2d2b888ee05cc2f7afd52693",
            "authors": "Yuhui Zhang, Brandon McKinzie, Zhe Gan, Vaishaal Shankar, Alexander Toshev",
            "EMNLP Paper ID": "157",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "0f5582605876d3dab492b4db6e961353f0695063",
            "title": "VIVA: A Benchmark for Vision-Grounded Decision-Making with Human Values",
            "abstract": "Large vision language models (VLMs) have demonstrated significant potential for integration into daily life, making it crucial for them to incorporate human values when making decisions in real-world situations. This paper introduces VIVA, a benchmark for VIsion-grounded decision-making driven by human VAlues. While most large VLMs focus on physical-level skills, our work is the first to examine their multimodal capabilities in leveraging human values to make decisions under a vision-depicted situation. VIVA contains 1,240 images depicting diverse real-world situations and the manually annotated decisions grounded in them. Given an image there, the model should select the most appropriate action to address the situation and provide the relevant human values and reason underlying the decision. Extensive experiments based on VIVA show the limitation of VLMs in using human values to make multimodal decisions. Further analyses indicate the potential benefits of exploiting action consequences and predicted human values.",
            "link": "https://www.semanticscholar.org/paper/0f5582605876d3dab492b4db6e961353f0695063",
            "authors": "Zhe Hu, Yixiao Ren, Jing Li, Yu Yin",
            "EMNLP Paper ID": "259",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "dcc58cfd3bacaa35f38a6476af61a66b3ef9f82e",
            "title": "Can visual language models resolve textual ambiguity with visual cues? Let visual puns tell you!",
            "abstract": "Humans possess multimodal literacy, allowing them to actively integrate information from various modalities to form reasoning. Faced with challenges like lexical ambiguity in text, we supplement this with other modalities, such as thumbnail images or textbook illustrations. Is it possible for machines to achieve a similar multimodal understanding capability? In response, we present Understanding Pun with Image Explanations (UNPIE), a novel benchmark designed to assess the impact of multimodal inputs in resolving lexical ambiguities. Puns serve as the ideal subject for this evaluation due to their intrinsic ambiguity. Our dataset includes 1,000 puns, each accompanied by an image that explains both meanings. We pose three multimodal challenges with the annotations to assess different aspects of multimodal literacy; Pun Grounding, Disambiguation, and Reconstruction. The results indicate that various Socratic Models and Visual-Language Models improve over the text-only models when given visual context, particularly as the complexity of the tasks increases.",
            "link": "https://www.semanticscholar.org/paper/dcc58cfd3bacaa35f38a6476af61a66b3ef9f82e",
            "authors": "Jiwan Chung, Seungwon Lim, Jaehyun Jeon, Seungbeen Lee, Youngjae Yu",
            "EMNLP Paper ID": "272",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "10b685acdf642c8c1534515941c5242e514ed270",
            "title": "African or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification",
            "abstract": "Recent Large Vision-Language Models (LVLMs) demonstrate impressive abilities on numerous image understanding and reasoning tasks. The task of fine-grained object classification (e.g., distinction between \\textit{animal species}), however, has been probed insufficiently, despite its downstream importance. We fill this evaluation gap by creating \\texttt{FOCI} (\\textbf{F}ine-grained \\textbf{O}bject \\textbf{C}lass\\textbf{I}fication), a difficult multiple-choice benchmark for fine-grained object classification, from existing object classification datasets: (1) multiple-choice avoids ambiguous answers associated with casting classification as open-ended QA task; (2) we retain classification difficulty by mining negative labels with a CLIP model. \\texttt{FOCI}\\xspace complements five popular classification datasets with four domain-specific subsets from ImageNet-21k. We benchmark 12 public LVLMs on \\texttt{FOCI} and show that it tests for a \\textit{complementary skill} to established image understanding and reasoning benchmarks. Crucially, CLIP models exhibit dramatically better performance than LVLMs. Since the image encoders of LVLMs come from these CLIP models, this points to inadequate alignment for fine-grained object distinction between the encoder and the LLM and warrants (pre)training data with more fine-grained annotation. We release our code at \\url{https://github.com/gregor-ge/FOCI-Benchmark}.",
            "link": "https://www.semanticscholar.org/paper/10b685acdf642c8c1534515941c5242e514ed270",
            "authors": "Gregor Geigle, R. Timofte, Goran Glavas",
            "EMNLP Paper ID": "293",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "e231a9aa85c0c26b74bfcc6b2aed46137dc60900",
            "title": "Pixology: Probing the Linguistic and Visual Capabilities of Pixel-based Language Models",
            "abstract": "Pixel-based language models have emerged as a compelling alternative to subword-based language modelling, particularly because they can represent virtually any script. PIXEL, a canonical example of such a model, is a vision transformer that has been pre-trained on rendered text. While PIXEL has shown promising cross-script transfer abilities and robustness to orthographic perturbations, it falls short of outperforming monolingual subword counterparts like BERT in most other contexts. This discrepancy raises questions about the amount of linguistic knowledge learnt by these models and whether their performance in language tasks stems more from their visual capabilities than their linguistic ones. To explore this, we probe PIXEL using a variety of linguistic and visual tasks to assess its position on the vision-to-language spectrum. Our findings reveal a substantial gap between the model's visual and linguistic understanding. The lower layers of PIXEL predominantly capture superficial visual features, whereas the higher layers gradually learn more syntactic and semantic abstractions. Additionally, we examine variants of PIXEL trained with different text rendering strategies, discovering that introducing certain orthographic constraints at the input level can facilitate earlier learning of surface-level features. With this study, we hope to provide insights that aid the further development of pixel-based language models.",
            "link": "https://www.semanticscholar.org/paper/e231a9aa85c0c26b74bfcc6b2aed46137dc60900",
            "authors": "Kushal Tatariya, Vladimir Araujo, Thomas Bauwens, Miryam de Lhoneux",
            "EMNLP Paper ID": "378",
            "Oral/Poster": "Oral",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "1f3a10478cb0b87880833a7860d216e66ebb1de6",
            "title": "World to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering",
            "abstract": "Recent advances in Vision-Language Models (VLMs) and the scarcity of high-quality multi-modal alignment data have inspired numerous researches on synthetic VLM data generation. The conventional norm in VLM data construction uses a mixture of specialists in caption and OCR, or stronger VLM APIs and expensive human annotation. In this paper, we present World to Code (W2C), a meticulously curated multi-modal data construction pipeline that organizes the final generation output into a Python code format. The pipeline leverages the VLM itself to extract cross-modal information via different prompts and filter the generated outputs again via a consistency filtering strategy. Experiments have demonstrated the high quality of W2C by improving various existing visual question answering and visual grounding benchmarks across different VLMs. Further analysis also demonstrates that the new code parsing ability of VLMs presents better cross-modal equivalence than the commonly used detail caption ability. Our code is available at https://github.com/foundation-multimodal-models/World2Code.",
            "link": "https://www.semanticscholar.org/paper/1f3a10478cb0b87880833a7860d216e66ebb1de6",
            "authors": "Jiacong Wang, Bohong Wu, Haiyong Jiang, Xun Zhou, Xin Xiao, Haoyuan Guo, Jun Xiao",
            "EMNLP Paper ID": "506",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "43d6919485cd520304b4c3572a3828278ea730e4",
            "title": "Words Worth a Thousand Pictures: Measuring and Understanding Perceptual Variability in Text-to-Image Generation",
            "abstract": "Diffusion models are the state of the art in text-to-image generation, but their perceptual variability remains understudied. In this paper, we examine how prompts affect image variability in black-box diffusion-based models. We propose W1KP, a human-calibrated measure of variability in a set of images, bootstrapped from existing image-pair perceptual distances. Current datasets do not cover recent diffusion models, thus we curate three test sets for evaluation. Our best perceptual distance outperforms nine baselines by up to 18 points in accuracy, and our calibration matches graded human judgements 78% of the time. Using W1KP, we study prompt reusability and show that Imagen prompts can be reused for 10-50 random seeds before new images become too similar to already generated images, while Stable Diffusion XL and DALL-E 3 can be reused 50-200 times. Lastly, we analyze 56 linguistic features of real prompts, finding that the prompt's length, CLIP embedding norm, concreteness, and word senses influence variability most. As far as we are aware, we are the first to analyze diffusion variability from a visuolinguistic perspective. Our project page is at http://w1kp.com",
            "link": "https://www.semanticscholar.org/paper/43d6919485cd520304b4c3572a3828278ea730e4",
            "authors": "Raphael Tang, Xinyu Crystina Zhang, Lixinyu Xu, Yao Lu, Wenyan Li, Pontus Stenetorp, Jimmy Lin, Ferhan Ture",
            "EMNLP Paper ID": "609",
            "Oral/Poster": "Oral",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "d838f425c2e5e7b0fabb4ac108fc3f57bb4a85c0",
            "title": "Benchmarking Vision Language Models for Cultural Understanding",
            "abstract": "Foundation models and vision-language pre-training have notably advanced Vision Language Models (VLMs), enabling multimodal processing of visual and linguistic data. However, their performance has been typically assessed on general scene understanding - recognizing objects, attributes, and actions - rather than cultural comprehension. This study introduces CulturalVQA, a visual question-answering benchmark aimed at assessing VLM's geo-diverse cultural understanding. We curate a collection of 2,378 image-question pairs with 1-5 answers per question representing cultures from 11 countries across 5 continents. The questions probe understanding of various facets of culture such as clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on CulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of cultural understanding across regions, with strong cultural understanding capabilities for North America while significantly lower performance for Africa. We observe disparity in their performance across cultural facets too, with clothing, rituals, and traditions seeing higher performances than food and drink. These disparities help us identify areas where VLMs lack cultural understanding and demonstrate the potential of CulturalVQA as a comprehensive evaluation set for gauging VLM progress in understanding diverse cultures.",
            "link": "https://www.semanticscholar.org/paper/d838f425c2e5e7b0fabb4ac108fc3f57bb4a85c0",
            "authors": "Shravan Nayak, Kanishk Jain, Rabiul Awal, Siva Reddy, Sjoerd van Steenkiste, Lisa Anne Hendricks, Karolina Sta'nczak, Aishwarya Agrawal",
            "EMNLP Paper ID": "640",
            "Oral/Poster": "Oral",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "c25f3d54f63a1725d31ecae60a24ddbaea7aa683",
            "title": "Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models",
            "abstract": "Recent advances in instruction-tuned Large Vision-Language Models (LVLMs) have imbued the models with the ability to generate high-level, image-grounded explanations with ease. While such capability is largely attributed to the rich world knowledge contained within the Large Language Models (LLMs), our work reveals their shortcomings in fine-grained visual categorization (FGVC) across six different benchmark settings. Most recent state-of-the-art LVLMs like LLaVa-1.5, InstructBLIP and GPT-4V not only severely deteriorate in terms of classification performance, e.g., average drop of 65.58 in EM for Stanford Dogs for LLaVA-1.5, but also struggle to generate an accurate explanation with detailed attributes based on the concept that appears within an input image despite their capability to generate holistic image-level descriptions. In-depth analyses show that instruction-tuned LVLMs exhibit modality gap, showing discrepancy when given textual and visual inputs that correspond to the same concept, preventing the image modality from leveraging the rich parametric knowledge within the LLMs. In an effort to further the community's endeavor in this direction, we propose a multiple granularity attribute-centric evaluation benchmark, Finer, which aims to establish a ground to evaluate LVLMs' fine-grained visual comprehension ability and provide significantly improved explainability.",
            "link": "https://www.semanticscholar.org/paper/c25f3d54f63a1725d31ecae60a24ddbaea7aa683",
            "authors": "Jeonghwan Kim, Heng Ji",
            "EMNLP Paper ID": "698",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "73eaadc7b2bd5e05b370405ac1fd352e95fd1973",
            "title": "VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment",
            "abstract": "As large vision-language models (LVLMs) evolve rapidly, the demand for high-quality and diverse data to align these models becomes increasingly crucial. However, the creation of such data with human supervision proves costly and time-intensive. In this paper, we investigate the efficacy of AI feedback to scale supervision for aligning LVLMs. We introduce VLFeedback, the first large-scale vision-language feedback dataset, comprising over 82K multi-modal instructions and comprehensive rationales generated by off-the-shelf models without human annotations. To evaluate the effectiveness of AI feedback for vision-language alignment, we train Silkie, an LVLM fine-tuned via direct preference optimization on VLFeedback. Silkie showcases exceptional performance regarding helpfulness, visual faithfulness, and safety metrics. It outperforms its base model by 6.9\\% and 9.5\\% in perception and cognition tasks, reduces hallucination issues on MMHal-Bench, and exhibits enhanced resilience against red-teaming attacks. Furthermore, our analysis underscores the advantage of AI feedback, particularly in fostering preference diversity to deliver more comprehensive improvements. Our dataset, training code and models are available at https://vlf-silkie.github.io.",
            "link": "https://www.semanticscholar.org/paper/73eaadc7b2bd5e05b370405ac1fd352e95fd1973",
            "authors": "Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, Lingpeng Kong, Qi Liu",
            "EMNLP Paper ID": "700",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "70e775fc042df9f4bcc1c4a55203802b2f5d9632",
            "title": "UOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models",
            "abstract": "Smaller-scale Vision-Langauge Models (VLMs) often claim to perform on par with larger models in general-domain visual grounding and question-answering benchmarks while offering advantages in computational efficiency and storage. However, their ability to handle rare objects, which fall into the long tail of data distributions, is less understood. To rigorously evaluate this aspect, we introduce the\"Uncontextualized Uncommon Objects\"(UOUO) benchmark. This benchmark focuses on systematically testing VLMs with both large and small parameter counts on rare and specialized objects. Our comprehensive analysis reveals that while smaller VLMs maintain competitive performance on common datasets, they significantly underperform on tasks involving uncommon objects. We also propose an advanced, scalable pipeline for data collection and cleaning, ensuring the UOUO benchmark provides high-quality, challenging instances. These findings highlight the need to consider long-tail distributions when assessing the true capabilities of VLMs.",
            "link": "https://www.semanticscholar.org/paper/70e775fc042df9f4bcc1c4a55203802b2f5d9632",
            "authors": "Xinyu Pi, Mingyuan Wu, Jize Jiang, Haozhen Zheng, Beitong Tian, Chengxiang Zhai, Klara Nahrstedt, Zhiting Hu",
            "EMNLP Paper ID": "722",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "5fecfeb9455af484cded3d9d48cd9a3da8277f7f",
            "title": "From Local Concepts to Universals: Evaluating the Multicultural Understanding of Vision-Language Models",
            "abstract": "Despite recent advancements in vision-language models, their performance remains suboptimal on images from non-western cultures due to underrepresentation in training datasets. Various benchmarks have been proposed to test models' cultural inclusivity, but they have limited coverage of cultures and do not adequately assess cultural diversity across universal as well as culture-specific local concepts. To address these limitations, we introduce the GlobalRG benchmark, comprising two challenging tasks: retrieval across universals and cultural visual grounding. The former task entails retrieving culturally diverse images for universal concepts from 50 countries, while the latter aims at grounding culture-specific concepts within images from 15 countries. Our evaluation across a wide range of models reveals that the performance varies significantly across cultures -- underscoring the necessity for enhancing multicultural understanding in vision-language models.",
            "link": "https://www.semanticscholar.org/paper/5fecfeb9455af484cded3d9d48cd9a3da8277f7f",
            "authors": "Mehar Bhatia, Sahithya Ravi, Aditya Chinchure, EunJeong Hwang, V. Shwartz",
            "EMNLP Paper ID": "754",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "8ad5e841ad2f7fd48c691986e001db3c2baf4c1c",
            "title": "Empowering Backbone Models for Visual Text Generation with Input Granularity Control and Glyph-Aware Training",
            "abstract": "Diffusion-based text-to-image models have demonstrated impressive achievements in diversity and aesthetics but struggle to generate images with legible visual texts. Existing backbone models have limitations such as misspelling, failing to generate texts, and lack of support for Chinese text, but their development shows promising potential. In this paper, we propose a series of methods, aiming to empower backbone models to generate visual texts in English and Chinese. We first conduct a preliminary study revealing that Byte Pair Encoding (BPE) tokenization and the insufficient learning of cross-attention modules restrict the performance of the backbone models. Based on these observations, we make the following improvements: (1) We design a mixed granularity input strategy to provide more suitable text representations; (2) We propose to augment the conventional training objective with three glyph-aware training losses, which enhance the learning of cross-attention modules and encourage the model to focus on visual texts. Through experiments, we demonstrate that our methods can effectively empower backbone models to generate semantic relevant, aesthetically appealing, and accurate visual text images, while maintaining their fundamental image generation quality.",
            "link": "https://www.semanticscholar.org/paper/8ad5e841ad2f7fd48c691986e001db3c2baf4c1c",
            "authors": "Wenbo Li, Guohao Li, Zhibin Lan, Xue Xu, Wanru Zhuang, Jiachen Liu, Xinyan Xiao, Jinsong Su",
            "EMNLP Paper ID": "912",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "75075cf3c9d5be99029b2970743f5ef726979fd4",
            "title": "Attribute Diversity Determines the Systematicity Gap in VQA",
            "abstract": "Although modern neural networks often generalize to new combinations of familiar concepts, the conditions that enable such compositionality have long been an open question. In this work, we study the systematicity gap in visual question answering: the performance difference between reasoning on previously seen and unseen combinations of object attributes. To test, we introduce a novel diagnostic dataset, CLEVR-HOPE. We find that the systematicity gap is not reduced by increasing the quantity of training data, but is reduced by increasing the diversity of training data. In particular, our experiments suggest that the more distinct attribute type combinations are seen during training, the more systematic we can expect the resulting model to be.",
            "link": "https://www.semanticscholar.org/paper/75075cf3c9d5be99029b2970743f5ef726979fd4",
            "authors": "Ian Berlot-Attwell, A. M. Carrell, Kumar Krishna Agrawal, Yash Sharma, Naomi Saphra",
            "EMNLP Paper ID": "1070",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "6073c323ad81fd776420bf3ba957eb4ce77ef445",
            "title": "VLEU: a Method for Automatic Evaluation for Generalizability of Text-to-Image Models",
            "abstract": "Progress in Text-to-Image (T2I) models has significantly improved the generation of images from textual descriptions. However, existing evaluation metrics do not adequately assess the models' ability to handle a diverse range of textual prompts, which is crucial for their generalizability. To address this, we introduce a new metric called Visual Language Evaluation Understudy (VLEU). VLEU uses large language models to sample from the visual text domain, the set of all possible input texts for T2I models, to generate a wide variety of prompts. The images generated from these prompts are evaluated based on their alignment with the input text using the CLIP model.VLEU quantifies a model's generalizability by computing the Kullback-Leibler divergence between the marginal distribution of the visual text and the conditional distribution of the images generated by the model. This metric provides a quantitative way to compare different T2I models and track improvements during model finetuning. Our experiments demonstrate the effectiveness of VLEU in evaluating the generalization capability of various T2I models, positioning it as an essential metric for future research in text-to-image synthesis.",
            "link": "https://www.semanticscholar.org/paper/6073c323ad81fd776420bf3ba957eb4ce77ef445",
            "authors": "Jingtao Cao, Zhengkun Zhang, Hongru Wang, Kam-Fai Wong",
            "EMNLP Paper ID": "1261",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "7e03326baec9b345fa90b5f12b480b268e3e6be8",
            "title": "FineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension",
            "abstract": "Referring Expression Comprehension (REC) is a crucial cross-modal task that objectively evaluates the capabilities of language understanding, image comprehension, and language-to-image grounding. Consequently, it serves as an ideal testing ground for Multi-modal Large Language Models (MLLMs). In pursuit of this goal, we have established a new REC dataset characterized by two key features: Firstly, it is designed with controllable varying levels of difficulty, necessitating multi-level fine-grained reasoning across object categories, attributes, and multi-hop relationships. Secondly, it includes negative text and images created through fine-grained editing and generation based on existing data, thereby testing the model's ability to correctly reject scenarios where the target object is not visible in the image--an essential aspect often overlooked in existing datasets and approaches. Utilizing this high-quality dataset, we conducted comprehensive evaluations of both state-of-the-art specialist models and MLLMs. Our findings indicate that there remains a significant gap in achieving satisfactory grounding performance. We anticipate that our dataset will inspire new approaches to enhance visual reasoning and develop more advanced cross-modal interaction strategies, ultimately unlocking the full potential of MLLMs. Our code and the datasets are available at https://github.com/liujunzhuo/FineCops-Ref.",
            "link": "https://www.semanticscholar.org/paper/7e03326baec9b345fa90b5f12b480b268e3e6be8",
            "authors": "Junzhuo Liu, Xuzheng Yang, Weiwei Li, Peng Wang",
            "EMNLP Paper ID": "1799",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "4898cfa615b4a7fa53387337151b0262d979a9e8",
            "title": "FoodieQA: A Multimodal Dataset for Fine-Grained Understanding of Chinese Food Culture",
            "abstract": "Food is a rich and varied dimension of cultural heritage, crucial to both individuals and social groups. To bridge the gap in the literature on the often-overlooked regional diversity in this domain, we introduce FoodieQA, a manually curated, fine-grained image-text dataset capturing the intricate features of food cultures across various regions in China. We evaluate vision-language Models (VLMs) and large language models (LLMs) on newly collected, unseen food images and corresponding questions. FoodieQA comprises three multiple-choice question-answering tasks where models need to answer questions based on multiple images, a single image, and text-only descriptions, respectively. While LLMs excel at text-based question answering, surpassing human accuracy, the open-sourced VLMs still fall short by 41% on multi-image and 21% on single-image VQA tasks, although closed-weights models perform closer to human levels (within 10%). Our findings highlight that understanding food and its cultural implications remains a challenging and under-explored direction.",
            "link": "https://www.semanticscholar.org/paper/4898cfa615b4a7fa53387337151b0262d979a9e8",
            "authors": "Wenyan Li, Xinyu Zhang, Jiaang Li, Qiwei Peng, Raphael Tang, Li Zhou, Weijia Zhang, Guimin Hu, Yifei Yuan, Anders Sogaard, Daniel Hershcovich, Desmond Elliott",
            "EMNLP Paper ID": "2404",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "12899407b9be8dac9a05a5e287cb62411adecacd",
            "title": "Grounding Language in Multi-Perspective Referential Communication",
            "abstract": "We introduce a task and dataset for referring expression generation and comprehension in multi-agent embodied environments. In this task, two agents in a shared scene must take into account one another's visual perspective, which may be different from their own, to both produce and understand references to objects in a scene and the spatial relations between them. We collect a dataset of 2,970 human-written referring expressions, each paired with human comprehension judgments, and evaluate the performance of automated models as speakers and listeners paired with human partners, finding that model performance in both reference generation and comprehension lags behind that of pairs of human agents. Finally, we experiment training an open-weight speaker model with evidence of communicative success when paired with a listener, resulting in an improvement from 58.9 to 69.3% in communicative success and even outperforming the strongest proprietary model.",
            "link": "https://www.semanticscholar.org/paper/12899407b9be8dac9a05a5e287cb62411adecacd",
            "authors": "Zineng Tang, Lingjun Mao, Alane Suhr",
            "EMNLP Paper ID": "2559",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "e06597454f3de9124b44117d14a7e03765ac9b9d",
            "title": "Unveiling the Mystery of Visual Attributes of Concrete and Abstract Concepts: Variability, Nearest Neighbors, and Challenging Categories",
            "abstract": "The visual representation of a concept varies significantly depending on its meaning and the context where it occurs; this poses multiple challenges both for vision and multimodal models. Our study focuses on concreteness, a well-researched lexical-semantic variable, using it as a case study to examine the variability in visual representations. We rely on images associated with approximately 1,000 abstract and concrete concepts extracted from two different datasets: Bing and YFCC. Our goals are: (i) evaluate whether visual diversity in the depiction of concepts can reliably distinguish between concrete and abstract concepts; (ii) analyze the variability of visual features across multiple images of the same concept through a nearest neighbor analysis; and (iii) identify challenging factors contributing to this variability by categorizing and annotating images. Our findings indicate that for classifying images of abstract versus concrete concepts, a combination of basic visual features such as color and texture is more effective than features extracted by more complex models like Vision Transformer (ViT). However, ViTs show better performances in the nearest neighbor analysis, emphasizing the need for a careful selection of visual features when analyzing conceptual variables through modalities other than text.",
            "link": "https://www.semanticscholar.org/paper/e06597454f3de9124b44117d14a7e03765ac9b9d",
            "authors": "Tarun Tater, Sabine Schulte im Walde, Diego Frassinelli",
            "EMNLP Paper ID": "2972",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "1b6f04a3cfab8804a1d16b83f1853fdd0b682865",
            "title": "Holistic Evaluation for Interleaved Text-and-Image Generation",
            "abstract": "Interleaved text-and-image generation has been an intriguing research direction, where the models are required to generate both images and text pieces in an arbitrary order. Despite the emerging advancements in interleaved generation, the progress in its evaluation still significantly lags behind. Existing evaluation benchmarks do not support arbitrarily interleaved images and text for both inputs and outputs, and they only cover a limited number of domains and use cases. Also, current works predominantly use similarity-based metrics which fall short in assessing the quality in open-ended scenarios. To this end, we introduce InterleavedBench, the first benchmark carefully curated for the evaluation of interleaved text-and-image generation. InterleavedBench features a rich array of tasks to cover diverse real-world use cases. In addition, we present InterleavedEval, a strong reference-free metric powered by GPT-4o to deliver accurate and explainable evaluation. We carefully define five essential evaluation aspects for InterleavedEval, including text quality, perceptual quality, image coherence, text-image coherence, and helpfulness, to ensure a comprehensive and fine-grained assessment. Through extensive experiments and rigorous human evaluation, we show that our benchmark and metric can effectively evaluate the existing models with a strong correlation with human judgments surpassing previous reference-based metrics. We also provide substantial findings and insights to foster future research in interleaved generation and its evaluation.",
            "link": "https://www.semanticscholar.org/paper/1b6f04a3cfab8804a1d16b83f1853fdd0b682865",
            "authors": "Minqian Liu, Zhiyang Xu, Zihao Lin, Trevor Ashby, Joy Rimchala, Jiaxin Zhang, Lifu Huang",
            "EMNLP Paper ID": "3069",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "aaa7e486cb41d0bd84af174bee80545fe3635769",
            "title": "MIBench: Evaluating Multimodal Large Language Models over Multiple Images",
            "abstract": "Built on the power of LLMs, numerous multimodal large language models (MLLMs) have recently achieved remarkable performance on various vision-language tasks. However, most existing MLLMs and benchmarks primarily focus on single-image input scenarios, leaving the performance of MLLMs when handling realistic multiple images underexplored. Although a few benchmarks consider multiple images, their evaluation dimensions and samples are very limited. In this paper, we propose a new benchmark MIBench, to comprehensively evaluate fine-grained abilities of MLLMs in multi-image scenarios. Specifically, MIBench categorizes the multi-image abilities into three scenarios: multi-image instruction (MII), multimodal knowledge-seeking (MKS) and multimodal in-context learning (MIC), and constructs 13 tasks with a total of 13K annotated samples. During data construction, for MII and MKS, we extract correct options from manual annotations and create challenging distractors to obtain multiple-choice questions. For MIC, to enable an in-depth evaluation, we set four sub-tasks and transform the original datasets into in-context learning formats. We evaluate several open-source and closed-source MLLMs on the proposed MIBench. The results reveal that although current models excel in single-image tasks, they exhibit significant shortcomings when faced with multi-image inputs, such as limited fine-grained perception, multi-image reasoning and in-context learning abilities. The annotated data of MIBench is available at https://huggingface.co/datasets/StarBottle/MIBench.",
            "link": "https://www.semanticscholar.org/paper/aaa7e486cb41d0bd84af174bee80545fe3635769",
            "authors": "Haowei Liu, Xi Zhang, Haiyang Xu, Yaya Shi, Chaoya Jiang, Mingshi Yan, Ji Zhang, Fei Huang, Chunfen Yuan, Bing Li, Weiming Hu",
            "EMNLP Paper ID": "3221",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "9eb67e6179ce1eba746a5c2d35b5187458442a95",
            "title": "Unleashing the Potentials of Likelihood Composition for Multi-modal Language Models",
            "abstract": "Model fusing has always been an important topic, especially in an era where large language models (LLM) and multi-modal language models (MLM) with different architectures, parameter sizes and training pipelines, are being created all the time. In this work, we propose a post-hoc framework, aiming at fusing heterogeneous models off-the-shell, which we call \\textit{likelihood composition}, and the basic idea is to compose multiple models' likelihood distribution when doing a multi-choice visual-question-answering task. Here the core concept, \\textit{likelihood}, is actually the log-probability of the candidate answer. In \\textit{likelihood composition}, we introduce some basic operations: \\textit{debias}, \\textit{highlight}, \\textit{majority-vote} and \\textit{ensemble}. By combining (composing) these basic elements, we get the mixed composition methods: \\textit{mix-composition}. Through conducting comprehensive experiments on 9 VQA datasets and 10 MLMs, we prove the effectiveness of \\textit{mix-composition} compared with simple \\textit{ensemble} or \\textit{majority-vote} methods. In this framework, people can propose new basic composition methods and combine them to get the new mixed composition methods. We hope our proposed \\textit{likelihood composition} can provide a new perspective of fusing heterogeneous models and inspire the exploration under this framework.",
            "link": "https://www.semanticscholar.org/paper/9eb67e6179ce1eba746a5c2d35b5187458442a95",
            "authors": "Shitian Zhao, Renrui Zhang, Xuan Luo, Yan Wang, Shanghang Zhang, Peng Gao",
            "matchScore": 238.50732,
            "original title": "Unleashing the Potentials of Likelihood Composition for Multi-modal Language Models",
            "original authors": "Shitian Zhao, Renrui Zhang, Xu Luo, Yan Wang, Shanghang Zhang, Peng Gao",
            "EMNLP Paper ID": "2083",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "1542ba4c0678bf425f3920a7b087bf99373e61a5",
            "title": "Repairing Catastrophic-Neglect in Text-to-Image Diffusion Models via Attention-Guided Feature Enhancement",
            "abstract": "Text-to-Image Diffusion Models (T2I DMs) have garnered significant attention for their ability to generate high-quality images from textual descriptions. However, these models often produce images that do not fully align with the input prompts, resulting in semantic inconsistencies. The most prominent issue among these semantic inconsistencies is catastrophic-neglect, where the images generated by T2I DMs miss key objects mentioned in the prompt. We first conduct an empirical study on this issue, exploring the prevalence of catastrophic-neglect, potential mitigation strategies with feature enhancement, and the insights gained. Guided by the empirical findings, we propose an automated repair approach named Patcher to address catastrophic-neglect in T2I DMs. Specifically, Patcher first determines whether there are any neglected objects in the prompt, and then applies attention-guided feature enhancement to these neglected objects, resulting in a repaired prompt. Experimental results on three versions of Stable Diffusion demonstrate that Patcher effectively repairs the issue of catastrophic-neglect, achieving 10.1%-16.3% higher Correct Rate in image generation compared to baselines.",
            "link": "https://www.semanticscholar.org/paper/1542ba4c0678bf425f3920a7b087bf99373e61a5",
            "authors": "Zhiyuan Chang, Mingyang Li, Junjie Wang, Yi Liu, Qing Wang, Yang Liu",
            "matchScore": 304.38495,
            "original title": "Repairing Catastrophic-Neglect in Text-to-Image Diffusion Models via Attention-Guided Feature Enhancement",
            "original authors": "Zhiyuan Chang, Mingyang Li, Junjie Wang, Yi Liu, Qing Wang, Yang Liu",
            "EMNLP Paper ID": "2251",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "55813d53793564d12b04fc58a863e4bac367389c",
            "title": "Why do LLaVA Vision-Language Models Reply to Images in English?",
            "abstract": "We uncover a surprising multilingual bias occurring in a popular class of multimodal vision-language models (VLMs). Including an image in the query to a LLaVA-style VLM significantly increases the likelihood of the model returning an English response, regardless of the language of the query. This paper investigates the causes of this loss with a two-pronged approach that combines extensive ablation of the design space with a mechanistic analysis of the models' internal representations of image and text inputs. Both approaches indicate that the issue stems in the language modelling component of the LLaVA model. Statistically, we find that switching the language backbone for a bilingual language model has the strongest effect on reducing this error. Mechanistically, we provide compelling evidence that visual inputs are not mapped to a similar space as text ones, and that intervening on intermediary attention layers can reduce this bias. Our findings provide important insights to researchers and engineers seeking to understand the crossover between multimodal and multilingual spaces, and contribute to the goal of developing capable and inclusive VLMs for non-English contexts.",
            "link": "https://www.semanticscholar.org/paper/55813d53793564d12b04fc58a863e4bac367389c",
            "authors": "Musashi Hinck, Carolin Holtermann, M. L. Olson, Florian Schneider, Sungduk Yu, Anahita Bhiwandiwalla, Anne Lauscher, Shao-Yen Tseng, Vasudev Lal",
            "matchScore": 261.25446,
            "original title": "Why do LLaVA Vision-Language Models Reply to Images in English?",
            "original authors": "Musashi Hinck, Carolin Holtermann, Matthew Lyle Olson, Florian Schneider, Sungduk Yu, Anahita Bhiwandiwalla, Anne Lauscher, Shao-Yen Tseng, Vasudev Lal",
            "EMNLP Paper ID": "2608",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "c970f116735f32465b83d883efde38e29a4d28ca",
            "title": "MVP-Bench: Can Large Vision--Language Models Conduct Multi-level Visual Perception Like Humans?",
            "abstract": "Humans perform visual perception at multiple levels, including low-level object recognition and high-level semantic interpretation such as behavior understanding. Subtle differences in low-level details can lead to substantial changes in high-level perception. For example, substituting the shopping bag held by a person with a gun suggests violent behavior, implying criminal or violent activity. Despite significant advancements in various multimodal tasks, Large Visual-Language Models (LVLMs) remain unexplored in their capabilities to conduct such multi-level visual perceptions. To investigate the perception gap between LVLMs and humans, we introduce MVP-Bench, the first visual-language benchmark systematically evaluating both low- and high-level visual perception of LVLMs. We construct MVP-Bench across natural and synthetic images to investigate how manipulated content influences model perception. Using MVP-Bench, we diagnose the visual perception of 10 open-source and 2 closed-source LVLMs, showing that high-level perception tasks significantly challenge existing LVLMs. The state-of-the-art GPT-4o only achieves an accuracy of $56\\%$ on Yes/No questions, compared with $74\\%$ in low-level scenarios. Furthermore, the performance gap between natural and manipulated images indicates that current LVLMs do not generalize in understanding the visual semantics of synthetic images as humans do. Our data and code are publicly available at https://github.com/GuanzhenLi/MVP-Bench.",
            "link": "https://www.semanticscholar.org/paper/c970f116735f32465b83d883efde38e29a4d28ca",
            "authors": "Guanzhen Li, Yuxi Xie, Min-Yen Kan",
            "matchScore": 352.42242,
            "original title": "MVP-Bench: Can Large Vision-Language Models Conduct Multi-level Visual Perception Like Humans?",
            "original authors": "Guanzhen Li, Yuxi Xie, Min-Yen Kan",
            "EMNLP Paper ID": "2625",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "e9883986681458534f408ee30192ba98f5708ce5",
            "title": "MATE: Meet At The Embedding - Connecting Images with Long Texts",
            "abstract": "While advancements in Vision Language Models (VLMs) have significantly improved the alignment of visual and textual data, these models primarily focus on aligning images with short descriptive captions. This focus limits their ability to handle complex text interactions, particularly with longer texts such as lengthy captions or documents, which have not been extensively explored yet. In this paper, we introduce Meet At The Embedding (MATE), a novel approach that combines the capabilities of VLMs with Large Language Models (LLMs) to overcome this challenge without the need for additional image-long text pairs. Specifically, we replace the text encoder of the VLM with a pretrained LLM-based encoder that excels in understanding long texts. To bridge the gap between VLM and LLM, MATE incorporates a projection module that is trained in a multi-stage manner. It starts by aligning the embeddings from the VLM text encoder with those from the LLM using extensive text pairs. This module is then employed to seamlessly align image embeddings closely with LLM embeddings. We propose two new cross-modal retrieval benchmarks to assess the task of connecting images with long texts (lengthy captions / documents). Extensive experimental results demonstrate that MATE effectively connects images with long texts, uncovering diverse semantic relationships.",
            "link": "https://www.semanticscholar.org/paper/e9883986681458534f408ee30192ba98f5708ce5",
            "authors": "Young Kyun Jang, Junmo Kang, Yong Jae Lee, Donghyun Kim",
            "matchScore": 231.58829,
            "original title": "MATE: Meet At The Embedding - Connecting Images with Long Texts",
            "original authors": "Young Kyun Jang, Junmo Kang, Yong Jae Lee, Donghyun Kim",
            "EMNLP Paper ID": "346",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Findings"
        },
        {
            "paperId": "bea1b0c578d08aa4aa72a34771150cd8c2af7d34",
            "title": "Learning from Mistakes: Iterative Prompt Relabeling for Text-to-Image Diffusion Model Training",
            "abstract": "Diffusion models have shown impressive performance in many domains. However, the model's capability to follow natural language instructions (e.g., spatial relationships between objects, generating complex scenes) is still unsatisfactory. In this work, we propose Iterative Prompt Relabeling (IPR), a novel algorithm that aligns images to text through iterative image sampling and prompt relabeling with feedback. IPR first samples a batch of images conditioned on the text, then relabels the text prompts of unmatched text-image pairs with classifier feedback. We conduct thorough experiments on SDv2 and SDXL, testing their capability to follow instructions on spatial relations. With IPR, we improved up to 15.22% (absolute improvement) on the challenging spatial relation VISOR benchmark, demonstrating superior performance compared to previous RL methods. Our code is publicly available at https://github.com/cxy000000/IPR-RLDF.",
            "link": "https://www.semanticscholar.org/paper/bea1b0c578d08aa4aa72a34771150cd8c2af7d34",
            "authors": "Xinyan Chen, Jiaxin Ge, Tianjun Zhang, Jiaming Liu, Shanghang Zhang",
            "matchScore": 291.80295,
            "original title": "Learning from Mistakes: Iterative Prompt Relabeling for Text-to-Image Diffusion Model Training",
            "original authors": "Xinyan Chen, Jiaxin Ge, Tianjun Zhang, Jiaming Liu, Shanghang Zhang",
            "EMNLP Paper ID": "592",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "677953fb705d1bb1f170dcee4ffab9824f8626de",
            "title": "M5 - A Diverse Benchmark to Assess the Performance of Large Multimodal Models Across Multilingual and Multicultural Vision-Language Tasks",
            "abstract": "Since the release of ChatGPT, the field of Natural Language Processing has experienced rapid advancements, particularly in Large Language Models (LLMs) and their multimodal counterparts, Large Multimodal Models (LMMs). Despite their impressive capabilities, LLMs often exhibit significant performance disparities across different languages and cultural contexts, as demonstrated by various text-only benchmarks. However, current research lacks such benchmarks for multimodal visio-linguistic settings. This work fills this gap by introducing M5, the first comprehensive benchmark designed to evaluate LMMs on diverse vision-language tasks within a multilingual and multicultural context. M5 includes eight datasets covering five tasks and $41$ languages, with a focus on underrepresented languages and culturally diverse images. Furthermore, we introduce two novel datasets, M5-VGR and M5-VLOD, including a new Visio-Linguistic Outlier Detection task, in which all evaluated open-source models fail to significantly surpass the random baseline. Through extensive evaluation and analyses, we highlight substantial task-agnostic performance disparities between high- and low-resource languages. Moreover, we show that larger models do not necessarily outperform smaller ones in a multilingual setting.",
            "link": "https://www.semanticscholar.org/paper/677953fb705d1bb1f170dcee4ffab9824f8626de",
            "authors": "Florian Schneider, Sunayana Sitaram",
            "matchScore": 357.40338,
            "original title": "M5 \u2013 A Diverse Benchmark to Assess the Performance of Large Multimodal Models Across Multilingual and Multicultural Vision-Language Tasks",
            "original authors": "Florian Schneider, Sunayana Sitaram",
            "EMNLP Paper ID": "865",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "f3bfdb5537af5caaf923d4d53ccc7f72ace92172",
            "title": "Navigating the Nuances: A Fine-grained Evaluation of Vision-Language Navigation",
            "abstract": "This study presents a novel evaluation framework for the Vision-Language Navigation (VLN) task. It aims to diagnose current models for various instruction categories at a finer-grained level. The framework is structured around the context-free grammar (CFG) of the task. The CFG serves as the basis for the problem decomposition and the core premise of the instruction categories design. We propose a semi-automatic method for CFG construction with the help of Large-Language Models (LLMs). Then, we induct and generate data spanning five principal instruction categories (i.e. direction change, landmark recognition, region recognition, vertical movement, and numerical comprehension). Our analysis of different models reveals notable performance discrepancies and recurrent issues. The stagnation of numerical comprehension, heavy selective biases over directional concepts, and other interesting findings contribute to the development of future language-guided navigation systems.",
            "link": "https://www.semanticscholar.org/paper/f3bfdb5537af5caaf923d4d53ccc7f72ace92172",
            "authors": "Zehao Wang, Minye Wu, Yixin Cao, Yubo Ma, Meiqi Chen, T. Tuytelaars",
            "matchScore": 264.93054,
            "original title": "Navigating the Nuances: A Fine-grained Evaluation of Vision-Language Navigation",
            "original authors": "Zehao Wang, Minye Wu, Yixin Cao, Yubo Ma, Meiqi Chen, Tinne Tuytelaars",
            "EMNLP Paper ID": "925",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        }
    ],
    "Applications and Enhancements of Large Language Models in Medical Domains": [
        {
            "paperId": "752c99713b95e14b135d6b9a64f0d27e3a82bcf8",
            "title": "An Unsupervised Approach to Achieve Supervised-Level Explainability in Healthcare Records",
            "abstract": "Electronic healthcare records are vital for patient safety as they document conditions, plans, and procedures in both free text and medical codes. Language models have significantly enhanced the processing of such records, streamlining workflows and reducing manual data entry, thereby saving healthcare providers significant resources. However, the black-box nature of these models often leaves healthcare professionals hesitant to trust them. State-of-the-art explainability methods increase model transparency but rely on human-annotated evidence spans, which are costly. In this study, we propose an approach to produce plausible and faithful explanations without needing such annotations. We demonstrate on the automated medical coding task that adversarial robustness training improves explanation plausibility and introduce AttInGrad, a new explanation method superior to previous ones. By combining both contributions in a fully unsupervised setup, we produce explanations of comparable quality, or better, to that of a supervised approach. We release our code and model weights.",
            "link": "https://www.semanticscholar.org/paper/752c99713b95e14b135d6b9a64f0d27e3a82bcf8",
            "authors": "Joakim Edin, Maria Maistro, Lars Maal\u00f8e, Lasse Borgholt, Jakob D. Havtorn, Tuukka Ruotsalo",
            "EMNLP Paper ID": "531",
            "Oral/Poster": "Oral",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "d4f15d9ffb118dacca60bbac0b4f91a0a8b0aaea",
            "title": "HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale",
            "abstract": "The rapid development of multimodal large language models (MLLMs), such as GPT-4V, has led to significant advancements. However, these models still face challenges in medical multimodal capabilities due to limitations in the quantity and quality of medical vision-text data, stemming from data privacy concerns and high annotation costs. While pioneering approaches utilize PubMed's large-scale, de-identified medical image-text pairs to address these limitations, they still fall short due to inherent data noise. To tackle this, we refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in an 'unblinded' capacity to denoise and reformat the data, resulting in the creation of the PubMedVision dataset with 1.3 million medical VQA samples. Our validation demonstrates that: (1) PubMedVision can significantly enhance the medical multimodal capabilities of current MLLMs, showing significant improvement in benchmarks including the MMMU Health&Medicine track; (2) manual checks by medical experts and empirical results validate the superior data quality of our dataset compared to other data construction methods. Using PubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows superior performance in medical multimodal scenarios among open-source MLLMs.",
            "link": "https://www.semanticscholar.org/paper/d4f15d9ffb118dacca60bbac0b4f91a0a8b0aaea",
            "authors": "Junying Chen, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, Guangjun Yu, Xiang Wan, Benyou Wang",
            "EMNLP Paper ID": "830",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "6138790da330f667f86ccac70c6e14803e5ea072",
            "title": "FEDKIM: Adaptive Federated Knowledge Injection into Medical Foundation Models",
            "abstract": "Foundation models have demonstrated remarkable capabilities in handling diverse modalities and tasks, outperforming conventional artificial intelligence (AI) approaches that are highly task-specific and modality-reliant. In the medical domain, however, the development of comprehensive foundation models is constrained by limited access to diverse modalities and stringent privacy regulations. To address these constraints, this study introduces a novel knowledge injection approach, FedKIM, designed to scale the medical foundation model within a federated learning framework. FedKIM leverages lightweight local models to extract healthcare knowledge from private data and integrates this knowledge into a centralized foundation model using a designed adaptive Multitask Multimodal Mixture Of Experts (M3OE) module. This method not only preserves privacy but also enhances the model's ability to handle complex medical tasks involving multiple modalities. Our extensive experiments across twelve tasks in seven modalities demonstrate the effectiveness of FedKIM in various settings, highlighting its potential to scale medical foundation models without direct access to sensitive data.",
            "link": "https://www.semanticscholar.org/paper/6138790da330f667f86ccac70c6e14803e5ea072",
            "authors": "Xiaochen Wang, Jiaqi Wang, Houping Xiao, Jinghui Chen, Fenglong Ma",
            "EMNLP Paper ID": "934",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "64f2abd0a47787fcb5f8773b0d737492a9d12904",
            "title": "CliMedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models in Clinical Scenarios",
            "abstract": "With the proliferation of Large Language Models (LLMs) in diverse domains, there is a particular need for unified evaluation standards in clinical medical scenarios, where models need to be examined very thoroughly. We present CliMedBench, a comprehensive benchmark with 14 expert-guided core clinical scenarios specifically designed to assess the medical ability of LLMs across 7 pivot dimensions. It comprises 33,735 questions derived from real-world medical reports of top-tier tertiary hospitals and authentic examination exercises. The reliability of this benchmark has been confirmed in several ways. Subsequent experiments with existing LLMs have led to the following findings: (i) Chinese medical LLMs underperform on this benchmark, especially where medical reasoning and factual consistency are vital, underscoring the need for advances in clinical knowledge and diagnostic accuracy. (ii) Several general-domain LLMs demonstrate substantial potential in medical clinics, while the limited input capacity of many medical LLMs hinders their practical use. These findings reveal both the strengths and limitations of LLMs in clinical scenarios and offer critical insights for medical research.",
            "link": "https://www.semanticscholar.org/paper/64f2abd0a47787fcb5f8773b0d737492a9d12904",
            "authors": "Zetian Ouyang, Yishuai Qiu, Linlin Wang, Gerard de Melo, Ya Zhang, Yanfeng Wang, Liang He",
            "EMNLP Paper ID": "974",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "fc68323cb259dced4d8fab2ac267b200f5321f5f",
            "title": "DKEC: Domain Knowledge Enhanced Multi-Label Classification for Diagnosis Prediction",
            "abstract": "Multi-label text classification (MLTC) tasks in the medical domain often face the long-tail label distribution problem. Prior works have explored hierarchical label structures to find relevant information for few-shot classes, but mostly neglected to incorporate external knowledge from medical guidelines. This paper presents DKEC, Domain Knowledge Enhanced Classification for diagnosis prediction with two innovations: (1) automated construction of heterogeneous knowledge graphs from external sources to capture semantic relations among diverse medical entities, (2) incorporating the heterogeneous knowledge graphs in few-shot classification using a label-wise attention mechanism. We construct DKEC using three online medical knowledge sources and evaluate it on a real-world Emergency Medical Services (EMS) dataset and a public electronic health record (EHR) dataset. Results show that DKEC outperforms the state-of-the-art label-wise attention networks and transformer models of different sizes, particularly for the few-shot classes. More importantly, it helps the smaller language models achieve comparable performance to large language models.",
            "link": "https://www.semanticscholar.org/paper/fc68323cb259dced4d8fab2ac267b200f5321f5f",
            "authors": "Xueren Ge, Satpathy Abhishek, Ronald D. Williams, John A. Stankovic, H. Alemzadeh",
            "EMNLP Paper ID": "1488",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "b2515168dd17240480a0462360d8e48ab2fc2639",
            "title": "Large Language Models in the Clinic: A Comprehensive Benchmark",
            "abstract": "The adoption of large language models (LLMs) to assist clinicians has attracted remarkable attention. Existing works mainly adopt the close-ended question-answering (QA) task with answer options for evaluation. However, many clinical decisions involve answering open-ended questions without pre-set options. To better understand LLMs in the clinic, we construct a benchmark ClinicBench. We first collect eleven existing datasets covering diverse clinical language generation, understanding, and reasoning tasks. Furthermore, we construct six novel datasets and clinical tasks that are complex but common in real-world practice, e.g., open-ended decision-making, long document processing, and emerging drug analysis. We conduct an extensive evaluation of twenty-two LLMs under both zero-shot and few-shot settings. Finally, we invite medical experts to evaluate the clinical usefulness of LLMs. The benchmark data is available at https://github.com/AI-in-Health/ClinicBench.",
            "link": "https://www.semanticscholar.org/paper/b2515168dd17240480a0462360d8e48ab2fc2639",
            "authors": "Andrew Liu, Hongjian Zhou, Yining Hua, Omid Rohanian, Anshul Thakur, Lei A. Clifton, David A. Clifton",
            "EMNLP Paper ID": "1579",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "2c8992e8acaeddd82df14e380022ffc9d2ef2eac",
            "title": "ERVQA: A Dataset to Benchmark the Readiness of Large Vision Language Models in Hospital Environments",
            "abstract": "The global shortage of healthcare workers has demanded the development of smart healthcare assistants, which can help monitor and alert healthcare workers when necessary. We examine the healthcare knowledge of existing Large Vision Language Models (LVLMs) via the Visual Question Answering (VQA) task in hospital settings through expert annotated open-ended questions. We introduce the Emergency Room Visual Question Answering (ERVQA) dataset, consisting oftriplets covering diverse emergency room scenarios, a seminal benchmark for LVLMs. By developing a detailed error taxonomy and analyzing answer trends, we reveal the nuanced nature of the task. We benchmark state-of-the-art open-source and closed LVLMs using traditional and adapted VQA metrics: Entailment Score and CLIPScore Confidence. Analyzing errors across models, we infer trends based on properties like decoder type, model size, and in-context examples. Our findings suggest the ERVQA dataset presents a highly complex task, highlighting the need for specialized, domain-specific solutions.",
            "link": "https://www.semanticscholar.org/paper/2c8992e8acaeddd82df14e380022ffc9d2ef2eac",
            "authors": "Sourjyadip Ray, Kushal Gupta, Soumi Kundu, P. Kasat, Somak Aditya, Pawan Goyal",
            "EMNLP Paper ID": "1828",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "e8c660658b84d6d73c8617df4eaa8149d1f74859",
            "title": "MediTOD: An English Dialogue Dataset for Medical History Taking with Comprehensive Annotations",
            "abstract": "Medical task-oriented dialogue systems can assist doctors by collecting patient medical history, aiding in diagnosis, or guiding treatment selection, thereby reducing doctor burnout and expanding access to medical services. However, doctor-patient dialogue datasets are not readily available, primarily due to privacy regulations. Moreover, existing datasets lack comprehensive annotations involving medical slots and their different attributes, such as symptoms and their onset, progression, and severity. These comprehensive annotations are crucial for accurate diagnosis. Finally, most existing datasets are non-English, limiting their utility for the larger research community. In response, we introduce MediTOD, a new dataset of doctor-patient dialogues in English for the medical history-taking task. Collaborating with doctors, we devise a questionnaire-based labeling scheme tailored to the medical domain. Then, medical professionals create the dataset with high-quality comprehensive annotations, capturing medical slots and their attributes. We establish benchmarks in supervised and few-shot settings on MediTOD for natural language understanding, policy learning, and natural language generation subtasks, evaluating models from both TOD and biomedical domains. We make MediTOD publicly available for future research.",
            "link": "https://www.semanticscholar.org/paper/e8c660658b84d6d73c8617df4eaa8149d1f74859",
            "authors": "Vishal Vivek Saley, Goonjan Saha, Rocktim Jyoti Das, Dinesh Raghu, Mausam",
            "EMNLP Paper ID": "1983",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "b8d703691dc7685fa48695a91c75e8e4215e4ab9",
            "title": "MedReadMe: A Systematic Study for Fine-grained Sentence Readability in Medical Domain",
            "abstract": "Medical texts are notoriously challenging to read. Properly measuring their readability is the first step towards making them more accessible. In this paper, we present a systematic study on fine-grained readability measurements in the medical domain at both sentence-level and span-level. We introduce a new dataset MedReadMe, which consists of manually annotated readability ratings and fine-grained complex span annotation for 4,520 sentences, featuring two novel\"Google-Easy\"and\"Google-Hard\"categories. It supports our quantitative analysis, which covers 650 linguistic features and automatic complex word and jargon identification. Enabled by our high-quality annotation, we benchmark and improve several state-of-the-art sentence-level readability metrics for the medical domain specifically, which include unsupervised, supervised, and prompting-based methods using recently developed large language models (LLMs). Informed by our fine-grained complex span annotation, we find that adding a single feature, capturing the number of jargon spans, into existing readability formulas can significantly improve their correlation with human judgments. We will publicly release the dataset and code.",
            "link": "https://www.semanticscholar.org/paper/b8d703691dc7685fa48695a91c75e8e4215e4ab9",
            "authors": "Chao Jiang, Wei Xu",
            "EMNLP Paper ID": "2047",
            "Oral/Poster": "Oral",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "f6021ada3ae58f2a9139d50d71306b3b395bb474",
            "title": "CasiMedicos-Arg: A Medical Question Answering Dataset Annotated with Explanatory Argumentative Structures",
            "abstract": "Explaining Artificial Intelligence (AI) decisions is a major challenge nowadays in AI, in particular when applied to sensitive scenarios like medicine and law. However, the need to explain the rationale behind decisions is a main issue also for human-based deliberation as it is important to justify \\textit{why} a certain decision has been taken. Resident medical doctors for instance are required not only to provide a (possibly correct) diagnosis, but also to explain how they reached a certain conclusion. Developing new tools to aid residents to train their explanation skills is therefore a central objective of AI in education. In this paper, we follow this direction, and we present, to the best of our knowledge, the first multilingual dataset for Medical Question Answering where correct and incorrect diagnoses for a clinical case are enriched with a natural language explanation written by doctors. These explanations have been manually annotated with argument components (i.e., premise, claim) and argument relations (i.e., attack, support), resulting in the Multilingual CasiMedicos-Arg dataset which consists of 558 clinical cases in four languages (English, Spanish, French, Italian) with explanations, where we annotated 5021 claims, 2313 premises, 2431 support relations, and 1106 attack relations. We conclude by showing how competitive baselines perform over this challenging dataset for the argument mining task.",
            "link": "https://www.semanticscholar.org/paper/f6021ada3ae58f2a9139d50d71306b3b395bb474",
            "authors": "Ekaterina Sviridova, Anar Yeginbergen, A. Estarrona, Elena Cabrio, S. Villata, R. Agerri",
            "EMNLP Paper ID": "2296",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "23f62dc6564f139a1298ff624b96332350594f62",
            "title": "STLLaVA-Med: Self-Training Large Language and Vision Assistant for Medical Question-Answering",
            "abstract": "Large Vision-Language Models (LVLMs) have shown significant potential in assisting medical diagnosis by leveraging extensive biomedical datasets. However, the advancement of medical image understanding and reasoning critically depends on building high-quality visual instruction data, which is costly and labor-intensive to obtain, particularly in the medical domain. To mitigate this data-starving issue, we introduce Self-Training Large Language and Vision Assistant for Medicine (STLLaVA-Med). The proposed method is designed to train a policy model (an LVLM) capable of auto-generating medical visual instruction data to improve data efficiency, guided through Direct Preference Optimization (DPO). Specifically, a more powerful and larger LVLM (e.g., GPT-4o) is involved as a biomedical expert to oversee the DPO fine-tuning process on the auto-generated data, encouraging the policy model to align efficiently with human preferences. We validate the efficacy and data efficiency of STLLaVA-Med across three major medical Visual Question Answering (VQA) benchmarks, demonstrating competitive zero-shot performance with the utilization of only 9% of the medical data.",
            "link": "https://www.semanticscholar.org/paper/23f62dc6564f139a1298ff624b96332350594f62",
            "authors": "Guohao Sun, Can Qin, Huazhu Fu, Linwei Wang, Zhiqiang Tao",
            "EMNLP Paper ID": "2615",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "7b654e072b2fa2103fd2631f70b4fcd88b48d972",
            "title": "MedAdapter: Efficient Test-Time Adaptation of Large Language Models towards Medical Reasoning",
            "abstract": "Despite their improved capabilities in generation and reasoning, adapting large language models (LLMs) to the biomedical domain remains challenging due to their immense size and corporate privacy. In this work, we propose MedAdapter, a unified post-hoc adapter for test-time adaptation of LLMs towards biomedical applications. Instead of fine-tuning the entire LLM, MedAdapter effectively adapts the original model by fine-tuning only a small BERT-sized adapter to rank candidate solutions generated by LLMs. Experiments demonstrate that MedAdapter effectively adapts both white-box and black-box LLMs in biomedical reasoning, achieving average performance improvements of 25.48% and 11.31%, respectively, without requiring extensive computational resources or sharing data with third parties. MedAdapter also yields superior performance when combined with train-time adaptation, highlighting a flexible and complementary solution to existing adaptation methods. Faced with the challenges of balancing model performance, computational resources, and data privacy, MedAdapter provides an efficient, privacy-preserving, cost-effective, and transparent solution for adapting LLMs to the biomedical domain.",
            "link": "https://www.semanticscholar.org/paper/7b654e072b2fa2103fd2631f70b4fcd88b48d972",
            "authors": "Wenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Hang Wu, Carl Yang, M. D. Wang",
            "EMNLP Paper ID": "3167",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "3721eb984e7e00a94dd9fc84e0183b2ab5991033",
            "title": "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records",
            "abstract": "Large language models (LLMs) have demonstrated exceptional capabilities in planning and tool utilization as autonomous agents, but few have been developed for medical problem-solving. We propose EHRAgent, an LLM agent empowered with a code interface, to autonomously generate and execute code for multi-tabular reasoning within electronic health records (EHRs). First, we formulate an EHR question-answering task into a tool-use planning process, efficiently decomposing a complicated task into a sequence of manageable actions. By integrating interactive coding and execution feedback, EHRAgent learns from error messages and improves the originally generated code through iterations. Furthermore, we enhance the LLM agent by incorporating long-term memory, which allows EHRAgent to effectively select and build upon the most relevant successful cases from past experiences. Experiments on three real-world multi-tabular EHR datasets show that EHRAgent outperforms the strongest baseline by up to 29.6% in success rate. EHRAgent leverages the emerging few-shot learning capabilities of LLMs, enabling autonomous code generation and execution to tackle complex clinical tasks with minimal demonstrations.",
            "link": "https://www.semanticscholar.org/paper/3721eb984e7e00a94dd9fc84e0183b2ab5991033",
            "authors": "Wenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Jieyu Zhang, Hang Wu, Yuanda Zhu, Joyce C. Ho, Carl Yang, M. D. Wang",
            "EMNLP Paper ID": "3169",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "d7fad950e9680d36a152e5c8ec3981440de89da6",
            "title": "When Raw Data Prevails: Are Large Language Model Embeddings Effective in Numerical Data Representation for Medical Machine Learning Applications?",
            "abstract": "The introduction of Large Language Models (LLMs) has advanced data representation and analysis, bringing significant progress in their use for medical questions and answering. Despite these advancements, integrating tabular data, especially numerical data pivotal in clinical contexts, into LLM paradigms has not been thoroughly explored. In this study, we examine the effectiveness of vector representations from last hidden states of LLMs for medical diagnostics and prognostics using electronic health record (EHR) data. We compare the performance of these embeddings with that of raw numerical EHR data when used as feature inputs to traditional machine learning (ML) algorithms that excel at tabular data learning, such as eXtreme Gradient Boosting. We focus on instruction-tuned LLMs in a zero-shot setting to represent abnormal physiological data and evaluating their utilities as feature extractors to enhance ML classifiers for predicting diagnoses, length of stay, and mortality. Furthermore, we examine prompt engineering techniques on zero-shot and few-shot LLM embeddings to measure their impact comprehensively. Although findings suggest the raw data features still prevails in medical ML tasks, zero-shot LLM embeddings demonstrate competitive results, suggesting a promising avenue for future research in medical applications.",
            "link": "https://www.semanticscholar.org/paper/d7fad950e9680d36a152e5c8ec3981440de89da6",
            "authors": "Yanjun Gao, Skatje Myers, Shan Chen, D. Dligach, Timothy A. Miller, Danielle S. Bitterman, M. Churpek, Majid Afshar",
            "matchScore": 358.85547,
            "original title": "When Raw Data Prevails: Are Large Language Model Embeddings Effective in Numerical Data Representation for Medical Machine Learning Applications",
            "original authors": "Yanjun Gao, Skatje Myers, Shan Chen, Dmitriy Dligach, Timothy A Miller, Danielle Bitterman, Matthew Churpek, Majid Afshar",
            "EMNLP Paper ID": "1086",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "322bdaa46ec0628c7f6fd5d4438cc16f350a4907",
            "title": "On Creating an English-Thai Code-switched Machine Translation in Medical Domain",
            "abstract": "Machine translation (MT) in the medical domain plays a pivotal role in enhancing healthcare quality and disseminating medical knowledge. Despite advancements in English-Thai MT technology, common MT approaches often underperform in the medical field due to their inability to precisely translate medical terminologies. Our research prioritizes not merely improving translation accuracy but also maintaining medical terminology in English within the translated text through code-switched (CS) translation. We developed a method to produce CS medical translation data, fine-tuned a CS translation model with this data, and evaluated its performance against strong baselines, such as Google Neural Machine Translation (NMT) and GPT-3.5/GPT-4. Our model demonstrated competitive performance in automatic metrics and was highly favored in human preference evaluations. Our evaluation result also shows that medical professionals significantly prefer CS translations that maintain critical English terms accurately, even if it slightly compromises fluency. Our code and test set are publicly available https://github.com/preceptorai-org/NLLB_CS_EM_NLP2024.",
            "link": "https://www.semanticscholar.org/paper/322bdaa46ec0628c7f6fd5d4438cc16f350a4907",
            "authors": "Parinthapat Pengpun, Krittamate Tiankanon, Amrest Chinkamol, Jiramet Kinchagawat, Pitchaya Chairuengjitjaras, Pasit Supholkhan, Pubordee Aussavavirojekul, C. Boonnag, Kanyakorn Veerakanjana, Hirunkul Phimsiri, Boonthicha Sae-jia, Nattawach Sataudom, Piyalitt Ittichaiwong, Peerat Limkonchotiwat",
            "matchScore": 244.9042,
            "original title": "On Creating an English-Thai Code-switched Machine Translation in Medical Domain",
            "original authors": "Parinthapat Pengpun, Krittamate Tiankanon, Amrest Chinkamol, Jiramet Kinchagawat, Pitchaya Chairuengjitjaras, Pasit Supholkhan, Pubordee Aussavavirojekul, Chiraphat Boonnag, Kanyakorn Veerakanjana, Hirunkul Phimsiri, Boonthicha Sae-jia, Nattawach Sataudom, Piyalitt Ittichaiwong, Peerat Limkonchotiwat",
            "EMNLP Paper ID": "1255",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Findings"
        },
        {
            "paperId": "ebe53532339772db4d79d053f7be8e385a4cd58b",
            "title": "MedINST: Meta Dataset of Biomedical Instructions",
            "abstract": "The integration of large language model (LLM) techniques in the field of medical analysis has brought about significant advancements, yet the scarcity of large, diverse, and well-annotated datasets remains a major challenge. Medical data and tasks, which vary in format, size, and other parameters, require extensive preprocessing and standardization for effective use in training LLMs. To address these challenges, we introduce MedINST, the Meta Dataset of Biomedical Instructions, a novel multi-domain, multi-task instructional meta-dataset. MedINST comprises 133 biomedical NLP tasks and over 7 million training samples, making it the most comprehensive biomedical instruction dataset to date. Using MedINST as the meta dataset, we curate MedINST32, a challenging benchmark with different task difficulties aiming to evaluate LLMs' generalization ability. We fine-tune several LLMs on MedINST and evaluate on MedINST32, showcasing enhanced cross-task generalization.",
            "link": "https://www.semanticscholar.org/paper/ebe53532339772db4d79d053f7be8e385a4cd58b",
            "authors": "Wenhan Han, Meng Fang, Zihan Zhang, Yu Yin, Zirui Song, Ling Chen, Mykola Pechenizkiy, Qingyu Chen",
            "matchScore": 209.96637,
            "original title": "MedINST: Meta Dataset of Biomedical Instructions",
            "original authors": "Wenhan Han, Meng Fang, Zihan Zhang, Yu Yin, Zirui Song, Ling Chen, Mykola Pechenizkiy, Qingyu Chen",
            "EMNLP Paper ID": "1741",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "8d36a67a6d7571f4dfd88c8f2df21f97de10eb4d",
            "title": "MMedAgent: Learning to Use Medical Tools with Multi-modal Agent",
            "abstract": "Multi-Modal Large Language Models (MLLMs), despite being successful, exhibit limited generality and often fall short when compared to specialized models. Recently, LLM-based agents have been developed to address these challenges by selecting appropriate specialized models as tools based on user inputs. However, such advancements have not been extensively explored within the medical domain. To bridge this gap, this paper introduces the first agent explicitly designed for the medical field, named \\textbf{M}ulti-modal \\textbf{Med}ical \\textbf{Agent} (MMedAgent). We curate an instruction-tuning dataset comprising six medical tools solving seven tasks across five modalities, enabling the agent to choose the most suitable tools for a given task. Comprehensive experiments demonstrate that MMedAgent achieves superior performance across a variety of medical tasks compared to state-of-the-art open-source methods and even the closed-source model, GPT-4o. Furthermore, MMedAgent exhibits efficiency in updating and integrating new medical tools. Codes and models are all available.",
            "link": "https://www.semanticscholar.org/paper/8d36a67a6d7571f4dfd88c8f2df21f97de10eb4d",
            "authors": "Binxu Li, Tiankai Yan, Yuanting Pan, Zhe Xu, Jie Luo, Ruiyang Ji, Shilong Liu, Haoyu Dong, Zihao Lin, Yixin Wang",
            "matchScore": 230.7605,
            "original title": "MMedAgent: Learning to Use Medical Tools with Multi-modal Agent",
            "original authors": "Binxu Li, Tiankai Yan, Yuanting Pan, Jie Luo, Ruiyang Ji, Jiayuan Ding, Zhe Xu, Shilong Liu, Haoyu Dong, Zihao Lin, Yixin Wang",
            "EMNLP Paper ID": "1821",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Findings"
        },
        {
            "paperId": "e36285c1c9b310cc6d5296bd4211be51f65320fa",
            "title": "ICON: Improving Inter-Report Consistency in Radiology Report Generation via Lesion-aware Mixup Augmentation",
            "abstract": "Previous research on radiology report generation has made significant progress in terms of increasing the clinical accuracy of generated reports. In this paper, we emphasize another crucial quality that it should possess, i.e., inter-report consistency, which refers to the capability of generating consistent reports for semantically equivalent radiographs. This quality is even of greater significance than the overall report accuracy in terms of ensuring the system's credibility, as a system prone to providing conflicting results would severely erode users' trust. Regrettably, existing approaches struggle to maintain inter-report consistency, exhibiting biases towards common patterns and susceptibility to lesion variants. To address this issue, we propose ICON, which improves the inter-report consistency of radiology report generation. Aiming to enhance the system's ability to capture similarities in semantically equivalent lesions, our approach first involves extracting lesions from input images and examining their characteristics. Then, we introduce a lesion-aware mixup technique to ensure that the representations of the semantically equivalent lesions align with the same attributes, achieved through a linear combination during the training phase. Extensive experiments on three publicly available chest X-ray datasets verify the effectiveness of our approach, both in terms of improving the consistency and accuracy of the generated reports.",
            "link": "https://www.semanticscholar.org/paper/e36285c1c9b310cc6d5296bd4211be51f65320fa",
            "authors": "Wenjun Hou, Yi Cheng, Kaishuai Xu, Yan Hu, Wenjie Li, Jiangming Liu",
            "matchScore": 303.60944,
            "original title": "ICON: Improving Inter-Report Consistency of Radiology Report Generation via Lesion-aware Mix-up Augmentation",
            "original authors": "Wenjun Hou, Yi Cheng, Kaishuai Xu, Yan Hu, Wenjie Li, Jiang Liu",
            "EMNLP Paper ID": "1889",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "d8fd3234ab624ffe8c9bd920144a49427ccbcc3d",
            "title": "Beyond Fine-tuning: Unleashing the Potential of Continuous Pretraining for Clinical LLMs",
            "abstract": "Large Language Models (LLMs) have demonstrated significant potential in transforming clinical applications. In this study, we investigate the efficacy of four techniques in adapting LLMs for clinical use-cases: continuous pretraining, instruct fine-tuning, NEFTune, and prompt engineering. We employ these methods on Mistral 7B and Mixtral 8x7B models, leveraging a large-scale clinical pretraining dataset of 50 billion tokens and an instruct fine-tuning dataset of 500 million tokens. Our evaluation across various clinical tasks reveals the impact of each technique. While continuous pretraining beyond 250 billion tokens yields marginal improvements on its own, it establishes a strong foundation for instruct fine-tuning. Notably, NEFTune, designed primarily to enhance generation quality, surprisingly demonstrates additional gains on our benchmark. Complex prompt engineering methods further enhance performance. These findings show the importance of tailoring fine-tuning strategies and exploring innovative techniques to optimize LLM performance in the clinical domain.",
            "link": "https://www.semanticscholar.org/paper/d8fd3234ab624ffe8c9bd920144a49427ccbcc3d",
            "authors": "Cl'ement Christophe, Tathagata Raha, Svetlana Maslenkova, Muhammad Umar Salman, Praveen K Kanithi, M. A. Pimentel, Shadab Khan",
            "matchScore": 265.51013,
            "original title": "Beyond Fine-tuning: Unleashing the Potential of Continuous Pretraining for Clinical LLMs.",
            "original authors": "Clement Christophe, Tathagata Raha, Svetlana Maslenkova, Muhammad Umar Salman, Praveenkumar Kanithi, Marco AF Pimentel, Shadab Khan",
            "EMNLP Paper ID": "2142",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "b260a9b76fa2d287d4dcaa2c5a02a02b8be05ece",
            "title": "MedCare: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation",
            "abstract": "Large language models (LLMs) have shown substantial progress in natural language understanding and generation, proving valuable especially in the medical field. Despite advancements, challenges persist due to the complexity and diversity inherent in medical tasks, which can be categorized as knowledge-intensive tasks and alignment-required tasks. Previous approaches either ignore the latter task or focus on a minority of tasks and hence lose generalization. To address these drawbacks, we propose a progressive fine-tuning pipeline. This pipeline employs a Knowledge Aggregator and a Noise aggregator to encode diverse knowledge in the first stage and filter out detrimental information. In the second stage, we drop the Noise Aggregator to avoid the interference of suboptimal representation and leverage an additional alignment module optimized towards an orthogonal direction to the knowledge space to mitigate knowledge forgetting. Based on this two-stage paradigm, we proposed a Medical LLM through decoupling Clinical Alignment and Knowledge Aggregation (MedCare), which is designed to achieve state-of-the-art (SOTA) performance on over 20 medical tasks, as well as SOTA results on specific medical alignment tasks. Various model sizes of MedCare (1.8B, 7B, 14B) all demonstrate significant improvements over existing models with similar model sizes.",
            "link": "https://www.semanticscholar.org/paper/b260a9b76fa2d287d4dcaa2c5a02a02b8be05ece",
            "authors": "Yusheng Liao, Shuyang Jiang, Yanfeng Wang, Yu Wang",
            "matchScore": 341.70703,
            "original title": "MedCare: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation",
            "original authors": "Yusheng Liao, Shuyang Jiang, Zhe Chen, Yu Wang, Yanfeng Wang",
            "EMNLP Paper ID": "2143",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "2adff5a102117c05186e05850dd5209c62cac228",
            "title": "HSDreport: Heart Sound Diagnosis with Echocardiography Reports",
            "abstract": "Heart sound auscultation holds significant importance in the diagnosis of congenital heart disease. However, existing methods for Heart Sound Diagnosis (HSD) tasks are predominantly limited to a few fixed categories, framing the HSD task as a rigid classification problem that does not fully align with medical practice and offers only limited information to physicians. Besides, such methods do not utilize echocardiography reports, the gold standard in the diagnosis of related diseases. To tackle this challenge, we introduce HSDreport, a new benchmark for HSD, which mandates the direct utilization of heart sounds obtained from auscultation to predict echocardiography reports. This benchmark aims to merge the convenience of auscultation with the comprehensive nature of echocardiography reports. First, we collect a new dataset for this benchmark, comprising 2,275 heart sound samples along with their corresponding reports. Subsequently, we develop a knowledge-aware query-based transformer to handle this task. The intent is to leverage the capabilities of medically pre-trained models and the internal knowledge of large language models (LLMs) to address the task's inherent complexity and variability, thereby enhancing the robustness and scientific validity of the method. Furthermore, our experimental results indicate that our method significantly outperforms traditional HSD approaches and existing multimodal LLMs in detecting key abnormalities in heart sounds.",
            "link": "https://www.semanticscholar.org/paper/2adff5a102117c05186e05850dd5209c62cac228",
            "authors": "Zihan Zhao, Pingjie Wang, Liudan Zhao, Yuchen Yang, Ya Zhang, Kun Sun, Xin Sun, Xin Zhou, Yu Wang, Yanfeng Wang",
            "matchScore": 218.52121,
            "original title": "HSDreport: Heart Sound Diagnosis with Echocardiography Reports",
            "original authors": "Zihan Zhao, Pingjie Wang, Liudan Zhao, Yuchen Yang, Ya Zhang, Kun Sun, Xin Sun, Xin Zhou, Yu Wang, Yanfeng Wang",
            "EMNLP Paper ID": "2250",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "39b2b3d543b01ec192cba9c7fce7c177bfc33869",
            "title": "Language Models are Surprisingly Fragile to Drug Names in Biomedical Benchmarks",
            "abstract": "Medical knowledge is context-dependent and requires consistent reasoning across various natural language expressions of semantically equivalent phrases. This is particularly crucial for drug names, where patients often use brand names like Advil or Tylenol instead of their generic equivalents. To study this, we create a new robustness dataset, RABBITS, to evaluate performance differences on medical benchmarks after swapping brand and generic drug names using physician expert annotations. We assess both open-source and API-based LLMs on MedQA and MedMCQA, revealing a consistent performance drop ranging from 1-10\\%. Furthermore, we identify a potential source of this fragility as the contamination of test data in widely used pre-training datasets. All code is accessible at https://github.com/BittermanLab/RABBITS, and a HuggingFace leaderboard is available at https://huggingface.co/spaces/AIM-Harvard/rabbits-leaderboard.",
            "link": "https://www.semanticscholar.org/paper/39b2b3d543b01ec192cba9c7fce7c177bfc33869",
            "authors": "Jack Gallifant, Shan Chen, Pedro Moreira, Nikolaj Munch, Mingye Gao, Jackson Pond, L. Celi, Hugo Aerts, Thomas Hartvigsen, Danielle S. Bitterman",
            "matchScore": 259.73236,
            "original title": "Language Models are Surprisingly Fragile to Drug Names in Biomedical Benchmarks",
            "original authors": "Jack Gallifant, Shan Chen, Pedro Jos\u00e9 Ferreira Moreira, Nikolaj Munch, Mingye Gao, Jackson Pond, Leo Anthony Celi, Hugo Aerts, Thomas Hartvigsen, Danielle Bitterman",
            "EMNLP Paper ID": "2433",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "df16e8c7599e1a5f7dbc1beebf454cd20e938a59",
            "title": "README: Bridging Medical Jargon and Lay Understanding for Patient Education through Data-Centric NLP",
            "abstract": "The advancement in healthcare has shifted focus toward patient-centric approaches, particularly in self-care and patient education, facilitated by access to Electronic Health Records (EHR). However, medical jargon in EHRs poses significant challenges in patient comprehension. To address this, we introduce a new task of automatically generating lay definitions, aiming to simplify complex medical terms into patient-friendly lay language. We first created the README dataset, an extensive collection of over 50,000 unique (medical term, lay definition) pairs and 300,000 mentions, each offering context-aware lay definitions manually annotated by domain experts. We have also engineered a data-centric Human-AI pipeline that synergizes data filtering, augmentation, and selection to improve data quality. We then used README as the training data for models and leveraged a Retrieval-Augmented Generation method to reduce hallucinations and improve the quality of model outputs. Our extensive automatic and human evaluations demonstrate that open-source mobile-friendly models, when fine-tuned with high-quality data, are capable of matching or even surpassing the performance of state-of-the-art closed-source large language models like ChatGPT. This research represents a significant stride in closing the knowledge gap in patient education and advancing patient-centric healthcare solutions.",
            "link": "https://www.semanticscholar.org/paper/df16e8c7599e1a5f7dbc1beebf454cd20e938a59",
            "authors": "Zonghai Yao, Nandyala Siddharth Kantu, Guanghao Wei, Hieu Tran, Zhangqi Duan, Sunjae Kwon, Zhichao Yang, Readme annotation team, Hong Yu",
            "matchScore": 314.35645,
            "original title": "README: Bridging Medical Jargon and Lay Understanding for Patient Education through Data-Centric NLP",
            "original authors": "Zonghai Yao, Nandyala Siddharth Kantu, Guanghao Wei, Hieu Tran, Zhangqi Duan, SUNJAE KWON, Zhichao Yang, hong yu",
            "EMNLP Paper ID": "2466",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "486047b5b14433a3ce28e190c724d5e5256542ec",
            "title": "See Detail Say Clear: Towards Brain CT Report Generation via Pathological Clue-driven Representation Learning",
            "abstract": "Brain CT report generation is significant to aid physicians in diagnosing cranial diseases. Recent studies concentrate on handling the consistency between visual and textual pathological features to improve the coherence of report. However, there exist some challenges: 1) Redundant visual representing: Massive irrelevant areas in 3D scans distract models from representing salient visual contexts. 2) Shifted semantic representing: Limited medical corpus causes difficulties for models to transfer the learned textual representations to generative layers. This study introduces a Pathological Clue-driven Representation Learning (PCRL) model to build cross-modal representations based on pathological clues and naturally adapt them for accurate report generation. Specifically, we construct pathological clues from perspectives of segmented regions, pathological entities, and report themes, to fully grasp visual pathological patterns and learn cross-modal feature representations. To adapt the representations for the text generation task, we bridge the gap between representation learning and report generation by using a unified large language model (LLM) with task-tailored instructions. These crafted instructions enable the LLM to be flexibly fine-tuned across tasks and smoothly transfer the semantic representation for report generation. Experiments demonstrate that our method outperforms previous methods and achieves SoTA performance. Our code is available at\"https://github.com/Chauncey-Jheng/PCRL-MRG\".",
            "link": "https://www.semanticscholar.org/paper/486047b5b14433a3ce28e190c724d5e5256542ec",
            "authors": "Chengxin Zheng, Junzhong Ji, Yanzhao Shi, Xiaodan Zhang, Liangqiong Qu",
            "matchScore": 410.2867,
            "original title": "See Detail Say Clear: Towards Brain CT Report Generation via Pathological Clue-driven Representation Learning",
            "original authors": "Chengxin Zheng, Junzhong Ji, Yanzhao Shi, Xiaodan Zhang, Liangqiong Qu",
            "EMNLP Paper ID": "3179",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 14): 13:00\u201314:00 (Afternoon)",
            "Type": "Findings"
        },
        {
            "paperId": "9b7985ed957b4739f0a686d66dc919aa37afe316",
            "title": "MedLogic-AQA: Enhancing Medical Question Answering with Abstractive Models Focusing on Logical Structures",
            "abstract": "In Medical question-answering (QA) tasks, the need for effective systems is pivotal in delivering accurate responses to intricate medical queries. However, existing approaches often struggle to grasp the intricate logical structures and relationships inherent in medical contexts, thus limiting their capacity to furnish precise and nuanced answers. In this work, we address this gap by proposing a novel Abstractive QA system MedLogic-AQA that harnesses First Order Logic (FOL) based rules extracted from both context and questions to generate well-grounded answers. Through initial experimentation, we identified six pertinent first-order logical rules, which were then used to train a Logic-Understanding (LU) model capable of generating logical triples for a given context, question, and answer. These logic triples are then integrated into the training of MedLogic-AQA, enabling effective and coherent reasoning during answer generation. This distinctive fusion of logical reasoning with abstractive QA equips our system to produce answers that are logically sound, relevant, and engaging. Evaluation with respect to both automated and human-based demonstrates the robustness of MedLogic-AQA against strong baselines. Through empirical assessments and case studies, we validate the efficacy of MedLogic-AQA in elevating the quality and comprehensiveness of answers in terms of reasoning as well as informativeness",
            "link": "https://www.semanticscholar.org/paper/9b7985ed957b4739f0a686d66dc919aa37afe316",
            "authors": "Aizan Zafar, Kshitij Mishra, Asif Ekbal",
            "matchScore": 251.88489,
            "original title": "MedLogic-AQA: Enhancing Medicare Question Answering with Abstractive Models Focusing on Logical Structures",
            "original authors": "Aizan Zafar, Kshitij Mishra, Asif Ekbal",
            "EMNLP Paper ID": "3232",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "2db51d392ba762af3703240325dc74df2c112b8f",
            "title": "BiMediX: Bilingual Medical Mixture of Experts LLM",
            "abstract": "In this paper, we introduce BiMediX, the first bilingual medical mixture of experts LLM designed for seamless interaction in both English and Arabic. Our model facilitates a wide range of medical interactions in English and Arabic, including multi-turn chats to inquire about additional details such as patient symptoms and medical history, multiple-choice question answering, and open-ended question answering. We propose a semi-automated English-to-Arabic translation pipeline with human refinement to ensure high-quality translations. We also introduce a comprehensive evaluation benchmark for Arabic medical LLMs. Furthermore, we introduce BiMed1.3M, an extensive Arabic-English bilingual instruction set covering 1.3 Million diverse medical interactions, resulting in over 632 million healthcare specialized tokens for instruction tuning. Our BiMed1.3M dataset includes 250k synthesized multi-turn doctor-patient chats and maintains a 1:2 Arabic-to-English ratio. Our model outperforms state-of-the-art Med42 and Meditron by average absolute gains of 2.5% and 4.1%, respectively, computed across multiple medical evaluation benchmarks in English, while operating at 8-times faster inference. Moreover, our BiMediX outperforms the generic Arabic-English bilingual LLM, Jais-30B, by average absolute gains of 10% on our Arabic medical benchmark and 15% on bilingual evaluations across multiple datasets. Our project page with source code and trained model is available at https://github.com/mbzuai-oryx/BiMediX .",
            "link": "https://www.semanticscholar.org/paper/2db51d392ba762af3703240325dc74df2c112b8f",
            "authors": "Sara Pieri, Sahal Shaji Mullappilly, F. Khan, R. Anwer, Salman H. Khan, Timothy Baldwin, Hisham Cholakkal",
            "matchScore": 231.57648,
            "original title": "BiMediX: Bilingual Medical Mixture of Experts LLM",
            "original authors": "Sara Pieri, Sahal Shaji Mullappilly, Fahad Shahbaz Khan, Rao Muhammad Anwer, Salman Khan, Timothy Baldwin, Hisham Cholakkal",
            "EMNLP Paper ID": "3247",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "d18f317e0faa60c5a229319021fde7a23944e29a",
            "title": "Augmenting Black-box LLMs with Medical Textbooks for Biomedical Question Answering (Published in Findings of EMNLP 2024)",
            "abstract": "Large-scale language models (LLMs) like ChatGPT have demonstrated impressive abilities in generating responses based on human instructions. However, their use in the medical field can be challenging due to their lack of specific, in-depth knowledge. In this study, we present a system called LLMs Augmented with Medical Textbooks (LLM-AMT) designed to enhance the proficiency of LLMs in specialized domains. LLM-AMT integrates authoritative medical textbooks into the LLMs' framework using plug-and-play modules. These modules include a Query Augmenter, a Hybrid Textbook Retriever, and a Knowledge Self-Refiner. Together, they incorporate authoritative medical knowledge. Additionally, an LLM Reader aids in contextual understanding. Our experimental results on three medical QA tasks demonstrate that LLMAMT significantly improves response quality, with accuracy gains ranging from 11.6% to 16.6%. Notably, with GPT-4-Turbo as the base model, LLM-AMT outperforms the specialized Med-PaLM 2 model pre-trained on a massive amount of medical corpus by 2-3%. We found that despite being 100x smaller in size, medical textbooks as a retrieval corpus is proven to be a more effective knowledge database than Wikipedia in the medical domain, boosting performance by 7.8%-13.7%.",
            "link": "https://www.semanticscholar.org/paper/d18f317e0faa60c5a229319021fde7a23944e29a",
            "authors": "Yubo Wang, Xueguang Ma, Wenhu Chen",
            "matchScore": 243.9562,
            "original title": "Augmenting Black-box LLMs with Medical Textbooks for Biomedical Question Answering",
            "original authors": "Yubo Wang, Xueguang Ma, Wenhu Chen",
            "EMNLP Paper ID": "361",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Findings"
        },
        {
            "paperId": "c1ca35c4dbfd8f2296436be73658eabed3c46d4b",
            "title": "GREEN: Generative Radiology Report Evaluation and Error Notation",
            "abstract": "Evaluating radiology reports is a challenging problem as factual correctness is extremely important due to the need for accurate medical communication about medical images. Existing automatic evaluation metrics either suffer from failing to consider factual correctness (e.g., BLEU and ROUGE) or are limited in their interpretability (e.g., F1CheXpert and F1RadGraph). In this paper, we introduce GREEN (Generative Radiology Report Evaluation and Error Notation), a radiology report generation metric that leverages the natural language understanding of language models to identify and explain clinically significant errors in candidate reports, both quantitatively and qualitatively. Compared to current metrics, GREEN offers: 1) a score aligned with expert preferences, 2) human interpretable explanations of clinically significant errors, enabling feedback loops with end-users, and 3) a lightweight open-source method that reaches the performance of commercial counterparts. We validate our GREEN metric by comparing it to GPT-4, as well as to error counts of 6 experts and preferences of 2 experts. Our method demonstrates not only higher correlation with expert error counts, but simultaneously higher alignment with expert preferences when compared to previous approaches.\"",
            "link": "https://www.semanticscholar.org/paper/c1ca35c4dbfd8f2296436be73658eabed3c46d4b",
            "authors": "Sophie Ostmeier, Justin Xu, Zhihong Chen, Maya Varma, Louis Blankemeier, Christian Bluethgen, Arne Edward Michalson, Michael E. Moseley, Curtis P. Langlotz, Akshay S. Chaudhari, Jean-Benoit Delbrouck",
            "matchScore": 242.93607,
            "original title": "GREEN: Generative Radiology Report Evaluation and Error Notation",
            "original authors": "Sophie Ostmeier, Justin Xu, Zhihong Chen, Maya Varma, Louis Blankemeier, Christian Bluethgen, Arne Edward Michalson MD, Michael Moseley, Curtis Langlotz, Akshay S Chaudhari, Jean-Benoit Delbrouck",
            "EMNLP Paper ID": "61",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "5c0a82558db7a885bf6b174c2ed4e52de558a000",
            "title": "Med-MoE: Mixture of Domain-Specific Experts for Lightweight Medical Vision-Language Models",
            "abstract": "Recent advancements in general-purpose or domain-specific multimodal large language models (LLMs) have witnessed remarkable progress for medical decision-making. However, they are designated for specific classification or generative tasks, and require model training or finetuning on large-scale datasets with sizeable parameters and tremendous computing, hindering their clinical utility across diverse resource-constrained scenarios in practice. In this paper, we propose a novel and lightweight framework Med-MoE (Mixture-of-Experts) that tackles both discriminative and generative multimodal medical tasks. The learning of Med-MoE consists of three steps: multimodal medical alignment, instruction tuning and routing, and domain-specific MoE tuning. After aligning multimodal medical images with LLM tokens, we then enable the model for different multimodal medical tasks with instruction tuning, together with a trainable router tailored for expert selection across input modalities. Finally, the model is tuned by integrating the router with multiple domain-specific experts, which are selectively activated and further empowered by meta expert. Comprehensive experiments on both open- and close-end medical question answering (Med-VQA) and image classification tasks across datasets such as VQA-RAD, SLAKE and Path-VQA demonstrate that our model can achieve performance superior to or on par with state-of-the-art baselines, while only requiring approximately 30\\%-50\\% of activated model parameters. Extensive analysis and ablations corroborate the effectiveness and practical utility of our method.",
            "link": "https://www.semanticscholar.org/paper/5c0a82558db7a885bf6b174c2ed4e52de558a000",
            "authors": "Songtao Jiang, Tuo Zheng, Yan Zhang, Yeying Jin, Li Yuan, Zuozhu Liu",
            "matchScore": 287.5643,
            "original title": "Med-MoE: Mixture of Domain-Specific Experts for Lightweight Medical Vision-Language Models",
            "original authors": "Songtao Jiang, Tuo zheng, Yan Zhang, YEYING JIN, Li Yuan, Zuozhu Liu",
            "EMNLP Paper ID": "767",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "e9e1e00e73c751e5aa7bc0d3583a0f9153df35ef",
            "title": "Geneverse: A collection of Open-source Multimodal Large Language Models for Genomic and Proteomic Research",
            "abstract": "The applications of large language models (LLMs) are promising for biomedical and healthcare research. Despite the availability of open-source LLMs trained using a wide range of biomedical data, current research on the applications of LLMs to genomics and proteomics is still limited. To fill this gap, we propose a collection of finetuned LLMs and multimodal LLMs (MLLMs), known as Geneverse, for three novel tasks in genomic and proteomic research. The models in Geneverse are trained and evaluated based on domain-specific datasets, and we use advanced parameter-efficient finetuning techniques to achieve the model adaptation for tasks including the generation of descriptions for gene functions, protein function inference from its structure, and marker gene selection from spatial transcriptomic data. We demonstrate that adapted LLMs and MLLMs perform well for these tasks and may outperform closed-source large-scale models based on our evaluations focusing on both truthfulness and structural correctness. All of the training strategies and base models we used are freely accessible.",
            "link": "https://www.semanticscholar.org/paper/e9e1e00e73c751e5aa7bc0d3583a0f9153df35ef",
            "authors": "Tianyu Liu, Yijia Xiao, Xiao Luo, Hua Xu, W. Zheng, Hongyu Zhao",
            "matchScore": 289.9174,
            "original title": "Geneverse: A Collection of Open-source Multimodal Large Language Models for Genomic and Proteomic Research",
            "original authors": "Tianyu Liu, Yijia Xiao, Xiao Luo, Hua Xu, Wenjin Zheng, Hongyu Zhao",
            "EMNLP Paper ID": "944",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        }
    ],
    "Large Language Models in NLP Data Management and Evaluation": [
        {
            "paperId": "123871c749f6eeafc151cc6ab476f104aa32ffa0",
            "title": "Multi-News+: Cost-efficient Dataset Cleansing via LLM-based Data Annotation",
            "abstract": "The quality of the dataset is crucial for ensuring optimal performance and reliability of downstream task models. However, datasets often contain noisy data inadvertently included during the construction process. Numerous attempts have been made to correct this issue through human annotators. However, hiring and managing human annotators is expensive and time-consuming. As an alternative, recent studies are exploring the use of large language models (LLMs) for data annotation. In this study, we present a case study that extends the application of LLM-based data annotation to enhance the quality of existing datasets through a cleansing strategy. Specifically, we leverage approaches such as chain-of-thought and majority voting to imitate human annotation and classify unrelated documents from the Multi-News dataset, which is widely used for the multi-document summarization task. Through our proposed cleansing method, we introduce an enhanced Multi-News+. By employing LLMs for data cleansing, we demonstrate an efficient and effective approach to improving dataset quality without relying on expensive human annotation efforts.",
            "link": "https://www.semanticscholar.org/paper/123871c749f6eeafc151cc6ab476f104aa32ffa0",
            "authors": "Juhwan Choi, Jungmin Yun, Kyohoon Jin, Youngbin Kim",
            "EMNLP Paper ID": "3",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "2df14dafc8486ff51575f8327159da1a021054b5",
            "title": "Large Language Models for Data Annotation: A Survey",
            "abstract": "Data annotation generally refers to the labeling or generating of raw data with relevant information, which could be used for improving the efficacy of machine learning models. The process, however, is labor-intensive and costly. The emergence of advanced Large Language Models (LLMs), exemplified by GPT-4, presents an unprecedented opportunity to automate the complicated process of data annotation. While existing surveys have extensively covered LLM architecture, training, and general applications, we uniquely focus on their specific utility for data annotation. This survey contributes to three core aspects: LLM-Based Annotation Generation, LLM-Generated Annotations Assessment, and LLM-Generated Annotations Utilization. Furthermore, this survey includes an in-depth taxonomy of data types that LLMs can annotate, a comprehensive review of learning strategies for models utilizing LLM-generated annotations, and a detailed discussion of the primary challenges and limitations associated with using LLMs for data annotation. Serving as a key guide, this survey aims to assist researchers and practitioners in exploring the potential of the latest LLMs for data annotation, thereby fostering future advancements in this critical field.",
            "link": "https://www.semanticscholar.org/paper/2df14dafc8486ff51575f8327159da1a021054b5",
            "authors": "Zhen Tan, Dawei Li, Alimohammad Beigi, Song Wang, Ruocheng Guo, Amrita Bhattacharjee, Bohan Jiang, Mansooreh Karami, Jundong Li, Lu Cheng, Huan Liu",
            "EMNLP Paper ID": "124",
            "Oral/Poster": "Oral",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "885eb9e8c57ebfdf9db1ed8cd757f4b56dd4635e",
            "title": "DecorateLM: Data Engineering through Corpus Rating, Tagging, and Editing with Language Models",
            "abstract": "The performance of Large Language Models (LLMs) is substantially influenced by the pretraining corpus, which consists of vast quantities of unsupervised data processed by the models. Despite its critical role in model performance, ensuring the quality of this data is challenging due to its sheer volume and the absence of sample-level quality annotations and enhancements. In this paper, we introduce DecorateLM, a data engineering method designed to refine the pretraining corpus through data rating, tagging and editing. Specifically, DecorateLM rates texts against quality criteria, tags texts with hierarchical labels, and edits texts into a more formalized format. Due to the massive size of the pretraining corpus, adopting an LLM for decorating the entire corpus is less efficient. Therefore, to balance performance with efficiency, we curate a meticulously annotated training corpus for DecorateLM using a large language model and distill data engineering expertise into a compact 1.2 billion parameter small language model (SLM). We then apply DecorateLM to enhance 100 billion tokens of the training corpus, selecting 45 billion tokens that exemplify high quality and diversity for the further training of another 1.2 billion parameter LLM. Our results demonstrate that employing such high-quality data can significantly boost model performance, showcasing a powerful approach to enhance the quality of the pretraining corpus.",
            "link": "https://www.semanticscholar.org/paper/885eb9e8c57ebfdf9db1ed8cd757f4b56dd4635e",
            "authors": "Ranchi Zhao, Zhen Leng Thai, Yifan Zhang, Shengding Hu, Yunqi Ba, Jie Zhou, Jie Cai, Zhiyuan Liu, Maosong Sun",
            "EMNLP Paper ID": "167",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "472251f534ddfae14d163874c7114a1894156efd",
            "title": "Large Language Model as an Assignment Evaluator: Insights, Feedback, and Challenges in a 1000+ Student Course",
            "abstract": "Using large language models (LLMs) for automatic evaluation has become an important evaluation method in NLP research. However, it is unclear whether these LLM-based evaluators can be applied in real-world classrooms to assess student assignments. This empirical report shares how we use GPT-4 as an automatic assignment evaluator in a university course with 1,028 students. Based on student responses, we find that LLM-based assignment evaluators are generally acceptable to students when students have free access to these LLM-based evaluators. However, students also noted that the LLM sometimes fails to adhere to the evaluation instructions. Additionally, we observe that students can easily manipulate the LLM-based evaluator to output specific strings, allowing them to achieve high scores without meeting the assignment rubric. Based on student feedback and our experience, we provide several recommendations for integrating LLM-based evaluators into future classrooms. Our observation also highlights potential directions for improving LLM-based evaluators, including their instruction-following ability and vulnerability to prompt hacking.",
            "link": "https://www.semanticscholar.org/paper/472251f534ddfae14d163874c7114a1894156efd",
            "authors": "Cheng-Han Chiang, Wei-Chih Chen, Chun-Yi Kuan, Chienchou Yang, Hung-yi Lee",
            "EMNLP Paper ID": "277",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "d21a201d38195dc07682239f3b5c2d2ceed1675b",
            "title": "How Hard is this Test Set? NLI Characterization by Exploiting Training Dynamics",
            "abstract": "Natural Language Inference (NLI) evaluation is crucial for assessing language understanding models; however, popular datasets suffer from systematic spurious correlations that artificially inflate actual model performance. To address this, we propose a method for the automated creation of a challenging test set without relying on the manual construction of artificial and unrealistic examples. We categorize the test set of popular NLI datasets into three difficulty levels by leveraging methods that exploit training dynamics. This categorization significantly reduces spurious correlation measures, with examples labeled as having the highest difficulty showing markedly decreased performance and encompassing more realistic and diverse linguistic phenomena. When our characterization method is applied to the training set, models trained with only a fraction of the data achieve comparable performance to those trained on the full dataset, surpassing other dataset characterization techniques. Our research addresses limitations in NLI dataset construction, providing a more authentic evaluation of model performance with implications for diverse NLU applications.",
            "link": "https://www.semanticscholar.org/paper/d21a201d38195dc07682239f3b5c2d2ceed1675b",
            "authors": "Adrian Cosma, Stefan Ruseti, Mihai Dascalu, Cornelia Caragea",
            "EMNLP Paper ID": "337",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "2f31b29e5901ad224cb94e20e833aee9e0e8331e",
            "title": "Quality Matters: Evaluating Synthetic Data for Tool-Using LLMs",
            "abstract": "Training large language models (LLMs) for external tool usage is a rapidly expanding field, with recent research focusing on generating synthetic data to address the shortage of available data. However, the absence of systematic data quality checks poses complications for properly training and testing models. To that end, we propose two approaches for assessing the reliability of data for training LLMs to use external tools. The first approach uses intuitive, human-defined correctness criteria. The second approach uses a model-driven assessment with in-context evaluation. We conduct a thorough evaluation of data quality on two popular benchmarks, followed by an extrinsic evaluation that showcases the impact of data quality on model performance. Our results demonstrate that models trained on high-quality data outperform those trained on unvalidated data, even when trained with a smaller quantity of data. These findings empirically support the significance of assessing and ensuring the reliability of training data for tool-using LLMs.",
            "link": "https://www.semanticscholar.org/paper/2f31b29e5901ad224cb94e20e833aee9e0e8331e",
            "authors": "Shadi Iskander, Nachshon Cohen, Zohar Karnin, Ori Shapira, Sofia Tolmach",
            "EMNLP Paper ID": "544",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "f2c7f69177d7972510e7a026c7fa20c26905a3df",
            "title": "LLMs Assist NLP Researchers: Critique Paper (Meta-)Reviewing",
            "abstract": "This work is motivated by two key trends. On one hand, large language models (LLMs) have shown remarkable versatility in various generative tasks such as writing, drawing, and question answering, significantly reducing the time required for many routine tasks. On the other hand, researchers, whose work is not only time-consuming but also highly expertise-demanding, face increasing challenges as they have to spend more time reading, writing, and reviewing papers. This raises the question: how can LLMs potentially assist researchers in alleviating their heavy workload? This study focuses on the topic of LLMs assist NLP Researchers, particularly examining the effectiveness of LLM in assisting paper (meta-)reviewing and its recognizability. To address this, we constructed the ReviewCritique dataset, which includes two types of information: (i) NLP papers (initial submissions rather than camera-ready) with both human-written and LLM-generated reviews, and (ii) each review comes with\"deficiency\"labels and corresponding explanations for individual segments, annotated by experts. Using ReviewCritique, this study explores two threads of research questions: (i)\"LLMs as Reviewers\", how do reviews generated by LLMs compare with those written by humans in terms of quality and distinguishability? (ii)\"LLMs as Metareviewers\", how effectively can LLMs identify potential issues, such as Deficient or unprofessional review segments, within individual paper reviews? To our knowledge, this is the first work to provide such a comprehensive analysis.",
            "link": "https://www.semanticscholar.org/paper/f2c7f69177d7972510e7a026c7fa20c26905a3df",
            "authors": "Jiangshu Du, Yibo Wang, Wenting Zhao, Zhongfen Deng, Shuaiqi Liu, Renze Lou, Henry Peng Zou, Pranav Narayanan Venkit, Nan Zhang, Mukund Srinath, Haoran Zhang, Vipul Gupta, Yinghui Li, Tao Li, Fei Wang, Qin Liu, Tianlin Liu, Pengzhi Gao, Congying Xia, Chen Xing, Jiayang Cheng, Zhaowei Wang, Yingjing Su, Raj Sanjay Shah, Ruohao Guo, Jing Gu, Haoran Li, Kang Wei, Zihao Wang, Lu Cheng, S. Ranathunga, Meng Fang, Jieyi Fu, Fei Liu, Rui Huang, Eduardo Blanco, Yixin Cao, Rui Zhang, Philip S. Yu, Wenpeng Yin",
            "EMNLP Paper ID": "564",
            "Oral/Poster": "Oral",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "3bc7877cc7e49af87f0b3f2e525aace6b49d0bd1",
            "title": "PARIKSHA : A Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data",
            "abstract": "Evaluation of multilingual Large Language Models (LLMs) is challenging due to a variety of factors -- the lack of benchmarks with sufficient linguistic diversity, contamination of popular benchmarks into LLM pre-training data and the lack of local, cultural nuances in translated benchmarks. In this work, we study human and LLM-based evaluation in a multilingual, multi-cultural setting. We evaluate 30 models across 10 Indic languages by conducting 90K human evaluations and 30K LLM-based evaluations and find that models such as GPT-4o and Llama-3 70B consistently perform best for most Indic languages. We build leaderboards for two evaluation settings - pairwise comparison and direct assessment and analyze the agreement between humans and LLMs. We find that humans and LLMs agree fairly well in the pairwise setting but the agreement drops for direct assessment evaluation especially for languages such as Bengali and Odia. We also check for various biases in human and LLM-based evaluation and find evidence of self-bias in the GPT-based evaluator. Our work presents a significant step towards scaling up multilingual evaluation of LLMs.",
            "link": "https://www.semanticscholar.org/paper/3bc7877cc7e49af87f0b3f2e525aace6b49d0bd1",
            "authors": "Ishaan Watts, Varun Gumma, Aditya Yadavalli, Vivek Seshadri, Manohar Swaminathan, Sunayana Sitaram",
            "EMNLP Paper ID": "906",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "c7d43357593ec96c4a18845a413ffe5073a47589",
            "title": "Efficient Performance Tracking: Leveraging Large Language Models for Automated Construction of Scientific Leaderboards",
            "abstract": "Scientific leaderboards are standardized ranking systems that facilitate evaluating and comparing competitive methods. Typically, a leaderboard is defined by a task, dataset, and evaluation metric (TDM) triple, allowing objective performance assessment and fostering innovation through benchmarking. However, the exponential increase in publications has made it infeasible to construct and maintain these leaderboards manually. Automatic leaderboard construction has emerged as a solution to reduce manual labor. Existing datasets for this task are based on the community-contributed leaderboards without additional curation. Our analysis shows that a large portion of these leaderboards are incomplete, and some of them contain incorrect information. In this work, we present SciLead, a manually-curated Scientific Leaderboard dataset that overcomes the aforementioned problems. Building on this dataset, we propose three experimental settings that simulate real-world scenarios where TDM triples are fully defined, partially defined, or undefined during leaderboard construction. While previous research has only explored the first setting, the latter two are more representative of real-world applications. To address these diverse settings, we develop a comprehensive LLM-based framework for constructing leaderboards. Our experiments and analysis reveal that various LLMs often correctly identify TDM triples while struggling to extract result values from publications. We make our code and data publicly available.",
            "link": "https://www.semanticscholar.org/paper/c7d43357593ec96c4a18845a413ffe5073a47589",
            "authors": "Furkan cSahinucc, Thy Thy Tran, Yulia Grishina, Yufang Hou, Bei Chen, Iryna Gurevych",
            "EMNLP Paper ID": "910",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "3dd5ad34012164c4ec9c571a12cc6a7561683dea",
            "title": "A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery",
            "abstract": "In many scientific fields, large language models (LLMs) have revolutionized the way text and other modalities of data (e.g., molecules and proteins) are handled, achieving superior performance in various applications and augmenting the scientific discovery process. Nevertheless, previous surveys on scientific LLMs often concentrate on one or two fields or a single modality. In this paper, we aim to provide a more holistic view of the research landscape by unveiling cross-field and cross-modal connections between scientific LLMs regarding their architectures and pre-training techniques. To this end, we comprehensively survey over 260 scientific LLMs, discuss their commonalities and differences, as well as summarize pre-training datasets and evaluation tasks for each field and modality. Moreover, we investigate how LLMs have been deployed to benefit scientific discovery. Resources related to this survey are available at https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models.",
            "link": "https://www.semanticscholar.org/paper/3dd5ad34012164c4ec9c571a12cc6a7561683dea",
            "authors": "Yu Zhang, Xiusi Chen, Bowen Jin, Sheng Wang, Shuiwang Ji, Wei Wang, Jiawei Han",
            "EMNLP Paper ID": "1007",
            "Oral/Poster": "Oral",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "205bd8f061f0adf67f953a48942981fe44e31062",
            "title": "SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading",
            "abstract": "With the rapid development of Large Language Models (LLMs), it is crucial to have benchmarks which can evaluate the ability of LLMs on different domains. One common use of LLMs is performing tasks on scientific topics, such as writing algorithms, querying databases or giving mathematical proofs. Inspired by the way university students are evaluated on such tasks, in this paper, we propose SciEx - a benchmark consisting of university computer science exam questions, to evaluate LLMs ability on solving scientific tasks. SciEx is (1) multilingual, containing both English and German exams, and (2) multi-modal, containing questions that involve images, and (3) contains various types of freeform questions with different difficulty levels, due to the nature of university exams. We evaluate the performance of various state-of-the-art LLMs on our new benchmark. Since SciEx questions are freeform, it is not straightforward to evaluate LLM performance. Therefore, we provide human expert grading of the LLM outputs on SciEx. We show that the free-form exams in SciEx remain challenging for the current LLMs, where the best LLM only achieves 59.4\\% exam grade on average. We also provide detailed comparisons between LLM performance and student performance on SciEx. To enable future evaluation of new LLMs, we propose using LLM-as-a-judge to grade the LLM answers on SciEx. Our experiments show that, although they do not perform perfectly on solving the exams, LLMs are decent as graders, achieving 0.948 Pearson correlation with expert grading.",
            "link": "https://www.semanticscholar.org/paper/205bd8f061f0adf67f953a48942981fe44e31062",
            "authors": "Tu Anh Dinh, Carlos Mullov, Leonard Barmann, Zhaolin Li, Danni Liu, Simon Rei\u00df, Jueun Lee, Nathan Lerzer, Fabian Ternava, Jianfeng Gao, Alexander Waibel, Tamim Asfour, Michael Beigl, Rainer Stiefelhagen, Carsten Dachsbacher, Klemens Bohm, Jan Niehues",
            "EMNLP Paper ID": "1350",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "4055e37ec2f1dd6c883001d12181a7131010882d",
            "title": "\"A good pun is its own reword\": Can Large Language Models Understand Puns?",
            "abstract": "Puns play a vital role in academic research due to their distinct structure and clear definition, which aid in the comprehensive analysis of linguistic humor. However, the understanding of puns in large language models (LLMs) has not been thoroughly examined, limiting their use in creative writing and humor creation. In this paper, we leverage three popular tasks, i.e., pun recognition, explanation and generation to systematically evaluate the capabilities of LLMs in pun understanding. In addition to adopting the automated evaluation metrics from prior research, we introduce new evaluation methods and metrics that are better suited to the in-context learning paradigm of LLMs. These new metrics offer a more rigorous assessment of an LLM's ability to understand puns and align more closely with human cognition than previous metrics. Our findings reveal the\"lazy pun generation\"pattern and identify the primary challenges LLMs encounter in understanding puns.",
            "link": "https://www.semanticscholar.org/paper/4055e37ec2f1dd6c883001d12181a7131010882d",
            "authors": "Zhijun Xu, Siyu Yuan, Lingjie Chen, Deqing Yang",
            "EMNLP Paper ID": "1366",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "df61f25e1f77163d82b8e52183b629404d535b88",
            "title": "A Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations",
            "abstract": "Large Language Models (LLMs) have recently gained significant attention due to their remarkable capabilities in performing diverse tasks across various domains. However, a thorough evaluation of these models is crucial before deploying them in real-world applications to ensure they produce reliable performance. Despite the well-established importance of evaluating LLMs in the community, the complexity of the evaluation process has led to varied evaluation setups, causing inconsistencies in findings and interpretations. To address this, we systematically review the primary challenges and limitations causing these inconsistencies and unreliable evaluations in various steps of LLM evaluation. Based on our critical review, we present our perspectives and recommendations to ensure LLM evaluations are reproducible, reliable, and robust.",
            "link": "https://www.semanticscholar.org/paper/df61f25e1f77163d82b8e52183b629404d535b88",
            "authors": "Md Tahmid Rahman Laskar, Sawsan Alqahtani, M Saiful Bari, Mizanur Rahman, Mohammad Abdullah Matin Khan, Haidar Khan, Israt Jahan, Amran Bhuiyan, Chee Wei Tan, Md. Rizwan Parvez, Enamul Hoque, Shafiq R. Joty, Jimmy X. Huang",
            "EMNLP Paper ID": "1591",
            "Oral/Poster": "Oral",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "4c42d8077db4e09936818aed0efa9dff5cf23e5d",
            "title": "Related Work and Citation Text Generation: A Survey",
            "abstract": "To convince readers of the novelty of their research paper, authors must perform a literature review and compose a coherent story that connects and relates prior works to the current work. This challenging nature of literature review writing makes automatic related work generation (RWG) academically and computationally interesting, and also makes it an excellent test bed for examining the capability of SOTA natural language processing (NLP) models. Since the initial proposal of the RWG task, its popularity has waxed and waned, following the capabilities of mainstream NLP approaches. In this work, we survey the zoo of RWG historical works, summarizing the key approaches and task definitions and discussing the ongoing challenges of RWG.",
            "link": "https://www.semanticscholar.org/paper/4c42d8077db4e09936818aed0efa9dff5cf23e5d",
            "authors": "Xiangci Li, Jessica Ouyang",
            "EMNLP Paper ID": "1598",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "e1ac3210172e4e2d0792b6f9ee9c5711e1f249ab",
            "title": "Human-LLM Hybrid Text Answer Aggregation for Crowd Annotations",
            "abstract": "The quality is a crucial issue for crowd annotations. Answer aggregation is an important type of solution. The aggregated answers estimated from multiple crowd answers to the same instance are the eventually collected annotations, rather than the individual crowd answers themselves. Recently, the capability of Large Language Models (LLMs) on data annotation tasks has attracted interest from researchers. Most of the existing studies mainly focus on the average performance of individual crowd workers; several recent works studied the scenarios of aggregation on categorical labels and LLMs used as label creators. However, the scenario of aggregation on text answers and the role of LLMs as aggregators are not yet well-studied. In this paper, we investigate the capability of LLMs as aggregators in the scenario of close-ended crowd text answer aggregation. We propose a human-LLM hybrid text answer aggregation method with a Creator-Aggregator Multi-Stage (CAMS) crowdsourcing framework. We make the experiments based on public crowdsourcing datasets. The results show the effectiveness of our approach based on the collaboration of crowd workers and LLMs.",
            "link": "https://www.semanticscholar.org/paper/e1ac3210172e4e2d0792b6f9ee9c5711e1f249ab",
            "authors": "Jiyi Li",
            "EMNLP Paper ID": "1833",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "cf77c76e89ccdc99b6fb687bb2d3639c02551ea8",
            "title": "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges",
            "abstract": "In the rapidly evolving domain of Natural Language Generation (NLG) evaluation, introducing Large Language Models (LLMs) has opened new avenues for assessing generated content quality, e.g., coherence, creativity, and context relevance. This paper aims to provide a thorough overview of leveraging LLMs for NLG evaluation, a burgeoning area that lacks a systematic analysis. We propose a coherent taxonomy for organizing existing LLM-based evaluation metrics, offering a structured framework to understand and compare these methods. Our detailed exploration includes critically assessing various LLM-based methodologies, as well as comparing their strengths and limitations in evaluating NLG outputs. By discussing unresolved challenges, including bias, robustness, domain-specificity, and unified evaluation, this paper seeks to offer insights to researchers and advocate for fairer and more advanced NLG evaluation techniques.",
            "link": "https://www.semanticscholar.org/paper/cf77c76e89ccdc99b6fb687bb2d3639c02551ea8",
            "authors": "Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, Yuxuan Lai, Chongyang Tao, Shuai Ma",
            "EMNLP Paper ID": "1886",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "384f91e20bd3516b35e564c7f9b43ddd46656b86",
            "title": "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists",
            "abstract": "Large Language Models (LLMs) are increasingly relied upon to evaluate text outputs of other LLMs, thereby influencing leaderboards and development decisions. However, concerns persist over the accuracy of these assessments and the potential for misleading conclusions. In this work, we investigate the effectiveness of LLMs as evaluators for text generation tasks. We propose FBI, a novel framework designed to examine the proficiency of Evaluator LLMs in assessing four critical abilities in other LLMs: factual accuracy, instruction following, coherence in long-form writing, and reasoning proficiency. By introducing targeted perturbations in answers generated by LLMs, that clearly impact one of these key capabilities, we test whether an Evaluator LLM can detect these quality drops. By creating a total of 2400 perturbed answers covering 22 perturbation categories, we conduct a comprehensive study using different evaluation strategies on five prominent LLMs commonly used as evaluators in the literature. Our findings reveal significant shortcomings in current Evaluator LLMs, which failed to identify quality drops in over 50\\% of cases on average. Single-answer and pairwise evaluations demonstrated notable limitations, whereas reference-based evaluations showed comparatively better performance. These results underscore the unreliable nature of current Evaluator LLMs and advocate for cautious implementation in practical applications. Code and data are available at https://github.com/AI4Bharat/FBI.",
            "link": "https://www.semanticscholar.org/paper/384f91e20bd3516b35e564c7f9b43ddd46656b86",
            "authors": "Sumanth Doddapaneni, Mohammed Safi Ur Rahman Khan, Sshubam Verma, Mitesh M. Khapra",
            "EMNLP Paper ID": "1921",
            "Oral/Poster": "Oral",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "fac4e94cb7f0813cbccbddc6e792ececd2814ca2",
            "title": "Evaluating Diversity in Automatic Poetry Generation",
            "abstract": "Natural Language Generation (NLG), and more generally generative AI, are among the currently most impactful research fields. Creative NLG, such as automatic poetry generation, is a fascinating niche in this area. While most previous research has focused on forms of the Turing test when evaluating automatic poetry generation - can humans distinguish between automatic and human generated poetry - we evaluate the diversity of automatically generated poetry, by comparing distributions of generated poetry to distributions of human poetry along structural, lexical, semantic and stylistic dimensions, assessing different model types (word vs. character-level, general purpose LLMs vs. poetry-specific models), including the very recent LLaMA3, and types of fine-tuning (conditioned vs. unconditioned). We find that current automatic poetry systems are considerably underdiverse along multiple dimensions - they often do not rhyme sufficiently, are semantically too uniform and even do not match the length distribution of human poetry. Our experiments reveal, however, that style-conditioning and character-level modeling clearly increases diversity across virtually all dimensions we explore. Our identified limitations may serve as the basis for more genuinely diverse future poetry generation models.",
            "link": "https://www.semanticscholar.org/paper/fac4e94cb7f0813cbccbddc6e792ececd2814ca2",
            "authors": "Yanran Chen, Hannes Groner, Sina Zarrie\u00df, Steffen Eger",
            "EMNLP Paper ID": "2539",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "f5c3c9791b004634ae80915cbae99911488357a1",
            "title": "From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items",
            "abstract": "LLMs can now perform a variety of complex writing tasks. They also excel in answering questions pertaining to natural language inference and commonsense reasoning. Composing these questions is itself a skilled writing task, so in this paper we consider LLMs as authors of commonsense assessment items. We prompt LLMs to generate items in the style of a prominent benchmark for commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine the outcome according to analyses facilitated by the LLMs and human annotation. We find that LLMs that succeed in answering the original COPA benchmark are also more successful in authoring their own items.",
            "link": "https://www.semanticscholar.org/paper/f5c3c9791b004634ae80915cbae99911488357a1",
            "authors": "Melissa Roemmele, Andrew S. Gordon",
            "matchScore": 315.9828,
            "original title": "From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items",
            "original authors": "Melissa Roemmele, Andrew Gordon",
            "EMNLP Paper ID": "1034",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "bfa86a8337ee786d96d2e664eec6b3fcc3a4f1a4",
            "title": "Self-Evaluation of Large Language Model based on Glass-box Features",
            "abstract": "The proliferation of open-source Large Language Models (LLMs) underscores the pressing need for evaluation methods. Existing works primarily rely on external evaluators, focusing on training and prompting strategies. However, a crucial aspect, model-aware glass-box features, is overlooked. In this study, we explore the utility of glass-box features under the scenario of self-evaluation, namely applying an LLM to evaluate its own output. We investigate various glass-box feature groups and discovered that the softmax distribution serves as a reliable quality indicator for self-evaluation. Experimental results on public benchmarks validate the feasibility of self-evaluation of LLMs using glass-box features.",
            "link": "https://www.semanticscholar.org/paper/bfa86a8337ee786d96d2e664eec6b3fcc3a4f1a4",
            "authors": "Hui Huang, Yingqi Qu, Jing Liu, Muyun Yang, Tiejun Zhao",
            "matchScore": 226.79141,
            "original title": "Self-Evaluation of Large Language Model based on Glass-box Features",
            "original authors": "Hui Huang, Yingqi Qu, Jing Liu, Muyun Yang, Bing Xu, Tiejun Zhao, Wenpeng Lu",
            "EMNLP Paper ID": "1177",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "5c57bc93a1f87da003eecd3435b0ce8f881c4604",
            "title": "On the Rigour of Scientific Writing: Criteria, Analysis, and Insights",
            "abstract": "Rigour is crucial for scientific research as it ensures the reproducibility and validity of results and findings. Despite its importance, little work exists on modelling rigour computationally, and there is a lack of analysis on whether these criteria can effectively signal or measure the rigour of scientific papers in practice. In this paper, we introduce a bottom-up, data-driven framework to automatically identify and define rigour criteria and assess their relevance in scientific writing. Our framework includes rigour keyword extraction, detailed rigour definition generation, and salient criteria identification. Furthermore, our framework is domain-agnostic and can be tailored to the evaluation of scientific rigour for different areas, accommodating the distinct salient criteria across fields. We conducted comprehensive experiments based on datasets collected from two high impact venues for Machine Learning and NLP (i.e., ICLR and ACL) to demonstrate the effectiveness of our framework in modelling rigour. In addition, we analyse linguistic patterns of rigour, revealing that framing certainty is crucial for enhancing the perception of scientific rigour, while suggestion certainty and probability uncertainty diminish it.",
            "link": "https://www.semanticscholar.org/paper/5c57bc93a1f87da003eecd3435b0ce8f881c4604",
            "authors": "Joseph James, Chenghao Xiao, Yucheng Li, Chenghua Lin",
            "matchScore": 223.01642,
            "original title": "On the Rigour of Scientific Writing: Criteria, Analysis, and Insights",
            "original authors": "Joseph James, Chenghao Xiao, YUCHENG LI, Chenghua Lin",
            "EMNLP Paper ID": "1326",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "a7871727332c22df98e0402cb0b0d751978687c7",
            "title": "SYNTHEVAL: Hybrid Behavioral Testing of NLP Models with Synthetic CheckLists",
            "abstract": "Traditional benchmarking in NLP typically involves using static held-out test sets. However, this approach often results in an overestimation of performance and lacks the ability to offer comprehensive, interpretable, and dynamic assessments of NLP models. Recently, works like DynaBench (Kiela et al., 2021) and CheckList (Ribeiro et al., 2020) have addressed these limitations through behavioral testing of NLP models with test types generated by a multistep human-annotated pipeline. Unfortunately, manually creating a variety of test types requires much human labor, often at prohibitive cost. In this work, we propose SYNTHEVAL, a hybrid behavioral testing framework that leverages large language models (LLMs) to generate a wide range of test types for a comprehensive evaluation of NLP models. SYNTHEVAL first generates sentences via LLMs using controlled generation, and then identifies challenging examples by comparing the predictions made by LLMs with task-specific NLP models. In the last stage, human experts investigate the challenging examples, manually design templates, and identify the types of failures the taskspecific models consistently exhibit. We apply SYNTHEVAL to two classification tasks, sentiment analysis and toxic language detection, and show that our framework is effective in identifying weaknesses of strong models on these tasks. We share our code in https://github.com/Loreley99/SynthEval_CheckList.",
            "link": "https://www.semanticscholar.org/paper/a7871727332c22df98e0402cb0b0d751978687c7",
            "authors": "Raoyuan Zhao, Abdullatif K\u00f6ksal, Yihong Liu, Leonie Weissweiler, Anna Korhonen, Hinrich Schutze",
            "matchScore": 222.45435,
            "original title": "SynthEval: Hybrid Behavioral Testing of NLP Models with Synthetic Evaluation",
            "original authors": "Raoyuan Zhao, Abdullatif K\u00f6ksal, Yihong Liu, Leonie Weissweiler, Anna Korhonen, Hinrich Schuetze",
            "EMNLP Paper ID": "1434",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "45afd4a3552b11de7ef9ac1c22f50716db558c68",
            "title": "Generalists vs. Specialists: Evaluating Large Language Models for Urdu",
            "abstract": "In this paper, we compare general-purpose models, GPT-4-Turbo and Llama-3-8b, with special-purpose models--XLM-Roberta-large, mT5-large, and Llama-3-8b--that have been fine-tuned on specific tasks. We focus on seven classification and seven generation tasks to evaluate the performance of these models on Urdu language. Urdu has 70 million native speakers, yet it remains underrepresented in Natural Language Processing (NLP). Despite the frequent advancements in Large Language Models (LLMs), their performance in low-resource languages, including Urdu, still needs to be explored. We also conduct a human evaluation for the generation tasks and compare the results with the evaluations performed by GPT-4-Turbo, Llama-3-8b and Claude 3.5 Sonnet. We find that special-purpose models consistently outperform general-purpose models across various tasks. We also find that the evaluation done by GPT-4-Turbo for generation tasks aligns more closely with human evaluation compared to the evaluation the evaluation done by Llama-3-8b. This paper contributes to the NLP community by providing insights into the effectiveness of general and specific-purpose LLMs for low-resource languages.",
            "link": "https://www.semanticscholar.org/paper/45afd4a3552b11de7ef9ac1c22f50716db558c68",
            "authors": "Samee Arif, Abdul Hameed Azeemi, Agha Ali Raza, Awais Athar",
            "matchScore": 240.00012,
            "original title": "Generalists vs. Specialists: Evaluating Large Language Models for Urdu",
            "original authors": "Samee Arif, Abdul Hameed Azeemi, Agha Ali Raza, Awais Athar",
            "EMNLP Paper ID": "1480",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "b8251f591ada6b935ce1ad1d0877bcb4d1b91597",
            "title": "A LLM-Based Ranking Method for the Evaluation of Automatic Counter-Narrative Generation",
            "abstract": "This paper proposes a novel approach to evaluate Counter Narrative (CN) generation using a Large Language Model (LLM) as an evaluator. We show that traditional automatic metrics correlate poorly with human judgements and fail to capture the nuanced relationship between generated CNs and human perception. To alleviate this, we introduce a model ranking pipeline based on pairwise comparisons of generated CNs from different models, organized in a tournament-style format. The proposed evaluation method achieves a high correlation with human preference, with a $\\rho$ score of 0.88. As an additional contribution, we leverage LLMs as zero-shot CN generators and provide a comparative analysis of chat, instruct, and base models, exploring their respective strengths and limitations. Through meticulous evaluation, including fine-tuning experiments, we elucidate the differences in performance and responsiveness to domain-specific data. We conclude that chat-aligned models in zero-shot are the best option for carrying out the task, provided they do not refuse to generate an answer due to security concerns.",
            "link": "https://www.semanticscholar.org/paper/b8251f591ada6b935ce1ad1d0877bcb4d1b91597",
            "authors": "I. Zubiaga, A. Soroa, R. Agerri",
            "matchScore": 241.68765,
            "original title": "A LLM-based Ranking Method for the Evaluation of Automatic Counter-Narrative Generation",
            "original authors": "Irune Zubiaga, Aitor Soroa, Rodrigo Agerri",
            "EMNLP Paper ID": "1980",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "9acda8593fb9c70f724f6a7161be348d5a415e36",
            "title": "Automated Peer Reviewing in Paper SEA: Standardization, Evaluation, and Analysis",
            "abstract": "In recent years, the rapid increase in scientific papers has overwhelmed traditional review mechanisms, resulting in varying quality of publications. Although existing methods have explored the capabilities of Large Language Models (LLMs) for automated scientific reviewing, their generated contents are often generic or partial. To address the issues above, we introduce an automated paper reviewing framework SEA. It comprises of three modules: Standardization, Evaluation, and Analysis, which are represented by models SEA-S, SEA-E, and SEA-A, respectively. Initially, SEA-S distills data standardization capabilities of GPT-4 for integrating multiple reviews for a paper. Then, SEA-E utilizes standardized data for fine-tuning, enabling it to generate constructive reviews. Finally, SEA-A introduces a new evaluation metric called mismatch score to assess the consistency between paper contents and reviews. Moreover, we design a self-correction strategy to enhance the consistency. Extensive experimental results on datasets collected from eight venues show that SEA can generate valuable insights for authors to improve their papers.",
            "link": "https://www.semanticscholar.org/paper/9acda8593fb9c70f724f6a7161be348d5a415e36",
            "authors": "Jianxiang Yu, Zichen Ding, Jiaqi Tan, Kangyang Luo, Zhenmin Weng, Chenghua Gong, Long Zeng, Renjing Cui, Chengcheng Han, Qiushi Sun, Zhiyong Wu, Yunshi Lan, Xiang Li",
            "matchScore": 228.98132,
            "original title": "Automated Peer Reviewing in Paper SEA: Standardization, Evaluation, and Analysis",
            "original authors": "Jianxiang Yu, Zichen Ding, Jiaqi Tan, Kangyang Luo, Zhenmin Weng, Chenghua Gong, Long Zeng, RenJing Cui, Chengcheng Han, Qiushi Sun, Zhiyong Wu, Yunshi Lan, Xiang Li",
            "EMNLP Paper ID": "2084",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "dda8031682684655744c7001374e6cb88c9503bd",
            "title": "Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs",
            "abstract": "Although human evaluation remains the gold standard for open-domain dialogue evaluation, the growing popularity of automated evaluation using Large Language Models (LLMs) has also extended to dialogue. However, most frameworks leverage benchmarks that assess older chatbots on aspects such as fluency and relevance, which are not reflective of the challenges associated with contemporary models. In fact, a qualitative analysis on Soda, a GPT-3.5 generated dialogue dataset, suggests that current chatbots may exhibit several recurring issues related to coherence and commonsense knowledge, but generally produce highly fluent and relevant responses. Noting the aforementioned limitations, this paper introduces Soda-Eval, an annotated dataset based on Soda that covers over 120K turn-level assessments across 10K dialogues, where the annotations were generated by GPT-4. Using Soda-Eval as a benchmark, we then study the performance of several open-access instruction-tuned LLMs, finding that dialogue evaluation remains challenging. Fine-tuning these models improves performance over few-shot inferences, both in terms of correlation and explanation.",
            "link": "https://www.semanticscholar.org/paper/dda8031682684655744c7001374e6cb88c9503bd",
            "authors": "John Mendon\u00e7a, Isabel Trancoso, A. Lavie",
            "matchScore": 262.0016,
            "original title": "Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs",
            "original authors": "John Mendon\u00e7a, Isabel Trancoso, Alon Lavie",
            "EMNLP Paper ID": "2293",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "a43438b65660e69f7c7341b5f3ced15d5ac98c8d",
            "title": "Towards More Robust NLP System Evaluation: Handling Missing Scores in Benchmarks",
            "abstract": "The evaluation of natural language processing (NLP) systems is crucial for advancing the field, but current benchmarking approaches often assume that all systems have scores available for all tasks, which is not always practical. In reality, several factors such as the cost of running baseline, private systems, computational limitations, or incomplete data may prevent some systems from being evaluated on entire tasks. This paper formalize an existing problem in NLP research: benchmarking when some systems scores are missing on the task, and proposes a novel approach to address it. Our method utilizes a compatible partial ranking approach to impute missing data, which is then aggregated using the Borda count method. It includes two refinements designed specifically for scenarios where either task-level or instance-level scores are available. We also introduce an extended benchmark, which contains over 131 million scores, an order of magnitude larger than existing benchmarks. We validate our methods and demonstrate their effectiveness in addressing the challenge of missing system evaluation on an entire task. This work highlights the need for more comprehensive benchmarking approaches that can handle real-world scenarios where not all systems are evaluated on the entire task.",
            "link": "https://www.semanticscholar.org/paper/a43438b65660e69f7c7341b5f3ced15d5ac98c8d",
            "authors": "Anas Himmi, Ekhine Irurozki, Nathan Noiry, S. Cl\u00e9men\u00e7on, Pierre Colombo",
            "matchScore": 314.6349,
            "original title": "Towards More Robust NLP System Evaluation: Handling Missing Scores in Benchmarks",
            "original authors": "Anas Himmi, Ekhine Irurozki, Nathan Noiry, Stephan Cl\u00e9men\u00e7on, Pierre Colombo",
            "EMNLP Paper ID": "2309",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "7094592049a00a3a5dc82d13b8087e77e39d0fc9",
            "title": "Self-Recognition in Language Models",
            "abstract": "A rapidly growing number of applications rely on a small set of closed-source language models (LMs). This dependency might introduce novel security risks if LMs develop self-recognition capabilities. Inspired by human identity verification methods, we propose a novel approach for assessing self-recognition in LMs using model-generated\"security questions\". Our test can be externally administered to monitor frontier models as it does not require access to internal model parameters or output probabilities. We use our test to examine self-recognition in ten of the most capable open- and closed-source LMs currently publicly available. Our extensive experiments found no empirical evidence of general or consistent self-recognition in any examined LM. Instead, our results suggest that given a set of alternatives, LMs seek to pick the\"best\"answer, regardless of its origin. Moreover, we find indications that preferences about which models produce the best answers are consistent across LMs. We additionally uncover novel insights on position bias considerations for LMs in multiple-choice settings.",
            "link": "https://www.semanticscholar.org/paper/7094592049a00a3a5dc82d13b8087e77e39d0fc9",
            "authors": "Tim R. Davidson, Viacheslav Surkov, V. Veselovsky, Giuseppe Russo Latona, Robert West, Caglar Gulcehre",
            "matchScore": 118.777214,
            "original title": "Self-Recognition in Language Models",
            "original authors": "Tim Ruben Davidson, Viacheslav Surkov, Veniamin Veselovsky, Giuseppe Russo, Robert West, Caglar Gulcehre",
            "EMNLP Paper ID": "2369",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "c3ba560b0ea35ec687eeafb5bc70a2b034359f46",
            "title": "Sonnet or Not, Bot? Poetry Evaluation for Large Models and Datasets",
            "abstract": "Large language models (LLMs) can now generate and recognize poetry. But what do LLMs really know about poetry? We develop a task to evaluate how well LLMs recognize one aspect of English-language poetry--poetic form--which captures many different poetic features, including rhyme scheme, meter, and word or line repetition. By using a benchmark dataset of over 4.1k human expert-annotated poems, we show that state-of-the-art LLMs can successfully identify both common and uncommon fixed poetic forms--such as sonnets, sestinas, and pantoums--with surprisingly high accuracy. However, performance varies significantly by poetic form; the models struggle to identify unfixed poetic forms, especially those based on topic or visual features. We additionally measure how many poems from our benchmark dataset are present in popular pretraining datasets or memorized by GPT-4, finding that pretraining presence and memorization may improve performance on this task, but results are inconclusive. We release a benchmark evaluation dataset with 1.4k public domain poems and form annotations, results of memorization experiments and data audits, and code.",
            "link": "https://www.semanticscholar.org/paper/c3ba560b0ea35ec687eeafb5bc70a2b034359f46",
            "authors": "Melanie Walsh, Anna Preus, Maria Antoniak",
            "matchScore": 241.83426,
            "original title": "Sonnet or Not, Bot? Poetry Evaluation for Large Models and Datasets",
            "original authors": "Melanie Walsh, Maria Antoniak, Anna Preus",
            "EMNLP Paper ID": "2997",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        }
    ],
    "Domain-Specific Applications of Vision-Language Models": [
        {
            "paperId": "64c8002bd1044d869cc18a4a8738e076bf22f4ea",
            "title": "UniFashion: A Unified Vision-Language Model for Multimodal Fashion Retrieval and Generation",
            "abstract": "The fashion domain encompasses a variety of real-world multimodal tasks, including multimodal retrieval and multimodal generation. The rapid advancements in artificial intelligence generated content, particularly in technologies like large language models for text generation and diffusion models for visual generation, have sparked widespread research interest in applying these multimodal models in the fashion domain. However, tasks involving embeddings, such as image-to-text or text-to-image retrieval, have been largely overlooked from this perspective due to the diverse nature of the multimodal fashion domain. And current research on multi-task single models lack focus on image generation. In this work, we present UniFashion, a unified framework that simultaneously tackles the challenges of multimodal generation and retrieval tasks within the fashion domain, integrating image generation with retrieval tasks and text generation tasks. UniFashion unifies embedding and generative tasks by integrating a diffusion model and LLM, enabling controllable and high-fidelity generation. Our model significantly outperforms previous single-task state-of-the-art models across diverse fashion tasks, and can be readily adapted to manage complex vision-language tasks. This work demonstrates the potential learning synergy between multimodal generation and retrieval, offering a promising direction for future research in the fashion domain. The source code is available at https://github.com/xiangyu-mm/UniFashion.",
            "link": "https://www.semanticscholar.org/paper/64c8002bd1044d869cc18a4a8738e076bf22f4ea",
            "authors": "Xiangyu Zhao, Yuehan Zhang, Wenlong Zhang, Xiao-Ming Wu",
            "EMNLP Paper ID": "177",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "2625479bfc769732411c591ba858f3ed40209cdf",
            "title": "MatchTime: Towards Automatic Soccer Game Commentary Generation",
            "abstract": "Soccer is a globally popular sport with a vast audience, in this paper, we consider constructing an automatic soccer game commentary model to improve the audiences' viewing experience. In general, we make the following contributions: First, observing the prevalent video-text misalignment in existing datasets, we manually annotate timestamps for 49 matches, establishing a more robust benchmark for soccer game commentary generation, termed as SN-Caption-test-align; Second, we propose a multi-modal temporal alignment pipeline to automatically correct and filter the existing dataset at scale, creating a higher-quality soccer game commentary dataset for training, denoted as MatchTime; Third, based on our curated dataset, we train an automatic commentary generation model, named MatchVoice. Extensive experiments and ablation studies have demonstrated the effectiveness of our alignment pipeline, and training model on the curated datasets achieves state-of-the-art performance for commentary generation, showcasing that better alignment can lead to significant performance improvements in downstream tasks.",
            "link": "https://www.semanticscholar.org/paper/2625479bfc769732411c591ba858f3ed40209cdf",
            "authors": "Jiayuan Rao, Haoning Wu, Chang Liu, Yanfeng Wang, Weidi Xie",
            "EMNLP Paper ID": "192",
            "Oral/Poster": "Oral",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "6b1da517a8699f0a62368d032fda3311871372c5",
            "title": "Self-Bootstrapped Visual-Language Model for Knowledge Selection and Question Answering",
            "abstract": "While large visual-language models (LVLM) have shown promising results on traditional visual question answering benchmarks, it is still challenging for them to answer complex VQA problems which requires diverse world knowledge. Motivated by the research of retrieval-augmented generation in the field of natural language processing, we use Dense Passage Retrieval (DPR) to retrieve related knowledge to help the model answer questions. However, DPR conduct retrieving in natural language space, which may not ensure comprehensive acquisition of image information. Thus, the retrieved knowledge is not truly conducive to helping answer the question, affecting the performance of the overall system. To address this issue, we propose a novel framework that leverages the visual-language model to select the key knowledge retrieved by DPR and answer questions. The framework consists of two modules: Selector and Answerer, where both are initialized by the LVLM and parameter-efficiently finetuned by self-bootstrapping: find key knowledge in the retrieved knowledge documents using the Selector, and then use them to finetune the Answerer to predict answers; obtain the pseudo-labels of key knowledge documents based on the predictions of the Answerer and weak supervision labels, and then finetune the Selector to select key knowledge; repeat. Our framework significantly enhances the performance of the baseline on the challenging open-domain Knowledge-based VQA benchmark, OK-VQA, achieving a state-of-the-art accuracy of 62.83%. Our code is publicly available at https://github.com/haodongze/Self-KSel-QAns.",
            "link": "https://www.semanticscholar.org/paper/6b1da517a8699f0a62368d032fda3311871372c5",
            "authors": "Dongze Hao, Qunbo Wang, Longteng Guo, Jie Jiang, Jing Liu",
            "EMNLP Paper ID": "209",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "d1a14ef92761331303db78ff2dd0b41cef4c6d8a",
            "title": "By My Eyes: Grounding Multimodal Large Language Models with Sensor Data via Visual Prompting",
            "abstract": "Large language models (LLMs) have demonstrated exceptional abilities across various domains. However, utilizing LLMs for ubiquitous sensing applications remains challenging as existing text-prompt methods show significant performance degradation when handling long sensor data sequences. We propose a visual prompting approach for sensor data using multimodal LLMs (MLLMs). We design a visual prompt that directs MLLMs to utilize visualized sensor data alongside the target sensory task descriptions. Additionally, we introduce a visualization generator that automates the creation of optimal visualizations tailored to a given sensory task, eliminating the need for prior task-specific knowledge. We evaluated our approach on nine sensory tasks involving four sensing modalities, achieving an average of 10% higher accuracy than text-based prompts and reducing token costs by 15.8 times. Our findings highlight the effectiveness and cost-efficiency of visual prompts with MLLMs for various sensory tasks. The source code is available at https://github.com/diamond264/ByMyEyes.",
            "link": "https://www.semanticscholar.org/paper/d1a14ef92761331303db78ff2dd0b41cef4c6d8a",
            "authors": "Hyungjun Yoon, Biniyam Aschalew Tolera, Taesik Gong, Kimin Lee, Sung-Ju Lee",
            "EMNLP Paper ID": "251",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "90348acb66a7bf76cbe0065b5b6126783e91f56b",
            "title": "Empowering Large Language Model for Continual Video Question Answering with Collaborative Prompting",
            "abstract": "In recent years, the rapid increase in online video content has underscored the limitations of static Video Question Answering (VideoQA) models trained on fixed datasets, as they struggle to adapt to new questions or tasks posed by newly available content. In this paper, we explore the novel challenge of VideoQA within a continual learning framework, and empirically identify a critical issue: fine-tuning a large language model (LLM) for a sequence of tasks often results in catastrophic forgetting. To address this, we propose Collaborative Prompting (ColPro), which integrates specific question constraint prompting, knowledge acquisition prompting, and visual temporal awareness prompting. These prompts aim to capture textual question context, visual content, and video temporal dynamics in VideoQA, a perspective underexplored in prior research. Experimental results on the NExT-QA and DramaQA datasets show that ColPro achieves superior performance compared to existing approaches, achieving 55.14\\% accuracy on NExT-QA and 71.24\\% accuracy on DramaQA, highlighting its practical relevance and effectiveness.",
            "link": "https://www.semanticscholar.org/paper/90348acb66a7bf76cbe0065b5b6126783e91f56b",
            "authors": "Chen Cai, Zheng Wang, Jianjun Gao, Wenyang Liu, Ye Lu, Runzhong Zhang, Kim-hui Yap",
            "EMNLP Paper ID": "442",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "4a0c0af49996fccad71fde7aa2a6b872762228ed",
            "title": "VIMI: Grounding Video Generation through Multi-modal Instruction",
            "abstract": "Existing text-to-video diffusion models rely solely on text-only encoders for their pretraining. This limitation stems from the absence of large-scale multimodal prompt video datasets, resulting in a lack of visual grounding and restricting their versatility and application in multimodal integration. To address this, we construct a large-scale multimodal prompt dataset by employing retrieval methods to pair in-context examples with the given text prompts and then utilize a two-stage training strategy to enable diverse video generation tasks within the same model. In the first stage, we propose a multimodal conditional video generation framework for pretraining on these augmented datasets, establishing a foundational model for grounded video generation. Secondly, we finetune the model from the first stage on three video generation tasks, incorporating multi-modal instructions. This process further refines the model's ability to handle diverse inputs and tasks, ensuring seamless integration of multi-modal information. After this two-stage train-ing process, VIMI demonstrates multimodal understanding capabilities, producing contextually rich and personalized videos grounded in the provided inputs, as shown in Figure 1. Compared to previous visual grounded video generation methods, VIMI can synthesize consistent and temporally coherent videos with large motion while retaining the semantic control. Lastly, VIMI also achieves state-of-the-art text-to-video generation results on UCF101 benchmark.",
            "link": "https://www.semanticscholar.org/paper/4a0c0af49996fccad71fde7aa2a6b872762228ed",
            "authors": "Yuwei Fang, Willi Menapace, Aliaksandr Siarohin, Tsai-Shien Chen, Kuan-Chien Wang, Ivan Skorokhodov, Graham Neubig, Sergey Tulyakov",
            "EMNLP Paper ID": "483",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "6d1b37ace2429bb6935d656a043ce6d4484be9de",
            "title": "Visual Prompting in LLMs for Enhancing Emotion Recognition",
            "abstract": "Vision Large Language Models (VLLMs) are transforming the intersection of computer vision and natural language processing. Nonetheless, the potential of using visual prompts for emotion recognition in these models remains largely unexplored and untapped. Traditional methods in VLLMs struggle with spatial localization and often discard valuable global context. To address this problem, we propose a Set-of-Vision prompting (SoV) approach that enhances zero-shot emotion recognition by using spatial information, such as bounding boxes and facial landmarks, to mark targets precisely. SoV improves accuracy in face count and emotion categorization while preserving the enriched image context. Through a battery of experimentation and analysis of recent commercial or open-source VLLMs, we evaluate the SoV model's ability to comprehend facial expressions in natural environments. Our findings demonstrate the effectiveness of integrating spatial visual prompts into VLLMs for improving emotion recognition performance.",
            "link": "https://www.semanticscholar.org/paper/6d1b37ace2429bb6935d656a043ce6d4484be9de",
            "authors": "Qixuan Zhang, Zhifeng Wang, Dylan Zhang, Wenjia Niu, Sabrina Caldwell, Tom Gedeon, Yang Liu, Zhen Qin",
            "EMNLP Paper ID": "487",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "6947191cc21675b3b1c0c61ac06dfa50403b7a7b",
            "title": "MaPPER: Multimodal Prior-guided Parameter Efficient Tuning for Referring Expression Comprehension",
            "abstract": "Referring Expression Comprehension (REC), which aims to ground a local visual region via natural language, is a task that heavily relies on multimodal alignment. Most existing methods utilize powerful pre-trained models to transfer visual/linguistic knowledge by full fine-tuning. However, full fine-tuning the entire backbone not only breaks the rich prior knowledge embedded in the pre-training, but also incurs significant computational costs. Motivated by the recent emergence of Parameter-Efficient Transfer Learning (PETL) methods, we aim to solve the REC task in an effective and efficient manner. Directly applying these PETL methods to the REC task is inappropriate, as they lack the specific-domain abilities for precise local visual perception and visual-language alignment. Therefore, we propose a novel framework of Multimodal Prior-guided Parameter Efficient Tuning, namely MaPPER. Specifically, MaPPER comprises Dynamic Prior Adapters guided by an aligned prior, and Local Convolution Adapters to extract precise local semantics for better visual perception. Moreover, the Prior-Guided Text module is proposed to further utilize the prior for facilitating the cross-modal alignment. Experimental results on three widely-used benchmarks demonstrate that MaPPER achieves the best accuracy compared to the full fine-tuning and other PETL methods with only 1.41% tunable backbone parameters. Our code is available at https://github.com/liuting20/MaPPER.",
            "link": "https://www.semanticscholar.org/paper/6947191cc21675b3b1c0c61ac06dfa50403b7a7b",
            "authors": "Ting Liu, Zunnan Xu, Yue Hu, Liangtao Shi, Zhiqiang Wang, Quanjun Yin",
            "EMNLP Paper ID": "549",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "107fb6eec2febbae12db29bf3e311aaf5680027c",
            "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
            "abstract": "The Large Vision-Language Model (LVLM) has enhanced the performance of various downstream tasks in visual-language understanding. Most existing approaches encode images and videos into separate feature spaces, which are then fed as inputs to large language models. However, due to the lack of unified tokenization for images and videos, namely misalignment before projection, it becomes challenging for a Large Language Model (LLM) to learn multi-modal interactions from several poor projection layers. In this work, we unify visual representation into the language feature space to advance the foundational LLM towards a unified LVLM. As a result, we establish a simple but robust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images and videos, mutually enhancing each other. Video-LLaVA achieves superior performances on a broad range of 9 image benchmarks across 5 image question-answering datasets and 4 image benchmark toolkits. Additionally, our Video-LLaVA also outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on MSRVTT, MSVD, TGIF, and ActivityNet, respectively. Notably, extensive experiments demonstrate that Video-LLaVA mutually benefits images and videos within a unified visual representation, outperforming models designed specifically for images or videos. We aim for this work to provide modest insights into the multi-modal inputs for the LLM. Code address: \\href{https://github.com/PKU-YuanGroup/Video-LLaVA}",
            "link": "https://www.semanticscholar.org/paper/107fb6eec2febbae12db29bf3e311aaf5680027c",
            "authors": "Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, Li Yuan",
            "EMNLP Paper ID": "665",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "269cba0e807f2aa9a1186d6a8854824b59f3f53a",
            "title": "Beyond Embeddings: The Promise of Visual Table in Visual Reasoning",
            "abstract": "Visual representation learning has been a cornerstone in computer vision, involving typical forms such as visual embeddings, structural symbols, and text-based representations. Despite the success of CLIP-type visual embeddings, they often lack access to world knowledge critical for visual reasoning. In this work, we propose Visual Table, a novel form of visual representation tailored for visual reasoning. Visual tables are constructed as hierarchical descriptions of visual scenes, featuring a scene description and multiple object-centric descriptions covering categories, attributes, and knowledge. Thanks to the structural and textual formats, visual tables offer unique advantages over mere visual embeddings, such as interpretability and controllable editing. Furthermore, they deliver instance-level world knowledge and detailed attributes that are essential for visual reasoning. To create visual tables, we develop a generator trained on the dataset with collected, small-scale annotations. Extensive results on 11 visual reasoning benchmarks demonstrate that the generated visual tables significantly outperform previous structural and text-based representations. Moreover, they consistently enhance state-of-the-art multimodal large language models across diverse benchmarks, showcasing their potential for advancing visual reasoning tasks. Our code is available at https://github.com/LaVi-Lab/Visual-Table.",
            "link": "https://www.semanticscholar.org/paper/269cba0e807f2aa9a1186d6a8854824b59f3f53a",
            "authors": "Yiwu Zhong, Zi-Yuan Hu, Michael R. Lyu, Liwei Wang",
            "EMNLP Paper ID": "774",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "50222763f11d6410458422b12c95691900a304fa",
            "title": "Encoding and Controlling Global Semantics for Long-form Video Question Answering",
            "abstract": "Seeking answers effectively for long videos is essential to build video question answering (videoQA) systems. Previous methods adaptively select frames and regions from long videos to save computations. However, this fails to reason over the whole sequence of video, leading to sub-optimal performance. To address this problem, we introduce a state space layer (SSL) into multi-modal Transformer to efficiently integrate global semantics of the video, which mitigates the video information loss caused by frame and region selection modules. Our SSL includes a gating unit to enable controllability over the flow of global semantics into visual representations. To further enhance the controllability, we introduce a cross-modal compositional congruence (C^3) objective to encourage global semantics aligned with the question. To rigorously evaluate long-form videoQA capacity, we construct two new benchmarks Ego-QA and MAD-QA featuring videos of considerably long length, i.e. 17.5 minutes and 1.9 hours, respectively. Extensive experiments demonstrate the superiority of our framework on these new as well as existing datasets. The code, model, and data have been made available at https://nguyentthong.github.io/Long_form_VideoQA.",
            "link": "https://www.semanticscholar.org/paper/50222763f11d6410458422b12c95691900a304fa",
            "authors": "Thong Nguyen, Zhiyuan Hu, Xiaobao Wu, Cong-Duy Nguyen, See-Kiong Ng, A. Luu",
            "EMNLP Paper ID": "791",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "516435c1a34adff37f02bb76a62def3ec610f416",
            "title": "TraveLER: A Modular Multi-LMM Agent Framework for Video Question-Answering",
            "abstract": "Recently, image-based Large Multimodal Models (LMMs) have made significant progress in video question-answering (VideoQA) using a frame-wise approach by leveraging large-scale pretraining in a zero-shot manner. Nevertheless, these models need to be capable of finding relevant information, extracting it, and answering the question simultaneously. Currently, existing methods perform all of these steps in a single pass without being able to adapt if insufficient or incorrect information is collected. To overcome this, we introduce a modular multi-LMM agent framework based on several agents with different roles, instructed by a Planner agent that updates its instructions using shared feedback from the other agents. Specifically, we propose TraveLER, a method that can create a plan to\"Traverse\"through the video, ask questions about individual frames to\"Locate\"and store key information, and then\"Evaluate\"if there is enough information to answer the question. Finally, if there is not enough information, our method is able to\"Replan\"based on its collected knowledge. Through extensive experiments, we find that the proposed TraveLER approach improves performance on several VideoQA benchmarks without the need to fine-tune on specific datasets. Our code is available at https://github.com/traveler-framework/TraveLER.",
            "link": "https://www.semanticscholar.org/paper/516435c1a34adff37f02bb76a62def3ec610f416",
            "authors": "Chuyi Shang, Amos You, Sanjay Subramanian, Trevor Darrell, Roei Herzig",
            "EMNLP Paper ID": "1083",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "1996de1fbb993e3d9eae4b0dc86108186bcc78d0",
            "title": "Efficient Temporal Extrapolation of Multimodal Large Language Models with Temporal Grounding Bridge",
            "abstract": "Despite progress in multimodal large language models (MLLMs), the challenge of interpreting long-form videos in response to linguistic queries persists, largely due to the inefficiency in temporal grounding and limited pre-trained context window size. In this work, we introduce Temporal Grounding Bridge (TGB), a novel framework that bootstraps MLLMs with advanced temporal grounding capabilities and broadens their contextual scope. Our framework significantly enhances the temporal capabilities of current MLLMs through three key innovations: an efficient multi-span temporal grounding algorithm applied to low-dimension temporal features projected from flow; a multimodal length extrapolation training paradigm that utilizes low-dimension temporal features to extend the training context window size; and a bootstrapping framework that bridges our model with pluggable MLLMs without requiring annotation. We validate TGB across seven video benchmarks and demonstrate substantial performance improvements compared with prior MLLMs. Notably, our model, initially trained on sequences of four frames, effectively handles sequences up to 16 longer without sacrificing performance, highlighting its scalability and effectiveness in real-world applications. Our code is publicly available at https://github.com/bigai-nlco/VideoTGB",
            "link": "https://www.semanticscholar.org/paper/1996de1fbb993e3d9eae4b0dc86108186bcc78d0",
            "authors": "Yuxuan Wang, Yueqian Wang, Pengfei Wu, Jianxin Liang, Dongyan Zhao, Zilong Zheng",
            "EMNLP Paper ID": "1115",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "ea1f6e28e8c6b08f18424ab4c0291916a750448e",
            "title": "OmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer",
            "abstract": "Recent advancements in Large Language Models (LLMs) have expanded their capabilities to multimodal contexts, including comprehensive video understanding. However, processing extensive videos such as 24-hour CCTV footage or full-length films presents significant challenges due to the vast data and processing demands. Traditional methods, like extracting key frames or converting frames to text, often result in substantial information loss. To address these shortcomings, we develop OmAgent, efficiently stores and retrieves relevant video frames for specific queries, preserving the detailed content of videos. Additionally, it features an Divide-and-Conquer Loop capable of autonomous reasoning, dynamically invoking APIs and tools to enhance query processing and accuracy. This approach ensures robust video understanding, significantly reducing information loss. Experimental results affirm OmAgent's efficacy in handling various types of videos and complex tasks. Moreover, we have endowed it with greater autonomy and a robust tool-calling system, enabling it to accomplish even more intricate tasks.",
            "link": "https://www.semanticscholar.org/paper/ea1f6e28e8c6b08f18424ab4c0291916a750448e",
            "authors": "Lu Zhang, Tiancheng Zhao, Heting Ying, Yibo Ma, Kyusong Lee",
            "EMNLP Paper ID": "1121",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "68c46db36ddc687551070711cc526230e34802e8",
            "title": "ECIS-VQG: Generation of Entity-centric Information-seeking Questions from Videos",
            "abstract": "Previous studies on question generation from videos have mostly focused on generating questions about common objects and attributes and hence are not entity-centric. In this work, we focus on the generation of entity-centric information-seeking questions from videos. Such a system could be useful for video-based learning, recommending ``People Also Ask'' questions, video-based chatbots, and fact-checking. Our work addresses three key challenges: identifying question-worthy information, linking it to entities, and effectively utilizing multimodal signals. Further, to the best of our knowledge, there does not exist a large-scale dataset for this task. Most video question generation datasets are on TV shows, movies, or human activities or lack entity-centric information-seeking questions. Hence, we contribute a diverse dataset of YouTube videos, VideoQuestions, consisting of 411 videos with 2265 manually annotated questions. We further propose a model architecture combining Transformers, rich context signals (titles, transcripts, captions, embeddings), and a combination of cross-entropy and contrastive loss function to encourage entity-centric question generation. Our best method yields BLEU, ROUGE, CIDEr, and METEOR scores of 71.3, 78.6, 7.31, and 81.9, respectively, demonstrating practical usability. We make the code and dataset publicly available. https://github.com/thePhukan/ECIS-VQG",
            "link": "https://www.semanticscholar.org/paper/68c46db36ddc687551070711cc526230e34802e8",
            "authors": "Arpan Phukan, Manish Gupta, Asif Ekbal",
            "EMNLP Paper ID": "1665",
            "Oral/Poster": "Oral",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "45d6b0140a488c2443f62ddeeaa226544387095c",
            "title": "On Efficient Language and Vision Assistants for Visually-Situated Natural Language Understanding: What Matters in Reading and Reasoning",
            "abstract": "Recent advancements in language and vision assistants have showcased impressive capabilities but suffer from a lack of transparency, limiting broader research and reproducibility. While open-source models handle general image tasks effectively, they face challenges with the high computational demands of complex visually-situated text understanding. Such tasks often require increased token inputs and large vision modules to harness high-resolution information. Striking a balance between model size and data importance remains an open question. This study aims to redefine the design of vision-language models by identifying key components and creating efficient models with constrained inference costs. By strategically formulating datasets, optimizing vision modules, and enhancing supervision techniques, we achieve significant improvements in inference throughput while maintaining high performance. Extensive experiments across models ranging from 160M to 13B parameters offer insights into model optimization. We will fully open-source our codebase, models, and datasets at https://github.com/naver-ai/elva.",
            "link": "https://www.semanticscholar.org/paper/45d6b0140a488c2443f62ddeeaa226544387095c",
            "authors": "Geewook Kim, Minjoon Seo",
            "EMNLP Paper ID": "1998",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "0042b9380f7da8335be040a3516e4f6765320834",
            "title": "TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning",
            "abstract": "It is challenging for models to understand complex, multimodal content such as television clips, and this is in part because video-language models often rely on single-modality reasoning and lack interpretability. To combat these issues we propose TV-TREES, the first multimodal entailment tree generator. TV-TREES serves as an approach to video understanding that promotes interpretable joint-modality reasoning by searching for trees of entailment relationships between simple text-video evidence and higher-level conclusions that prove question-answer pairs. We also introduce the task of multimodal entailment tree generation to evaluate reasoning quality. Our method's performance on the challenging TVQA benchmark demonstrates interpretable, state-of-the-art zero-shot performance on full clips, illustrating that multimodal entailment tree generation can be a best-of-both-worlds alternative to black-box systems.",
            "link": "https://www.semanticscholar.org/paper/0042b9380f7da8335be040a3516e4f6765320834",
            "authors": "Kate Sanders, Nathaniel Weir, Benjamin Van Durme",
            "EMNLP Paper ID": "2394",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "efb91bed09e03b8a0e9cae23052b1ae537906254",
            "title": "Eliciting In-Context Learning in Vision-Language Models for Videos Through Curated Data Distributional Properties",
            "abstract": "A major reason behind the recent success of large language models (LLMs) is their \\textit{in-context learning} capability, which makes it possible to rapidly adapt them to downstream text-based tasks by prompting them with a small number of relevant demonstrations. While large vision-language models (VLMs) have recently been developed for tasks requiring both text and images, they largely lack in-context learning over visual information, especially in understanding and generating text about videos. In this work, we implement \\textbf{E}mergent \\textbf{I}n-context \\textbf{Le}arning on \\textbf{V}ideos (\\eilev{}), a novel training paradigm that induces in-context learning over video and text by capturing key properties of pre-training data found by prior work to be essential for in-context learning in transformers. In our experiments, we show that \\eilev-trained models outperform other off-the-shelf VLMs in few-shot video narration for novel, rare actions. Furthermore, we demonstrate that these key properties of bursty distributions, skewed marginal distributions, and dynamic meaning each contribute to varying degrees to VLMs' in-context learning capability in narrating procedural videos. Our results, analysis, and \\eilev{}-trained models yield numerous insights about the emergence of in-context learning over video and text, creating a foundation for future work to optimize and scale VLMs for open-domain video understanding and reasoning. Our code and demo are available at \\url{https://github.com/yukw777/EILEV}.",
            "link": "https://www.semanticscholar.org/paper/efb91bed09e03b8a0e9cae23052b1ae537906254",
            "authors": "Keunwoo Peter Yu, Zheyuan Zhang, Fengyuan Hu, Joyce Chai",
            "EMNLP Paper ID": "2669",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "d2d7cb6ba2123816eb5849a01cdaeaa1a200f896",
            "title": "Visual Text Matters: Improving Text-KVQA with Visual Text Entity Knowledge-aware Large Multimodal Assistant",
            "abstract": "We revisit knowledge-aware text-based visual question answering, also known as Text-KVQA, in the light of modern advancements in large multimodal models (LMMs), and make the following contributions: (i) We propose VisTEL - a principled approach to perform visual text entity linking. The proposed VisTEL module harnesses a state-of-the-art visual text recognition engine and the power of a large multimodal model to jointly reason using textual and visual context obtained using surrounding cues in the image to link the visual text entity to the correct knowledge base entity. (ii) We present KaLMA - a knowledge-aware large multimodal assistant that augments an LMM with knowledge associated with visual text entity in the image to arrive at an accurate answer. Further, we provide a comprehensive experimental analysis and comparison of our approach with traditional visual question answering, pre-large multimodal models, and large multimodal models, as well as prior top-performing approaches. Averaging over three splits of Text-KVQA, our proposed approach surpasses the previous best approach by a substantial 23.3% on an absolute scale and establishes a new state of the art. We make our implementation publicly available.",
            "link": "https://www.semanticscholar.org/paper/d2d7cb6ba2123816eb5849a01cdaeaa1a200f896",
            "authors": "A. S. Penamakuri, Anand Mishra",
            "EMNLP Paper ID": "2741",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "8cfaa39909c7c8d05b9e664abd554a90b0dc7762",
            "title": "Show and Guide: Instructional-Plan Grounded Vision and Language Model",
            "abstract": "Guiding users through complex procedural plans is an inherently multimodal task in which having visually illustrated plan steps is crucial to deliver an effective plan guidance. However, existing works on plan-following language models (LMs) often are not capable of multimodal input and output. In this work, we present MM-PlanLLM, the first multimodal LLM designed to assist users in executing instructional tasks by leveraging both textual plans and visual information. Specifically, we bring cross-modality through two key tasks: Conversational Video Moment Retrieval, where the model retrieves relevant step-video segments based on user queries, and Visually-Informed Step Generation, where the model generates the next step in a plan, conditioned on an image of the user's current progress. MM-PlanLLM is trained using a novel multitask-multistage approach, designed to gradually expose the model to multimodal instructional-plans semantic layers, achieving strong performance on both multimodal and textual dialogue in a plan-grounded setting. Furthermore, we show that the model delivers cross-modal temporal and plan-structure representations aligned between textual plan steps and instructional video moments.",
            "link": "https://www.semanticscholar.org/paper/8cfaa39909c7c8d05b9e664abd554a90b0dc7762",
            "authors": "Diogo Gl'oria-Silva, David Semedo, Jo\u00e3o Magalh\u00e3es",
            "EMNLP Paper ID": "2937",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "4641fe56cd44144b6cabea583233ed952f97f4c0",
            "title": "A Simple LLM Framework for Long-Range Video Question-Answering",
            "abstract": "We present LLoVi, a language-based framework for long-range video question-answering (LVQA). Unlike prior long-range video understanding methods, which are often costly and require specialized long-range video modeling design (e.g., memory queues, state-space layers, etc.), our approach uses a frame/clip-level visual captioner (e.g., BLIP2, LaViLa, LLaVA) coupled with a Large Language Model (GPT-3.5, GPT-4) leading to a simple yet surprisingly effective LVQA framework. Specifically, we decompose short and long-range modeling aspects of LVQA into two stages. First, we use a short-term visual captioner to generate textual descriptions of short video clips (0.5-8s in length) densely sampled from a long input video. Afterward, an LLM aggregates the densely extracted short-term captions to perform long-range temporal reasoning needed to understand the whole video and answer a question. To analyze what makes our simple framework so effective, we thoroughly evaluate various components of our system. Our empirical analysis reveals that the choice of the visual captioner and LLM is critical for good LVQA performance. Furthermore, we show that a specialized prompt that asks the LLM first to summarize the noisy short-term visual captions and then answer a given input question leads to a significant LVQA performance boost. On EgoSchema, which is best known as a very long-form video question-answering benchmark, our method achieves 50.3% accuracy, outperforming the previous best-performing approach by 18.1% (absolute gain). In addition, our approach outperforms the previous state-of-the-art by 4.1% and 3.1% on NeXT-QA and IntentQA. We also extend LLoVi to grounded LVQA and show that it outperforms all prior methods on the NeXT-GQA dataset. We will release our code at https://github.com/CeeZh/LLoVi.",
            "link": "https://www.semanticscholar.org/paper/4641fe56cd44144b6cabea583233ed952f97f4c0",
            "authors": "Ce Zhang, Taixi Lu, Md Mohaiminul Islam, Ziyang Wang, Shoubin Yu, Mohit Bansal, Gedas Bertasius",
            "EMNLP Paper ID": "2993",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "2b070923f46f1a6592e7975cfb01bcd4ba911b96",
            "title": "Unveiling the Invisible: Captioning Videos with Metaphors",
            "abstract": "Metaphors are a common communication tool used in our day-to-day life. The detection and generation of metaphors in textual form have been studied extensively but metaphors in other forms have been under-explored. Recent studies have shown that Vision-Language (VL) models cannot understand visual metaphors in memes and adverts. As of now, no probing studies have been done that involve complex language phenomena like metaphors with videos. Hence, we introduce a new VL task of describing the metaphors present in the videos in our work. To facilitate this novel task, we construct and release a manually created dataset with 705 videos and 2115 human-written captions, along with a new metric called Average Concept Distance (ACD), to automatically evaluate the creativity of the metaphors generated. We also propose a novel low-resource video metaphor captioning system: GIT-LLaVA, which obtains comparable performance to SoTA video language models on the proposed task. We perform a comprehensive analysis of existing video language models on this task and publish our dataset, models, and benchmark results to enable further research.",
            "link": "https://www.semanticscholar.org/paper/2b070923f46f1a6592e7975cfb01bcd4ba911b96",
            "authors": "Abisek Rajakumar Kalarani, Pushpak Bhattacharyya, Sumit Shekhar",
            "matchScore": 207.33745,
            "original title": "Unveiling the Invisible: Captioning Videos with Metaphors",
            "original authors": "Abisek Rajakumar Kalarani, Pushpak Bhattacharyya, Sumit Shekhar",
            "EMNLP Paper ID": "1289",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "a769b0830148f1023e414a7eca6e9468f37c0d43",
            "title": "VideoINSTA: Zero-shot Long Video Understanding via Informative Spatial-Temporal Reasoning with LLMs",
            "abstract": "In the video-language domain, recent works in leveraging zero-shot Large Language Model-based reasoning for video understanding have become competitive challengers to previous end-to-end models. However, long video understanding presents unique challenges due to the complexity of reasoning over extended timespans, even for zero-shot LLM-based approaches. The challenge of information redundancy in long videos prompts the question of what specific information is essential for large language models (LLMs) and how to leverage them for complex spatial-temporal reasoning in long-form video analysis. We propose a framework VideoINSTA, i.e. INformative Spatial-TemporAl Reasoning for zero-shot long-form video understanding. VideoINSTA contributes (1) a zero-shot framework for long video understanding using LLMs; (2) an event-based temporal reasoning and content-based spatial reasoning approach for LLMs to reason over spatial-temporal information in videos; (3) a self-reflective information reasoning scheme balancing temporal factors based on information sufficiency and prediction confidence. Our model significantly improves the state-of-the-art on three long video question-answering benchmarks: EgoSchema, NextQA, and IntentQA, and the open question answering dataset ActivityNetQA. The code is released here: https://github.com/mayhugotong/VideoINSTA.",
            "link": "https://www.semanticscholar.org/paper/a769b0830148f1023e414a7eca6e9468f37c0d43",
            "authors": "Ruotong Liao, Max Erler, Huiyu Wang, Guangyao Zhai, Gengyuan Zhang, Yunpu Ma, Volker Tresp",
            "matchScore": 263.09024,
            "original title": "VideoINSTA: Zero-shot Long-Form Video Understanding via Informative Spatial-Temporal Reasoning",
            "original authors": "Ruotong Liao, Max Erler, Huiyu Wang, Guangyao Zhai, Gengyuan Zhang, Yunpu Ma, Volker Tresp",
            "EMNLP Paper ID": "1342",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "cd17dc060ca82b3014f375cb8162fd0949d59337",
            "title": "MobileVLM: A Vision-Language Model for Better Intra- and Inter-UI Understanding",
            "abstract": "Recently, mobile AI agents based on VLMs have been gaining increasing attention. These works typically utilize VLM as a foundation, fine-tuning it with instruction-based mobile datasets. However, these VLMs are typically pre-trained on general-domain data, which often results in a lack of fundamental capabilities specific to the mobile domain. Therefore, they may struggle to recognize specific UI elements and understand intra-UI fine-grained information. In addition, the current fine-tuning task focuses on interacting with the most relevant element for the given instruction. These fine-tuned VLMs may still ignore the relationships between UI pages, neglect the roles of elements in page transitions and lack inter-UI understanding. To address issues, we propose a VLM called MobileVLM, which includes two additional pre-training stages to enhance both intra- and inter-UI understanding. We defined four UI-based pre-training tasks, enabling the model to better perceive fine-grained elements and capture page transition actions. To address the lack of mobile pre-training data, we built a large Chinese mobile dataset Mobile3M from scratch, which contains 3 million UI pages, and real-world transition actions, forming a directed graph structure. Experimental results show MobileVLM excels on both our test set and public mobile benchmarks, outperforming existing VLMs.",
            "link": "https://www.semanticscholar.org/paper/cd17dc060ca82b3014f375cb8162fd0949d59337",
            "authors": "Qinzhuo Wu, Weikai Xu, Wei Liu, Tao Tan, Jianfeng Liu, Ang Li, Jian Luan, Bin Wang, Shuo Shang",
            "matchScore": 264.75934,
            "original title": "MobileVLM: A Vision-Language Model for Better Intra- and Inter-UI Understanding",
            "original authors": "Qinzhuo Wu, Weikai Xu, Wei Liu, Tao Tan, Liujianfeng, Ang Li, Jian Luan, Bin Wang, Shuo Shang",
            "EMNLP Paper ID": "2090",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Findings"
        },
        {
            "paperId": "b908824639d18f11883abcab21efeb22e315ab9c",
            "title": "Multimodal Procedural Planning via Dual Text-Image Prompting",
            "abstract": "Embodied agents have achieved prominent performance in following human instructions to complete tasks. However, the potential of providing instructions informed by texts and images to assist humans in completing tasks remains underexplored. To uncover this capability, we present the multimodal procedural planning (MPP) task, in which models are given a high-level goal and generate plans of paired text-image steps, providing more complementary and informative guidance than unimodal plans. The key challenges of MPP are to ensure the informativeness, temporal coherence,and accuracy of plans across modalities. To tackle this, we propose Text-Image Prompting (TIP), a dual-modality prompting method that jointly leverages zero-shot reasoning ability in large language models (LLMs) and compelling text-to-image generation ability from diffusion-based models. TIP improves the interaction in the dual modalities using Text-to-Image Bridge and Image-to-Text Bridge, allowing LLMs to guide the textual-grounded image plan generation and leveraging the descriptions of image plans to ground the textual plan reversely. To address the lack of relevant datasets, we collect WIKIPLAN and RECIPEPLAN as a testbed for MPP. Our results show compelling human preferences and automatic scores against unimodal and multimodal baselines on WIKIPLAN and RECIPEPLAN in terms of informativeness, temporal coherence, and plan accuracy. Our code and data: https://github.com/YujieLu10/MPP.",
            "link": "https://www.semanticscholar.org/paper/b908824639d18f11883abcab21efeb22e315ab9c",
            "authors": "Yujie Lu, Pan Lu, Zhiyu Chen, Wanrong Zhu, X. Wang, William Yang Wang",
            "matchScore": 267.4668,
            "original title": "Multimodal Procedural Planning via Dual Text-Image Prompting",
            "original authors": "Yujie Lu, Pan Lu, Zhiyu Chen, Wanrong Zhu, Xin Eric Wang, William Yang Wang",
            "EMNLP Paper ID": "2188",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "1872b0a2ad3d44ca325ac80ccea5788c9b4a6574",
            "title": "AlanaVLM: A Multimodal Embodied AI Foundation Model for Egocentric Video Understanding",
            "abstract": "AI personal assistants deployed via robots or wearables require embodied understanding to collaborate with humans effectively. However, current Vision-Language Models (VLMs) primarily focus on third-person view videos, neglecting the richness of egocentric perceptual experience. To address this gap, we propose three key contributions. First, we introduce the Egocentric Video Understanding Dataset (EVUD) for training VLMs on video captioning and question answering tasks specific to egocentric videos. Second, we present AlanaVLM, a 7B parameter VLM trained using parameter-efficient methods on EVUD. Finally, we evaluate AlanaVLM's capabilities on OpenEQA, a challenging benchmark for embodied video question answering. Our model achieves state-of-the-art performance, outperforming open-source models including strong Socratic models using GPT-4 as a planner by 3.6%. Additionally, we outperform Claude 3 and Gemini Pro Vision 1.0 and showcase competitive results compared to Gemini Pro 1.5 and GPT-4V, even surpassing the latter in spatial reasoning. This research paves the way for building efficient VLMs that can be deployed in robots or wearables, leveraging embodied video understanding to collaborate seamlessly with humans in everyday tasks, contributing to the next generation of Embodied AI.",
            "link": "https://www.semanticscholar.org/paper/1872b0a2ad3d44ca325ac80ccea5788c9b4a6574",
            "authors": "Alessandro Suglia, Claudio Greco, Katie Baker, Jose L. Part, Ioannis Papaionnou, Arash Eshghi, Ioannis Konstas, Oliver Lemon",
            "matchScore": 324.0122,
            "original title": "AlanaVLM: A Multimodal Embodied AI Foundation Model for Egocentric Video Understanding",
            "original authors": "Alessandro Suglia, Claudio Greco, Katie Baker, Jose L. Part, Ioannis Papaioannou, Arash Eshghi, Ioannis Konstas, Oliver Lemon",
            "EMNLP Paper ID": "2211",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "7242478180247ef7bf335112aed16d4fc5d2d133",
            "title": "EchoSight: Advancing Visual-Language Models with Wiki Knowledge",
            "abstract": "Knowledge-based Visual Question Answering (KVQA) tasks require answering questions about images using extensive background knowledge. Despite significant advancements, generative models often struggle with these tasks due to the limited integration of external knowledge. In this paper, we introduce EchoSight, a novel multimodal Retrieval-Augmented Generation (RAG) framework that enables large language models (LLMs) to answer visual questions requiring fine-grained encyclopedic knowledge. To strive for high-performing retrieval, EchoSight first searches wiki articles by using visual-only information, subsequently, these candidate articles are further reranked according to their relevance to the combined text-image query. This approach significantly improves the integration of multimodal knowledge, leading to enhanced retrieval outcomes and more accurate VQA responses. Our experimental results on the Encyclopedic VQA and InfoSeek datasets demonstrate that EchoSight establishes new state-of-the-art results in knowledge-based VQA, achieving an accuracy of 41.8% on Encyclopedic VQA and 31.3% on InfoSeek.",
            "link": "https://www.semanticscholar.org/paper/7242478180247ef7bf335112aed16d4fc5d2d133",
            "authors": "Yibin Yan, Weidi Xie",
            "matchScore": 244.40643,
            "original title": "EchoSight: Advancing Visual-Language Models with Wiki Knowledge",
            "original authors": "Yibin Yan, Weidi Xie",
            "EMNLP Paper ID": "307",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "6d9690ab7674d70a3d8e41870186acba7325485b",
            "title": "SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM",
            "abstract": "Vision-extended LLMs have made significant strides in Visual Question Answering (VQA). Despite these advancements, VLLMs still encounter substantial difficulties in handling queries involving long-tail entities, with a tendency to produce erroneous or hallucinated responses. In this work, we introduce a novel evaluative benchmark named \\textbf{SnapNTell}, specifically tailored for entity-centric VQA. This task aims to test the models' capabilities in identifying entities and providing detailed, entity-specific knowledge. We have developed the \\textbf{SnapNTell Dataset}, distinct from traditional VQA datasets: (1) It encompasses a wide range of categorized entities, each represented by images and explicitly named in the answers; (2) It features QA pairs that require extensive knowledge for accurate responses. The dataset is organized into 22 major categories, containing 7,568 unique entities in total. For each entity, we curated 10 illustrative images and crafted 10 knowledge-intensive QA pairs. To address this novel task, we devised a scalable, efficient, and transparent retrieval-augmented multimodal LLM. Our approach markedly outperforms existing methods on the SnapNTell dataset, achieving a 66.5\\% improvement in the BELURT score. We will soon make the dataset and the source code publicly accessible.",
            "link": "https://www.semanticscholar.org/paper/6d9690ab7674d70a3d8e41870186acba7325485b",
            "authors": "Jielin Qiu, Andrea Madotto, Zhaojiang Lin, Paul A. Crook, Y. Xu, Xin Luna Dong, Christos Faloutsos, Lei Li, Babak Damavandi, Seungwhan Moon",
            "matchScore": 299.3404,
            "original title": "SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM",
            "original authors": "Jielin Qiu, Andrea Madotto, Zhaojiang Lin, Paul A. Crook, Yifan Ethan Xu, Xin Luna Dong, Christos Faloutsos, Lei Li, Babak Damavandi, Seungwhan Moon",
            "EMNLP Paper ID": "44",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "d283e26eded435f9e6cc3db9bb305295b4315c86",
            "title": "Enhancing Temporal Modeling of Video LLMs via Time Gating",
            "abstract": "Video Large Language Models (Video LLMs) have achieved impressive performance on video-and-language tasks, such as video question answering. However, most existing Video LLMs neglect temporal information in video data, leading to struggles with temporal-aware video understanding. To address this gap, we propose a Time Gating Video LLM (TG-Vid) designed to enhance temporal modeling through a novel Time Gating module (TG). The TG module employs a time gating mechanism on its sub-modules, comprising gating spatial attention, gating temporal attention, and gating MLP. This architecture enables our model to achieve a robust understanding of temporal information within videos. Extensive evaluation of temporal-sensitive video benchmarks (i.e., MVBench, TempCompass, and NExT-QA) demonstrates that our TG-Vid model significantly outperforms the existing Video LLMs. Further, comprehensive ablation studies validate that the performance gains are attributed to the designs of our TG module. Our code is available at https://github.com/LaVi-Lab/TG-Vid.",
            "link": "https://www.semanticscholar.org/paper/d283e26eded435f9e6cc3db9bb305295b4315c86",
            "authors": "Zi-Yuan Hu, Yiwu Zhong, Shijia Huang, Michael R. Lyu, Liwei Wang",
            "matchScore": 232.25156,
            "original title": "Enhancing Temporal Modeling of Video LLMs via Time Gating",
            "original authors": "Zi-Yuan Hu, Yiwu Zhong, Shijia Huang, Michael Lyu, Liwei Wang",
            "EMNLP Paper ID": "583",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        }
    ],
    "Applications of Large Language Models in Mental Health Care": [
        {
            "paperId": "7b75b3d9f08aea9498baef8426f954106c3b5802",
            "title": "When LLMs Meets Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection",
            "abstract": "Depression is a critical concern in global mental health, prompting extensive research into AI-based detection methods. Among various AI technologies, Large Language Models (LLMs) stand out for their versatility in mental healthcare applications. However, their primary limitation arises from their exclusive dependence on textual input, which constrains their overall capabilities. Furthermore, the utilization of LLMs in identifying and analyzing depressive states is still relatively untapped. In this paper, we present an innovative approach to integrating acoustic speech information into the LLMs framework for multimodal depression detection. We investigate an efficient method for depression detection by integrating speech signals into LLMs utilizing Acoustic Landmarks. By incorporating acoustic landmarks, which are specific to the pronunciation of spoken words, our method adds critical dimensions to text transcripts. This integration also provides insights into the unique speech patterns of individuals, revealing the potential mental states of individuals. Evaluations of the proposed approach on the DAIC-WOZ dataset reveal state-of-the-art results when compared with existing Audio-Text baselines. In addition, this approach is not only valuable for the detection of depression but also represents a new perspective in enhancing the ability of LLMs to comprehend and process speech signals.",
            "link": "https://www.semanticscholar.org/paper/7b75b3d9f08aea9498baef8426f954106c3b5802",
            "authors": "Xiangyu Zhang, Hexin Liu, Kaishuai Xu, Qiquan Zhang, Daijiao Liu, Beena Ahmed, Julien Epps",
            "EMNLP Paper ID": "11",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "6ae28b27d5e81aa7ad2dfdc7e3d712870159a7bc",
            "title": "Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems",
            "abstract": "Intelligent Tutoring Systems (ITSs) can provide personalized and self-paced learning experience. The emergence of large language models (LLMs) further enables better human-machine interaction, and facilitates the development of conversational ITSs in various disciplines such as math and language learning. In dialogic teaching, recognizing and adapting to individual characteristics can significantly enhance student engagement and learning efficiency. However, characterizing and simulating student's persona remain challenging in training and evaluating conversational ITSs. In this work, we propose a framework to construct profiles of different student groups by refining and integrating both cognitive and noncognitive aspects, and leverage LLMs for personality-aware student simulation in a language learning scenario. We further enhance the framework with multi-aspect validation, and conduct extensive analysis from both teacher and student perspectives. Our experimental results show that state-of-the-art LLMs can produce diverse student responses according to the given language ability and personality traits, and trigger teacher's adaptive scaffolding strategies.",
            "link": "https://www.semanticscholar.org/paper/6ae28b27d5e81aa7ad2dfdc7e3d712870159a7bc",
            "authors": "Zhengyuan Liu, Stella Xin Yin, Geyu Lin, Nancy F. Chen",
            "EMNLP Paper ID": "84",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "c0bf1f4672b318541df036ab118dc2c127845bbf",
            "title": "PsyGUARD: An Automated System for Suicide Detection and Risk Assessment in Psychological Counseling",
            "abstract": "As awareness of mental health issues grows, online counseling support services are becoming increasingly prevalent worldwide. Detecting whether users express suicidal ideation in text-based counseling services is crucial for identifying and prioritizing at-risk individuals. However, the lack of domain-specific systems to facilitate fine-grained suicide detection and corresponding risk assessment in online counseling poses a significant challenge for automated crisis intervention aimed at suicide prevention. In this paper, we propose PsyGUARD, an automated system for detecting suicide ideation and assessing risk in psychological counseling. To achieve this, we first develop a detailed taxonomy for detecting suicide ideation based on foundational theories. We then curate a large-scale, high-quality dataset called PsySUICIDE for suicide detection. To evaluate the capabilities of automated systems in fine-grained suicide detection, we establish a range of baselines. Subsequently, to assist automated services in providing safe, helpful, and tailored responses for further assessment, we propose to build a suite of risk assessment frameworks. Our study not only provides an insightful analysis of the effectiveness of automated risk assessment systems based on fine-grained suicide detection but also highlights their potential to improve mental health services on online counseling platforms. Code, data, and models are available at https://github.com/qiuhuachuan/PsyGUARD.",
            "link": "https://www.semanticscholar.org/paper/c0bf1f4672b318541df036ab118dc2c127845bbf",
            "authors": "Huachuan Qiu, Lizhi Ma, Zhenzhong Lan",
            "EMNLP Paper ID": "502",
            "Oral/Poster": "Oral",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "5678a1ee9d5542785115555b856e51a1dd9eb0e9",
            "title": "\"In Dialogues We Learn\": Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning",
            "abstract": "Personalized dialogue systems have gained significant attention in recent years for their ability to generate responses in alignment with different personas. However, most existing approaches rely on pre-defined personal profiles, which are not only time-consuming and labor-intensive to create but also lack flexibility. We propose In-Dialogue Learning (IDL), a fine-tuning framework that enhances the ability of pre-trained large language models to leverage dialogue history to characterize persona for completing personalized dialogue generation tasks without pre-defined profiles. Our experiments on three datasets demonstrate that IDL brings substantial improvements, with BLEU and ROUGE scores increasing by up to 200% and 247%, respectively. Additionally, the results of human evaluations further validate the efficacy of our proposed method.",
            "link": "https://www.semanticscholar.org/paper/5678a1ee9d5542785115555b856e51a1dd9eb0e9",
            "authors": "Chuanqi Cheng, Quan Tu, Wei Wu, Shuo Shang, Cunli Mao, Zhengtao Yu, Rui Yan",
            "EMNLP Paper ID": "1179",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "2e4774b6feff0ed8af83e31bf99fc1cd3c24c5cf",
            "title": "Roleplay-doh: Enabling Domain-Experts to Create LLM-simulated Patients via Eliciting and Adhering to Principles",
            "abstract": "Recent works leverage LLMs to roleplay realistic social scenarios, aiding novices in practicing their social skills. However, simulating sensitive interactions, such as in mental health, is challenging. Privacy concerns restrict data access, and collecting expert feedback, although vital, is laborious. To address this, we develop Roleplay-doh, a novel human-LLM collaboration pipeline that elicits qualitative feedback from a domain-expert, which is transformed into a set of principles, or natural language rules, that govern an LLM-prompted roleplay. We apply this pipeline to enable senior mental health supporters to create customized AI patients for simulated practice partners for novice counselors. After uncovering issues in GPT-4 simulations not adhering to expert-defined principles, we also introduce a novel principle-adherence prompting pipeline which shows 30% improvements in response quality and principle following for the downstream task. Via a user study with 25 counseling experts, we demonstrate that the pipeline makes it easy and effective to create AI patients that more faithfully resemble real patients, as judged by creators and third-party counselors. See our project website at https://roleplay-doh.github.io/ for code and data.",
            "link": "https://www.semanticscholar.org/paper/2e4774b6feff0ed8af83e31bf99fc1cd3c24c5cf",
            "authors": "Ryan Louie, Ananjan Nandi, William Fang, Cheng Chang, E. Brunskill, Diyi Yang",
            "EMNLP Paper ID": "1194",
            "Oral/Poster": "Oral",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "4daff69178cdc5b3c0ba3caf56232ed3500dea92",
            "title": "PATIENT-\u03a8: Using Large Language Models to Simulate Patients for Training Mental Health Professionals",
            "abstract": "Mental illness remains one of the most critical public health issues. Despite its importance, many mental health professionals highlight a disconnect between their training and actual real-world patient practice. To help bridge this gap, we propose PATIENT-{\\Psi}, a novel patient simulation framework for cognitive behavior therapy (CBT) training. To build PATIENT-{\\Psi}, we construct diverse patient cognitive models based on CBT principles and use large language models (LLMs) programmed with these cognitive models to act as a simulated therapy patient. We propose an interactive training scheme, PATIENT-{\\Psi}-TRAINER, for mental health trainees to practice a key skill in CBT -- formulating the cognitive model of the patient -- through role-playing a therapy session with PATIENT-{\\Psi}. To evaluate PATIENT-{\\Psi}, we conducted a comprehensive user study of 13 mental health trainees and 20 experts. The results demonstrate that practice using PATIENT-{\\Psi}-TRAINER enhances the perceived skill acquisition and confidence of the trainees beyond existing forms of training such as textbooks, videos, and role-play with non-patients. Based on the experts' perceptions, PATIENT-{\\Psi} is perceived to be closer to real patient interactions than GPT-4, and PATIENT-{\\Psi}-TRAINER holds strong promise to improve trainee competencies. Our code and data are released at \\url{https://github.com/ruiyiw/patient-psi}.",
            "link": "https://www.semanticscholar.org/paper/4daff69178cdc5b3c0ba3caf56232ed3500dea92",
            "authors": "Ruiyi Wang, Stephanie Milani, Jamie C. Chiu, S. Eack, Travis Labrum, Samuel M. Murphy, Nev Jones, Kate Hardy, Hong Shen, Fei Fang, Z. Chen",
            "EMNLP Paper ID": "1487",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "202f3a1341a9e614c2611de4809dd12fe7850f36",
            "title": "ESC-Eval: Evaluating Emotion Support Conversations in Large Language Models",
            "abstract": "Emotion Support Conversation (ESC) is a crucial application, which aims to reduce human stress, offer emotional guidance, and ultimately enhance human mental and physical well-being. With the advancement of Large Language Models (LLMs), many researchers have employed LLMs as the ESC models. However, the evaluation of these LLM-based ESCs remains uncertain. Inspired by the awesome development of role-playing agents, we propose an ESC Evaluation framework (ESC-Eval), which uses a role-playing agent to interact with ESC models, followed by a manual evaluation of the interactive dialogues. In detail, we first re-organize 2,801 role-playing cards from seven existing datasets to define the roles of the role-playing agent. Second, we train a specific role-playing model called ESC-Role which behaves more like a confused person than GPT-4. Third, through ESC-Role and organized role cards, we systematically conduct experiments using 14 LLMs as the ESC models, including general AI-assistant LLMs (ChatGPT) and ESC-oriented LLMs (ExTES-Llama). We conduct comprehensive human annotations on interactive multi-turn dialogues of different ESC models. The results show that ESC-oriented LLMs exhibit superior ESC abilities compared to general AI-assistant LLMs, but there is still a gap behind human performance. Moreover, to automate the scoring process for future ESC models, we developed ESC-RANK, which trained on the annotated data, achieving a scoring performance surpassing 35 points of GPT-4. Our data and code are available at https://github.com/AIFlames/Esc-Eval.",
            "link": "https://www.semanticscholar.org/paper/202f3a1341a9e614c2611de4809dd12fe7850f36",
            "authors": "Haiquan Zhao, Lingyu Li, Shisong Chen, Shuqi Kong, Jiaan Wang, Kexin Huang, Tianle Gu, Yixu Wang, Dandan Liang, Zhixu Li, Yan Teng, Yanghua Xiao, Yingchun Wang",
            "EMNLP Paper ID": "1852",
            "Oral/Poster": "Oral",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "7c70f359f489a01271f0055718cb9d156f7a50c4",
            "title": "Still Not Quite There! Evaluating Large Language Models for Comorbid Mental Health Diagnosis",
            "abstract": "In this study, we introduce ANGST, a novel, first-of-its kind benchmark for depression-anxiety comorbidity classification from social media posts. Unlike contemporary datasets that often oversimplify the intricate interplay between different mental health disorders by treating them as isolated conditions, ANGST enables multi-label classification, allowing each post to be simultaneously identified as indicating depression and/or anxiety. Comprising 2876 meticulously annotated posts by expert psychologists and an additional 7667 silver-labeled posts, ANGST posits a more representative sample of online mental health discourse. Moreover, we benchmark ANGST using various state-of-the-art language models, ranging from Mental-BERT to GPT-4. Our results provide significant insights into the capabilities and limitations of these models in complex diagnostic scenarios. While GPT-4 generally outperforms other models, none achieve an F1 score exceeding 72% in multi-class comorbid classification, underscoring the ongoing challenges in applying language models to mental health diagnostics.",
            "link": "https://www.semanticscholar.org/paper/7c70f359f489a01271f0055718cb9d156f7a50c4",
            "authors": "Amey Hengle, Atharva Kulkarni, Shantanu Patankar, Madhumitha Chandrasekaran, Sneha D'Silva, Jemima Jacob, Rashmi Gupta",
            "EMNLP Paper ID": "1970",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "55d322ddac05dcf89fc43cc859ca136471877bbd",
            "title": "Knowledge Planning in Large Language Models for Domain-Aligned Counseling Summarization",
            "abstract": "In mental health counseling, condensing dialogues into concise and relevant summaries (aka counseling notes) holds pivotal significance. Large Language Models (LLMs) exhibit remarkable capabilities in various generative tasks; however, their adaptation to domain-specific intricacies remains challenging, especially within mental health contexts. Unlike standard LLMs, mental health experts first plan to apply domain knowledge in writing summaries. Our work enhances LLMs' ability by introducing a novel planning engine to orchestrate structuring knowledge alignment. To achieve high-order planning, we divide knowledge encapsulation into two major phases: (i) holding dialogue structure and (ii) incorporating domain-specific knowledge. We employ a planning engine on Llama-2, resulting in a novel framework, PIECE. Our proposed system employs knowledge filtering-cum-scaffolding to encapsulate domain knowledge. Additionally, PIECE leverages sheaf convolution learning to enhance its understanding of the dialogue's structural nuances. We compare PIECE with 14 baseline methods and observe a significant improvement across ROUGE and Bleurt scores. Further, expert evaluation and analyses validate the generation quality to be effective, sometimes even surpassing the gold standard. We further benchmark PIECE with other LLMs and report improvement, including Llama-2 (+2.72%), Mistral (+2.04%), and Zephyr (+1.59%), to justify the generalizability of the planning engine.",
            "link": "https://www.semanticscholar.org/paper/55d322ddac05dcf89fc43cc859ca136471877bbd",
            "authors": "Aseem Srivastava, Smriti Joshi, Tanmoy Chakraborty, Md. Shad Akhtar",
            "EMNLP Paper ID": "2135",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "0bc6cd458b53555a667ea9c06c285288b5fe0ccb",
            "title": "Emotion Granularity from Text: An Aggregate-Level Indicator of Mental Health",
            "abstract": "We are united in how emotions are central to shaping our experiences; and yet, individuals differ greatly in how we each identify, categorize, and express emotions. In psychology, variation in the ability of individuals to differentiate between emotion concepts is called emotion granularity (determined through self-reports of one's emotions). High emotion granularity has been linked with better mental and physical health; whereas low emotion granularity has been linked with maladaptive emotion regulation strategies and poor health outcomes. In this work, we propose computational measures of emotion granularity derived from temporally-ordered speaker utterances in social media (in lieu of self-reports that suffer from various biases). We then investigate the effectiveness of such text-derived measures of emotion granularity in functioning as markers of various mental health conditions (MHCs). We establish baseline measures of emotion granularity derived from textual utterances, and show that, at an aggregate level, emotion granularities are significantly lower for people self-reporting as having an MHC than for the control population. This paves the way towards a better understanding of the MHCs, and specifically the role emotions play in our well-being.",
            "link": "https://www.semanticscholar.org/paper/0bc6cd458b53555a667ea9c06c285288b5fe0ccb",
            "authors": "Krishnapriya Vishnubhotla, Daniela Teodorescu, Mallory J. Feldman, Kristen A. Lindquist, Saif M. Mohammad",
            "EMNLP Paper ID": "2429",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "eeed21017b19fde7f29294be8e72cc7e59b9ee94",
            "title": "BLSP-Emo: Towards Empathetic Large Speech-Language Models",
            "abstract": "The recent release of GPT-4o showcased the potential of end-to-end multimodal models, not just in terms of low latency but also in their ability to understand and generate expressive speech with rich emotions. While the details are unknown to the open research community, it likely involves significant amounts of curated data and compute, neither of which is readily accessible. In this paper, we present BLSP-Emo (Bootstrapped Language-Speech Pretraining with Emotion support), a novel approach to developing an end-to-end speech-language model capable of understanding both semantics and emotions in speech and generate empathetic responses. BLSP-Emo utilizes existing speech recognition (ASR) and speech emotion recognition (SER) datasets through a two-stage process. The first stage focuses on semantic alignment, following recent work on pretraining speech-language models using ASR data. The second stage performs emotion alignment with the pretrained speech-language model on an emotion-aware continuation task constructed from SER data. Our experiments demonstrate that the BLSP-Emo model excels in comprehending speech and delivering empathetic responses, both in instruction-following tasks and conversations.",
            "link": "https://www.semanticscholar.org/paper/eeed21017b19fde7f29294be8e72cc7e59b9ee94",
            "authors": "Chen Wang, Minpeng Liao, Zhongqiang Huang, Junhong Wu, Chengqing Zong, Jiajun Zhang",
            "EMNLP Paper ID": "2435",
            "Oral/Poster": "Oral",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "ccd819bcacc0a37a799a0c50136f94f3d5da2a04",
            "title": "MASIVE: Open-Ended Affective State Identification in English and Spanish",
            "abstract": "In the field of emotion analysis, much NLP research focuses on identifying a limited number of discrete emotion categories, often applied across languages. These basic sets, however, are rarely designed with textual data in mind, and culture, language, and dialect can influence how particular emotions are interpreted. In this work, we broaden our scope to a practically unbounded set of \\textit{affective states}, which includes any terms that humans use to describe their experiences of feeling. We collect and publish MASIVE, a dataset of Reddit posts in English and Spanish containing over 1,000 unique affective states each. We then define the new problem of \\textit{affective state identification} for language generation models framed as a masked span prediction task. On this task, we find that smaller finetuned multilingual models outperform much larger LLMs, even on region-specific Spanish affective states. Additionally, we show that pretraining on MASIVE improves model performance on existing emotion benchmarks. Finally, through machine translation experiments, we find that native speaker-written data is vital to good performance on this task.",
            "link": "https://www.semanticscholar.org/paper/ccd819bcacc0a37a799a0c50136f94f3d5da2a04",
            "authors": "Nicholas Deas, Elsbeth Turcan, Iv'an P'erez Mej'ia, Kathleen McKeown",
            "EMNLP Paper ID": "2677",
            "Oral/Poster": "Oral",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "83bc360fa9874564cf8ccbd3a00e6e14dbe4e4e6",
            "title": "SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support",
            "abstract": "Developing specialized dialogue systems for mental health support requires multi-turn conversation data, which has recently garnered increasing attention. However, gathering and releasing large-scale, real-life multi-turn conversations that could facilitate advancements in mental health support presents challenges in data privacy protection and the time and cost involved in crowdsourcing. To address these challenges, we introduce SMILE, a single-turn to multi-turn inclusive language expansion technique that prompts ChatGPT to rewrite public single-turn dialogues into multi-turn ones. Our work begins by analyzing language transformation and validating the feasibility of our proposed method. We conduct a study on dialogue diversity, including lexical features, semantic features, and dialogue topics, demonstrating the effectiveness of our method. Further, we employ our method to generate a large-scale, lifelike, and diverse dialogue dataset named SMILECHAT, consisting of 55k dialogues. Finally, we utilize the collected corpus to develop a mental health chatbot, MeChat. To better assess the quality of SMILECHAT, we collect a small-scale real-life counseling dataset conducted by data anonymization. Both automatic and human evaluations demonstrate significant improvements in our dialogue system and confirm that SMILECHAT is high-quality. Code, data, and model are publicly available at https://github.com/qiuhuachuan/smile.",
            "link": "https://www.semanticscholar.org/paper/83bc360fa9874564cf8ccbd3a00e6e14dbe4e4e6",
            "authors": "Huachuan Qiu, Hongliang He, Shuai Zhang, Anqi Li, Zhenzhong Lan",
            "matchScore": 350.37177,
            "original title": "SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support",
            "original authors": "Huachuan Qiu, Hongliang He, Shuai Zhang, Anqi Li, Zhenzhong Lan",
            "EMNLP Paper ID": "111",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "9ab5cf4423547ac0ff37db234ff5d65290c11c8f",
            "title": "Mental Disorder Classification via Temporal Representation of Text",
            "abstract": "Mental disorders pose a global challenge, aggravated by the shortage of qualified mental health professionals. Mental disorder prediction from social media posts by current LLMs is challenging due to the complexities of sequential text data and the limited context length of language models. Current language model-based approaches split a single data instance into multiple chunks to compensate for limited context size. The predictive model is then applied to each chunk individually, and the most voted output is selected as the final prediction. This results in the loss of inter-post dependencies and important time variant information, leading to poor performance. We propose a novel framework which first compresses the large sequence of chronologically ordered social media posts into a series of numbers. We then use this time variant representation for mental disorder classification. We demonstrate the generalization capabilities of our framework by outperforming the current SOTA in three different mental conditions: depression, self-harm, and anorexia, with an absolute improvement of 5% in the F1 score. We investigate the situation where current data instances fall within the context length of language models and present empirical results highlighting the importance of temporal properties of textual data. Furthermore, we utilize the proposed framework for a cross-domain study, exploring commonalities across disorders and the possibility of inter-domain data usage.",
            "link": "https://www.semanticscholar.org/paper/9ab5cf4423547ac0ff37db234ff5d65290c11c8f",
            "authors": "Raja Kumar, Kishan Maharaj, Ashita Saxena, Pushpak Bhattacharyya",
            "matchScore": 220.03445,
            "original title": "Mental Disorder Classification via Temporal Representation of Text",
            "original authors": "Raja Kumar, Kishan Maharaj, Ashita Saxena, Pushpak Bhattacharyya",
            "EMNLP Paper ID": "2184",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "1acd860ddff9ccd6e30d6f3597f8719fb0100de4",
            "title": "Mixed-Session Conversation with Egocentric Memory",
            "abstract": "Recently introduced dialogue systems have demonstrated high usability. However, they still fall short of reflecting real-world conversation scenarios. Current dialogue systems exhibit an inability to replicate the dynamic, continuous, long-term interactions involving multiple partners. This shortfall arises because there have been limited efforts to account for both aspects of real-world dialogues: deeply layered interactions over the long-term dialogue and widely expanded conversation networks involving multiple participants. As the effort to incorporate these aspects combined, we introduce Mixed-Session Conversation, a dialogue system designed to construct conversations with various partners in a multi-session dialogue setup. We propose a new dataset called MiSC to implement this system. The dialogue episodes of MiSC consist of 6 consecutive sessions, with four speakers (one main speaker and three partners) appearing in each episode. Also, we propose a new dialogue model with a novel memory management mechanism, called Egocentric Memory Enhanced Mixed-Session Conversation Agent (EMMA). EMMA collects and retains memories from the main speaker's perspective during conversations with partners, enabling seamless continuity in subsequent interactions. Extensive human evaluations validate that the dialogues in MiSC demonstrate a seamless conversational flow, even when conversation partners change in each session. EMMA trained with MiSC is also evaluated to maintain high memorability without contradiction throughout the entire conversation.",
            "link": "https://www.semanticscholar.org/paper/1acd860ddff9ccd6e30d6f3597f8719fb0100de4",
            "authors": "Jihyoung Jang, Taeyoung Kim, Hyounghun Kim",
            "matchScore": 215.05264,
            "original title": "Mixed-Session Conversation with Egocentric Memory",
            "original authors": "Jihyoung Jang, Taeyoung Kim, Hyounghun Kim",
            "EMNLP Paper ID": "2323",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "fcc97a38c5eb912bb6629894a69a1a3eb17c092a",
            "title": "Stark: Social Long-Term Multi-Modal Conversation with Persona Commonsense Knowledge",
            "abstract": "Humans share a wide variety of images related to their personal experiences within conversations via instant messaging tools. However, existing works focus on (1) image-sharing behavior in singular sessions, leading to limited long-term social interaction, and (2) a lack of personalized image-sharing behavior. In this work, we introduce Stark, a large-scale long-term multi-modal conversation dataset that covers a wide range of social personas in a multi-modality format, time intervals, and images. To construct Stark automatically, we propose a novel multi-modal contextualization framework, Mcu, that generates long-term multi-modal dialogue distilled from ChatGPT and our proposed Plan-and-Execute image aligner. Using our Stark, we train a multi-modal conversation model, Ultron 7B, which demonstrates impressive visual imagination ability. Furthermore, we demonstrate the effectiveness of our dataset in human evaluation. We make our source code and dataset publicly available.",
            "link": "https://www.semanticscholar.org/paper/fcc97a38c5eb912bb6629894a69a1a3eb17c092a",
            "authors": "Young-Jun Lee, Dokyong Lee, Junyoung Youn, Kyeongjin Oh, ByungSoo Ko, Jonghwan Hyeon, Ho-Jin Choi",
            "matchScore": 313.9723,
            "original title": "Stark: Social Long-Term Multi-Modal Conversation with Persona Commonsense Knowledge",
            "original authors": "Young-Jun Lee, Dokyong Lee, junyoung youn, Kyeong-Jin Oh, Byungsoo Ko, Jonghwan Hyeon, Ho-Jin Choi",
            "EMNLP Paper ID": "2385",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "9a1b11b562d3959e53c9fb7ea8027e132ac209ae",
            "title": "Understanding the Therapeutic Relationship between Counselors and Clients in Online Text-based Counseling using LLMs",
            "abstract": "Robust therapeutic relationships between counselors and clients are fundamental to counseling effectiveness. The assessment of therapeutic alliance is well-established in traditional face-to-face therapy but may not directly translate to text-based settings. With millions of individuals seeking support through online text-based counseling, understanding the relationship in such contexts is crucial. In this paper, we present an automatic approach using large language models (LLMs) to understand the development of therapeutic alliance in text-based counseling. We adapt a theoretically grounded framework specifically to the context of online text-based counseling and develop comprehensive guidelines for characterizing the alliance. We collect a comprehensive counseling dataset and conduct multiple expert evaluations on a subset based on this framework. Our LLM-based approach, combined with guidelines and simultaneous extraction of supportive evidence underlying its predictions, demonstrates effectiveness in identifying the therapeutic alliance. Through further LLM-based evaluations on additional conversations, our findings underscore the challenges counselors face in cultivating strong online relationships with clients. Furthermore, we demonstrate the potential of LLM-based feedback mechanisms to enhance counselors' ability to build relationships, supported by a small-scale proof-of-concept.",
            "link": "https://www.semanticscholar.org/paper/9a1b11b562d3959e53c9fb7ea8027e132ac209ae",
            "authors": "Anqi Li, Yu Lu, Nirui Song, Shuai Zhang, Lizhi Ma, Zhenzhong Lan",
            "matchScore": 306.56342,
            "original title": "Understanding the Therapeutic Relationship between Counselors and Clients in Online Text-based Counseling using LLMs",
            "original authors": "Anqi Li, Yu Lu, Nirui Song, Shuai Zhang, Lizhi Ma, Zhenzhong Lan",
            "EMNLP Paper ID": "258",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "28f2d200e056b214d8e038d86fb7c57744df7c02",
            "title": "The Language of Trauma: Modeling Traumatic Event Descriptions Across Domains with Explainable AI",
            "abstract": "Psychological trauma can manifest following various distressing events and is captured in diverse online contexts. However, studies traditionally focus on a single aspect of trauma, often neglecting the transferability of findings across different scenarios. We address this gap by training language models with progressing complexity on trauma-related datasets, including genocide-related court data, a Reddit dataset on post-traumatic stress disorder (PTSD), counseling conversations, and Incel forum posts. Our results show that the fine-tuned RoBERTa model excels in predicting traumatic events across domains, slightly outperforming large language models like GPT-4. Additionally, SLALOM-feature scores and conceptual explanations effectively differentiate and cluster trauma-related language, highlighting different trauma aspects and identifying sexual abuse and experiences related to death as a common traumatic event across all datasets. This transferability is crucial as it allows for the development of tools to enhance trauma detection and intervention in diverse populations and settings.",
            "link": "https://www.semanticscholar.org/paper/28f2d200e056b214d8e038d86fb7c57744df7c02",
            "authors": "Miriam Schirmer, Tobias Leemann, Gjergji Kasneci, J\u00fcrgen Pfeffer, David Jurgens",
            "matchScore": 284.35687,
            "original title": "The Language of Trauma: Modeling Traumatic Event Descriptions Across Domains with Explainable AI",
            "original authors": "Miriam Schirmer, Tobias Leemann, Gjergji Kasneci, J\u00fcrgen Pfeffer, David Jurgens",
            "EMNLP Paper ID": "2580",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "962c67e5e563e45bb493664941b93822848bb977",
            "title": "Can LLMs Understand the Implication of Emphasized Sentences in Dialogue?",
            "abstract": "Emphasis is a crucial component in human communication, which indicates the speaker's intention and implication beyond pure text in dialogue. While Large Language Models (LLMs) have revolutionized natural language processing, their ability to understand emphasis in dialogue remains unclear. This paper introduces Emphasized-Talk, a benchmark with emphasis-annotated dialogue samples capturing the implications of emphasis. We evaluate various LLMs, both open-source and commercial, to measure their performance in understanding emphasis. Additionally, we propose an automatic evaluation pipeline using GPT-4, which achieves a high correlation with human rating. Our findings reveal that although commercial LLMs generally perform better, there is still significant room for improvement in comprehending emphasized sentences.",
            "link": "https://www.semanticscholar.org/paper/962c67e5e563e45bb493664941b93822848bb977",
            "authors": "Guan-Ting Lin, Hung-yi Lee",
            "matchScore": 261.47754,
            "original title": "Can LLMs Understand the Implication of Emphasized Sentences in Dialogue?",
            "original authors": "Guan-Ting Lin, Hung-yi Lee",
            "EMNLP Paper ID": "2603",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "378702db60e4e9761dcdb0b73f0b9a1549bbbb58",
            "title": "Cactus: Towards Psychological Counseling Conversations using Cognitive Behavioral Theory",
            "abstract": "Recently, the demand for psychological counseling has significantly increased as more individuals express concerns about their mental health. This surge has accelerated efforts to improve the accessibility of counseling by using large language models (LLMs) as counselors. To ensure client privacy, training open-source LLMs faces a key challenge: the absence of realistic counseling datasets. To address this, we introduce Cactus, a multi-turn dialogue dataset that emulates real-life interactions using the goal-oriented and structured approach of Cognitive Behavioral Therapy (CBT). We create a diverse and realistic dataset by designing clients with varied, specific personas, and having counselors systematically apply CBT techniques in their interactions. To assess the quality of our data, we benchmark against established psychological criteria used to evaluate real counseling sessions, ensuring alignment with expert evaluations. Experimental results demonstrate that Camel, a model trained with Cactus, outperforms other models in counseling skills, highlighting its effectiveness and potential as a counseling agent. We make our data, model, and code publicly available.",
            "link": "https://www.semanticscholar.org/paper/378702db60e4e9761dcdb0b73f0b9a1549bbbb58",
            "authors": "Suyeon Lee, Sunghwan Kim, Minju Kim, Dongjin Kang, Dongil Yang, Harim Kim, Minseok Kang, Dayi Jung, Min Hee Kim, Seungbeen Lee, Kyoung-Mee Chung, Youngjae Yu, Dongha Lee, Jinyoung Yeo",
            "matchScore": 265.1166,
            "original title": "Cactus: Towards Psychological Counseling Conversations using Cognitive Behavioral Theory",
            "original authors": "Suyeon Lee, Sunghwan Kim, Minju Kim, Dongjin Kang, Dongil Yang, Harim Kim, Minseok Kang, Dayi jung, Min Hee Kim, Seungbeen Lee, Kyong-Mee Chung, Youngjae Yu, Dongha Lee, Jinyoung Yeo",
            "EMNLP Paper ID": "2757",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "aa335755b411de74ca37536f636b76ab87bcda07",
            "title": "Can Machines Resonate with Humans? Evaluating the Emotional and Empathic Comprehension of LMs",
            "abstract": "Empathy plays a pivotal role in fostering prosocial behavior, often triggered by the sharing of personal experiences through narratives. However, modeling empathy using NLP approaches remains challenging due to its deep interconnection with human interaction dynamics. Previous approaches, which involve fine-tuning language models (LMs) on human-annotated empathic datasets, have had limited success. In our pursuit of improving empathy understanding in LMs, we propose several strategies, including contrastive learning with masked LMs and supervised fine-tuning with Large Language Models (LLMs). While these methods show improvements over previous methods, the overall results remain unsatisfactory. To better understand this trend, we performed an analysis which reveals a low agreement among annotators. This lack of consensus hinders training and highlights the subjective nature of the task. We also explore the cultural impact on annotations. To study this, we meticulously collected story pairs in Urdu language and find that subjectivity in interpreting empathy among annotators appears to be independent of cultural background. The insights from our systematic exploration of LMs' understanding of empathy suggest that there is considerable room for exploration in both task formulation and modeling.",
            "link": "https://www.semanticscholar.org/paper/aa335755b411de74ca37536f636b76ab87bcda07",
            "authors": "Muhammad Arslan Manzoor, Yuxia Wang, Minghan Wang, Preslav Nakov",
            "matchScore": 275.04065,
            "original title": "Can Machines Resonate with Humans? Evaluating the Emotional and Empathic Comprehension of LMs",
            "original authors": "Muhammad Arslan Manzoor, Yuxia Wang, Minghan Wang, Preslav Nakov",
            "EMNLP Paper ID": "2834",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "1fefae6fbcc0c2bed713dc1571231f596e0a86dd",
            "title": "When\"A Helpful Assistant\"Is Not Really Helpful: Personas in System Prompts Do Not Improve Performances of Large Language Models",
            "abstract": "Prompting serves as the major way humans interact with Large Language Models (LLM). Commercial AI systems commonly define the role of the LLM in system prompts. For example, ChatGPT uses ``You are a helpful assistant'' as part of its default system prompt. Despite current practices of adding personas to system prompts, it remains unclear how different personas affect a model's performance on objective tasks. In this study, we present a systematic evaluation of personas in system prompts. We curate a list of 162 roles covering 6 types of interpersonal relationships and 8 domains of expertise. Through extensive analysis of 4 popular families of LLMs and 2,410 factual questions, we demonstrate that adding personas in system prompts does not improve model performance across a range of questions compared to the control setting where no persona is added. Nevertheless, further analysis suggests that the gender, type, and domain of the persona can all influence the resulting prediction accuracies. We further experimented with a list of persona search strategies and found that, while aggregating results from the best persona for each question significantly improves prediction accuracy, automatically identifying the best persona is challenging, with predictions often performing no better than random selection. Overall, our findings suggest that while adding a persona may lead to performance gains in certain settings, the effect of each persona can be largely random. Code and data are available at https://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.",
            "link": "https://www.semanticscholar.org/paper/1fefae6fbcc0c2bed713dc1571231f596e0a86dd",
            "authors": "Mingqian Zheng, Jiaxin Pei, David Jurgens",
            "matchScore": 348.3634,
            "original title": "When \u2018\u2018A Helpful Assistant\u2019\u2019 Is Not Really Helpful: Personas in System Prompts Do Not Improve Performances of Large Language Models",
            "original authors": "Mingqian Zheng, Jiaxin Pei, Lajanugen Logeswaran, Moontae Lee, David Jurgens",
            "EMNLP Paper ID": "2895",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "1b92b9da5925f852cfbe39a5b68f66b32c515252",
            "title": "CASE: Efficient Curricular Data Pre-training for Building Assistive Psychology Expert Models",
            "abstract": "The limited availability of psychologists necessitates efficient identification of individuals requiring urgent mental healthcare. This study explores the use of Natural Language Processing (NLP) pipelines to analyze text data from online mental health forums used for consultations. By analyzing forum posts, these pipelines can flag users who may require immediate professional attention. A crucial challenge in this domain is data privacy and scarcity. To address this, we propose utilizing readily available curricular texts used in institutes specializing in mental health for pre-training the NLP pipelines. This helps us mimic the training process of a psychologist. Our work presents CASE-BERT that flags potential mental health disorders based on forum text. CASE-BERT demonstrates superior performance compared to existing methods, achieving an f1 score of 0.91 for Depression and 0.88 for Anxiety, two of the most commonly reported mental health disorders. Our code and data are publicly available.",
            "link": "https://www.semanticscholar.org/paper/1b92b9da5925f852cfbe39a5b68f66b32c515252",
            "authors": "Sarthak Harne, Monjoy Narayan Choudhury, Madhav Rao, T. Srikanth, Seema Mehrotra, Apoorva Vashisht, Aarushi Basu, Manjit Sodhi",
            "matchScore": 285.40073,
            "original title": "CASE: Efficient Curricular Data Pre-training for Building Assistive Psychology Expert Models",
            "original authors": "Sarthak Harne, Monjoy Narayan Choudhury, Madhav Rao, T K Srikanth, Seema Mehrotra, Apoorva Vashisht, Aarushi Basu, Manjit singh sodhi",
            "EMNLP Paper ID": "3021",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "394c1272a672ddbfdde581c677617b960029c253",
            "title": "Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization",
            "abstract": "The concept of persona, originally adopted in dialogue literature, has re-surged as a promising framework for tailoring large language models (LLMs) to specific context (e.g., personalized search, LLM-as-a-judge). However, the growing research on leveraging persona in LLMs is relatively disorganized and lacks a systematic taxonomy. To close the gap, we present a comprehensive survey to categorize the current state of the field. We identify two lines of research, namely (1) LLM Role-Playing, where personas are assigned to LLMs, and (2) LLM Personalization, where LLMs take care of user personas. Additionally, we introduce existing methods for LLM personality evaluation. To the best of our knowledge, we present the first survey for role-playing and personalization in LLMs under the unified view of persona. We continuously maintain a paper collection to foster future endeavors: https://github.com/MiuLab/PersonaLLM-Survey",
            "link": "https://www.semanticscholar.org/paper/394c1272a672ddbfdde581c677617b960029c253",
            "authors": "Yu-Min Tseng, Yu-Chao Huang, Teng-Yun Hsiao, Yu-Ching Hsu, Jia-Yin Foo, Chao-Wei Huang, Yun-Nung Chen",
            "matchScore": 242.12358,
            "original title": "Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization",
            "original authors": "Yu-Min Tseng, Yu-Chao Huang, Teng-Yun Hsiao, Wei-Lin Chen, Chao-Wei Huang, Yu Meng, Yun-Nung Chen",
            "EMNLP Paper ID": "3195",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "6907de99f81d7bff76571968d44ea7349684aa38",
            "title": "Can AI Relate: Testing Large Language Model Response for Mental Health Support",
            "abstract": "Large language models (LLMs) are already being piloted for clinical use in hospital systems like NYU Langone, Dana-Farber and the NHS. A proposed deployment use case is psychotherapy, where a LLM-powered chatbot can treat a patient undergoing a mental health crisis. Deployment of LLMs for mental health response could hypothetically broaden access to psychotherapy and provide new possibilities for personalizing care. However, recent high-profile failures, like damaging dieting advice offered by the Tessa chatbot to patients with eating disorders, have led to doubt about their reliability in high-stakes and safety-critical settings. In this work, we develop an evaluation framework for determining whether LLM response is a viable and ethical path forward for the automation of mental health treatment. Our framework measures equity in empathy and adherence of LLM responses to motivational interviewing theory. Using human evaluation with trained clinicians and automatic quality-of-care metrics grounded in psychology research, we compare the responses provided by peer-to-peer responders to those provided by a state-of-the-art LLM. We show that LLMs like GPT-4 use implicit and explicit cues to infer patient demographics like race. We then show that there are statistically significant discrepancies between patient subgroups: Responses to Black posters consistently have lower empathy than for any other demographic group (2%-13% lower than the control group). Promisingly, we do find that the manner in which responses are generated significantly impacts the quality of the response. We conclude by proposing safety guidelines for the potential deployment of LLMs for mental health response.",
            "link": "https://www.semanticscholar.org/paper/6907de99f81d7bff76571968d44ea7349684aa38",
            "authors": "Saadia Gabriel, Isha Puri, Xuhai Xu, Matteo Malgaroli, Marzyeh Ghassemi",
            "matchScore": 246.6553,
            "original title": "Can AI Relate: Testing Large Language Model Response for Mental Health Support",
            "original authors": "Saadia Gabriel, Isha Puri, Xuhai Xu, Matteo Malgaroli, Marzyeh Ghassemi",
            "EMNLP Paper ID": "437",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 14): 13:00\u201314:00 (Afternoon)",
            "Type": "Findings"
        },
        {
            "paperId": "a4694af9214241b6131f2b97563657a7ddd5a1a3",
            "title": "LLM Questionnaire Completion for Automatic Psychiatric Assessment",
            "abstract": "We employ a Large Language Model (LLM) to convert unstructured psychological interviews into structured questionnaires spanning various psychiatric and personality domains. The LLM is prompted to answer these questionnaires by impersonating the interviewee. The obtained answers are coded as features, which are used to predict standardized psychiatric measures of depression (PHQ-8) and PTSD (PCL-C), using a Random Forest regressor. Our approach is shown to enhance diagnostic accuracy compared to multiple baselines. It thus establishes a novel framework for interpreting unstructured psychological interviews, bridging the gap between narrative-driven and data-driven approaches for mental health assessment.",
            "link": "https://www.semanticscholar.org/paper/a4694af9214241b6131f2b97563657a7ddd5a1a3",
            "authors": "Gony Rosenman, Lior Wolf, Talma Hendler",
            "matchScore": 223.90863,
            "original title": "LLM Questionnaire Completion for Automatic Psychiatric Assessment",
            "original authors": "Gony Rosenman, Talma Hendler, Lior Wolf",
            "EMNLP Paper ID": "70",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "56b028c127cfca5d55000afebcdd16231926d103",
            "title": "EDEN: Empathetic Dialogues for English learning",
            "abstract": "Dialogue systems have been used as conversation partners in English learning, but few have studied whether these systems improve learning outcomes. Student passion and perseverance, or grit, has been associated with language learning success. Recent work establishes that as students perceive their English teachers to be more supportive, their grit improves. Hypothesizing that the same pattern applies to English-teaching chatbots, we create EDEN, a robust open-domain chatbot for spoken conversation practice that provides empathetic feedback. To construct EDEN, we first train a specialized spoken utterance grammar correction model and a high-quality social chit-chat conversation model. We then conduct a preliminary user study with a variety of strategies for empathetic feedback. Our experiment suggests that using adaptive empathetic feedback leads to higher perceived affective support. Furthermore, elements of perceived affective support positively correlate with student grit.",
            "link": "https://www.semanticscholar.org/paper/56b028c127cfca5d55000afebcdd16231926d103",
            "authors": "Siyan Li, Teresa Shao, Zhou Yu, Julia Hirschberg",
            "matchScore": 189.97783,
            "original title": "EDEN: Empathetic Dialogues for English learning",
            "original authors": "Siyan Li, Teresa Shao, Zhou Yu, Julia Hirschberg",
            "EMNLP Paper ID": "710",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        }
    ],
    "Advanced Techniques for Instruction Tuning in Large Language Models": [
        {
            "paperId": "2403ca4ff39727bb1c922891d0320c07004bc17e",
            "title": "Clustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation",
            "abstract": "With contributions from the open-source community, a vast amount of instruction tuning (IT) data has emerged. Given the significant resource allocation required for training and evaluating models, it is advantageous to have an efficient method for selecting high-quality IT data. However, existing methods for instruction data selection have limitations such as relying on fragile external APIs, being affected by biases in GPT models, or reducing the diversity of the selected instruction dataset. In this paper, we propose an industrial-friendly, expert-aligned and diversity-preserved instruction data selection method: Clustering and Ranking (CaR). CaR employs a two-step process: first, it ranks instruction pairs using a high-accuracy (84.25%) scoring model aligned with expert preferences; second, it preserves dataset diversity through clustering. In our experiment, CaR efficiently selected a mere 1.96% of Alpaca's IT data, yet the resulting AlpaCaR model surpassed Alpaca's performance by an average of 32.1% in GPT-4 evaluations. Moreover, we find that data selecting is a consistent paradigm whether the pre-trained model is more capable or the model parameters scaling up. Our approach employs compact models with 550M parameters and incurs just 11.2% of the financial outlay of current methods, enhancing its industrial deployability.",
            "link": "https://www.semanticscholar.org/paper/2403ca4ff39727bb1c922891d0320c07004bc17e",
            "authors": "Yuan Ge, Yilun Liu, Chi Hu, Weibin Meng, Shimin Tao, Xiaofeng Zhao, Hongxia Ma, Li Zhang, Hao Yang, Tong Xiao",
            "EMNLP Paper ID": "66",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "d58ac8e6329c7059ef3ab3d593ba23b22220af1c",
            "title": "AMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation",
            "abstract": "The impressive performance of proprietary LLMs like GPT4 in code generation has led to a trend to replicate these capabilities in open-source models through knowledge distillation (e.g. Code Evol-Instruct). However, these efforts often neglect the crucial aspect of response quality, relying heavily on teacher models for direct response distillation. This paradigm, especially for complex instructions, can degrade the quality of synthesized data, compromising the knowledge distillation process. To this end, our study introduces the Adaptive Modular Response Evolution (AMR-Evol) framework, which employs a two-stage process to refine response distillation. The first stage, modular decomposition, breaks down the direct response into more manageable sub-modules. The second stage, adaptive response evolution, automatically evolves the response with the related function modules. Our experiments with three popular code benchmarks (HumanEval, MBPP, and EvalPlus) attest to the superiority of the AMR-Evol framework over baseline response distillation methods. By comparing with the open-source Code LLMs trained on a similar scale of data, we observed performance enhancements: more than +3.0 points on HumanEval-Plus and +1.0 points on MBPP-Plus, which underscores the effectiveness of our framework. Our codes are available at https://github.com/ChiYeungLaw/AMR-Evol.",
            "link": "https://www.semanticscholar.org/paper/d58ac8e6329c7059ef3ab3d593ba23b22220af1c",
            "authors": "Ziyang Luo, Xin Li, Hongzhan Lin, Jing Ma, Li Bing",
            "EMNLP Paper ID": "140",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "5b7cc3c440147f13f032570213eab40eca8d70c1",
            "title": "Instruction Pre-Training: Language Models are Supervised Multitask Learners",
            "abstract": "Unsupervised multitask pre-training has been the critical method behind the recent success of language models (LMs). However, supervised multitask learning still holds significant promise, as scaling it in the post-training stage trends towards better generalization. In this paper, we explore supervised multitask pre-training by proposing Instruction Pre-Training, a framework that scalably augments massive raw corpora with instruction-response pairs to pre-train LMs. The instruction-response pairs are generated by an efficient instruction synthesizer built on open-source models. In our experiments, we synthesize 200M instruction-response pairs covering 40+ task categories to verify the effectiveness of Instruction Pre-Training. In pre-training from scratch, Instruction Pre-Training not only consistently enhances pre-trained base models but also benefits more from further instruction tuning. In continual pre-training, Instruction Pre-Training enables Llama3-8B to be comparable to or even outperform Llama3-70B. Our model, code, and data are available at https://github.com/microsoft/LMOps.",
            "link": "https://www.semanticscholar.org/paper/5b7cc3c440147f13f032570213eab40eca8d70c1",
            "authors": "Daixuan Cheng, Yuxian Gu, Shaohan Huang, Junyu Bi, Minlie Huang, Furu Wei",
            "EMNLP Paper ID": "282",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 12): 17:45\u201318:45 (Evening)",
            "Type": "Main"
        },
        {
            "paperId": "e38638e31403407e277f86e2f90681d49d2d5679",
            "title": "Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models",
            "abstract": "Modern language models (LMs) need to follow human instructions while being faithful; yet, they often fail to achieve both. Here, we provide concrete evidence of a trade-off between instruction following (i.e., follow open-ended instructions) and faithfulness (i.e., ground responses in given context) when training LMs with these objectives. For instance, fine-tuning LLaMA-7B on instruction following datasets renders it less faithful. Conversely, instruction-tuned Vicuna-7B shows degraded performance at following instructions when further optimized on tasks that require contextual grounding. One common remedy is multi-task learning (MTL) with data mixing, yet it remains far from achieving a synergic outcome. We propose a simple yet effective method that relies on Rejection Sampling for Continued Self-instruction Tuning (ReSet), which significantly outperforms vanilla MTL. Surprisingly, we find that less is more, as training ReSet with high-quality, yet substantially smaller data (three-fold less) yields superior results. Our findings offer a better understanding of objective discrepancies in alignment training of LMs.",
            "link": "https://www.semanticscholar.org/paper/e38638e31403407e277f86e2f90681d49d2d5679",
            "authors": "Zhengxuan Wu, Yuhao Zhang, Peng Qi, Yumo Xu, Rujun Han, Yian Zhang, Jifan Chen, Bonan Min, Zhiheng Huang",
            "EMNLP Paper ID": "444",
            "Oral/Poster": "Oral",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "1e56e3956a86b57d351c305d391cdb65a10c67bb",
            "title": "CoEvol: Constructing Better Responses for Instruction Finetuning through Multi-Agent Cooperation",
            "abstract": "In recent years, instruction fine-tuning (IFT) on large language models (LLMs) has garnered considerable attention to enhance model performance on unseen tasks. Attempts have been made on automatic construction and effective selection for IFT data. However, we posit that previous methods have not fully harnessed the potential of LLMs for enhancing data quality. The responses within IFT data could be further enhanced by leveraging the capabilities of LLMs themselves. In this paper, we propose CoEvol, an LLM-based multi-agent cooperation framework for the improvement of responses to instructions. To effectively refine the responses, we develop an iterative framework following a debate-advise-edit-judge paradigm. A two-stage multi-agent debate strategy is further devised to ensure the diversity and reliability of editing suggestions within the framework. Empirically, models equipped with CoEvol outperform competitive baselines evaluated by MT-Bench and AlpacaEval, demonstrating its effectiveness in enhancing instruction-following capabilities for LLMs.",
            "link": "https://www.semanticscholar.org/paper/1e56e3956a86b57d351c305d391cdb65a10c67bb",
            "authors": "Renhao Li, Minghuan Tan, Derek F. Wong, Min Yang",
            "EMNLP Paper ID": "517",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "83cd74cb7d525cf387bba7b02ebdf9ab30093c68",
            "title": "Automatic Instruction Evolving for Large Language Models",
            "abstract": "Fine-tuning large pre-trained language models with Evol-Instruct has achieved encouraging results across a wide range of tasks. However, designing effective evolving methods for instruction evolution requires substantial human expertise. This paper proposes Auto Evol-Instruct, an end-to-end framework that evolves instruction datasets using large language models without any human effort. The framework automatically analyzes and summarizes suitable evolutionary strategies for the given instruction data and iteratively improves the evolving method based on issues exposed during the instruction evolution process. Our extensive experiments demonstrate that the best method optimized by Auto Evol-Instruct outperforms human-designed methods on various benchmarks, including MT-Bench, AlpacaEval, GSM8K, and HumanEval.",
            "link": "https://www.semanticscholar.org/paper/83cd74cb7d525cf387bba7b02ebdf9ab30093c68",
            "authors": "Weihao Zeng, Can Xu, Yingxiu Zhao, Jianguang Lou, Weizhu Chen",
            "EMNLP Paper ID": "783",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "f51b3338c731e11cd2574de5a02cc28d306df2cf",
            "title": "CommonIT: Commonality-Aware Instruction Tuning for Large Language Models via Data Partitions",
            "abstract": "With instruction tuning, Large Language Models (LLMs) can enhance their ability to adhere to commands. Diverging from most works focusing on data mixing, our study concentrates on enhancing the model's capabilities from the perspective of data sampling during training. Drawing inspiration from the human learning process, where it is generally easier to master solutions to similar topics through focused practice on a single type of topic, we introduce a novel instruction tuning strategy termed CommonIT: Commonality-aware Instruction Tuning. Specifically, we cluster instruction datasets into distinct groups with three proposed metrics (Task, Embedding and Length). We ensure each training mini-batch, or\"partition\", consists solely of data from a single group, which brings about both data randomness across mini-batches and intra-batch data similarity. Rigorous testing on LLaMa models demonstrates CommonIT's effectiveness in enhancing the instruction-following capabilities of LLMs through IT datasets (FLAN, CoT, and Alpaca) and models (LLaMa2-7B, Qwen2-7B, LLaMa 13B, and BLOOM 7B). CommonIT consistently boosts an average improvement of 2.1\\% on the general domain (i.e., the average score of Knowledge, Reasoning, Multilinguality and Coding) with the Length metric, and 5.2\\% on the special domain (i.e., GSM, Openfunctions and Code) with the Task metric, and 3.8\\% on the specific tasks (i.e., MMLU) with the Embedding metric. Code is available at \\url{https://github.com/raojay7/CommonIT}.",
            "link": "https://www.semanticscholar.org/paper/f51b3338c731e11cd2574de5a02cc28d306df2cf",
            "authors": "Jun Rao, Xuebo Liu, Lian Lian, Shengjun Cheng, Yunjie Liao, Min Zhang",
            "EMNLP Paper ID": "1128",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "de4f65ea0c4b029beb2317c0bdc5c16149c18024",
            "title": "COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities",
            "abstract": "Social scientists use surveys to probe the opinions and beliefs of populations, but these methods are slow, costly, and prone to biases. Recent advances in large language models (LLMs) enable the creating of computational representations or\"digital twins\"of populations that generate human-like responses mimicking the population's language, styles, and attitudes. We introduce Community-Cross-Instruct, an unsupervised framework for aligning LLMs to online communities to elicit their beliefs. Given a corpus of a community's online discussions, Community-Cross-Instruct automatically generates instruction-output pairs by an advanced LLM to (1) finetune a foundational LLM to faithfully represent that community, and (2) evaluate the alignment of the finetuned model to the community. We demonstrate the method's utility in accurately representing political and diet communities on Reddit. Unlike prior methods requiring human-authored instructions, Community-Cross-Instruct generates instructions in a fully unsupervised manner, enhancing scalability and generalization across domains. This work enables cost-effective and automated surveying of diverse online communities.",
            "link": "https://www.semanticscholar.org/paper/de4f65ea0c4b029beb2317c0bdc5c16149c18024",
            "authors": "Zihao He, Rebecca Dorn, Siyi Guo, Minh Duc Hoang Chu, Kristina Lerman",
            "EMNLP Paper ID": "1999",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "043aa8e8b4768e2b36c5097141b88a6ad2b591b7",
            "title": "Inductive-Deductive Strategy Reuse for Multi-Turn Instructional Dialogues",
            "abstract": "Aligning large language models (LLMs) with human expectations requires high-quality instructional dialogues, which usually require instructions that are diverse and in-depth. Existing methods leverage two LLMs to interact for automatic collection: one simulating a user to pose instructions, and the other acting as a system agent to respond. However, these user simulators struggle to model the rules behind how dialogues can pose different instructions without explicit guidance, resulting in general instructions. In this paper, we propose to explicitly capture the complex rules to help the user simulator pose diverse and in-depth instruction. Specifically, we first induce high-level instruction strategies from various real instruction dialogues serving as rules. Afterward, different possible strategies are applied to the newly given dialogue scenario deductively to pose various instructions. Experimental results show that our method can generate diverse and in-depth instructions. The constructed multi-turn instructional dialogues can outperform competitive baselines on the downstream chat model.",
            "link": "https://www.semanticscholar.org/paper/043aa8e8b4768e2b36c5097141b88a6ad2b591b7",
            "authors": "Jiao Ou, Jiayu Wu, Che Liu, Fuzheng Zhang, Di Zhang, Kun Gai",
            "EMNLP Paper ID": "2076",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 12): 17:45\u201318:45 (Evening)",
            "Type": "Main"
        },
        {
            "paperId": "1265bc74328b30123530da1ba867edcaf8ca1d5d",
            "title": "Instruction Matters: A Simple yet Effective Task Selection for Optimized Instruction Tuning of Specific Tasks",
            "abstract": "Instruction tuning has been proven effective in enhancing zero-shot generalization across various tasks and in improving the performance of specific tasks. For task-specific improvements, strategically selecting and training on related tasks that provide meaningful supervision is crucial, as this approach enhances efficiency and prevents performance degradation from learning irrelevant tasks. In this light, we introduce a simple yet effective task selection method that leverages instruction information alone to identify relevant tasks, optimizing instruction tuning for specific tasks. Our method is significantly more efficient than traditional approaches, which require complex measurements of pairwise transferability between tasks or the creation of data samples for the target task. Additionally, by aligning the model with the unique instructional template style of the meta-dataset, we enhance its ability to granularly discern relevant tasks, leading to improved overall performance. Experimental results demonstrate that training on a small set of tasks, chosen solely based on the instructions, results in substantial improvements in performance on benchmarks such as P3, Big-Bench, NIV2, and Big-Bench Hard. Significantly, these improvements surpass those achieved by prior task selection methods, highlighting the superiority of our approach.",
            "link": "https://www.semanticscholar.org/paper/1265bc74328b30123530da1ba867edcaf8ca1d5d",
            "authors": "Changho Lee, Janghoon Han, Seonghyeon Ye, Stanley Jungkyu Choi, Honglak Lee, Kyunghoon Bae",
            "EMNLP Paper ID": "2322",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "f970e9ff0c07e639a96772c644a2b464afd01141",
            "title": "Investigating the Role of Instruction Variety and Task Difficulty in Robotic Manipulation Tasks",
            "abstract": "Evaluating the generalisation capabilities of multimodal models based solely on their performance on out-of-distribution data fails to capture their true robustness. This work introduces a comprehensive evaluation framework that systematically examines the role of instructions and inputs in the generalisation abilities of such models, considering architectural design, input perturbations across language and vision modalities, and increased task complexity. The proposed framework uncovers the resilience of multimodal models to extreme instruction perturbations and their vulnerability to observational changes, raising concerns about overfitting to spurious correlations. By employing this evaluation framework on current Transformer-based multimodal models for robotic manipulation tasks, we uncover limitations and suggest future advancements should focus on architectural and training innovations that better integrate multimodal inputs, enhancing a model's generalisation prowess by prioritising sensitivity to input content over incidental correlations.",
            "link": "https://www.semanticscholar.org/paper/f970e9ff0c07e639a96772c644a2b464afd01141",
            "authors": "Amit Parekh, Nikolas Vitsakis, Alessandro Suglia, Ioannis Konstas",
            "EMNLP Paper ID": "2470",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "ed8ff544a41ddb64f7cceaf924f479f6bd9c0855",
            "title": "Evolutionary Contrastive Distillation for Language Model Alignment",
            "abstract": "The ability of large language models (LLMs) to execute complex instructions is essential for their real-world applications. However, several recent studies indicate that LLMs struggle with challenging instructions. In this paper, we propose Evolutionary Contrastive Distillation (ECD), a novel method for generating high-quality synthetic preference data designed to enhance the complex instruction-following capability of language models. ECD generates data that specifically illustrates the difference between a response that successfully follows a set of complex instructions and a response that is high-quality, but nevertheless makes some subtle mistakes. This is done by prompting LLMs to progressively evolve simple instructions to more complex instructions. When the complexity of an instruction is increased, the original successful response to the original instruction becomes a\"hard negative\"response for the new instruction, mostly meeting requirements of the new instruction, but barely missing one or two. By pairing a good response with such a hard negative response, and employing contrastive learning algorithms such as DPO, we improve language models' ability to follow complex instructions. Empirically, we observe that our method yields a 7B model that exceeds the complex instruction-following performance of current SOTA 7B models and is competitive even with open-source 70B models.",
            "link": "https://www.semanticscholar.org/paper/ed8ff544a41ddb64f7cceaf924f479f6bd9c0855",
            "authors": "Julian Katz-Samuels, Zheng Li, Hyokun Yun, Priyank Nigam, Yi Xu, Vaclav Petricek, Bing Yin, Trishul A Chilimbi",
            "matchScore": 199.60715,
            "original title": "Evolutionary Contrastive Distillation for Language Model Alignment",
            "original authors": "Julian Katz-Samuels, Zheng Li, Hyokun Yun, Priyanka Nigam, Yi Xu, Vaclav Petricek, Bing Yin, Trishul Chilimbi",
            "EMNLP Paper ID": "1074",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "4aca2c5186e9a97a8ac917e9e4a159c2a3a0a0da",
            "title": "Distilling Instruction-following Abilities of Large Language Models with Task-aware Curriculum Planning",
            "abstract": "Instruction tuning aims to align large language models (LLMs) with open-domain instructions and human-preferred responses. While several studies have explored autonomous approaches to distilling and annotating instructions from powerful proprietary LLMs, such as ChatGPT, they often neglect the impact of the distributions and characteristics of tasks, together with the varying difficulty of instructions in training sets. This oversight can lead to imbalanced knowledge capabilities and poor generalization powers of student LLMs. To address these challenges, we introduce Task-Aware Curriculum Planning for Instruction Refinement (TAPIR), a multi-round distillation framework that utilizes an oracle LLM to select instructions that are difficult for a student LLM to follow. To balance the student's capabilities, task distributions in training sets are adjusted with responses automatically refined according to their corresponding tasks. In addition, by incorporating curriculum planning, our approach systematically escalates the difficulty levels of tasks, progressively enhancing the student LLM's capabilities. We rigorously evaluate TAPIR using several widely recognized benchmarks (such as AlpacaEval 2.0, MT-Bench, etc.) and multiple student LLMs. Empirical results demonstrate that student LLMs, trained with our method and less training data, outperform larger instruction-tuned models and strong distillation baselines.",
            "link": "https://www.semanticscholar.org/paper/4aca2c5186e9a97a8ac917e9e4a159c2a3a0a0da",
            "authors": "Yuanhao Yue, Chengyu Wang, Jun Huang, Peng Wang",
            "matchScore": 277.0346,
            "original title": "Distilling Instruction-following Abilities of Large Language Models with Task-aware Curriculum Planning",
            "original authors": "Yuanhao Yue, Chengyu Wang, Jun Huang, Peng Wang",
            "EMNLP Paper ID": "1244",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Findings"
        },
        {
            "paperId": "6f39442852656f8a9decc95854a2ed461b3a83ab",
            "title": "Ada-Instruct: Adapting Instruction Generators for Complex Reasoning",
            "abstract": "Instructions augmentation is a crucial step for unleashing the full potential of large language models (LLMs) in downstream tasks. Existing Self-Instruct methods primarily simulate new instructions from a few initial instructions with in-context learning. However, our study identifies a critical flaw in this approach: even with GPT4o, Self-Instruct cannot generate complex instructions of length $\\ge 100$, which is necessary in complex tasks such as code completion. To address this issue, our key insight is that fine-tuning open source LLMs with only ten examples can produce complex instructions that maintain distributional consistency for complex reasoning tasks. We introduce Ada-Instruct, an adaptive instruction generator developed through fine-tuning. We empirically validated Ada-Instruct's efficacy across different applications. The results highlight Ada-Instruct's capacity to generate long, intricate, and distributionally consistent instructions.",
            "link": "https://www.semanticscholar.org/paper/6f39442852656f8a9decc95854a2ed461b3a83ab",
            "authors": "Wanyun Cui, Qianle Wang",
            "matchScore": 257.0561,
            "original title": "Ada-Instruct: Adapting Instruction Generators for Complex Reasoning",
            "original authors": "Wanyun Cui, Qianle Wang",
            "EMNLP Paper ID": "1421",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "05bf0ea7bc89adb92e583f4fc91dc994fa77bb85",
            "title": "LongForm: Effective Instruction Tuning with Reverse Instructions",
            "abstract": "Instruction tuning enables language models to more effectively generalize and better follow user intent. However, obtaining instruction data is costly and challenging. Prior work employs methods such as expensive human annotation, crowd-sourced datasets with alignment issues, and generating noisy examples via LLMs. We introduce the LongForm-C dataset, which is created by reverse instructions. We generate instructions via LLMs for human-written corpus examples using reverse instructions. First we select a diverse set of human-written documents from corpora such as C4 and Wikipedia; then we generate instructions for these documents via LLMs. This approach provides a cheaper and cleaner instruction-tuning dataset with natural output and one suitable for long text generation. Our models outperform 10x larger language models without instruction tuning on tasks such as story/recipe generation and long-form question answering. Moreover, LongForm models outperform prior instruction-tuned models such as FLAN-T5 and Alpaca by a large margin, and improve language understanding capabilities further. We publicly release our data and models: https://github.com/akoksal/LongForm.",
            "link": "https://www.semanticscholar.org/paper/05bf0ea7bc89adb92e583f4fc91dc994fa77bb85",
            "authors": "Abdullatif K\u00f6ksal, Timo Schick, A. Korhonen, Hinrich Sch\u00fctze",
            "matchScore": 222.90714,
            "original title": "LongForm: Effective Instruction Tuning with Reverse Instructions",
            "original authors": "Abdullatif K\u00f6ksal, Timo Schick, Anna Korhonen, Hinrich Schuetze",
            "EMNLP Paper ID": "1436",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Findings"
        },
        {
            "paperId": "b8384139339e4487840fef42e73b4ead65f227d3",
            "title": "LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for Enhanced Following of Instructions with Multiple Constraints",
            "abstract": "Instruction following is a key capability for LLMs. However, recent studies have shown that LLMs often struggle with instructions containing multiple constraints (e.g. a request to create a social media post\"in a funny tone\"with\"no hashtag\"). Despite this, most evaluations focus solely on synthetic data. To address this, we introduce RealInstruct, the first benchmark designed to evaluate LLMs' ability to follow real-world multi-constrained instructions by leveraging queries real users asked AI assistants. We also investigate model-based evaluation as a cost-effective alternative to human annotation for this task. Our findings reveal that even the proprietary GPT-4 model fails to meet at least one constraint on over 21% of instructions, highlighting the limitations of state-of-the-art models. To address the performance gap between open-source and proprietary models, we propose the Decompose, Critique and Refine (DeCRIM) self-correction pipeline, which enhances LLMs' ability to follow constraints. DeCRIM works by decomposing the original instruction into a list of constraints and using a Critic model to decide when and where the LLM's response needs refinement. Our results show that DeCRIM improves Mistral's performance by 7.3% on RealInstruct and 8.0% on IFEval even with weak feedback. Moreover, we demonstrate that with strong feedback, open-source LLMs with DeCRIM can outperform GPT-4 on both benchmarks.",
            "link": "https://www.semanticscholar.org/paper/b8384139339e4487840fef42e73b4ead65f227d3",
            "authors": "Thomas Palmeira Ferraz, Kartik Mehta, Yu-Hsiang Lin, Haw-Shiuan Chang, Shereen Oraby, Sijia Liu, Vivek Subramanian, Tagyoung Chung, Mohit Bansal, Nanyun Peng",
            "matchScore": 356.40613,
            "original title": "LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for Enhanced Following of Instructions with Multiple Constraints",
            "original authors": "Thomas Palmeira Ferraz, Kartik Mehta, Yu-Hsiang Lin, Haw-Shiuan Chang, Shereen Oraby, Sijia Liu, Vivek Subramanian, Tagyoung Chung, Mohit Bansal, Nanyun Peng",
            "EMNLP Paper ID": "1629",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Findings"
        },
        {
            "paperId": "4d9d8635c4a849b84db9d95e3c1b3249c05c1193",
            "title": "Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging",
            "abstract": "Socratic questioning is an effective teaching strategy, encouraging critical thinking and problem-solving. The conversational capabilities of large language models (LLMs) show great potential for providing scalable, real-time student guidance. However, current LLMs often give away solutions directly, making them ineffective instructors. We tackle this issue in the code debugging domain with TreeInstruct, an Instructor agent guided by a novel state space-based planning algorithm. TreeInstruct asks probing questions to help students independently identify and resolve errors. It estimates a student's conceptual and syntactical knowledge to dynamically construct a question tree based on their responses and current knowledge state, effectively addressing both independent and dependent mistakes concurrently in a multi-turn interaction setting. In addition to using an existing single-bug debugging benchmark, we construct a more challenging multi-bug dataset of 150 coding problems, incorrect solutions, and bug fixes -- all carefully constructed and annotated by experts. Extensive evaluation shows TreeInstruct's state-of-the-art performance on both datasets, proving it to be a more effective instructor than baselines. Furthermore, a real-world case study with five students of varying skill levels further demonstrates TreeInstruct's ability to guide students to debug their code efficiently with minimal turns and highly Socratic questioning. We provide our code and datasets at http://github.com/agarwalishika/TreeInstruct .",
            "link": "https://www.semanticscholar.org/paper/4d9d8635c4a849b84db9d95e3c1b3249c05c1193",
            "authors": "Priyanka Kargupta, Ishika Agarwal, Dilek Hakkani-Tur, Jiawei Han",
            "matchScore": 357.2125,
            "original title": "Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging",
            "original authors": "Priyanka Kargupta, Ishika Agarwal, Dilek Hakkani Tur, Jiawei Han",
            "EMNLP Paper ID": "1972",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "ad5c359094dbfb68b1e09b22e2be3ce6dac33e3d",
            "title": "From Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following Ability of Large Language Models",
            "abstract": "It is imperative for Large language models (LLMs) to follow instructions with elaborate requirements (i.e. Complex Instructions Following). Yet, it remains under-explored how to enhance the ability of LLMs to follow complex instructions with multiple constraints. To bridge the gap, we initially study what training data is effective in enhancing complex constraints following abilities. We found that training LLMs with instructions containing multiple constraints enhances their understanding of complex instructions, especially those with lower complexity levels. The improvement can even generalize to compositions of out-of-domain constraints. Additionally, we further propose methods addressing how to obtain and utilize the effective training data. Finally, we conduct extensive experiments to prove the effectiveness of our methods in terms of overall performance and training efficiency. We also demonstrate that our methods improve models' ability to follow instructions generally and generalize effectively across out-of-domain, in-domain, and adversarial settings, while maintaining general capabilities.",
            "link": "https://www.semanticscholar.org/paper/ad5c359094dbfb68b1e09b22e2be3ce6dac33e3d",
            "authors": "Qi He, Jie Zeng, Qianxi He, Jiaqing Liang, Yanghua Xiao",
            "matchScore": 315.68863,
            "original title": "From Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following Ability of Large Language Models",
            "original authors": "Qianyu He, Jie Zeng, Qianxi He, Jiaqing Liang, Yanghua Xiao",
            "EMNLP Paper ID": "2181",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "06ba45793c761583ffdd408141f81517b92f649f",
            "title": "Functionality learning through specification instructions",
            "abstract": "Test suites assess natural language processing models' performance on specific functionalities: cases of interest involving model robustness, fairness, or particular linguistic capabilities. This paper introduces specification instructions: text descriptions specifying fine-grained task-specific behaviors. For each functionality in a suite, we generate an instruction that describes it. We combine the specification instructions to create specification-augmented prompts, which we feed to language models pre-trained on natural instruction data. We conduct experiments to measure how optimizing for some functionalities may negatively impact functionalities that are not covered by the specification set. Our analyses across four tasks and models of diverse sizes and families show that smaller models struggle to follow specification instructions. However, larger models (>~3B params.) can benefit from specifications and -- surprisingly -- even generalize certain desirable behaviors across functionalities.",
            "link": "https://www.semanticscholar.org/paper/06ba45793c761583ffdd408141f81517b92f649f",
            "authors": "Pedro Henrique Luz de Araujo, Benjamin Roth",
            "matchScore": 180.86269,
            "original title": "Functionality learning through specification instructions",
            "original authors": "Pedro Henrique Luz de Araujo, Benjamin Roth",
            "EMNLP Paper ID": "2190",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "78b502c3f6f404d623ead6cd12759b839d80c320",
            "title": "Better Alignment with Instruction Back-and-Forth Translation",
            "abstract": "We propose a new method, instruction back-and-forth translation, to construct high-quality synthetic data grounded in world knowledge for aligning large language models (LLMs). Given documents from a web corpus, we generate and curate synthetic instructions using the backtranslation approach proposed by Li et al.(2023a), and rewrite the responses to improve their quality further based on the initial documents. Fine-tuning with the resulting (backtranslated instruction, rewritten response) pairs yields higher win rates on AlpacaEval than using other common instruction datasets such as Humpback, ShareGPT, Open Orca, Alpaca-GPT4 and Self-instruct. We also demonstrate that rewriting the responses with an LLM outperforms direct distillation, and the two generated text distributions exhibit significant distinction in embedding space. Further analysis shows that our backtranslated instructions are of higher quality than other sources of synthetic instructions, while our responses are more diverse and complex than those obtained from distillation. Overall we find that instruction back-and-forth translation combines the best of both worlds -- making use of the information diversity and quantity found on the web, while ensuring the quality of the responses which is necessary for effective alignment.",
            "link": "https://www.semanticscholar.org/paper/78b502c3f6f404d623ead6cd12759b839d80c320",
            "authors": "Thao Nguyen, Jeffrey Li, Sewoong Oh, Ludwig Schmidt, Jason Weston, Luke Zettlemoyer, Xian Li",
            "matchScore": 242.13254,
            "original title": "Better Alignment with Instruction Back-and-Forth Translation",
            "original authors": "Thao Nguyen, Jeffrey Li, Sewoong Oh, Ludwig Schmidt, Jason E Weston, Luke Zettlemoyer, Xian Li",
            "EMNLP Paper ID": "2595",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "897580ae25ef697a0c2ec6548a19844f8f480aab",
            "title": "TOWER: Tree Organized Weighting for Evaluating Complex Instructions",
            "abstract": "Evaluating the ability of large language models (LLMs) to follow complex human-written instructions is essential for their deployment in real-world applications. While benchmarks like Chatbot Arena use human judges to assess model performance, they are resource-intensive and time-consuming. Alternative methods using LLMs as judges, such as AlpacaEval, MT Bench, WildBench, and InFoBench offer improvements but still do not capture that certain complex instruction aspects are more important than others to follow. To address this gap, we propose a novel evaluation metric, \\textsc{TOWER}, that incorporates human-judged importance into the assessment of complex instruction following. We show that human annotators agree with tree-based representations of these complex instructions nearly as much as they agree with other human annotators. We release tree-based annotations of the InFoBench dataset and the corresponding evaluation code to facilitate future research.",
            "link": "https://www.semanticscholar.org/paper/897580ae25ef697a0c2ec6548a19844f8f480aab",
            "authors": "Noah Ziems, Zhihan Zhang, Meng Jiang",
            "matchScore": 264.47705,
            "original title": "TOWER: Tree Organized Weighting for Evaluating Complex Instructions",
            "original authors": "Noah Ziems, Zhihan Zhang, Meng Jiang",
            "EMNLP Paper ID": "2690",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "3c7e78eddd6d3b4ade5cf63cda94c85829fff159",
            "title": "The SIFo Benchmark: Investigating the Sequential Instruction Following Ability of Large Language Models",
            "abstract": "Following multiple instructions is a crucial ability for large language models (LLMs). Evaluating this ability comes with significant challenges: (i) limited coherence between multiple instructions, (ii) positional bias where the order of instructions affects model performance, and (iii) a lack of objectively verifiable tasks. To address these issues, we introduce a benchmark designed to evaluate models' abilities to follow multiple instructions through sequential instruction following (SIFo) tasks. In SIFo, the successful completion of multiple instructions is verifiable by examining only the final instruction. Our benchmark evaluates instruction following using four tasks (text modification, question answering, mathematics, and security rules), each assessing different aspects of sequential instruction following. Our evaluation of popular LLMs, both closed-source and open-source, shows that more recent and larger models significantly outperform their older and smaller counterparts on the SIFo tasks, validating the benchmark's effectiveness. All models struggle with following sequences of instructions, hinting at an important lack of robustness of today's language models.",
            "link": "https://www.semanticscholar.org/paper/3c7e78eddd6d3b4ade5cf63cda94c85829fff159",
            "authors": "Xinyi Chen, Baohao Liao, Jirui Qi, Panagiotis Eustratiadis, C. Monz, Arianna Bisazza, M. D. Rijke",
            "matchScore": 287.3506,
            "original title": "The SIFo Benchmark: Investigating the Sequential Instruction Following Ability of Large Language Models",
            "original authors": "Xinyi Chen, Baohao Liao, Jirui Qi, Panagiotis Eustratiadis, Christof Monz, Arianna Bisazza, Maarten de Rijke",
            "EMNLP Paper ID": "357",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "92c0c44d89287de52e150dc985f0e40e26a0583f",
            "title": "Optimizing Instruction Synthesis: Effective Exploration of Evolutionary Space with Tree Search",
            "abstract": "Instruction tuning is a crucial technique for aligning language models with humans' actual goals in the real world. Extensive research has highlighted the quality of instruction data is essential for the success of this alignment. However, creating high-quality data manually is labor-intensive and time-consuming, which leads researchers to explore using LLMs to synthesize data. Recent studies have focused on using a stronger LLM to iteratively enhance existing instruction data, showing promising results. Nevertheless, previous work often lacks control over the evolution direction, resulting in high uncertainty in the data synthesis process and low-quality instructions. In this paper, we introduce a general and scalable framework, IDEA-MCTS (Instruction Data Enhancement using Monte Carlo Tree Search), a scalable framework for efficiently synthesizing instructions. With tree search and evaluation models, it can efficiently guide each instruction to evolve into a high-quality form, aiding in instruction fine-tuning. Experimental results show that IDEA-MCTS significantly enhances the seed instruction data, raising the average evaluation scores of quality, diversity, and complexity from 2.19 to 3.81. Furthermore, in open-domain benchmarks, experimental results show that IDEA-MCTS improves the accuracy of real-world instruction-following skills in LLMs by an average of 5\\% in low-resource settings.",
            "link": "https://www.semanticscholar.org/paper/92c0c44d89287de52e150dc985f0e40e26a0583f",
            "authors": "Chenglin Li, Qianglong Chen, Zhi Li, Feng Tao, Yicheng Li, Hao Chen, Fei Yu, Yin Zhang",
            "matchScore": 292.28235,
            "original title": "Optimizing Instruction Synthesis: Effective Exploration of Evolutionary Space with Tree Search",
            "original authors": "Li Chenglin, Qianglong Chen, Zhi Li, FengTao, Yicheng Li, Hao Chen, Fei Yu, Yin Zhang",
            "EMNLP Paper ID": "358",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "26944f7cc9c05dd99df5c7e86ee10c286824bc14",
            "title": "Suri: Multi-constraint Instruction Following for Long-form Text Generation",
            "abstract": "Existing research on instruction following largely focuses on tasks with simple instructions and short responses. In this work, we explore multi-constraint instruction following for generating long-form text. We create Suri, a dataset with 20K human-written long-form texts paired with LLM-generated backtranslated instructions that contain multiple complex constraints. Because of prohibitive challenges associated with collecting human preference judgments on long-form texts, preference-tuning algorithms such as DPO are infeasible in our setting; thus, we propose Instructional ORPO (I-ORPO), an alignment method based on the ORPO algorithm. Instead of receiving negative feedback from dispreferred responses, I-ORPO obtains negative feedback from synthetically corrupted instructions generated by an LLM. Using Suri, we perform supervised and I-ORPO fine-tuning on Mistral-7b-Instruct-v0.2. The resulting models, Suri-SFT and Suri-I-ORPO, generate significantly longer texts (~5K tokens) than base models without significant quality deterioration. Our human evaluation shows that while both SFT and I-ORPO models satisfy most constraints, Suri-I-ORPO generations are generally preferred for their coherent and informative incorporation of the constraints. We release our code at https://github.com/chtmp223/suri.",
            "link": "https://www.semanticscholar.org/paper/26944f7cc9c05dd99df5c7e86ee10c286824bc14",
            "authors": "Chau Minh Pham, Simeng Sun, Mohit Iyyer",
            "matchScore": 282.112,
            "original title": "Suri: Multi-constraint Instruction Following in Long-form Text Generation",
            "original authors": "Chau Minh Pham, Simeng Sun, Mohit Iyyer",
            "EMNLP Paper ID": "360",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "240ac1cc47a00d2c4fe62e228c1fb0dfe85b0c39",
            "title": "Eliciting Instruction-tuned Code Language Models' Capabilities to Utilize Auxiliary Function for Code Generation",
            "abstract": "We study the code generation behavior of instruction-tuned models built on top of code pre-trained language models when they could access an auxiliary function to implement a function. We design several ways to provide auxiliary functions to the models by adding them to the query or providing a response prefix to incorporate the ability to utilize auxiliary functions with the instruction-following capability. Our experimental results show the effectiveness of combining the base models' auxiliary function utilization ability with the instruction following ability. In particular, the performance of adopting our approaches with the open-sourced language models surpasses that of the recent powerful proprietary language models, i.e., gpt-4o.",
            "link": "https://www.semanticscholar.org/paper/240ac1cc47a00d2c4fe62e228c1fb0dfe85b0c39",
            "authors": "Seonghyeon Lee, Suyeon Kim, Joonwon Jang, Heejae Chon, Dongha Lee, Hwanjo Yu",
            "matchScore": 317.13696,
            "original title": "Eliciting Instruction-tuned Code Language Models\u2019 Capabilities to Utilize Auxiliary Function for Code Generation",
            "original authors": "Seonghyeon Lee, Suyeon Kim, Joonwon Jang, HeeJae Chon, Dongha Lee, Hwanjo Yu",
            "EMNLP Paper ID": "370",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "d034a77636c0b155eb3f752f161203f7891135cb",
            "title": "Data Diversity Matters for Robust Instruction Tuning",
            "abstract": "Recent works have shown that by curating high quality and diverse instruction tuning datasets, we can significantly improve instruction-following capabilities. However, creating such datasets is difficult and most works rely on manual curation or proprietary language models. Automatic data curation is difficult as it is still not clear how we can define diversity for instruction tuning, how diversity and quality depend on one other, and how we can optimize dataset quality and diversity. To resolve these issue, we propose a new algorithm, Quality-Diversity Instruction Tuning (QDIT). QDIT provides a simple method to simultaneously control dataset diversity and quality, allowing us to conduct an in-depth study on the effect of diversity and quality on instruction tuning performance. From this study we draw two key insights (1) there is a natural tradeoff between data diversity and quality and (2) increasing data diversity significantly improves the worst case instruction following performance, therefore improving robustness. We validate the performance of QDIT on several large scale instruction tuning datasets, where we find it can substantially improve worst and average case performance compared to quality-driven data selection.",
            "link": "https://www.semanticscholar.org/paper/d034a77636c0b155eb3f752f161203f7891135cb",
            "authors": "Alexander Bukharin, Tuo Zhao",
            "matchScore": 200.2381,
            "original title": "Data Diversity Matters for Robust Instruction Tuning",
            "original authors": "Alexander Bukharin, Shiyang Li, Zhengyang Wang, Jingfeng Yang, Bing Yin, Xian Li, Chao Zhang, Tuo Zhao, Haoming Jiang",
            "EMNLP Paper ID": "696",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "ef010a9038dc02462312f6c1c97bf8d296c1abb5",
            "title": "Self-Evolution Fine-Tuning for Policy Optimization",
            "abstract": "The alignment of large language models (LLMs) is crucial not only for unlocking their potential in specific tasks but also for ensuring that responses meet human expectations and adhere to safety and ethical principles. Current alignment methodologies face considerable challenges. For instance, supervised fine-tuning (SFT) requires extensive, high-quality annotated samples, while reinforcement learning from human feedback (RLHF) is complex and often unstable. In this paper, we introduce self-evolution fine-tuning (SEFT) for policy optimization, with the aim of eliminating the need for annotated samples while retaining the stability and efficiency of SFT. SEFT first trains an adaptive reviser to elevate low-quality responses while maintaining high-quality ones. The reviser then gradually guides the policy's optimization by fine-tuning it with enhanced responses. One of the prominent features of this method is its ability to leverage unlimited amounts of unannotated data for policy optimization through supervised fine-tuning. Our experiments on AlpacaEval 2.0 and MT-Bench demonstrate the effectiveness of SEFT. We also provide a comprehensive analysis of its advantages over existing alignment techniques.",
            "link": "https://www.semanticscholar.org/paper/ef010a9038dc02462312f6c1c97bf8d296c1abb5",
            "authors": "Ruijun Chen, Jiehao Liang, Shiping Gao, Fanqi Wan, Xiaojun Quan",
            "matchScore": 193.89012,
            "original title": "Self-Evolution Fine-Tuning for Policy Optimization",
            "original authors": "Ruijun Chen, Jiehao Liang, Shiping Gao, Fanqi Wan, Xiaojun Quan",
            "EMNLP Paper ID": "820",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        }
    ],
    "Advancements in Language Models and NLP Techniques": [
        {
            "paperId": "bdd07688083de2fc792a48ba935cd33256066827",
            "title": "LongEmbed: Extending Embedding Models for Long Context Retrieval",
            "abstract": "Embedding models play a pivot role in modern NLP applications such as IR and RAG. While the context limit of LLMs has been pushed beyond 1 million tokens, embedding models are still confined to a narrow context window not exceeding 8k tokens, refrained from application scenarios requiring long inputs such as legal contracts. This paper explores context window extension of existing embedding models, pushing the limit to 32k without requiring additional training. First, we examine the performance of current embedding models for long context retrieval on our newly constructed LongEmbed benchmark. LongEmbed comprises two synthetic tasks and four carefully chosen real-world tasks, featuring documents of varying length and dispersed target information. Benchmarking results underscore huge room for improvement in these models. Based on this, comprehensive experiments show that training-free context window extension strategies like position interpolation can effectively extend the context window of existing embedding models by several folds, regardless of their original context being 512 or beyond 4k. Furthermore, for models employing absolute position encoding (APE), we show the possibility of further fine-tuning to harvest notable performance gains while strictly preserving original behavior for short inputs. For models using rotary position embedding (RoPE), significant enhancements are observed when employing RoPE-specific methods, such as NTK and SelfExtend, indicating RoPE's superiority over APE for context window extension. To facilitate future research, we release E5-Base-4k and E5-RoPE-Base, along with the LongEmbed benchmark.",
            "link": "https://www.semanticscholar.org/paper/bdd07688083de2fc792a48ba935cd33256066827",
            "authors": "Dawei Zhu, Liang Wang, Nan Yang, Yifan Song, Wenhao Wu, Furu Wei, Sujian Li",
            "EMNLP Paper ID": "102",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "eea73ab4eb59dc1fdebeab5805b530b50bcd35e3",
            "title": "MiniConGTS: A Near Ultimate Minimalist Contrastive Grid Tagging Scheme for Aspect Sentiment Triplet Extraction",
            "abstract": "Aspect Sentiment Triplet Extraction (ASTE) aims to co-extract the sentiment triplets in a given corpus. Existing approaches within the pretraining-finetuning paradigm tend to either meticulously craft complex tagging schemes and classification heads, or incorporate external semantic augmentation to enhance performance. In this study, we, for the first time, re-evaluate the redundancy in tagging schemes and the internal enhancement in pretrained representations. We propose a method to improve and utilize pretrained representations by integrating a minimalist tagging scheme and a novel token-level contrastive learning strategy. The proposed approach demonstrates comparable or superior performance compared to state-of-the-art techniques while featuring a more compact design and reduced computational overhead. Additionally, we are the first to formally evaluate GPT-4's performance in few-shot learning and Chain-of-Thought scenarios for this task. The results demonstrate that the pretraining-finetuning paradigm remains highly effective even in the era of large language models.",
            "link": "https://www.semanticscholar.org/paper/eea73ab4eb59dc1fdebeab5805b530b50bcd35e3",
            "authors": "Qiao Sun, Liujia Yang, Minghao Ma, Nanyang Ye, Qinying Gu",
            "EMNLP Paper ID": "316",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "45868ce5c5091f09ba76f3a9ca8fed5ee6121d23",
            "title": "Extending Context Window of Large Language Models from a Distributional Perspective",
            "abstract": "Scaling the rotary position embedding (RoPE) has become a common method for extending the context window of RoPE-based large language models (LLMs). However, existing scaling methods often rely on empirical approaches and lack a profound understanding of the internal distribution within RoPE, resulting in suboptimal performance in extending the context window length. In this paper, we propose to optimize the context window extending task from the view of rotary angle distribution. Specifically, we first estimate the distribution of the rotary angles within the model and analyze the extent to which length extension perturbs this distribution. Then, we present a novel extension strategy that minimizes the disturbance between rotary angle distributions to maintain consistency with the pre-training phase, enhancing the model's capability to generalize to longer sequences. Experimental results compared to the strong baseline methods demonstrate that our approach reduces by up to 72% of the distributional disturbance when extending LLaMA2's context window to 8k, and reduces by up to 32% when extending to 16k. On the LongBench-E benchmark, our method achieves an average improvement of up to 4.33% over existing state-of-the-art methods. Furthermore, Our method maintains the model's performance on the Hugging Face Open LLM benchmark after context window extension, with only an average performance fluctuation ranging from -0.12 to +0.22.",
            "link": "https://www.semanticscholar.org/paper/45868ce5c5091f09ba76f3a9ca8fed5ee6121d23",
            "authors": "Yingsheng Wu, Yuxuan Gu, Xiaocheng Feng, Weihong Zhong, Dongliang Xu, Qing Yang, Hongtao Liu, Bing Qin",
            "EMNLP Paper ID": "822",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "519a2ec4a0c7f5b189b9b4eed40606c96ad5adc1",
            "title": "On Eliciting Syntax from Language Models via Hashing",
            "abstract": "Unsupervised parsing, also known as grammar induction, aims to infer syntactic structure from raw text. Recently, binary representation has exhibited remarkable information-preserving capabilities at both lexicon and syntax levels. In this paper, we explore the possibility of leveraging this capability to deduce parsing trees from raw text, relying solely on the implicitly induced grammars within models. To achieve this, we upgrade the bit-level CKY from zero-order to first-order to encode the lexicon and syntax in a unified binary representation space, switch training from supervised to unsupervised under the contrastive hashing framework, and introduce a novel loss function to impose stronger yet balanced alignment signals. Our model shows competitive performance on various datasets, therefore, we claim that our method is effective and efficient enough to acquire high-quality parsing trees from pre-trained language models at a low cost.",
            "link": "https://www.semanticscholar.org/paper/519a2ec4a0c7f5b189b9b4eed40606c96ad5adc1",
            "authors": "Yiran Wang, Masao Utiyama",
            "EMNLP Paper ID": "973",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "f0be312fa5d4b979a768680bf3ebac9153ac9fce",
            "title": "CSSL: Contrastive Self-Supervised Learning for Dependency Parsing on Relatively Free Word Ordered and Morphologically Rich Low Resource Languages",
            "abstract": "Neural dependency parsing has achieved remarkable performance for low resource morphologically rich languages. It has also been well-studied that morphologically rich languages exhibit relatively free word order. This prompts a fundamental investigation: Is there a way to enhance dependency parsing performance, making the model robust to word order variations utilizing the relatively free word order nature of morphologically rich languages? In this work, we examine the robustness of graph-based parsing architectures on 7 relatively free word order languages. We focus on scrutinizing essential modifications such as data augmentation and the removal of position encoding required to adapt these architectures accordingly. To this end, we propose a contrastive self-supervised learning method to make the model robust to word order variations. Furthermore, our proposed modification demonstrates a substantial average gain of 3.03/2.95 points in 7 relatively free word order languages, as measured by the UAS/LAS Score metric when compared to the best performing baseline.",
            "link": "https://www.semanticscholar.org/paper/f0be312fa5d4b979a768680bf3ebac9153ac9fce",
            "authors": "Pretam Ray, Jivnesh Sandhan, Amrith Krishna, Pawan Goyal",
            "EMNLP Paper ID": "978",
            "Oral/Poster": "Oral",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "f22974e36c86aa3e0578deb309b5e238e4674f09",
            "title": "Can Transformers Learn $n$-gram Language Models?",
            "abstract": "Much theoretical work has described the ability of transformers to represent formal languages. However, linking theoretical results to empirical performance is not straightforward due to the complex interplay between the architecture, the learning algorithm, and training data. To test whether theoretical lower bounds imply \\emph{learnability} of formal languages, we turn to recent work relating transformers to $n$-gram language models (LMs). We study transformers' ability to learn random $n$-gram LMs of two kinds: ones with arbitrary next-symbol probabilities and ones where those are defined with shared parameters. We find that classic estimation techniques for $n$-gram LMs such as add-$\\lambda$ smoothing outperform transformers on the former, while transformers perform better on the latter, outperforming methods specifically designed to learn $n$-gram LMs.",
            "link": "https://www.semanticscholar.org/paper/f22974e36c86aa3e0578deb309b5e238e4674f09",
            "authors": "Anej Svete, Nadav Borenstein, Mike Zhou, Isabelle Augenstein, Ryan Cotterell",
            "EMNLP Paper ID": "1099",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "d865562b6f1a074e51cb96c4677cb10b1d3860f1",
            "title": "Strengthening Structural Inductive Biases by Pre-training to Perform Syntactic Transformations",
            "abstract": "Models need appropriate inductive biases to effectively learn from small amounts of data and generalize systematically outside of the training distribution. While Transformers are highly versatile and powerful, they can still benefit from enhanced structural inductive biases for seq2seq tasks, especially those involving syntactic transformations, such as converting active to passive voice or semantic parsing. In this paper, we propose to strengthen the structural inductive bias of a Transformer by intermediate pre-training to perform synthetically generated syntactic transformations of dependency trees given a description of the transformation. Our experiments confirm that this helps with few-shot learning of syntactic tasks such as chunking, and also improves structural generalization for semantic parsing. Our analysis shows that the intermediate pre-training leads to attention heads that keep track of which syntactic transformation needs to be applied to which token, and that the model can leverage these attention heads on downstream tasks.",
            "link": "https://www.semanticscholar.org/paper/d865562b6f1a074e51cb96c4677cb10b1d3860f1",
            "authors": "Matthias Lindemann, Alexander Koller, Ivan Titov",
            "EMNLP Paper ID": "1346",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "48967585b4334f112f023619daff33f3bc16e4b4",
            "title": "Advancing Semantic Textual Similarity Modeling: A Regression Framework with Translated ReLU and Smooth K2 Loss",
            "abstract": "Since the introduction of BERT and RoBERTa, research on Semantic Textual Similarity (STS) has made groundbreaking progress. Particularly, the adoption of contrastive learning has substantially elevated state-of-the-art performance across various STS benchmarks. However, contrastive learning categorizes text pairs as either semantically similar or dissimilar, failing to leverage fine-grained annotated information and necessitating large batch sizes to prevent model collapse. These constraints pose challenges for researchers engaged in STS tasks that involve nuanced similarity levels or those with limited computational resources, compelling them to explore alternatives like Sentence-BERT. Despite its efficiency, Sentence-BERT tackles STS tasks from a classification perspective, overlooking the progressive nature of semantic relationships, which results in suboptimal performance. To bridge this gap, this paper presents an innovative regression framework and proposes two simple yet effective loss functions: Translated ReLU and Smooth K2 Loss. Experimental results demonstrate that our method achieves convincing performance across seven established STS benchmarks and offers the potential for further optimization of contrastive learning pre-trained models.",
            "link": "https://www.semanticscholar.org/paper/48967585b4334f112f023619daff33f3bc16e4b4",
            "authors": "Bowen Zhang, Chunping Li",
            "EMNLP Paper ID": "1382",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "598d18792b7f0e33daa56a60800b5a73033bace6",
            "title": "MolTRES: Improving Chemical Language Representation Learning for Molecular Property Prediction",
            "abstract": "Chemical representation learning has gained increasing interest due to the limited availability of supervised data in fields such as drug and materials design. This interest particularly extends to chemical language representation learning, which involves pre-training Transformers on SMILES sequences -- textual descriptors of molecules. Despite its success in molecular property prediction, current practices often lead to overfitting and limited scalability due to early convergence. In this paper, we introduce a novel chemical language representation learning framework, called MolTRES, to address these issues. MolTRES incorporates generator-discriminator training, allowing the model to learn from more challenging examples that require structural understanding. In addition, we enrich molecular representations by transferring knowledge from scientific literature by integrating external materials embedding. Experimental results show that our model outperforms existing state-of-the-art models on popular molecular property prediction tasks.",
            "link": "https://www.semanticscholar.org/paper/598d18792b7f0e33daa56a60800b5a73033bace6",
            "authors": "Jun-Hyung Park, Yeachan Kim, Mingyu Lee, Hyuntae Park, SangKeun Lee",
            "EMNLP Paper ID": "1643",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "31249eaaf157270af7c99c684cf91d754883d2b0",
            "title": "Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity",
            "abstract": "Semantic Textual Similarity (STS) constitutes a critical research direction in computational linguistics and serves as a key indicator of the encoding capabilities of embedding models. Driven by advances in pre-trained language models and contrastive learning, leading sentence representation methods have reached an average Spearman's correlation score of approximately 86 across seven STS benchmarks in SentEval. However, further progress has become increasingly marginal, with no existing method attaining an average score higher than 86.5 on these tasks. This paper conducts an in-depth analysis of this phenomenon and concludes that the upper limit for Spearman's correlation scores under contrastive learning is 87.5. To transcend this ceiling, we propose an innovative approach termed Pcc-tuning, which employs Pearson's correlation coefficient as a loss function to refine model performance beyond contrastive learning. Experimental results demonstrate that Pcc-tuning can markedly surpass previous state-of-the-art strategies with only a minimal amount of fine-grained annotated samples.",
            "link": "https://www.semanticscholar.org/paper/31249eaaf157270af7c99c684cf91d754883d2b0",
            "authors": "Bowen Zhang, Chunping Li",
            "EMNLP Paper ID": "1651",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "9ffbf1b0633fea99d45b8155d752e6f6e1b14b76",
            "title": "Semformer: Transformer Language Models with Semantic Planning",
            "abstract": "Next-token prediction serves as the dominant component in current neural language models. During the training phase, the model employs teacher forcing, which predicts tokens based on all preceding ground truth tokens. However, this approach has been found to create shortcuts, utilizing the revealed prefix to spuriously fit future tokens, potentially compromising the accuracy of the next-token predictor. In this paper, we introduce Semformer, a novel method of training a Transformer language model that explicitly models the semantic planning of response. Specifically, we incorporate a sequence of planning tokens into the prefix, guiding the planning token representations to predict the latent semantic representations of the response, which are induced by an autoencoder. In a minimal planning task (i.e., graph path-finding), our model exhibits near-perfect performance and effectively mitigates shortcut learning, a feat that standard training methods and baseline models have been unable to accomplish. Furthermore, we pretrain Semformer from scratch with 125M parameters, demonstrating its efficacy through measures of perplexity, in-context learning, and fine-tuning on summarization tasks.",
            "link": "https://www.semanticscholar.org/paper/9ffbf1b0633fea99d45b8155d752e6f6e1b14b76",
            "authors": "Yongjing Yin, Junran Ding, Kai Song, Yue Zhang",
            "EMNLP Paper ID": "2326",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "20038b50f93623bd495c0078a21e21444d0d2d85",
            "title": "TempoFormer: A Transformer for Temporally-aware Representations in Change Detection",
            "abstract": "Dynamic representation learning plays a pivotal role in understanding the evolution of linguistic content over time. On this front both context and time dynamics as well as their interplay are of prime importance. Current approaches model context via pre-trained representations, which are typically temporally agnostic. Previous work on modelling context and temporal dynamics has used recurrent methods, which are slow and prone to overfitting. Here we introduce TempoFormer, the first task-agnostic transformer-based and temporally-aware model for dynamic representation learning. Our approach is jointly trained on inter and intra context dynamics and introduces a novel temporal variation of rotary positional embeddings. The architecture is flexible and can be used as the temporal representation foundation of other models or applied to different transformer-based architectures. We show new SOTA performance on three different real-time change detection tasks.",
            "link": "https://www.semanticscholar.org/paper/20038b50f93623bd495c0078a21e21444d0d2d85",
            "authors": "Talia Tseriotou, Adam Tsakalidis, M. Liakata",
            "EMNLP Paper ID": "2536",
            "Oral/Poster": "Oral",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "88210ae860510a459429d7955ddf0e5a512859a8",
            "title": "Contrastive Classification via Linear Layer Extrapolation",
            "abstract": ": Early-exiting predictions in a deep Transformer network evolve from layer to layer in a somewhat smooth process. This has been exploited in language modeling to improve factuality (Chuang et al., 2023), with the observation that factual associations emerge in later layers. We \ufffdnd a similar process multiway emotion classi\ufffdcation, motivating Linear Layer Extrapolation, which \ufffdnds stable improvements by recasting contrastive inference as linear extrapolation. Experiments across multiple models and emotion classi\ufffdcation datasets \ufffdnd that Linear Layer Extrapolation outperforms standard classi\ufffdcation on ne-grained emotion analysis tasks.",
            "link": "https://www.semanticscholar.org/paper/88210ae860510a459429d7955ddf0e5a512859a8",
            "authors": "",
            "EMNLP Paper ID": "2771",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "d64a4a9aa01b1e50b659a077cb9f2c9bde85ad1a",
            "title": "A Morphology-Based Investigation of Positional Encodings",
            "abstract": "Contemporary deep learning models effectively handle languages with diverse morphology despite not being directly integrated into them. Morphology and word order are closely linked, with the latter incorporated into transformer-based models through positional encodings. This prompts a fundamental inquiry: Is there a correlation between the morphological complexity of a language and the utilization of positional encoding in pre-trained language models? In pursuit of an answer, we present the first study addressing this question, encompassing 22 languages and 5 downstream tasks. Our findings reveal that the importance of positional encoding diminishes with increasing morphological complexity in languages. Our study motivates the need for a deeper understanding of positional encoding, augmenting them to better reflect the different languages under consideration.",
            "link": "https://www.semanticscholar.org/paper/d64a4a9aa01b1e50b659a077cb9f2c9bde85ad1a",
            "authors": "Poulami Ghosh, Shikhar Vashishth, Raj Dabre, Pushpak Bhattacharyya",
            "EMNLP Paper ID": "2819",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "ba5f6b9399a7a32266c4849876b2d788f82cfdd7",
            "title": "I love pineapple on pizza != I hate pineapple on pizza : Stance-Aware Sentence Transformers for Opinion Mining",
            "abstract": "Sentence transformers excel at grouping topically similar texts, but struggle to differentiate opposing viewpoints on the same topic. This shortcoming hinders their utility in applications where understanding nuanced differences in opinion is essential, such as those re-lated to social and political discourse analysis. This paper addresses this issue by fine-tuning sentence transformers with arguments for and against human-generated controversial claims. We demonstrate how our fine-tuned model enhances the utility of sentence transformers for social computing tasks such as opinion mining and stance detection. We elaborate that applying stance-aware sentence transformers to opinion mining is more computationally efficient than the classic classification-based approaches.",
            "link": "https://www.semanticscholar.org/paper/ba5f6b9399a7a32266c4849876b2d788f82cfdd7",
            "authors": "V. Ghafouri, Jose Such, Guillermo Suarez-Tangil",
            "EMNLP Paper ID": "2821",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 12): 17:45\u201318:45 (Evening)",
            "Type": "Main"
        },
        {
            "paperId": "ee371c2027b654f7f0120865064f9e166963c39b",
            "title": "On the token distance modeling ability of higher RoPE attention dimension",
            "abstract": "Length extrapolation algorithms based on Rotary position embedding (RoPE) have shown promising results in extending the context length of language models. However, understanding how position embedding can capture longer-range contextual information remains elusive. Based on the intuition that different dimensions correspond to different frequency of changes in RoPE encoding, we conducted a dimension-level analysis to investigate the correlation between a hidden dimension of an attention head and its contribution to capturing long-distance dependencies. Using our correlation metric, we identified a particular type of attention heads, which we named Positional Heads, from various length-extrapolated models. These heads exhibit a strong focus on long-range information interaction and play a pivotal role in long input processing, as evidence by our ablation. We further demonstrate the correlation between the efficiency of length extrapolation and the extension of the high-dimensional attention allocation of these heads. The identification of Positional Heads provides insights for future research in long-text comprehension.",
            "link": "https://www.semanticscholar.org/paper/ee371c2027b654f7f0120865064f9e166963c39b",
            "authors": "Xiangyu Hong, Che Jiang, Biqing Qi, Fandong Meng, Mo Yu, Bowen Zhou, Jie Zhou",
            "matchScore": 285.3396,
            "original title": "On the token distance modeling ability of higher RoPE attention dimension",
            "original authors": "Xiangyu Hong, Che Jiang, Biqing Qi, Fandong Meng, Mo Yu, Bowen Zhou, Jie Zhou",
            "EMNLP Paper ID": "1196",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "e133126dc7c59019c5f6f671c2d91f25507f470a",
            "title": "Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity",
            "abstract": "While BERT produces high-quality sentence embeddings, its pre-training computational cost is a significant drawback. In contrast, ELECTRA provides a cost-effective pre-training objective and downstream task performance improvements, but worse sentence embeddings. The community tacitly stopped utilizing ELECTRA's sentence embeddings for semantic textual similarity (STS). We notice a significant drop in performance for the ELECTRA discriminator's last layer in comparison to prior layers. We explore this drop and propose a way to repair the embeddings using a novel truncated model fine-tuning (TMFT) method. TMFT improves the Spearman correlation coefficient by over $8$ points while increasing parameter efficiency on the STS Benchmark. We extend our analysis to various model sizes, languages, and two other tasks. Further, we discover the surprising efficacy of ELECTRA's generator model, which performs on par with BERT, using significantly fewer parameters and a substantially smaller embedding size. Finally, we observe boosts by combining TMFT with word similarity or domain adaptive pre-training.",
            "link": "https://www.semanticscholar.org/paper/e133126dc7c59019c5f6f671c2d91f25507f470a",
            "authors": "Ivan Rep, David Duki'c, Jan \u0160najder",
            "matchScore": 266.82144,
            "original title": "Are ELECTRA\u2019s Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity",
            "original authors": "Ivan Rep, David Duki\u0107, Jan Snajder",
            "EMNLP Paper ID": "1916",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "4c6ef5a1e5655a27d32ee5810583a074474a1a3e",
            "title": "Length Extrapolation of Transformers: A Survey from the Perspective of Positional Encoding",
            "abstract": "Built upon the Transformer, large language models (LLMs) have captured worldwide attention due to their remarkable abilities. Nevertheless, all Transformer-based models including LLMs suffer from a preset length limit and can hardly generalize from short training sequences to longer inference ones, namely, they cannot perform length extrapolation to handle long sequences, which severely hinders their application in scenarios demanding long input sequences such as legal or scientific documents. Thus, numerous methods have emerged to enhance the length extrapolation of Transformers. Despite the great research efforts, a systematic survey is still lacking. To fill this gap, we delve into these advances in a unified notation from the perspective of positional encoding (PE), as it has been considered the primary factor on length extrapolation. Specifically, we begin with extrapolatable PEs that have dominated this research field. Then, we dive into extrapolation methods based on them, covering position interpolation and randomized position methods. Finally, several challenges and future directions in this area are highlighted. Through this survey, we aim to enable the reader to gain a deep understanding of existing methods and provide stimuli for future research.",
            "link": "https://www.semanticscholar.org/paper/4c6ef5a1e5655a27d32ee5810583a074474a1a3e",
            "authors": "Liang Zhao, Xiachong Feng, Xiaocheng Feng, Weihong Zhong, Dongliang Xu, Qing Yang, Hongtao Liu, Bing Qin, Ting Liu",
            "matchScore": 227.10172,
            "original title": "Length Extrapolation of Transformers: A Survey from the Perspective of Positional Encoding",
            "original authors": "Liang Zhao, Xiachong Feng, Xiaocheng Feng, Weihong Zhong, Dongliang Xu, Qing Yang, Hongtao Liu, Bing Qin, Ting Liu",
            "EMNLP Paper ID": "2054",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "5928bc20378279e5b609af50e3f7a35d36a24f17",
            "title": "PRESTO: Progressive Pretraining Enhances Synthetic Chemistry Outcomes",
            "abstract": "Multimodal Large Language Models (MLLMs) have seen growing adoption across various scientific disciplines. These advancements encourage the investigation of molecule-text modeling within synthetic chemistry, a field dedicated to designing and conducting chemical reactions to synthesize new compounds with desired properties and applications. Current approaches, however, often neglect the critical role of multiple molecule graph interaction in understanding chemical reactions, leading to suboptimal performance in synthetic chemistry tasks. This study introduces PRESTO(Progressive Pretraining Enhances Synthetic Chemistry Outcomes), a new framework that bridges the molecule-text modality gap by integrating a comprehensive benchmark of pretraining strategies and dataset configurations. It progressively improves multimodal LLMs through cross-modal alignment and multi-graph understanding. Our extensive experiments demonstrate that PRESTO offers competitive results in downstream synthetic chemistry tasks. The code can be found at https://github.com/IDEA-XL/PRESTO.",
            "link": "https://www.semanticscholar.org/paper/5928bc20378279e5b609af50e3f7a35d36a24f17",
            "authors": "He Cao, Yanjun Shao, Zhiyuan Liu, Zijing Liu, Xiangru Tang, Yuan Yao, Yu Li",
            "matchScore": 265.46124,
            "original title": "PRESTO: Progressive Pretraining Enhances Synthetic Chemistry Outcomes",
            "original authors": "He CAO, Yanjun Shao, Zhiyuan Liu, Zijing Liu, Xiangru Tang, Yuan Yao, Yu Li",
            "EMNLP Paper ID": "2087",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "cd6ce958e3456b51d8fa7049bc81a3c27092fa5d",
            "title": "MELT: Materials-aware Continued Pre-training for Language Model Adaptation to Materials Science",
            "abstract": "We introduce a novel continued pre-training method, MELT (MatEriaLs-aware continued pre-Training), specifically designed to efficiently adapt the pre-trained language models (PLMs) for materials science. Unlike previous adaptation strategies that solely focus on constructing domain-specific corpus, MELT comprehensively considers both the corpus and the training strategy, given that materials science corpus has distinct characteristics from other domains. To this end, we first construct a comprehensive materials knowledge base from the scientific corpus by building semantic graphs. Leveraging this extracted knowledge, we integrate a curriculum into the adaptation process that begins with familiar and generalized concepts and progressively moves toward more specialized terms. We conduct extensive experiments across diverse benchmarks to verify the effectiveness and generality of MELT. A comprehensive evaluation convincingly supports the strength of MELT, demonstrating superior performance compared to existing continued pre-training methods. The in-depth analysis also shows that MELT enables PLMs to effectively represent materials entities compared to the existing adaptation methods, thereby highlighting its broad applicability across a wide spectrum of materials science.",
            "link": "https://www.semanticscholar.org/paper/cd6ce958e3456b51d8fa7049bc81a3c27092fa5d",
            "authors": "Junho Kim, Yeachan Kim, Jun-Hyung Park, Yerim Oh, Suho Kim, SangKeun Lee",
            "matchScore": 273.07117,
            "original title": "MELT: Materials-aware Continued Pre-training for Language Model Adaptation to Materials Science",
            "original authors": "Junho Kim, Yeachan Kim, Jun-Hyung Park, Yerim Oh, Suho Kim, SangKeun Lee",
            "EMNLP Paper ID": "2160",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "26afe6e897bb29e4f8a962141e14db89b3ad579d",
            "title": "Make Compound Sentences Simple to Analyze: Learning to Split Sentences for Aspect-based Sentiment Analysis",
            "abstract": "In the domain of Aspect-Based Sentiment Analysis (ABSA), generative methods have shown promising results and achieved substantial advancements. However, despite these advancements, the tasks of extracting sentiment quadruplets, which capture the nuanced sentiment expressions within a sentence, remain significant challenges. In particular, compound sentences can potentially contain multiple quadruplets, making the extraction task increasingly difficult as sentence complexity grows. To address this issue, we are focusing on simplifying sentence structures to facilitate the easier recognition of these elements and crafting a model that integrates seamlessly with various ABSA tasks. In this paper, we propose Aspect Term Oriented Sentence Splitter (ATOSS), which simplifies compound sentence into simpler and clearer forms, thereby clarifying their structure and intent. As a plug-and-play module, this approach retains the parameters of the ABSA model while making it easier to identify essential intent within input sentences. Extensive experimental results show that utilizing ATOSS outperforms existing methods in both ASQP and ACOS tasks, which are the primary tasks for extracting sentiment quadruplets.",
            "link": "https://www.semanticscholar.org/paper/26afe6e897bb29e4f8a962141e14db89b3ad579d",
            "authors": "Yongsik Seo, Sungwon Song, Ryang Heo, Jieyong Kim, Dongha Lee",
            "matchScore": 328.7339,
            "original title": "Make Compound Sentences Simple to Analyze: Learning to Split Sentences for Aspect-based Sentiment Analysis",
            "original authors": "Yongsik Seo, Sungwon Song, Ryang Heo, Jieyong Kim, Dongha Lee",
            "EMNLP Paper ID": "2221",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "d64b4af5cb0ed20451b851f31572da9497da07a4",
            "title": "PositionID: LLMs can Control Lengths, Copy and Paste with Explicit Positional Awareness",
            "abstract": "Large Language Models (LLMs) demonstrate impressive capabilities across various domains, including role-playing, creative writing, mathematical reasoning, and coding. Despite these advancements, LLMs still encounter challenges with length control, frequently failing to adhere to specific length constraints due to their token-level operations and insufficient training on data with strict length limitations. We identify this issue as stemming from a lack of positional awareness and propose novel approaches--PositionID Prompting and PositionID Fine-Tuning--to address it. These methods enhance the model's ability to continuously monitor and manage text length during generation. Additionally, we introduce PositionID CP Prompting to enable LLMs to perform copy and paste operations accurately. Furthermore, we develop two benchmarks for evaluating length control and copy-paste abilities. Our experiments demonstrate that our methods significantly improve the model's adherence to length constraints and copy-paste accuracy without compromising response quality.",
            "link": "https://www.semanticscholar.org/paper/d64b4af5cb0ed20451b851f31572da9497da07a4",
            "authors": "Z. Wang, Feiyu Duan, Yibo Zhang, Wangchunshu Zhou, Ke Xu, Wenhao Huang, Jie Fu",
            "matchScore": 321.60895,
            "original title": "PositionID: LLMs can Control Lengths, Copy and Paste with Explicit Positional Awareness",
            "original authors": "Noah Wang, Feiyu Duan, Yibo Zhang, Wangchunshu Zhou, Ke Xu, Wenhao Huang, Jie Fu",
            "EMNLP Paper ID": "3234",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "885d20148852ae14e37a9cd5b1229cfbe39704fd",
            "title": "ASTE Transformer Modelling Dependencies in Aspect-Sentiment Triplet Extraction",
            "abstract": "Aspect-Sentiment Triplet Extraction (ASTE) is a recently proposed task of aspect-based sentiment analysis that consists in extracting (aspect phrase, opinion phrase, sentiment polarity) triples from a given sentence. Recent state-of-the-art methods approach this task by first extracting all possible text spans from a given text, then filtering the potential aspect and opinion phrases with a classifier, and finally considering all their pairs with another classifier that additionally assigns sentiment polarity to them. Although several variations of the above scheme have been proposed, the common feature is that the final result is constructed by a sequence of independent classifier decisions. This hinders the exploitation of dependencies between extracted phrases and prevents the use of knowledge about the interrelationships between classifier predictions to improve performance. In this paper, we propose a new ASTE approach consisting of three transformer-inspired layers, which enables the modelling of dependencies both between phrases and between the final classifier decisions. Experimental results show that the method achieves higher performance in terms of F1 measure than other methods studied on popular benchmarks. In addition, we show that a simple pre-training technique further improves the performance of the model.",
            "link": "https://www.semanticscholar.org/paper/885d20148852ae14e37a9cd5b1229cfbe39704fd",
            "authors": "Iwo Naglik, Mateusz Lango",
            "matchScore": 262.50858,
            "original title": "ASTE-Transformer: Modelling Dependencies in Aspect-Sentiment Triplet Extraction",
            "original authors": "Iwo Naglik, Mateusz Lango",
            "EMNLP Paper ID": "466",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "f7ccf8ecd508e0b2d423169588dd1c1a82dd3b4d",
            "title": "Scaling Sentence Embeddings with Large Language Models",
            "abstract": "Large language models (LLMs) have recently garnered significant interest. With in-context learning, LLMs achieve impressive results in various natural language tasks. However, the application of LLMs to sentence embeddings remains an area of ongoing research. In this work, we propose an in-context learning-based method aimed at improving sentence embeddings performance. Our approach involves adapting the previous prompt-based representation method for autoregressive models, constructing a demonstration set that enables LLMs to perform in-context learning, and scaling up the LLMs to different model sizes. Through extensive experiments, in-context learning enables LLMs to generate high-quality sentence embeddings without any fine-tuning. It helps LLMs achieve performance comparable to current contrastive learning methods. By scaling model size, we find scaling to more than tens of billion parameters harms the performance on semantic textual similarity (STS) tasks. However, the largest model outperforms other counterparts and achieves the new state-of-the-art result on transfer tasks. We also fine-tune LLMs with current contrastive learning approach, and the 2.7B OPT model, incorporating our prompt-based method, surpasses the performance of 4.8B ST5, achieving the new state-of-the-art results on STS tasks. Our code is available at https://github.com/kongds/scaling_sentemb.",
            "link": "https://www.semanticscholar.org/paper/f7ccf8ecd508e0b2d423169588dd1c1a82dd3b4d",
            "authors": "Ting Jiang, Shaohan Huang, Zhongzhi Luan, Deqing Wang, Fuzhen Zhuang",
            "matchScore": 176.27754,
            "original title": "Scaling Sentence Embeddings with Large Language Models",
            "original authors": "Ting Jiang, Shaohan Huang, Zhongzhi Luan, deqing wang, Fuzhen Zhuang",
            "EMNLP Paper ID": "650",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "0a52a97ec49da2560d96d431f15831bc7e1389e1",
            "title": "Taking a Deep Breath: Enhancing Language Modeling of Large Language Models with Sentinel Tokens",
            "abstract": "Large language models (LLMs) have shown promising efficacy across various tasks, becoming powerful tools in numerous aspects of human life. However, Transformer-based LLMs suffer a performance degradation when modeling long-term contexts due to they discard some information to reduce computational overhead. In this work, we propose a simple yet effective method to enable LLMs to take a deep breath, encouraging them to summarize information contained within discrete text chunks. Specifically, we segment the text into multiple chunks and insert special tokenat the end of each chunk. We then modify the attention mask to integrate the chunk's information into the correspondingtoken. This facilitates LLMs to interpret information not only from historical individual tokens but also from thetoken, aggregating the chunk's semantic information. Experiments on language modeling and out-of-domain downstream tasks validate the superiority of our approach.",
            "link": "https://www.semanticscholar.org/paper/0a52a97ec49da2560d96d431f15831bc7e1389e1",
            "authors": "Weiyao Luo, Suncong Zheng, Heming Xia, Weikang Wang, Yan Lei, Tianyu Liu, Shuang Chen, Zhifang Sui",
            "matchScore": 282.65506,
            "original title": "Taking a Deep Breath: Enhancing Language Modeling of Large Language Models with Sentinel Tokens",
            "original authors": "Weiyao Luo, Suncong Zheng, Heming Xia, weikang wang, Yan Lei, Tianyu Liu, Shuang Chen, Zhifang Sui",
            "EMNLP Paper ID": "794",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "e616f85b4d00204e495bbd1c7984c055ede99d7b",
            "title": "Leveraging Grammar Induction for Language Understanding and Generation",
            "abstract": "Grammar induction has made significant progress in recent years. However, it is not clear how the application of induced grammar could enhance practical performance in downstream tasks. In this work, we introduce an unsupervised grammar induction method for language understanding and generation. We construct a grammar parser to induce constituency structures and dependency relations, which is simultaneously trained on downstream tasks without additional syntax annotations. The induced grammar features are subsequently incorporated into Transformer as a syntactic mask to guide self-attention. We evaluate and apply our method to multiple machine translation tasks and natural language understanding tasks. Our method demonstrates superior performance compared to the original Transformer and other models enhanced with external parsers. Experimental results indicate that our method is effective in both from-scratch and pre-trained scenarios. Additionally, our research highlights the contribution of explicitly modeling the grammatical structure of texts to neural network models.",
            "link": "https://www.semanticscholar.org/paper/e616f85b4d00204e495bbd1c7984c055ede99d7b",
            "authors": "Jushi Kai, Shengyuan Hou, Yusheng Huang, Zhouhan Lin",
            "matchScore": 205.7185,
            "original title": "Leveraging Grammar Induction for Language Understanding and Generation",
            "original authors": "Jushi Kai, Shengyuan Hou, Yusheng Huang, Zhouhan Lin",
            "EMNLP Paper ID": "894",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "ea1ed23c409f29f42e983f23d805b8a0694310a7",
            "title": "Stanceformer: Target-Aware Transformer for Stance Detection",
            "abstract": "The task of Stance Detection involves discerning the stance expressed in a text towards a specific subject or target. Prior works have relied on existing transformer models that lack the capability to prioritize targets effectively. Consequently, these models yield similar performance regardless of whether we utilize or disregard target information, undermining the task's significance. To address this challenge, we introduce Stanceformer, a target-aware transformer model that incorporates enhanced attention towards the targets during both training and inference. Specifically, we design a \\textit{Target Awareness} matrix that increases the self-attention scores assigned to the targets. We demonstrate the efficacy of the Stanceformer with various BERT-based models, including state-of-the-art models and Large Language Models (LLMs), and evaluate its performance across three stance detection datasets, alongside a zero-shot dataset. Our approach Stanceformer not only provides superior performance but also generalizes even to other domains, such as Aspect-based Sentiment Analysis. We make the code publicly available.\\footnote{\\scriptsize\\url{https://github.com/kgarg8/Stanceformer}}",
            "link": "https://www.semanticscholar.org/paper/ea1ed23c409f29f42e983f23d805b8a0694310a7",
            "authors": "Krishna Garg, Cornelia Caragea",
            "matchScore": 233.25914,
            "original title": "Stanceformer: Target-Aware Transformer for Stance Detection",
            "original authors": "Krishna Garg, Cornelia Caragea",
            "EMNLP Paper ID": "975",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        }
    ],
    "Advanced Techniques in Chain-of-Thought Reasoning for LLMs": [
        {
            "paperId": "0654d0addc15c0c76e39ef71c48164e395c6993f",
            "title": "ARES: Alternating Reinforcement Learning and Supervised Fine-Tuning for Enhanced Multi-Modal Chain-of-Thought Reasoning Through Diverse AI Feedback",
            "abstract": "Large Multimodal Models (LMMs) excel at comprehending human instructions and demonstrate remarkable results across a broad spectrum of tasks. Reinforcement Learning from Human Feedback (RLHF) and AI Feedback (RLAIF) further refine LLMs by aligning them with specific preferences. These methods primarily use ranking-based feedback for entire generations. With advanced AI models (Teacher), such as GPT-4 and Claude 3 Opus, we can request various types of detailed feedback that are expensive for humans to provide. We propose a two-stage algorithm ARES that Alternates REinforcement Learning (RL) and Supervised Fine-Tuning (SFT). First, we request the Teacher to score how much each sentence contributes to solving the problem in a Chain-of-Thought (CoT). This sentence-level feedback allows us to consider individual valuable segments, providing more granular rewards for the RL procedure. Second, we ask the Teacher to correct the wrong reasoning after the RL stage. The RL procedure requires massive efforts for hyperparameter tuning and often generates errors like repetitive words and incomplete sentences. With the correction feedback, we stabilize the RL fine-tuned model through SFT. We conduct experiments on multi-model dataset ScienceQA and A-OKVQA to demonstrate the effectiveness of our proposal. ARES rationale reasoning achieves around 70% win rate against baseline models judged by GPT-4o. Additionally, we observe that the improved rationale reasoning leads to a 2.5% increase in inference answer accuracy on average for the multi-modal datasets.",
            "link": "https://www.semanticscholar.org/paper/0654d0addc15c0c76e39ef71c48164e395c6993f",
            "authors": "Ju-Seung Byun, Jiyun Chun, Jihyung Kil, Andrew Perrault",
            "EMNLP Paper ID": "481",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "6d25e201deddad04534e37ccbe0f7566938e10f3",
            "title": "Investigating Mysteries of CoT-Augmented Distillation",
            "abstract": "Eliciting\"chain of thought\"(CoT) rationales -- sequences of token that convey a\"reasoning\"process -- has been shown to consistently improve LLM performance on tasks like question answering. More recent efforts have shown that such rationales can also be used for model distillation: Including CoT sequences (elicited from a large\"teacher\"model) in addition to target labels when fine-tuning a small student model yields (often substantial) improvements. In this work we ask: Why and how does this additional training signal help in model distillation? We perform ablations to interrogate this, and report some potentially surprising results. Specifically: (1) Placing CoT sequences after labels (rather than before) realizes consistently better downstream performance -- this means that no student\"reasoning\"is necessary at test time to realize gains. (2) When rationales are appended in this way, they need not be coherent reasoning sequences to yield improvements; performance increases are robust to permutations of CoT tokens, for example. In fact, (3) a small number of key tokens are sufficient to achieve improvements equivalent to those observed when full rationales are used in model distillation.",
            "link": "https://www.semanticscholar.org/paper/6d25e201deddad04534e37ccbe0f7566938e10f3",
            "authors": "Somin Wadhwa, Silvio Amir, Byron C. Wallace",
            "EMNLP Paper ID": "678",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "215c345579e6f230191d2b5a591bdaa75e2fe2f5",
            "title": "Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs",
            "abstract": "Reasoning is a fundamental component of language understanding. Recent prompting techniques, such as chain of thought, have consistently improved LLMs' performance on various reasoning tasks. Nevertheless, there is still little understanding of what triggers reasoning abilities in LLMs in the inference stage. In this paper, we introduce code prompting, a chain of prompts that transforms a natural language problem into code and directly prompts the LLM using the generated code without resorting to external code execution. We hypothesize that code prompts can elicit certain reasoning capabilities of LLMs trained on text and code and utilize the proposed method to improve conditional reasoning, the ability to infer different conclusions depending on the fulfillment of certain conditions. We find that code prompting exhibits a high-performance boost for multiple LLMs (up to 22.52 percentage points on GPT 3.5, 7.75 on Mixtral, and 16.78 on Mistral) across multiple conditional reasoning datasets. We then conduct comprehensive experiments to understand how code prompts trigger reasoning abilities and which capabilities are elicited in the underlying models. Our analysis of GPT 3.5 reveals that the code formatting of the input problem is essential for performance improvement. Furthermore, code prompts improve sample efficiency of in-context learning and facilitate state tracking of variables or entities.",
            "link": "https://www.semanticscholar.org/paper/215c345579e6f230191d2b5a591bdaa75e2fe2f5",
            "authors": "Haritz Puerto, Martin Tutek, Somak Aditya, Xiaodan Zhu, Iryna Gurevych",
            "EMNLP Paper ID": "1306",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "2deca962f38f442eaa2d8cac3fdee3db7f2bde14",
            "title": "MARE: Multi-Aspect Rationale Extractor on Unsupervised Rationale Extraction",
            "abstract": "Unsupervised rationale extraction aims to extract text snippets to support model predictions without explicit rationale annotation. Researchers have made many efforts to solve this task. Previous works often encode each aspect independently, which may limit their ability to capture meaningful internal correlations between aspects. While there has been significant work on mitigating spurious correlations, our approach focuses on leveraging the beneficial internal correlations to improve multi-aspect rationale extraction. In this paper, we propose a Multi-Aspect Rationale Extractor (MARE) to explain and predict multiple aspects simultaneously. Concretely, we propose a Multi-Aspect Multi-Head Attention (MAMHA) mechanism based on hard deletion to encode multiple text chunks simultaneously. Furthermore, multiple special tokens are prepended in front of the text with each corresponding to one certain aspect. Finally, multi-task training is deployed to reduce the training overhead. Experimental results on two unsupervised rationale extraction benchmarks show that MARE achieves state-of-the-art performance. Ablation studies further demonstrate the effectiveness of our method. Our codes have been available at https://github.com/CSU-NLP-Group/MARE.",
            "link": "https://www.semanticscholar.org/paper/2deca962f38f442eaa2d8cac3fdee3db7f2bde14",
            "authors": "Han Jiang, Junwen Duan, Zhe Qu, Jianxin Wang",
            "EMNLP Paper ID": "1363",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "1f5b4e393a1e02ab49e5ca2e6819cc28841918a2",
            "title": "Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning",
            "abstract": "Large Language Models (LLMs) have shown impressive capabilities, yet they still struggle with math reasoning. In this work, we propose CoT-Influx, a novel approach that pushes the boundary of few-shot Chain-of-Thoughts (CoT) learning to improve LLM mathematical reasoning. Motivated by the observation that adding more concise CoT examples in the prompt can improve LLM reasoning performance, CoT-Influx employs a coarse-to-fine pruner to maximize the input of effective and concise CoT examples. The pruner first selects as many crucial CoT examples as possible and then prunes unimportant tokens to fit the context window. A math reasoning dataset with diverse difficulty levels and reasoning steps is used to train the pruner, along with a math-specialized reinforcement learning approach. As a result, by enabling more CoT examples with double the context window size in tokens, CoT-Influx significantly outperforms various prompting baselines across various LLMs (LLaMA2-7B, 13B, 70B) and 5 math datasets, achieving up to 4.55% absolute improvements. Remarkably, without any fine-tuning, LLaMA2-70B with CoT-Influx surpasses GPT-3.5 and a wide range of larger LLMs (PaLM, Minerva 540B, etc.) on the GSM8K. CoT-Influx serves as a plug-and-play module for LLMs and is compatible with most existing reasoning prompting techniques, such as self-consistency and self-verification.",
            "link": "https://www.semanticscholar.org/paper/1f5b4e393a1e02ab49e5ca2e6819cc28841918a2",
            "authors": "Xijie Huang, L. Zhang, Kwang-Ting Cheng, Fan Yang, Mao Yang",
            "EMNLP Paper ID": "1578",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "9289bc1f0c4578c1b819f95edd91161d5169e893",
            "title": "First Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning",
            "abstract": "Multi-step reasoning instruction, such as chain-of-thought prompting, is widely adopted to explore better language models (LMs) performance. We report on the systematic strategy that LMs employ in such a multi-step reasoning process. Our controlled experiments reveal that LMs rely more heavily on heuristics, such as lexical overlap, in the earlier stages of reasoning, where more reasoning steps remain to reach a goal. Conversely, their reliance on heuristics decreases as LMs progress closer to the final answer through multiple reasoning steps. This suggests that LMs can backtrack only a limited number of future steps and dynamically combine heuristic strategies with rationale ones in tasks involving multi-step reasoning.",
            "link": "https://www.semanticscholar.org/paper/9289bc1f0c4578c1b819f95edd91161d5169e893",
            "authors": "Yoichi Aoki, Keito Kudo, Tatsuki Kuribayashi, Shusaku Sone, Masaya Taniguchi, Keisuke Sakaguchi, Kentaro Inui",
            "EMNLP Paper ID": "1644",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "5161e996b9d6c83990d69c456423edf9ee9ff12e",
            "title": "Nash CoT: Multi-Path Inference with Preference Equilibrium",
            "abstract": "Chain-of-thought (CoT) prompting has emerged as a powerful technique for enhancing the reasoning capabilities of Large Language Models (LLMs) on complex problems. Among CoT-related studies, self-consistency (Multi-path inference with answer filtering through voting) involves generating multiple reasoning paths using the CoT framework and then selecting the most frequently produced outputs standing out as a concise yet competitive approach. While self-consistency has indeed led to the improvements in LLM inference, the use of multi-path inference also escalates deployment costs. Therefore, maintaining the performance benefits of self-consistency inherited from multi-path inference while reducing the inference costs holds significant value. In this research, we conceptualize language decoding as a preference consensus game, constructing a bi-player gaming system within each local path, and introduce Nash Chain-of-Thought (Nash CoT). Specifically, for a given question, we leverage LLM to autonomously select the contextually relevant template and generate outputs guided by this template, aiming to reach Nash Equilibrium alongside normal generation in each path. This approach allows us to achieve comparable or improved performance compared to self-consistency while using fewer inference paths on various inference tasks, including Arabic reasoning, Commonsense Question answering, and Symbolic inference.",
            "link": "https://www.semanticscholar.org/paper/5161e996b9d6c83990d69c456423edf9ee9ff12e",
            "authors": "Ziqi Zhang, Cunxiang Wang, Xiong Xiao, Yue Zhang, Donglin Wang",
            "EMNLP Paper ID": "1679",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 12): 17:45\u201318:45 (Evening)",
            "Type": "Main"
        },
        {
            "paperId": "4b508ba98a180f27fd93b702d7044adad91620eb",
            "title": "Re-Reading Improves Reasoning in Large Language Models",
            "abstract": "To enhance the reasoning capabilities of off-the-shelf Large Language Models (LLMs), we introduce a simple, yet general and effective prompting method, Re2, i.e., \\textbf{Re}-\\textbf{Re}ading the question as input. Unlike most thought-eliciting prompting methods, such as Chain-of-Thought (CoT), which aim to elicit the reasoning process in the output, Re2 shifts the focus to the input by processing questions twice, thereby enhancing the understanding process. Consequently, Re2 demonstrates strong generality and compatibility with most thought-eliciting prompting methods, including CoT. Crucially, Re2 facilitates a\"bidirectional\"encoding in unidirectional decoder-only LLMs because the first pass could provide global information for the second pass. We begin with a preliminary empirical study as the foundation of Re2, illustrating its potential to enable\"bidirectional\"attention mechanisms. We then evaluate Re2 on extensive reasoning benchmarks across 14 datasets, spanning 112 experiments, to validate its effectiveness and generality. Our findings indicate that, with the exception of a few scenarios on vanilla ChatGPT, Re2 consistently enhances the reasoning performance of LLMs through a simple re-reading strategy. Further analyses reveal Re2's adaptability, showing how it can be effectively integrated with different LLMs, thought-eliciting prompting, and ensemble strategies. Our code is available at \\url{https://github.com/Tebmer/Rereading-LLM-Reasoning/}",
            "link": "https://www.semanticscholar.org/paper/4b508ba98a180f27fd93b702d7044adad91620eb",
            "authors": "Xiaohan Xu, Chongyang Tao, Tao Shen, Can Xu, Hongbo Xu, Guodong Long, Jian-Guang Lou",
            "EMNLP Paper ID": "1826",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "04503eb8c96d1978171803cf3bb54273f6ad9a19",
            "title": "Improve Student's Reasoning Generalizability through Cascading Decomposed CoTs Distillation",
            "abstract": "Large language models (LLMs) exhibit enhanced reasoning at larger scales, driving efforts to distill these capabilities into smaller models via teacher-student learning. Previous works simply fine-tune student models on teachers' generated Chain-of-Thoughts (CoTs) data. Although these methods enhance in-domain (IND) reasoning performance, they struggle to generalize to out-of-domain (OOD) tasks. We believe that the widespread spurious correlations between questions and answers may lead the model to preset a specific answer which restricts the diversity and generalizability of its reasoning process. In this paper, we propose Cascading Decomposed CoTs Distillation (CasCoD) to address these issues by decomposing the traditional single-step learning process into two cascaded learning steps. Specifically, by restructuring the training objectives -- removing the answer from outputs and concatenating the question with the rationale as input -- CasCoD's two-step learning process ensures that students focus on learning rationales without interference from the preset answers, thus improving reasoning generalizability. Extensive experiments demonstrate the effectiveness of CasCoD on both IND and OOD benchmark reasoning datasets. Code can be found at https://github.com/C-W-D/CasCoD.",
            "link": "https://www.semanticscholar.org/paper/04503eb8c96d1978171803cf3bb54273f6ad9a19",
            "authors": "Chengwei Dai, Kun Li, Wei Zhou, Song Hu",
            "EMNLP Paper ID": "1834",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "4ab7306990bd92a3f01a4d53a23e9059b63d1f5e",
            "title": "Mentor-KD: Making Small Language Models Better Multi-step Reasoners",
            "abstract": "Large Language Models (LLMs) have displayed remarkable performances across various complex tasks by leveraging Chain-of-Thought (CoT) prompting. Recently, studies have proposed a Knowledge Distillation (KD) approach, reasoning distillation, which transfers such reasoning ability of LLMs through fine-tuning language models of multi-step rationales generated by LLM teachers. However, they have inadequately considered two challenges regarding insufficient distillation sets from the LLM teacher model, in terms of 1) data quality and 2) soft label provision. In this paper, we propose Mentor-KD, which effectively distills the multi-step reasoning capability of LLMs to smaller LMs while addressing the aforementioned challenges. Specifically, we exploit a mentor, intermediate-sized task-specific fine-tuned model, to augment additional CoT annotations and provide soft labels for the student model during reasoning distillation. We conduct extensive experiments and confirm Mentor-KD's effectiveness across various models and complex reasoning tasks.",
            "link": "https://www.semanticscholar.org/paper/4ab7306990bd92a3f01a4d53a23e9059b63d1f5e",
            "authors": "Hojae Lee, Junho Kim, SangKeun Lee",
            "EMNLP Paper ID": "2113",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "544afd4f3395d7517bbea69000be1b52a45e41c7",
            "title": "Tree of Problems: Improving structured problem solving with compositionality",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across multiple tasks through in-context learning. For complex reasoning tasks that require step-by-step thinking, Chain-of-Thought (CoT) prompting has given impressive results, especially when combined with self-consistency. Nonetheless, some tasks remain particularly difficult for LLMs to solve. Tree of Thoughts (ToT) and Graph of Thoughts (GoT) emerged as alternatives, dividing the complex problem into paths of subproblems. In this paper, we propose Tree of Problems (ToP), a simpler version of ToT, which we hypothesise can work better for complex tasks that can be divided into identical subtasks. Our empirical results show that our approach outperforms ToT and GoT, and in addition performs better than CoT on complex reasoning tasks. All code for this paper is publicly available here: https://github.com/ArmelRandy/tree-of-problems.",
            "link": "https://www.semanticscholar.org/paper/544afd4f3395d7517bbea69000be1b52a45e41c7",
            "authors": "Armel Zebaze, Beno\u00eet Sagot, Rachel Bawden",
            "EMNLP Paper ID": "2203",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "7ef00b45f7e9c220a8672bff48cf4bd273d01d32",
            "title": "Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities",
            "abstract": "When presented with questions involving visual thinking, humans naturally switch reasoning modalities, often forming mental images or drawing visual aids. Large language models have shown promising results in arithmetic and symbolic reasoning by expressing intermediate reasoning in text as a chain of thought, yet struggle to extend this capability to answer text queries that are easily solved by visual reasoning, even with extensive multimodal pretraining. We introduce a simple method, whiteboard-of-thought prompting, to unlock the visual reasoning capabilities of multimodal large language models across modalities. Whiteboard-of-thought prompting provides multimodal large language models with a metaphorical `whiteboard' to draw out reasoning steps as images, then returns these images back to the model for further processing. We find this can be accomplished with no demonstrations or specialized modules, instead leveraging models' existing ability to write code with libraries such as Matplotlib and Turtle. This simple approach shows state-of-the-art results on four difficult natural language tasks that involve visual and spatial reasoning. We identify multiple settings where GPT-4o using chain-of-thought fails dramatically, including more than one where it achieves $0\\%$ accuracy, while whiteboard-of-thought enables up to $92\\%$ accuracy in these same settings. We present a detailed exploration of where the technique succeeds as well as its sources of error.",
            "link": "https://www.semanticscholar.org/paper/7ef00b45f7e9c220a8672bff48cf4bd273d01d32",
            "authors": "Sachit Menon, Richard Zemel, Carl Vondrick",
            "EMNLP Paper ID": "2607",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "d5f0731d2c6e145ce6d978c8936d41ec64856e5e",
            "title": "Any Other Thoughts, Hedgehog? Linking Deliberation Chains in Collaborative Dialogues",
            "abstract": "Question-asking in collaborative dialogue has long been established as key to knowledge construction, both in internal and collaborative problem solving. In this work, we examine probing questions in collaborative dialogues: questions that explicitly elicit responses from the speaker's interlocutors. Specifically, we focus on modeling the causal relations that lead directly from utterances earlier in the dialogue to the emergence of the probing question. We model these relations using a novel graph-based framework of deliberation chains, and reframe the problem of constructing such chains as a coreference-style clustering problem. Our framework jointly models probing and causal utterances and the links between them, and we evaluate on two challenging collaborative task datasets: the Weights Task and DeliData. Our results demonstrate the effectiveness of our theoretically-grounded approach compared to both baselines and stronger coreference approaches, and establish a standard of performance in this novel task.",
            "link": "https://www.semanticscholar.org/paper/d5f0731d2c6e145ce6d978c8936d41ec64856e5e",
            "authors": "Abhijnan Nath, Videep Venkatesha, Mariah Bradford, Avyakta Chelle, Austin Youngren, Carlos Mabrey, Nathaniel Blanchard, Nikhil Krishnaswamy",
            "matchScore": 304.17255,
            "original title": "\u201cAny Other Thoughts, Hedgehog?\u201d Linking Deliberation Chains in Collaborative Dialogues",
            "original authors": "Abhijnan Nath, Videep Venkatesha, Mariah Bradford, Avyakta Chelle, Austin Collin Youngren, Carlos Mabrey, Nathaniel Blanchard, Nikhil Krishnaswamy",
            "EMNLP Paper ID": "1069",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "84c7ca14b9dc95fc8a21cf6f4a2194ffa99579cc",
            "title": "Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring",
            "abstract": "Generating rationales that justify scoring decisions has been a promising way to facilitate explainability in automated scoring systems. However, existing methods do not match the accuracy of classifier-based methods. Plus, the generated rationales often contain hallucinated information. To address these issues, we propose a novel framework capable of generating more faithful rationales and, more importantly, matching performance with classifier-based black-box scoring systems. We first mimic the human assessment process by querying Large Language Models (LLMs) to generate a thought tree. We then summarise intermediate assessment decisions from each thought tree path for creating synthetic rationale data and rationale preference data. Finally, we utilise the generated synthetic data to calibrate LLMs through a two-step training process: supervised fine-tuning and preference optimization. Extensive experimental results demonstrate that our framework achieves a 38% assessment performance improvement in the QWK score compared to prior work while producing higher-quality rationales, as recognised by human evaluators and LLMs. Our work sheds light on the effectiveness of performing preference optimization using synthetic preference data obtained from thought tree paths. Data and code are available at https://github.com/lijiazheng99/thought_tree_assessment.",
            "link": "https://www.semanticscholar.org/paper/84c7ca14b9dc95fc8a21cf6f4a2194ffa99579cc",
            "authors": "Jiazheng Li, Hainiu Xu, ZHAOYUE SUN, Yuxiang Zhou, David West, Cesare Aloisi, Yulan He",
            "matchScore": 320.33374,
            "original title": "Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring",
            "original authors": "Jiazheng Li, Hainiu Xu, ZHAOYUE SUN, Yuxiang Zhou, David West, Cesare Aloisi, Yulan He",
            "EMNLP Paper ID": "1089",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Findings"
        },
        {
            "paperId": "ac82fbd7122ef5814f8bbff01d12819fc44c4d2c",
            "title": "Wrong-of-Thought: An Integrated Reasoning Framework with Multi-Perspective Verification and Wrong Information",
            "abstract": "Chain-of-Thought (CoT) has become a vital technique for enhancing the performance of Large Language Models (LLMs), attracting increasing attention from researchers. One stream of approaches focuses on the iterative enhancement of LLMs by continuously verifying and refining their reasoning outputs for desired quality. Despite its impressive results, this paradigm faces two critical issues: (1) Simple verification methods: The current paradigm relies solely on a single verification method. (2) Wrong Information Ignorance: Traditional paradigms directly ignore wrong information during reasoning and refine the logic paths from scratch each time. To address these challenges, we propose Wrong-of-Thought (WoT), which includes two core modules: (1) Multi-Perspective Verification: A multi-perspective verification method for accurately refining the reasoning process and result, and (2) Wrong Information Utilization: Utilizing wrong information to alert LLMs and reduce the probability of LLMs making same mistakes. Experiments on 8 popular datasets and 5 LLMs demonstrate that WoT surpasses all previous baselines. In addition, WoT exhibits powerful capabilities in difficult computation tasks.",
            "link": "https://www.semanticscholar.org/paper/ac82fbd7122ef5814f8bbff01d12819fc44c4d2c",
            "authors": "Yongheng Zhang, Qiguang Chen, Jingxuan Zhou, Peng Wang, Jiasheng Si, Jin Wang, Wenpeng Lu, Libo Qin",
            "matchScore": 296.02063,
            "original title": "Wrong-of-Thought: An Integrated Reasoning Framework with Multi-Perspective Verification and Wrong Information",
            "original authors": "Yongheng Zhang, Qiguang Chen, Jingxuan Zhou, Peng Wang, Jiasheng Si, Jin Wang, Wenpeng Lu, Libo Qin",
            "EMNLP Paper ID": "1349",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "5b02900dd920a4c663dbe96023c2f1ac6e485276",
            "title": "Weak-to-Strong Reasoning",
            "abstract": "When large language models (LLMs) exceed human-level capabilities, it becomes increasingly challenging to provide full-scale and accurate supervision for these models. Weak-to-strong learning, which leverages a less capable model to unlock the latent abilities of a stronger model, proves valuable in this context. Yet, the efficacy of this approach for complex reasoning tasks is still untested. Furthermore, tackling reasoning tasks under the weak-to-strong setting currently lacks efficient methods to avoid blindly imitating the weak supervisor including its errors. In this paper, we introduce a progressive learning framework that enables the strong model to autonomously refine its training data, without requiring input from either a more advanced model or human-annotated data. This framework begins with supervised fine-tuning on a selective small but high-quality dataset, followed by preference optimization on contrastive samples identified by the strong model itself. Extensive experiments on the GSM8K and MATH datasets demonstrate that our method significantly enhances the reasoning capabilities of Llama2-70b using three separate weak models. This method is further validated in a forward-looking experimental setup, where Llama3-8b-instruct effectively supervises Llama3-70b on the highly challenging OlympicArena dataset. This work paves the way for a more scalable and sophisticated strategy to enhance AI reasoning powers. All relevant code and resources are available in \\url{https://github.com/GAIR-NLP/weak-to-strong-reasoning}.",
            "link": "https://www.semanticscholar.org/paper/5b02900dd920a4c663dbe96023c2f1ac6e485276",
            "authors": "Yuqing Yang, Yan Ma, Pengfei Liu",
            "matchScore": 116.492584,
            "original title": "Weak-to-Strong Reasoning",
            "original authors": "Yuqing Yang, Yan Ma, Pengfei Liu",
            "EMNLP Paper ID": "1764",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "5c23d83e1dad580160f58afb84d12e61f369b8ab",
            "title": "MDCR: A Dataset for Multi-Document Conditional Reasoning",
            "abstract": "The same real-life questions posed to different individuals may lead to different answers based on their unique situations. For instance, whether a student is eligible for a scholarship depends on eligibility conditions, such as major or degree required. ConditionalQA was proposed to evaluate models' capability of reading a document and answering eligibility questions, considering unmentioned conditions. However, it is limited to questions on single documents, neglecting harder cases that may require cross-document reasoning and optimization, for example,\"What is the maximum number of scholarships attainable?\"Such questions over multiple documents are not only more challenging due to more context having to understand, but also because the model has to (1) explore all possible combinations of unmentioned conditions and (2) understand the relationship between conditions across documents, to reason about the optimal outcome. To evaluate models' capability of answering such questions, we propose a new dataset MDCR, which can reflect real-world challenges and serve as a new test bed for complex conditional reasoning that requires optimization. We evaluate this dataset using the most recent LLMs and demonstrate their limitations in solving this task. We believe this dataset will facilitate future research in answering optimization questions with unknown conditions.",
            "link": "https://www.semanticscholar.org/paper/5c23d83e1dad580160f58afb84d12e61f369b8ab",
            "authors": "Peter Baile Chen, Yi Zhang, Chunwei Liu, Sejal Gupta, Yoon Kim, Michael Cafarella",
            "matchScore": 241.08658,
            "original title": "MDCR: A Dataset for Multi-Document Conditional Reasoning",
            "original authors": "Peter Baile Chen, Yi Zhang, Chunwei Liu, Sejal Gupta, Yoon Kim, Mike Cafarella",
            "EMNLP Paper ID": "2255",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "a3d2c1c932da66f48af673bcb860ba0b8fb335d1",
            "title": "Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning",
            "abstract": "Large language models (LLMs) have been shown to perform better when asked to reason step-by-step before answering a question. However, it is unclear to what degree the model's final answer is faithful to the stated reasoning steps. In this paper, we perform a causal mediation analysis on twelve LLMs to examine how intermediate reasoning steps generated by the LLM influence the final outcome and find that LLMs do not reliably use their intermediate reasoning steps when generating an answer. To address this issue, we introduce FRODO, a framework to tailor small-sized LMs to generate correct reasoning steps and robustly reason over these steps. FRODO consists of an inference module that learns to generate correct reasoning steps using an implicit causal reward function and a reasoning module that learns to faithfully reason over these intermediate inferences using a counterfactual and causal preference objective. Our experiments show that FRODO significantly outperforms four competitive baselines. Furthermore, FRODO improves the robustness and generalization ability of the reasoning LM, yielding higher performance on out-of-distribution test sets. Finally, we find that FRODO's rationales are more faithful to its final answer predictions than standard supervised fine-tuning.",
            "link": "https://www.semanticscholar.org/paper/a3d2c1c932da66f48af673bcb860ba0b8fb335d1",
            "authors": "Debjit Paul, Robert West, Antoine Bosselut, Boi Faltings",
            "matchScore": 266.07324,
            "original title": "Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning",
            "original authors": "Debjit Paul, Robert West, Antoine Bosselut, Boi Faltings",
            "EMNLP Paper ID": "2882",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "41a6acb85168316c333daf529a678afd2e668855",
            "title": "Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations",
            "abstract": "We introduce a structured chain-of-thought (SCoT) prompting approach to generating content-grounded multi-turn question-answer conversations using a pre-trained large language model (LLM). At the core of our proposal is a structured breakdown of the complex task into a number of states in a state machine, so that actions corresponding to various subtasks, e.g., content reading and utterance generation, can be executed in their own dedicated states. Each state leverages a unique set of resources including prompts and (optionally) additional tools to augment the generation process. Our experimental results show that SCoT prompting with designated states for hallucination mitigation increases agent faithfulness to grounding documents by up to 16.8%. When used as training data, our open-domain conversations synthesized from only 6 Wikipedia-based seed demonstrations train strong conversational QA agents; in out-of-domain evaluation, for example, we observe improvements of up to 13.9% over target domain gold data when the latter is augmented with our generated examples.",
            "link": "https://www.semanticscholar.org/paper/41a6acb85168316c333daf529a678afd2e668855",
            "authors": "M. Sultan, Jatin Ganhotra, Ram\u00f3n Fernandez Astudillo",
            "matchScore": 305.79953,
            "original title": "Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations",
            "original authors": "Md Arafat Sultan, Jatin Ganhotra, Ram\u00f3n Fernandez Astudillo",
            "EMNLP Paper ID": "3110",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "d8a3d1088540065be6bc41365750055125039a40",
            "title": "Chain of Condition: Construct, Verify and Solve Conditions for Conditional Question Answering",
            "abstract": "Conditional question answering (CQA) is an important task that aims to find probable answers and identify missing conditions. Existing approaches struggle with CQA due to two challenges: (1) precisely identifying necessary conditions and the logical relationship, and (2) verifying conditions to detect any that are missing. In this paper, we propose a novel prompting approach, Chain of condition, by first identifying all conditions and constructing their logical relationships explicitly according to the document, then verifying whether these conditions are satisfied, finally solving the logical expression to indicate any missing conditions and generating the answer accordingly. Experiments on two CQA benchmark datasets show our chain of condition outperforms existing prompting baselines, establishing a new state of the art. Furthermore, with only a few examples, our method can facilitate GPT-3.5-Turbo or GPT-4 to outperform all existing supervised models.",
            "link": "https://www.semanticscholar.org/paper/d8a3d1088540065be6bc41365750055125039a40",
            "authors": "Jiuheng Lin, Yuxuan Lai, Yansong Feng",
            "matchScore": 255.01923,
            "original title": "Chain of Condition: Construct, Verify and Solve Conditions for Conditional Question Answering",
            "original authors": "Jiuheng Lin, Yuxuan Lai, Yansong Feng",
            "EMNLP Paper ID": "3190",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "913f65763476dfc440bda5d389c41a5227646816",
            "title": "Abstraction-of-Thought Makes Language Models Better Reasoners",
            "abstract": "Abstract reasoning, the ability to reason from the abstract essence of a problem, serves as a key to generalization in human reasoning. However, eliciting language models to perform reasoning with abstraction remains unexplored. This paper seeks to bridge this gap by introducing a novel structured reasoning format called Abstraction-of-Thought (AoT). The uniqueness of AoT lies in its explicit requirement for varying levels of abstraction within the reasoning process. This approach could elicit language models to first contemplate on the abstract level before incorporating concrete details, which is overlooked by the prevailing step-by-step Chain-of-Thought (CoT) method. To align models with the AoT format, we present AoT Collection, a generic finetuning dataset consisting of 348k high-quality samples with AoT reasoning processes, collected via an automated and scalable pipeline. We finetune a wide range of language models with AoT Collection and conduct extensive evaluations on 23 unseen tasks from the challenging benchmark Big-Bench Hard. Experimental results indicate that models aligned to AoT reasoning format substantially outperform those aligned to CoT in many reasoning tasks.",
            "link": "https://www.semanticscholar.org/paper/913f65763476dfc440bda5d389c41a5227646816",
            "authors": "Ruixin Hong, Hongming Zhang, Xiaoman Pan, Dong Yu, Changshui Zhang",
            "matchScore": 250.25058,
            "original title": "Abstraction-of-Thought Makes Language Models Better Reasoners",
            "original authors": "Ruixin Hong, Hongming Zhang, Xiaoman Pan, Dong Yu, Changshui Zhang",
            "EMNLP Paper ID": "395",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "545ef40143ad9e749571bf8607bb42fefd5f7cf5",
            "title": "CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level Granularity",
            "abstract": "State-of-the-art performance in QA tasks is currently achieved by systems employing Large Language Models (LLMs), however these models tend to hallucinate information in their responses. One approach focuses on enhancing the generation process by incorporating attribution from the given input to the output. However, the challenge of identifying appropriate attributions and verifying their accuracy against a source is a complex task that requires significant improvements in assessing such systems. We introduce an attribution-oriented Chain-of-Thought reasoning method to enhance the accuracy of attributions. This approach focuses the reasoning process on generating an attribution-centric output. Evaluations on two context-enhanced question-answering datasets using GPT-4 demonstrate improved accuracy and correctness of attributions. In addition, the combination of our method with finetuning enhances the response and attribution accuracy of two smaller LLMs, showing their potential to outperform GPT-4 in some cases.",
            "link": "https://www.semanticscholar.org/paper/545ef40143ad9e749571bf8607bb42fefd5f7cf5",
            "authors": "Moshe Berchansky, Daniel Fleischer, Moshe Wasserblat, Peter Izsak",
            "matchScore": 263.42657,
            "original title": "CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level Granularity",
            "original authors": "Moshe Berchansky, Daniel Fleischer, Moshe Wasserblat, Peter Izsak",
            "EMNLP Paper ID": "41",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "5e7d4bb5431bc91d0ffd1b4e1575d7227021eaf8",
            "title": "Divide-or-Conquer? Which Part Should You Distill Your LLM?",
            "abstract": "Recent methods have demonstrated that Large Language Models (LLMs) can solve reasoning tasks better when they are encouraged to solve subtasks of the main task first. In this paper we devise a similar strategy that breaks down reasoning tasks into a problem decomposition phase and a problem solving phase and show that the strategy is able to outperform a single stage solution. Further, we hypothesize that the decomposition should be easier to distill into a smaller model compared to the problem solving because the latter requires large amounts of domain knowledge while the former only requires learning general problem solving strategies. We propose methods to distill these two capabilities and evaluate their impact on reasoning outcomes and inference cost. We find that we can distill the problem decomposition phase and at the same time achieve good generalization across tasks, datasets, and models. However, it is harder to distill the problem solving capability without losing performance and the resulting distilled model struggles with generalization. These results indicate that by using smaller, distilled problem decomposition models in combination with problem solving LLMs we can achieve reasoning with cost-efficient inference and local adaptation.",
            "link": "https://www.semanticscholar.org/paper/5e7d4bb5431bc91d0ffd1b4e1575d7227021eaf8",
            "authors": "Zhuofeng Wu, Richard He Bai, Aonan Zhang, Jiatao Gu, V. Vydiswaran, N. Jaitly, Yizhe Zhang",
            "matchScore": 311.91663,
            "original title": "Divide-or-Conquer? Which Part Should You Distill Your LLM?",
            "original authors": "Zhuofeng Wu, Richard He Bai, Aonan Zhang, Jiatao Gu, V.G.Vinod Vydiswaran, Navdeep Jaitly, Yizhe Zhang",
            "EMNLP Paper ID": "536",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "cb4ce9d960c0c91aa72b23fdf4b4de27dc0bd1db",
            "title": "On the Empirical Complexity of Reasoning and Planning in LLMs",
            "abstract": "Chain-of-thought (CoT), tree-of-thought (ToT), and related techniques work surprisingly well in practice for some complex reasoning tasks with Large Language Models (LLMs), but why? This work seeks the underlying reasons by conducting experimental case studies and linking the performance benefits to well-established sample and computational complexity principles in machine learning. We experimented with 6 reasoning tasks, ranging from grade school math, air travel planning, ..., to Blocksworld. The results suggest that (i) both CoT and ToT benefit significantly from task decomposition, which breaks a complex reasoning task into a sequence of steps with low sample complexity and explicitly outlines the reasoning structure, and (ii) for computationally hard reasoning tasks, the more sophisticated tree structure of ToT outperforms the linear structure of CoT. These findings provide useful guidelines for the use of LLM in solving reasoning tasks in practice.",
            "link": "https://www.semanticscholar.org/paper/cb4ce9d960c0c91aa72b23fdf4b4de27dc0bd1db",
            "authors": "Liwei Kang, Zirui Zhao, David Hsu, Wee Sun Lee",
            "matchScore": 194.46684,
            "original title": "On the Empirical Complexity of Reasoning and Planning in LLMs",
            "original authors": "Liwei Kang, Zirui Zhao, David Hsu, Wee Sun Lee",
            "EMNLP Paper ID": "591",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "eb2b132b14ed1c2cc5b119390a8624cd28bcd74e",
            "title": "LaRS: Latent Reasoning Skills for Chain-of-Thought Reasoning",
            "abstract": "Chain-of-thought (CoT) prompting is a popular in-context learning (ICL) approach for large language models (LLMs), especially when tackling complex reasoning tasks. Traditional ICL approaches construct prompts using examples that contain questions similar to the input question. However, CoT prompting, which includes crucial intermediate reasoning steps (rationales) within its examples, necessitates selecting examples based on these rationales rather than the questions themselves. Existing methods require human experts or pre-trained LLMs to describe the skill, a high-level abstraction of rationales, to guide the selection. These methods, however, are often costly and difficult to scale. Instead, this paper introduces a new approach named Latent Reasoning Skills (LaRS) that employs unsupervised learning to create a latent space representation of rationales, with a latent variable called a reasoning skill. Concurrently, LaRS learns a reasoning policy to determine the required reasoning skill for a given question. Then the ICL examples are selected by aligning the reasoning skills between past examples and the question. This approach is theoretically grounded and compute-efficient, eliminating the need for auxiliary LLM inference or manual prompt design. Empirical results demonstrate that LaRS consistently outperforms SOTA skill-based selection methods, processing example banks four times faster, reducing LLM inferences during the selection stage by half, and showing greater robustness to sub-optimal example banks.",
            "link": "https://www.semanticscholar.org/paper/eb2b132b14ed1c2cc5b119390a8624cd28bcd74e",
            "authors": "Zifan Xu, Haozhu Wang, Dmitriy Bespalov, Xian Wu, Peter Stone, Yanjun Qi",
            "matchScore": 248.93015,
            "original title": "LaRS: Latent Reasoning Skills for Chain-of-Thought Reasoning",
            "original authors": "Zifan Xu, Haozhu Wang, Dmitriy Bespalov, Xian Wu, Peter Stone, Yanjun Qi",
            "EMNLP Paper ID": "733",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "d9fb3f98c870891bae33aa5161fd222fb21cf1a5",
            "title": "Deciphering the Factors Influencing the Efficacy of Chain-of-Thought: Probability, Memorization, and Noisy Reasoning",
            "abstract": "Chain-of-Thought (CoT) prompting has been shown to enhance the multi-step reasoning capabilities of Large Language Models (LLMs). However, debates persist about whether LLMs exhibit abstract generalization or rely on shallow heuristics when given CoT prompts. To understand the factors influencing CoT reasoning we provide a detailed case study of the symbolic reasoning task of decoding shift ciphers, where letters are shifted forward some number of steps in the alphabet. We analyze the pattern of results produced by three LLMs -- GPT-4, Claude 3, and Llama 3.1 -- performing this task using CoT prompting. By focusing on a single relatively simple task, we are able to identify three factors that systematically affect CoT performance: the probability of the task's expected output (probability), what the model has implicitly learned during pre-training (memorization), and the number of intermediate operations involved in reasoning (noisy reasoning). We show that these factors can drastically influence task accuracy across all three LLMs; e.g., when tested with GPT-4, varying the output's probability of occurrence shifts accuracy from 26% to 70%. Overall, we conclude that CoT prompting performance reflects both memorization and a probabilistic version of genuine reasoning. Code and data at this https://github.com/aksh555/deciphering_cot",
            "link": "https://www.semanticscholar.org/paper/d9fb3f98c870891bae33aa5161fd222fb21cf1a5",
            "authors": "Akshara Prabhakar, Thomas L. Griffiths, R. Thomas McCoy",
            "matchScore": 293.888,
            "original title": "Deciphering the Factors Influencing the Efficacy of Chain-of-Thought: Probability, Memorization, and Noisy Reasoning",
            "original authors": "Akshara Prabhakar, Thomas L. Griffiths, R. Thomas McCoy",
            "EMNLP Paper ID": "747",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "14ad24cc03b0e997f84a07547bb4c179acf896b6",
            "title": "Few shot chain-of-thought driven reasoning to prompt LLMs for open ended medical question answering",
            "abstract": "In this paper, we propose a modified version of the MedQA-USMLE dataset, named MEDQA-OPEN, which contains open-ended medical questions without options to mimic clinical scenarios, along with clinician-approved reasoned answers. Additionally, we implement a prompt driven by Chain of Thought (CoT) reasoning, CLINICR, to mirror the prospective process of incremental reasoning, reaching a correct response to medical questions. We empirically demonstrate how CLINICR outperforms the state-of-the-art 5-shot CoT-based prompt (Li\\'evin et al., 2022). We also present an approach that mirrors real-life clinical practice by first exploring multiple differential diagnoses through MCQ-CLINICR and subsequently narrowing down to a final diagnosis using MCQ-ELIMINATIVE. Finally, emphasizing the importance of response verification in medical settings, we utilize a reward model mechanism, replacing the elimination process performed by MCQ-ELIMINATIVE.",
            "link": "https://www.semanticscholar.org/paper/14ad24cc03b0e997f84a07547bb4c179acf896b6",
            "authors": "Ojas Gramopadhye, Saeel Sandeep Nachane, Prateek Chanda, Ganesh Ramakrishnan, K. Jadhav, Yatin Nandwani, Dinesh Raghu, Sachindra Joshi",
            "matchScore": 365.198,
            "original title": "Few shot chain-of-thought driven reasoning to prompt LLMs for open ended medical question answering",
            "original authors": "Saeel Sandeep Nachane, Ojas Gramopadhye, Prateek Chanda, Ganesh Ramakrishnan, Kshitij Sharad Jadhav, Yatin Nandwani, Dinesh Raghu, Sachindra Joshi",
            "EMNLP Paper ID": "96",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        }
    ],
    "In-Context Learning in Large Language Models": [
        {
            "paperId": "30c0cdc414f68211d5d0514df027cec22e005174",
            "title": "A Survey on In-context Learning",
            "abstract": "With the increasing capabilities of large language models (LLMs), in-context learning (ICL) has emerged as a new paradigm for natural language processing (NLP), where LLMs make predictions based on contexts augmented with a few examples. It has been a significant trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, prompt designing strategies, and related analysis. Additionally, we explore various ICL application scenarios, such as data engineering and knowledge updating. Finally, we address the challenges of ICL and suggest potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.",
            "link": "https://www.semanticscholar.org/paper/30c0cdc414f68211d5d0514df027cec22e005174",
            "authors": "Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Zhifang Sui",
            "EMNLP Paper ID": "137",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "a88353f9070c4e9ba63e410b351f193728eb21ef",
            "title": "Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process",
            "abstract": "In-context learning (ICL) is a few-shot learning paradigm that involves learning mappings through input-output pairs and appropriately applying them to new instances. Despite the remarkable ICL capabilities demonstrated by Large Language Models (LLMs), existing works are highly dependent on large-scale labeled support sets, not always feasible in practical scenarios. To refine this approach, we focus primarily on an innovative selective annotation mechanism, which precedes the standard demonstration retrieval. We introduce the Language Model-based Determinant Point Process (LM-DPP) that simultaneously considers the uncertainty and diversity of unlabeled instances for optimal selection. Consequently, this yields a subset for annotation that strikes a trade-off between the two factors. We apply LM-DPP to various language models, including GPT-J, LlaMA, and GPT-3. Experimental results on 9 NLU and 2 Generation datasets demonstrate that LM-DPP can effectively select canonical examples. Further analysis reveals that LLMs benefit most significantly from subsets that are both low uncertainty and high diversity.",
            "link": "https://www.semanticscholar.org/paper/a88353f9070c4e9ba63e410b351f193728eb21ef",
            "authors": "Peng Wang, Xiaobin Wang, Chao Lou, Shengyu Mao, Pengjun Xie, Yong Jiang",
            "EMNLP Paper ID": "151",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "602dbe8dcf866e17dbd52d8640d09cd03f5182bf",
            "title": "To Word Senses and Beyond: Inducing Concepts with Contextualized Language Models",
            "abstract": "Polysemy and synonymy are two crucial interrelated facets of lexical ambiguity. While both phenomena have been studied extensively in NLP, leading to dedicated systems, they are often been considered independently. While many tasks dealing with polysemy (e.g. Word Sense Disambiguiation or Induction) highlight the role of a word's senses, the study of synonymy is rooted in the study of concepts, i.e. meaning shared across the lexicon. In this paper, we introduce Concept Induction, the unsupervised task of learning a soft clustering among words that defines a set of concepts directly from data. This task generalizes that of Word Sense Induction. We propose a bi-level approach to Concept Induction that leverages both a local lemma-centric view and a global cross-lexicon perspective to induce concepts. We evaluate the obtained clustering on SemCor's annotated data and obtain good performances (BCubed F1 above 0.60). We find that the local and the global levels are mutually beneficial to induce concepts and also senses in our setting. Finally, we create static embeddings representing our induced concepts and use them on the Word-in-Context task, obtaining competitive performances with the State-of-the-Art.",
            "link": "https://www.semanticscholar.org/paper/602dbe8dcf866e17dbd52d8640d09cd03f5182bf",
            "authors": "Bastien Li\u00e9tard, Pascal Denis, Mikaella Keller",
            "EMNLP Paper ID": "299",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "64f97659d1348843956f24c5aa39e264aa891f28",
            "title": "Understanding Higher-Order Correlations Among Semantic Components in Embeddings",
            "abstract": "Independent Component Analysis (ICA) offers interpretable semantic components of embeddings. While ICA theory assumes that embeddings can be linearly decomposed into independent components, real-world data often do not satisfy this assumption. Consequently, non-independencies remain between the estimated components, which ICA cannot eliminate. We quantified these non-independencies using higher-order correlations and demonstrated that when the higher-order correlation between two components is large, it indicates a strong semantic association between them, along with many words sharing common meanings with both components. The entire structure of non-independencies was visualized using a maximum spanning tree of semantic components. These findings provide deeper insights into embeddings through ICA.",
            "link": "https://www.semanticscholar.org/paper/64f97659d1348843956f24c5aa39e264aa891f28",
            "authors": "Momose Oyama, Hiroaki Yamagiwa, H. Shimodaira",
            "EMNLP Paper ID": "324",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "46955794ab197d56b40595fcb8e74b6948097075",
            "title": "How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning",
            "abstract": "We investigate the mechanism of in-context learning (ICL) on sentence classification tasks with semantically-unrelated labels (\"foo\"/\"bar\"). We find intervening in only 1\\% heads (named\"in-context heads\") significantly affects ICL accuracy from 87.6\\% to 24.4\\%. To understand this phenomenon, we analyze the value-output vectors in these heads and discover that the vectors at each label position contain substantial information about the corresponding labels. Furthermore, we observe that the prediction shift from\"foo\"to\"bar\"is due to the respective reduction and increase in these heads' attention scores at\"foo\"and\"bar\"positions. Therefore, we propose a hypothesis for ICL: in in-context heads, the value-output matrices extract label features, while the query-key matrices compute the similarity between the features at the last position and those at each label position. The query and key matrices can be considered as two towers that learn the similarity metric between the last position's features and each demonstration at label positions. Using this hypothesis, we explain the majority label bias and recency bias in ICL and propose two methods to reduce these biases by 22\\% and 17\\%, respectively.",
            "link": "https://www.semanticscholar.org/paper/46955794ab197d56b40595fcb8e74b6948097075",
            "authors": "Zeping Yu, Sophia Ananiadou",
            "EMNLP Paper ID": "369",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "e1ff3202f8a8cb9d0813531ed3a7a487c6279ec8",
            "title": "Focused Large Language Models are Stable Many-Shot Learners",
            "abstract": "In-Context Learning (ICL) enables large language models (LLMs) to achieve rapid task adaptation by learning from demonstrations. With the increase in available context length of LLMs, recent experiments have shown that the performance of ICL does not necessarily scale well in many-shot (demonstration) settings. We theoretically and experimentally confirm that the reason lies in more demonstrations dispersing the model attention from the query, hindering its understanding of key content. Inspired by how humans learn from examples, we propose a training-free method FocusICL, which conducts triviality filtering to avoid attention being diverted by unimportant contents at token-level and operates hierarchical attention to further ensure sufficient attention towards current query at demonstration-level. We also design an efficient hyperparameter searching strategy for FocusICL based on model perplexity of demonstrations. Comprehensive experiments validate that FocusICL achieves an average performance improvement of 5.2% over vanilla ICL and scales well with many-shot demonstrations.",
            "link": "https://www.semanticscholar.org/paper/e1ff3202f8a8cb9d0813531ed3a7a487c6279ec8",
            "authors": "Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Yueqi Zhang, Chuyi Tan, Boyuan Pan, Heda Wang, Yao Hu, Kan Li",
            "EMNLP Paper ID": "703",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "73b30c8bd96c91915aa66263be4fa54da2be7d87",
            "title": "Learning to Retrieve Iteratively for In-Context Learning",
            "abstract": "We introduce iterative retrieval, a novel framework that empowers retrievers to make iterative decisions through policy optimization. Finding an optimal portfolio of retrieved items is a combinatorial optimization problem, generally considered NP-hard. This approach provides a learned approximation to such a solution, meeting specific task requirements under a given family of large language models (LLMs). We propose a training procedure based on reinforcement learning, incorporating feedback from LLMs. We instantiate an iterative retriever for composing in-context learning (ICL) exemplars and apply it to various semantic parsing tasks that demand synthesized programs as outputs. By adding only 4M additional parameters for state encoding, we convert an off-the-shelf dense retriever into a stateful iterative retriever, outperforming previous methods in selecting ICL exemplars on semantic parsing datasets such as CalFlow, TreeDST, and MTOP. Additionally, the trained iterative retriever generalizes across different inference LLMs beyond the one used during training.",
            "link": "https://www.semanticscholar.org/paper/73b30c8bd96c91915aa66263be4fa54da2be7d87",
            "authors": "Yunmo Chen, Tongfei Chen, Harsh Jhamtani, Patrick Xia, Richard Shin, Jason Eisner, Benjamin Van Durme",
            "EMNLP Paper ID": "800",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "843c88c660272d04a959aa4b93a4823ad73e512b",
            "title": "Strategic Demonstration Selection for Improved Fairness in LLM In-Context Learning",
            "abstract": "Recent studies highlight the effectiveness of using in-context learning (ICL) to steer large language models (LLMs) in processing tabular data, a challenging task given the structured nature of such data. Despite advancements in performance, the fairness implications of these methods are less understood. This study investigates how varying demonstrations within ICL prompts influence the fairness outcomes of LLMs. Our findings reveal that deliberately including minority group samples in prompts significantly boosts fairness without sacrificing predictive accuracy. Further experiments demonstrate that the proportion of minority to majority samples in demonstrations affects the trade-off between fairness and prediction accuracy. Based on these insights, we introduce a mitigation technique that employs clustering and evolutionary strategies to curate a diverse and representative sample set from the training data. This approach aims to enhance both predictive performance and fairness in ICL applications. Experimental results validate that our proposed method dramatically improves fairness across various metrics, showing its efficacy in real-world scenarios.",
            "link": "https://www.semanticscholar.org/paper/843c88c660272d04a959aa4b93a4823ad73e512b",
            "authors": "Jingyu Hu, Weiru Liu, Mengnan Du",
            "EMNLP Paper ID": "843",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "cd4f264a72bcdd6ab519fe7d0a407679d85278c7",
            "title": "Retrieved In-Context Principles from Previous Mistakes",
            "abstract": "In-context learning (ICL) has been instrumental in adapting Large Language Models (LLMs) to downstream tasks using correct input-output examples. Recent advances have attempted to improve model performance through principles derived from mistakes, yet these approaches suffer from lack of customization and inadequate error coverage. To address these limitations, we propose Retrieved In-Context Principles (RICP), a novel teacher-student framework. In RICP, the teacher model analyzes mistakes from the student model to generate reasons and insights for preventing similar mistakes. These mistakes are clustered based on their underlying reasons for developing task-level principles, enhancing the error coverage of principles. During inference, the most relevant mistakes for each question are retrieved to create question-level principles, improving the customization of the provided guidance. RICP is orthogonal to existing prompting methods and does not require intervention from the teacher model during inference. Experimental results across seven reasoning benchmarks reveal that RICP effectively enhances performance when applied to various prompting strategies.",
            "link": "https://www.semanticscholar.org/paper/cd4f264a72bcdd6ab519fe7d0a407679d85278c7",
            "authors": "Hao Sun, Yong Jiang, Bo Wang, Yingyan Hou, Yan Zhang, Pengjun Xie, Fei Huang",
            "EMNLP Paper ID": "939",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "7533a8aecce8c31b740c81471df7f5a4332c32d3",
            "title": "SCOI: Syntax-augmented Coverage-based In-context Example Selection for Machine Translation",
            "abstract": "In-context learning (ICL) greatly improves the performance of large language models (LLMs) on various down-stream tasks, where the improvement highly depends on the quality of demonstrations. In this work, we introduce syntactic knowledge to select better in-context examples for machine translation (MT). We propose a new strategy, namely Syntax-augmented COverage-based In-context example selection (SCOI), leveraging the deep syntactic structure beyond conventional word matching. Specifically, we measure the set-level syntactic coverage by computing the coverage of polynomial terms with the help of a simplified tree-to-polynomial algorithm, and lexical coverage using word overlap. Furthermore, we devise an alternate selection approach to combine both coverage measures, taking advantage of syntactic and lexical information. We conduct experiments with two multi-lingual LLMs on six translation directions. Empirical results show that our proposed SCOI obtains the highest average COMET score among all learning-free methods, indicating that combining syntactic and lexical coverage successfully helps to select better in-context examples for MT. Our code is available at https://github.com/JamyDon/SCOI.",
            "link": "https://www.semanticscholar.org/paper/7533a8aecce8c31b740c81471df7f5a4332c32d3",
            "authors": "Chenming Tang, Zhixiang Wang, Yunfang Wu",
            "EMNLP Paper ID": "1114",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "8d722b52383a3a1209a17643219bc5471ae05c19",
            "title": "When Parts are Greater Than Sums: Individual LLM Components Can Outperform Full Models",
            "abstract": "This paper studies in-context learning by decomposing the output of large language models into the individual contributions of attention heads and MLPs (components). We observe curious components: good-performing ones that individually do well on a classification task, even when the model performs poorly; bad-performing ones that do much worse than chance; and label-biased components that always predict the same label. We find that component accuracies are well-correlated across different demonstration sets and perturbations of prompt templates. Based on our findings, we propose component reweighting, which learns to linearly re-scale the component activations from a few labeled examples. Given 24 labeled examples, our method improves by an average of 6.0% accuracy points over 24-shot ICL across 8 tasks on Llama-2-7B. Overall, this paper both enriches our understanding of ICL and provides a practical method for improvement by examining model internals.",
            "link": "https://www.semanticscholar.org/paper/8d722b52383a3a1209a17643219bc5471ae05c19",
            "authors": "Ting-Yun Chang, Jesse Thomason, Robin Jia",
            "EMNLP Paper ID": "1162",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "8b4faf263cebc2a05e0dd3efaf21b24572c615b2",
            "title": "How to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective",
            "abstract": "This paper introduces a novel generalized self-imitation learning ($\\textbf{GSIL}$) framework, which effectively and efficiently aligns large language models with offline demonstration data. We develop $\\textbf{GSIL}$ by deriving a surrogate objective of imitation learning with density ratio estimates, facilitating the use of self-generated data and optimizing the imitation learning objective with simple classification losses. $\\textbf{GSIL}$ eliminates the need for complex adversarial training in standard imitation learning, achieving lightweight and efficient fine-tuning for large language models. In addition, $\\textbf{GSIL}$ encompasses a family of offline losses parameterized by a general class of convex functions for density ratio estimation and enables a unified view for alignment with demonstration data. Extensive experiments show that $\\textbf{GSIL}$ consistently and significantly outperforms baselines in many challenging benchmarks, such as coding (HuamnEval), mathematical reasoning (GSM8K) and instruction-following benchmark (MT-Bench).",
            "link": "https://www.semanticscholar.org/paper/8b4faf263cebc2a05e0dd3efaf21b24572c615b2",
            "authors": "Teng Xiao, Mingxiao Li, Yige Yuan, Huaisheng Zhu, Chao Cui, V.G. Honavar",
            "EMNLP Paper ID": "1553",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "ae16932164b3be704671f25af7989f2346a689a5",
            "title": "The Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis",
            "abstract": "Understanding in-context learning (ICL) capability that enables large language models (LLMs) to excel in proficiency through demonstration examples is of utmost importance. This importance stems not only from the better utilization of this capability across various tasks, but also from the proactive identification and mitigation of potential risks, including concerns regarding truthfulness, bias, and toxicity, that may arise alongside the capability. In this paper, we present a thorough survey on the interpretation and analysis of in-context learning. First, we provide a concise introduction to the background and definition of in-context learning. Then, we give an overview of advancements from two perspectives: 1) a theoretical perspective, emphasizing studies on mechanistic interpretability and delving into the mathematical foundations behind ICL; and 2) an empirical perspective, concerning studies that empirically analyze factors associated with ICL. We conclude by highlighting the challenges encountered and suggesting potential avenues for future research. We believe that our work establishes the basis for further exploration into the interpretation of in-context learning. Additionally, we have created a repository containing the resources referenced in our survey.",
            "link": "https://www.semanticscholar.org/paper/ae16932164b3be704671f25af7989f2346a689a5",
            "authors": "Yuxiang Zhou, Jiazheng Li, Yanzheng Xiang, Hanqi Yan, Lin Gui, Yulan He",
            "EMNLP Paper ID": "1659",
            "Oral/Poster": "Oral",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "923e13b4de6816d75cd48b63f941eb02289502d1",
            "title": "Exploring the Learning Capabilities of Language Models using LEVERWORLDS",
            "abstract": "Learning a model of a stochastic setting often involves learning both general structure rules and specific properties of the instance. This paper investigates the interplay between learning the general and the specific in various learning methods, with emphasis on sample efficiency. We design a framework called {\\sc LeverWorlds}, which allows the generation of simple physics-inspired worlds that follow a similar generative process with different distributions, and their instances can be expressed in natural language. These worlds allow for controlled experiments to assess the sample complexity of different learning methods. We experiment with classic learning algorithms as well as Transformer language models, both with fine-tuning and In-Context Learning (ICL). Our general finding is that (1) Transformers generally succeed in the task; but (2) they are considerably less sample efficient than classic methods that make stronger assumptions about the structure, such as Maximum Likelihood Estimation and Logistic Regression. This finding is in tension with the recent tendency to use Transformers as general-purpose estimators. We propose an approach that leverages the ICL capabilities of contemporary language models to apply simple algorithms for this type of data. Our experiments show that models currently struggle with the task but show promising potential.",
            "link": "https://www.semanticscholar.org/paper/923e13b4de6816d75cd48b63f941eb02289502d1",
            "authors": "Eitan Wagner, Amir Feder, Omri Abend",
            "EMNLP Paper ID": "1807",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "64e55b23dee00019fb293350c6c95444c425515a",
            "title": "Are Large Language Models In-Context Personalized Summarizers? Get an iCOPERNICUS Test Done!",
            "abstract": "Large Language Models (LLMs) have succeeded considerably in In-Context-Learning (ICL) based summarization. However, saliency is subject to the users' specific preference histories. Hence, we need reliable In-Context Personalization Learning (ICPL) capabilities within such LLMs. For any arbitrary LLM to exhibit ICPL, it needs to have the ability to discern contrast in user profiles. A recent study proposed a measure for degree-of-personalization called EGISES for the first time. EGISES measures a model's responsiveness to user profile differences. However, it cannot test if a model utilizes all three types of cues provided in ICPL prompts: (i) example summaries, (ii) user's reading histories, and (iii) contrast in user profiles. To address this, we propose the iCOPERNICUS framework, a novel In-COntext PERsonalization learNIng sCrUtiny of Summarization capability in LLMs that uses EGISES as a comparative measure. As a case-study, we evaluate 17 state-of-the-art LLMs based on their reported ICL performances and observe that 15 models' ICPL degrades (min: 1.6%; max: 3.6%) when probed with richer prompts, thereby showing lack of true ICPL.",
            "link": "https://www.semanticscholar.org/paper/64e55b23dee00019fb293350c6c95444c425515a",
            "authors": "Divya Patel, Pathik Patel, Ankush Chander, Sourish Dasgupta, Tanmoy Chakraborty",
            "EMNLP Paper ID": "1982",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "5f6f04e96ea1946fb52b8196e0ad16afe25c2c70",
            "title": "Exploring Intra and Inter-language Consistency in Embeddings with ICA",
            "abstract": "Word embeddings represent words as multidimensional real vectors, facilitating data analysis and processing, but are often challenging to interpret. Independent Component Analysis (ICA) creates clearer semantic axes by identifying independent key features. Previous research has shown ICA's potential to reveal universal semantic axes across languages. However, it lacked verification of the consistency of independent components within and across languages. We investigated the consistency of semantic axes in two ways: both within a single language and across multiple languages. We first probed into intra-language consistency, focusing on the reproducibility of axes by performing ICA multiple times and clustering the outcomes. Then, we statistically examined inter-language consistency by verifying those axes' correspondences using statistical tests. We newly applied statistical methods to establish a robust framework that ensures the reliability and universality of semantic axes.",
            "link": "https://www.semanticscholar.org/paper/5f6f04e96ea1946fb52b8196e0ad16afe25c2c70",
            "authors": "Rongzhi Li, Takeru Matsuda, Hitomi Yanaka",
            "EMNLP Paper ID": "2415",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "729c6421c049d47350e04e32256a14d6fb74f9f1",
            "title": "Bayesian Example Selection Improves In-Context Learning for Speech, Text, and Visual Modalities",
            "abstract": "Large language models (LLMs) can adapt to new tasks through in-context learning (ICL) based on a few examples presented in dialogue history without any model parameter update. Despite such convenience, the performance of ICL heavily depends on the quality of the in-context examples presented, which makes the in-context example selection approach a critical choice. This paper proposes a novel Bayesian in-Context example Selection method (ByCS) for ICL. Extending the inference probability conditioned on in-context examples based on Bayes' theorem, ByCS focuses on the inverse inference conditioned on test input. Following the assumption that accurate inverse inference probability (likelihood) will result in accurate inference probability (posterior), in-context examples are selected based on their inverse inference results. Diverse and extensive cross-tasking and cross-modality experiments are performed with speech, text, and image examples. Experimental results show the efficacy and robustness of our ByCS method on various models, tasks and modalities.",
            "link": "https://www.semanticscholar.org/paper/729c6421c049d47350e04e32256a14d6fb74f9f1",
            "authors": "Siyin Wang, Chao-Han Huck Yang, Ji Wu, Chao Zhang",
            "EMNLP Paper ID": "2762",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "b217b6bc340af9a10bebbf8acc36ea30871769bd",
            "title": "In-Context Learning with Iterative Demonstration Selection",
            "abstract": "Spurred by advancements in scale, large language models (LLMs) have demonstrated strong few-shot learning ability via in-context learning (ICL). However, the performance of ICL has been shown to be highly sensitive to the selection of few-shot demonstrations. Selecting the most suitable examples as context remains an ongoing challenge and an open problem. Existing literature has highlighted the importance of selecting examples that are diverse or semantically similar to the test sample while ignoring the fact that the optimal selection dimension, i.e., diversity or similarity, is task-specific. Based on how the test sample is answered, we propose Iterative Demonstration Selection (IDS) to leverage the merits of both dimensions. Using zero-shot chain-of-thought reasoning (Zero-shot-CoT), IDS iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations. Specifically, IDS applies Zero-shot-CoT to the test sample before demonstration selection. The output reasoning path is then used to choose demonstrations that are prepended to the test sample for inference. The generated answer is followed by its corresponding reasoning path for extracting a new set of demonstrations in the next iteration. After several iterations, IDS adopts majority voting to obtain the final result. Through extensive experiments on tasks including reasoning, question answering, and topic classification, we demonstrate that IDS can consistently outperform existing ICL demonstration selection methods.",
            "link": "https://www.semanticscholar.org/paper/b217b6bc340af9a10bebbf8acc36ea30871769bd",
            "authors": "Chengwei Qin, Aston Zhang, Anirudh Dagar, Wenming Ye",
            "matchScore": 184.39767,
            "original title": "In-Context Learning with Iterative Demonstration Selection",
            "original authors": "Chengwei Qin, Aston Zhang, Chen Chen, Anirudh Dagar, Wenming Ye",
            "EMNLP Paper ID": "1533",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "838d6098bee69f78d17d6a66d9ea5befced82c43",
            "title": "Pseudo-Label Enhanced Prototypical Contrastive Learning for Uniformed Intent Discovery",
            "abstract": "New intent discovery is a crucial capability for task-oriented dialogue systems. Existing methods focus on transferring in-domain (IND) prior knowledge to out-of-domain (OOD) data through pre-training and clustering stages. They either handle the two processes in a pipeline manner, which exhibits a gap between intent representation and clustering process or use typical contrastive clustering that overlooks the potential supervised signals from the whole data. Besides, they often individually deal with open intent discovery or OOD settings. To this end, we propose a Pseudo-Label enhanced Prototypical Contrastive Learning (PLPCL) model for uniformed intent discovery. We iteratively utilize pseudo-labels to explore potential positive/negative samples for contrastive learning and bridge the gap between representation and clustering. To enable better knowledge transfer, we design a prototype learning method integrating the supervised and pseudo signals from IND and OOD samples. In addition, our method has been proven effective in two different settings of discovering new intents. Experiments on three benchmark datasets and two task settings demonstrate the effectiveness of our approach.",
            "link": "https://www.semanticscholar.org/paper/838d6098bee69f78d17d6a66d9ea5befced82c43",
            "authors": "Yimin Deng, Yuxia Wu, Guoshuai Zhao, Li Zhu, Xueming Qian",
            "matchScore": 293.67596,
            "original title": "Pseudo-Label Enhanced Prototypical Contrastive Learning for Uniformed Intent Discovery",
            "original authors": "Yimin Deng, Yuxia Wu, Li Zhu, Guoshuai Zhao, Xueming Qian",
            "EMNLP Paper ID": "1570",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "853513bf50bc9e8c77a0141776d15710a4673fe5",
            "title": "How Far Can In-Context Alignment Go? Exploring the State of In-Context Alignment",
            "abstract": "Recent studies have demonstrated that In-Context Learning (ICL), through the use of specific demonstrations, can align Large Language Models (LLMs) with human preferences known as In-Context Alignment (ICA), indicating that models can comprehend human instructions without requiring parameter adjustments. However, the exploration of the mechanism and applicability of ICA remains limited. In this paper, we begin by dividing the context text used in ICA into three categories: format, system prompt, and example. Through ablation experiments, we investigate the effectiveness of each part in enabling ICA to function effectively. We then examine how variants in these parts impact the model's alignment performance. Our findings indicate that the example part is crucial for enhancing the model's alignment capabilities, with changes in examples significantly affecting alignment performance. We also conduct a comprehensive evaluation of ICA's zero-shot capabilities in various alignment tasks. The results indicate that compared to parameter fine-tuning methods, ICA demonstrates superior performance in knowledge-based tasks and tool-use tasks. However, it still exhibits certain limitations in areas such as multi-turn dialogues and instruction following.",
            "link": "https://www.semanticscholar.org/paper/853513bf50bc9e8c77a0141776d15710a4673fe5",
            "authors": "Heyan Huang, Yinghao Li, Huashan Sun, Yu Bai, Yang Gao",
            "matchScore": 316.1253,
            "original title": "How Far Can In-Context Alignment Go? Exploring the State of In-Context Alignment",
            "original authors": "Heyan Huang, Yinghao Li, Huashan Sun, Yu Bai, Yang Gao",
            "EMNLP Paper ID": "1808",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "522240c89c7ac6b1365e8308c6a88c4784adc62e",
            "title": "Improving Diversity of Commonsense Generation by Large Language Models via In-Context Learning",
            "abstract": "Generative Commonsense Reasoning (GCR) requires a model to reason about a situation using commonsense knowledge, while generating coherent sentences. Although the quality of the generated sentences is crucial, the diversity of the generation is equally important because it reflects the model's ability to use a range of commonsense knowledge facts. Large Language Models (LLMs) have shown proficiency in enhancing the generation quality across various tasks through in-context learning (ICL) using given examples without the need for any fine-tuning. However, the diversity aspect in LLM outputs has not been systematically studied before. To address this, we propose a simple method that diversifies the LLM generations, while preserving their quality. Experimental results on three benchmark GCR datasets show that our method achieves an ideal balance between the quality and diversity. Moreover, the sentences generated by our proposed method can be used as training data to improve diversity in existing commonsense generators.",
            "link": "https://www.semanticscholar.org/paper/522240c89c7ac6b1365e8308c6a88c4784adc62e",
            "authors": "Tianhui Zhang, Bei Peng, D. Bollegala",
            "matchScore": 248.21306,
            "original title": "Improving Diversity of Commonsense Generation by Large Language Models via In-Context Learning",
            "original authors": "Tianhui Zhang, Bei Peng, Danushka Bollegala",
            "EMNLP Paper ID": "1928",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "28e9fb24bfb25f6e40d3897a5cf9c39f6f6b8d1f",
            "title": "C-ICL: Contrastive In-context Learning for Information Extraction",
            "abstract": "There has been increasing interest in exploring the capabilities of advanced large language models (LLMs) in the field of information extraction (IE), specifically focusing on tasks related to named entity recognition (NER) and relation extraction (RE). Although researchers are exploring the use of few-shot information extraction through in-context learning with LLMs, they tend to focus only on using correct or positive examples for demonstration, neglecting the potential value of incorporating incorrect or negative examples into the learning process. In this paper, we present c-ICL, a novel few-shot technique that leverages both correct and incorrect sample constructions to create in-context learning demonstrations. This approach enhances the ability of LLMs to extract entities and relations by utilizing prompts that incorporate not only the positive samples but also the reasoning behind them. This method allows for the identification and correction of potential interface errors. Specifically, our proposed method taps into the inherent contextual information and valuable information in hard negative samples and the nearest positive neighbors to the test and then applies the in-context learning demonstrations based on LLMs. Our experiments on various datasets indicate that c-ICL outperforms previous few-shot in-context learning methods, delivering substantial enhancements in performance across a broad spectrum of related tasks. These improvements are noteworthy, showcasing the versatility of our approach in miscellaneous scenarios.",
            "link": "https://www.semanticscholar.org/paper/28e9fb24bfb25f6e40d3897a5cf9c39f6f6b8d1f",
            "authors": "Ying Mo, Jian Yang, Jiahao Liu, Shun Zhang, Jingang Wang, Zhoujun Li",
            "matchScore": 217.57251,
            "original title": "C-ICL: Contrastive In-context Learning for Information Extraction",
            "original authors": "Ying Mo, Jiahao Liu, Jian Yang, Qifan Wang, Shun Zhang, Jingang Wang, Zhoujun Li",
            "EMNLP Paper ID": "2070",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "88277da950eb18a3ae5e7718ddb8dcaf7ebe8d8c",
            "title": "Analyzing Context Contributions in LLM-based Machine Translation",
            "abstract": "Large language models (LLMs) have achieved state-of-the-art performance in machine translation (MT) and demonstrated the ability to leverage in-context learning through few-shot examples. However, the mechanisms by which LLMs use different parts of the input context remain largely unexplored. In this work, we provide a comprehensive analysis of context utilization in MT, studying how LLMs use various context parts, such as few-shot examples and the source text, when generating translations. We highlight several key findings: (1) the source part of few-shot examples appears to contribute more than its corresponding targets, irrespective of translation direction; (2) finetuning LLMs with parallel data alters the contribution patterns of different context parts; and (3) there is a positional bias where earlier few-shot examples have higher contributions to the translated sequence. Finally, we demonstrate that inspecting anomalous context contributions can potentially uncover pathological translations, such as hallucinations. Our findings shed light on the internal workings of LLM-based MT which go beyond those known for standard encoder-decoder MT models.",
            "link": "https://www.semanticscholar.org/paper/88277da950eb18a3ae5e7718ddb8dcaf7ebe8d8c",
            "authors": "Emmanouil Zaranis, Nuno M. Guerreiro, Andr'e F. T. Martins",
            "matchScore": 234.94727,
            "original title": "Analyzing Context Contributions in LLM-based Machine Translation",
            "original authors": "Emmanouil Zaranis, Nuno M Guerreiro, Andre Martins",
            "EMNLP Paper ID": "2866",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "5e2f1c720ede6f78c897416b7a6ac056088d4ab4",
            "title": "Exploring the Relationship between In-Context Learning and Instruction Tuning",
            "abstract": "In-Context Learning (ICL) and Instruction Tuning (IT) are two primary paradigms of adopting Large Language Models (LLMs) to downstream applications. However, they are significantly different. In ICL, a set of demonstrations are provided at inference time but the LLM's parameters are not updated. In IT, a set of demonstrations are used to tune LLM's parameters in training time but no demonstrations are used at inference time. Although a growing body of literature has explored ICL and IT, studies on these topics have largely been conducted in isolation, leading to a disconnect between these two paradigms. In this work, we explore the relationship between ICL and IT by examining how the hidden states of LLMs change in these two paradigms. Through carefully designed experiments conducted with LLaMA-2 (7B and 13B), we find that ICL is implicit IT. In other words, ICL changes an LLM's hidden states as if the demonstrations were used to instructionally tune the model. Furthermore, the convergence between ICL and IT is largely contingent upon several factors related to the provided demonstrations. Overall, this work offers a unique perspective to explore the connection between ICL and IT and sheds light on understanding the behaviors of LLM.",
            "link": "https://www.semanticscholar.org/paper/5e2f1c720ede6f78c897416b7a6ac056088d4ab4",
            "authors": "Hanyu Duan, Yixuan Tang, Yi Yang, Ahmed Abbasi, K. Tam",
            "matchScore": 191.38785,
            "original title": "Exploring the Relationship between In-Context Learning and Instruction Tuning",
            "original authors": "Hanyu Duan, Yixuan Tang, Yi Yang, Ahmed Abbasi, KAR YAN TAM",
            "EMNLP Paper ID": "658",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "76221312f1cabc5d026f1cdd665e4b7aeb03ea1c",
            "title": "Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning",
            "abstract": "Fine-tuning and in-context learning (ICL) are two prevalent methods in imbuing large language models with task-specific knowledge. It is commonly believed that fine-tuning can surpass ICL given sufficient training samples as it allows the model to adjust its internal parameters based on the data. However, this paper presents a counterintuitive finding: For tasks with implicit patterns, ICL captures these patterns significantly better than fine-tuning. We developed several datasets featuring implicit patterns, such as sequences determining answers through parity or identifying reducible terms in calculations. We then evaluated the models' understanding of these patterns under both fine-tuning and ICL across models ranging from 0.5B to 7B parameters. The results indicate that models employing ICL can quickly grasp deep patterns and significantly improve accuracy. In contrast, fine-tuning, despite utilizing thousands of times more training samples than ICL, achieved only limited improvements. We also proposed circuit shift theory from a mechanistic interpretability's view to explain why ICL wins.",
            "link": "https://www.semanticscholar.org/paper/76221312f1cabc5d026f1cdd665e4b7aeb03ea1c",
            "authors": "Qingyu Yin, Xuzheng He, Luoao Deng, Chak Tou Leong, Fan Wang, Yanzhao Yan, Xiaoyu Shen, Qiang Zhang",
            "matchScore": 257.9798,
            "original title": "Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning",
            "original authors": "Qingyu Yin, Xuzheng He, Chak Tou Leong, Fan Wang, Yanzhao Yan, Xiaoyu Shen, Qiang Zhang",
            "EMNLP Paper ID": "821",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "ed91e349a4273d6a2ae0131069ba78c5c9995eea",
            "title": "Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed Embeddings",
            "abstract": "Word embedding is one of the most important components in natural language processing, but interpreting high-dimensional embeddings remains a challenging problem. To address this problem, Independent Component Analysis (ICA) is identified as an effective solution. ICA-transformed word embeddings reveal interpretable semantic axes; however, the order of these axes are arbitrary. In this study, we focus on this property and propose a novel method, Axis Tour, which optimizes the order of the axes. Inspired by Word Tour, a one-dimensional word embedding method, we aim to improve the clarity of the word embedding space by maximizing the semantic continuity of the axes. Furthermore, we show through experiments on downstream tasks that Axis Tour yields better or comparable low-dimensional embeddings compared to both PCA and ICA.",
            "link": "https://www.semanticscholar.org/paper/ed91e349a4273d6a2ae0131069ba78c5c9995eea",
            "authors": "Hiroaki Yamagiwa, Yusuke Takase, Hidetoshi Shimodaira",
            "matchScore": 318.82812,
            "original title": "Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed Embeddings",
            "original authors": "Hiroaki Yamagiwa, Yusuke Takase, Hidetoshi Shimodaira",
            "EMNLP Paper ID": "86",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        }
    ],
    "Enhancing Knowledge Graph Question Answering with Large Language Models": [
        {
            "paperId": "04847af01a0ba0beadb98d36a3059b7cac701f1a",
            "title": "Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering",
            "abstract": "Recent progress with LLM-based agents has shown promising results across various tasks. However, their use in answering questions from knowledge bases remains largely unexplored. Implementing a KBQA system using traditional methods is challenging due to the shortage of task-specific training data and the complexity of creating task-focused model structures. In this paper, we present Triad, a unified framework that utilizes an LLM-based agent with three roles for KBQA tasks. The agent is assigned three roles to tackle different KBQA subtasks: agent as a generalist for mastering various subtasks, as a decision maker for the selection of candidates, and as an advisor for answering questions with knowledge. Our KBQA framework is executed in four phases, involving the collaboration of the agent's multiple roles. We evaluated the performance of our framework using three benchmark datasets, and the results show that our framework outperforms state-of-the-art systems on the LC-QuAD and YAGO-QA benchmarks, yielding F1 scores of 11.8% and 20.7%, respectively.",
            "link": "https://www.semanticscholar.org/paper/04847af01a0ba0beadb98d36a3059b7cac701f1a",
            "authors": "Chang Zong, Yuchen Yan, Weiming Lu, Eliot Huang, Jian Shao, Y. Zhuang",
            "EMNLP Paper ID": "196",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 12): 17:45\u201318:45 (Evening)",
            "Type": "Main"
        },
        {
            "paperId": "db62b76906ab2663c79a149aab58b2b08473ab16",
            "title": "KB-Plugin: A Plug-and-play Framework for Large Language Models to Induce Programs over Low-resourced Knowledge Bases",
            "abstract": "Program induction (PI) has become a promising paradigm for using knowledge bases (KBs) to help large language models (LLMs) answer complex knowledge-intensive questions. Nonetheless, PI typically relies on a large number of parallel question-program pairs to make the LLM aware of the schema of the given KB, and is thus challenging for many low-resourced KBs that lack annotated data. To this end, we propose KB-Plugin, a plug-and-play framework that enables LLMs to induce programs over any low-resourced KB. Firstly, KB-Plugin adopts self-supervised learning to encode the detailed schema information of a given KB into a pluggable module, namely schema plugin. Secondly, KB-Plugin utilizes abundant annotated data from a rich-resourced KB to train another pluggable module, namely PI plugin, which can help the LLM extract question-relevant schema information from the schema plugin of any KB and utilize this information to induce programs over this KB. Experiments on five heterogeneous KBQA datasets show that KB-Plugin achieves better or comparable performance with 25$\\times$ smaller backbone LLM compared to SoTA PI methods for low-resourced KBs, and even approaches the performance of supervised methods. Our code and data are available at https://github.com/THU-KEG/KB-Plugin.",
            "link": "https://www.semanticscholar.org/paper/db62b76906ab2663c79a149aab58b2b08473ab16",
            "authors": "Jiajie Zhang, S. Cao, Linmei Hu, Ling Feng, Lei Hou, Juanzi Li",
            "EMNLP Paper ID": "321",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "f355bef0df69eaa9c51a6805757eb2c55ae7281b",
            "title": "CoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex Knowledge Graph Question Answering",
            "abstract": "Recent studies have explored the use of Large Language Models (LLMs) with Retrieval Augmented Generation (RAG) for Knowledge Graph Question Answering (KGQA). They typically require rewriting retrieved subgraphs into natural language formats comprehensible to LLMs. However, when tackling complex questions, the knowledge rewritten by existing methods may include irrelevant information, omit crucial details, or fail to align with the question's semantics. To address them, we propose a novel rewriting method CoTKR, Chain-of-Thought Enhanced Knowledge Rewriting, for generating reasoning traces and corresponding knowledge in an interleaved manner, thereby mitigating the limitations of single-step knowledge rewriting. Additionally, to bridge the preference gap between the knowledge rewriter and the question answering (QA) model, we propose a training strategy PAQAF, Preference Alignment from Question Answering Feedback, for leveraging feedback from the QA model to further optimize the knowledge rewriter. We conduct experiments using various LLMs across several KGQA benchmarks. Experimental results demonstrate that, compared with previous knowledge rewriting methods, CoTKR generates the most beneficial knowledge representation for QA models, which significantly improves the performance of LLMs in KGQA.",
            "link": "https://www.semanticscholar.org/paper/f355bef0df69eaa9c51a6805757eb2c55ae7281b",
            "authors": "Yike Wu, Yi Huang, Nan Hu, Yuncheng Hua, Guilin Qi, Jiaoyan Chen, Jeff Z. Pan",
            "EMNLP Paper ID": "398",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "d9cbe6cdf9bc397d53f196b5846966960d4336ad",
            "title": "Multi-Granularity History and Entity Similarity Learning for Temporal Knowledge Graph Reasoning",
            "abstract": "Temporal Knowledge Graph (TKG) reason-001 ing, aiming to predict future unknown facts 002 based on historical information, has attracted 003 considerable attention due to its great practi-004 cal value. Insight into history is the key to 005 predict the future. However, most existing 006 TKG reasoning models singly capture repet-007 itive history, ignoring the entity\u2019s multi-hop 008 neighbour history which can provide valuable 009 background knowledge for TKG reasoning. In 010 this paper, we propose M ulti-G ranularity His-011 tory and E ntity S imilarity L earning (MGESL) 012 model for Temporal Knowledge Graph Reason-013 ing, which models historical information from 014 both coarse-grained and fine-grained history. 015 Since similar entities tend to exhibit similar 016 behavioural patterns, we also design a hyper-017 graph convolution aggregator to capture the 018 similarity between entities. Furthermore, we 019 introduce a more realistic setting for the TKG 020 reasoning, where candidate entities are already 021 known at the timestamp to be predicted. Exten-022 sive experiments on three benchmark datasets 023 demonstrate the effectiveness of our proposed 024 model. 025",
            "link": "https://www.semanticscholar.org/paper/d9cbe6cdf9bc397d53f196b5846966960d4336ad",
            "authors": "G. G, J. Devlin, Ming-Wei Chang, Kenton Lee, Pengfei Wang, Yuanchun Zhou, Yanjie Fu",
            "EMNLP Paper ID": "574",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 12): 17:45\u201318:45 (Evening)",
            "Type": "Main"
        },
        {
            "paperId": "59395cf4f9346ef4ccb37499a3a7e52c2978fc61",
            "title": "Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering",
            "abstract": "Knowledge Graph Question Answering (KGQA) methods seek to answer Natural Language questions using the relational information stored in Knowledge Graphs (KGs). With the recent advancements of Large Language Models (LLMs) and their remarkable reasoning abilities, there is a growing trend to leverage them for KGQA. However, existing methodologies have only focused on answering factual questions, e.g.,\"In which city was Silvio Berlusconi's first wife born?\", leaving questions involving commonsense reasoning that real-world users may pose more often, e.g.,\"Do I need separate visas to see the Venus of Willendorf and attend the Olympics this summer?\"unaddressed. In this work, we first observe that existing LLM-based methods for KGQA struggle with hallucination on such questions, especially on queries targeting long-tail entities (e.g., non-mainstream and recent entities), thus hindering their applicability in real-world applications especially since their reasoning processes are not easily verifiable. In response, we propose Right for Right Reasons (R3), a commonsense KGQA methodology that allows for a verifiable reasoning procedure by axiomatically surfacing intrinsic commonsense knowledge of LLMs and grounding every factual reasoning step on KG triples. Through experimental evaluations across three different tasks--question answering, claim verification, and preference matching--our findings showcase R3 as a superior approach, outperforming existing methodologies and notably reducing instances of hallucination and reasoning errors.",
            "link": "https://www.semanticscholar.org/paper/59395cf4f9346ef4ccb37499a3a7e52c2978fc61",
            "authors": "Armin Toroghi, Willis Guo, Mohammad Mahdi Torabi pour, Scott Sanner",
            "EMNLP Paper ID": "741",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "db96e019410006c3ee0ae0184800ab206f8704dd",
            "title": "Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction",
            "abstract": "In this work, we are interested in automated methods for knowledge graph creation (KGC) from input text. Progress on large language models (LLMs) has prompted a series of recent works applying them to KGC, e.g., via zero/few-shot prompting. Despite successes on small domain-specific datasets, these models face difficulties scaling up to text common in many real-world applications. A principal issue is that, in prior methods, the KG schema has to be included in the LLM prompt to generate valid triplets; larger and more complex schemas easily exceed the LLMs' context window length. Furthermore, there are scenarios where a fixed pre-defined schema is not available and we would like the method to construct a high-quality KG with a succinct self-generated schema. To address these problems, we propose a three-phase framework named Extract-Define-Canonicalize (EDC): open information extraction followed by schema definition and post-hoc canonicalization. EDC is flexible in that it can be applied to settings where a pre-defined target schema is available and when it is not; in the latter case, it constructs a schema automatically and applies self-canonicalization. To further improve performance, we introduce a trained component that retrieves schema elements relevant to the input text; this improves the LLMs' extraction performance in a retrieval-augmented generation-like manner. We demonstrate on three KGC benchmarks that EDC is able to extract high-quality triplets without any parameter tuning and with significantly larger schemas compared to prior works. Code for EDC is available at https://github.com/clear-nus/edc.",
            "link": "https://www.semanticscholar.org/paper/db96e019410006c3ee0ae0184800ab206f8704dd",
            "authors": "Bowen Zhang, Harold Soh",
            "EMNLP Paper ID": "1093",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "398978c84ca8dab093d0b7fa73c6d380f5fa914c",
            "title": "MQuinE: a cure for \"Z-paradox\" in knowledge graph embedding models",
            "abstract": "Knowledge graph embedding (KGE) models achieved state-of-the-art results on many knowledge graph tasks including link prediction and information retrieval. Despite the superior performance of KGE models in practice, we discover a deficiency in the expressiveness of some popular existing KGE models called \\emph{Z-paradox}. Motivated by the existence of Z-paradox, we propose a new KGE model called \\emph{MQuinE} that does not suffer from Z-paradox while preserves strong expressiveness to model various relation patterns including symmetric/asymmetric, inverse, 1-N/N-1/N-N, and composition relations with theoretical justification. Experiments on real-world knowledge bases indicate that Z-paradox indeed degrades the performance of existing KGE models, and can cause more than 20\\% accuracy drop on some challenging test samples. Our experiments further demonstrate that MQuinE can mitigate the negative impact of Z-paradox and outperform existing KGE models by a visible margin on link prediction tasks.",
            "link": "https://www.semanticscholar.org/paper/398978c84ca8dab093d0b7fa73c6d380f5fa914c",
            "authors": "Yang Liu, Huang Fang, Yunfeng Cai, Mingming Sun",
            "EMNLP Paper ID": "1098",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "df4801ac7ac99f54ba9e645a4961c1d6f8877edc",
            "title": "Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering",
            "abstract": "Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance Quesetion Answering (QA) performance of Large Language Models (LLMs), yet structured KG verbalization remains challengin. Existing methods, such as triple-form or free-form textual conversion of triple-form facts, encounter several issues. These include reduced evidence density due to duplicated entities or relationships, and reduced evidence clarity due to an inability to emphasize crucial evidence. To address these issues, we propose EFSum, an Evidence-focused Fact Summarization framework for enhanced QA with knowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer through distillation and preference alignment. Our extensive experiments show that EFSum improves LLM's zero-shot QA performance, and it is possible to ensure both the helpfulness and faithfulness of the summary.",
            "link": "https://www.semanticscholar.org/paper/df4801ac7ac99f54ba9e645a4961c1d6f8877edc",
            "authors": "Sungho Ko, Hyunjin Cho, Hyungjoo Chae, Jinyoung Yeo, Dongha Lee",
            "EMNLP Paper ID": "1201",
            "Oral/Poster": "Oral",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "08436b3ddafd2edc798753ebc87f6ceffed6e8df",
            "title": "KnowTuning: Knowledge-aware Fine-tuning for Large Language Models",
            "abstract": "Despite their success at many natural language processing (NLP) tasks, large language models still struggle to effectively leverage knowledge for knowledge-intensive tasks, manifesting limitations such as generating incomplete, non-factual, or illogical answers. These limitations stem from inadequate knowledge awareness of LLMs during vanilla fine-tuning. To address these problems, we propose a knowledge-aware fine-tuning (KnowTuning) method to improve fine-grained and coarse-grained knowledge awareness of LLMs. We devise a fine-grained knowledge augmentation stage to train LLMs to identify difficult fine-grained knowledge in answers. We also propose a coarse-grained knowledge comparison stage to train LLMs to distinguish between reliable and unreliable knowledge, in three aspects: completeness, factuality, and logicality. Extensive experiments on both generic and medical question answering (QA) datasets confirm the effectiveness of KnowTuning, through automatic and human evaluations, across various sizes of LLMs. We further verify that KnowTuning generates more facts with less factual error rate under fine-grained facts evaluation.",
            "link": "https://www.semanticscholar.org/paper/08436b3ddafd2edc798753ebc87f6ceffed6e8df",
            "authors": "Yougang Lyu, Lingyong Yan, Shuaiqiang Wang, Haibo Shi, Dawei Yin, Pengjie Ren, Zhumin Chen, M. D. Rijke, Zhaochun Ren",
            "EMNLP Paper ID": "1675",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "384dd8509978e62cb9ada1279733de01fafeec47",
            "title": "StorySparkQA: Expert-Annotated QA Pairs with Real-World Knowledge for Children's Story-Based Learning",
            "abstract": "Interactive story reading is a common parent-child activity, where parents expect to teach both language skills and real-world knowledge beyond the story. While increasing storytelling and reading systems have been developed for this activity, they often fail to infuse real-world knowledge into the conversation. This limitation can be attributed to the existing question-answering (QA) datasets used for children's education, upon which the systems are built, failing to capture the nuances of how education experts think when conducting interactive story reading activities. To bridge this gap, we design an annotation framework, empowered by existing knowledge graph to capture experts' annotations and thinking process, and leverage this framework to construct StorySparkQA dataset, which comprises 5,868 expert-annotated QA pairs with real-world knowledge. We conduct automated and human expert evaluations across various QA pair generation settings to demonstrate that our StorySparkQA can effectively support models in generating QA pairs that target real-world knowledge beyond story content. StorySparkQA is available at https://huggingface.co/datasets/NEU-HAI/StorySparkQA.",
            "link": "https://www.semanticscholar.org/paper/384dd8509978e62cb9ada1279733de01fafeec47",
            "authors": "Jiaju Chen, Yuxuan Lu, Shao Zhang, Bingsheng Yao, Yuanzhe Dong, Ying Xu, Yunyao Li, Qianwen Wang, Dakuo Wang, Yuling Sun",
            "EMNLP Paper ID": "2060",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "648f11073506154d061c8d2bfd7bfba546d3c2be",
            "title": "Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering",
            "abstract": "To address the issues of insufficient knowledge and hallucination in Large Language Models (LLMs), numerous studies have explored integrating LLMs with Knowledge Graphs (KGs). However, these methods are typically evaluated on conventional Knowledge Graph Question Answering (KGQA) with complete KGs, where all factual triples required for each question are entirely covered by the given KG. In such cases, LLMs primarily act as an agent to find answer entities within the KG, rather than effectively integrating the internal knowledge of LLMs and external knowledge sources such as KGs. In fact, KGs are often incomplete to cover all the knowledge required to answer questions. To simulate these real-world scenarios and evaluate the ability of LLMs to integrate internal and external knowledge, we propose leveraging LLMs for QA under Incomplete Knowledge Graph (IKGQA), where the provided KG lacks some of the factual triples for each question, and construct corresponding datasets. To handle IKGQA, we propose a training-free method called Generate-on-Graph (GoG), which can generate new factual triples while exploring KGs. Specifically, GoG performs reasoning through a Thinking-Searching-Generating framework, which treats LLM as both Agent and KG in IKGQA. Experimental results on two datasets demonstrate that our GoG outperforms all previous methods.",
            "link": "https://www.semanticscholar.org/paper/648f11073506154d061c8d2bfd7bfba546d3c2be",
            "authors": "Yao Xu, Shizhu He, Jiabei Chen, Zihao Wang, Yangqiu Song, Hanghang Tong, Kang Liu, Jun Zhao",
            "EMNLP Paper ID": "2289",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "c864998ef6dad7fab65f9968089e44313f077210",
            "title": "Generative Subgraph Retrieval for Knowledge Graph-Grounded Dialog Generation",
            "abstract": "Knowledge graph-grounded dialog generation requires retrieving a dialog-relevant subgraph from the given knowledge base graph and integrating it with the dialog history. Previous works typically represent the graph using an external encoder, such as graph neural networks, and retrieve relevant triplets based on the similarity between single-vector representations of triplets and the dialog history. However, these external encoders fail to leverage the rich knowledge of pretrained language models, and the retrieval process is also suboptimal due to the information bottleneck caused by the single-vector abstraction of the dialog history. In this work, we propose Dialog generation with Generative Subgraph Retrieval (DialogGSR), which retrieves relevant knowledge subgraphs by directly generating their token sequences on top of language models. For effective generative subgraph retrieval, we introduce two key methods: (i) structure-aware knowledge graph linearization with self-supervised graph-specific tokens and (ii) graph-constrained decoding utilizing graph structural proximity-based entity informativeness scores for valid and relevant generative retrieval. DialogGSR achieves state-of-the-art performance in knowledge graph-grounded dialog generation, as demonstrated on OpenDialKG and KOMODIS datasets.",
            "link": "https://www.semanticscholar.org/paper/c864998ef6dad7fab65f9968089e44313f077210",
            "authors": "Jinyoung Park, Minseok Joo, Jooyeon Kim, Hyunwoo J. Kim",
            "EMNLP Paper ID": "2859",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "3f7983ee14ef0efb9b8fdf4a7593d1fc7ae28609",
            "title": "Synthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models",
            "abstract": "Large language models (LLMs) are proficient in capturing factual knowledge across various domains. However, refining their capabilities on previously seen knowledge or integrating new knowledge from external sources remains a significant challenge. In this work, we propose a novel synthetic knowledge ingestion method called Ski, which leverages fine-grained synthesis, interleaved generation, and assemble augmentation strategies to construct high-quality data representations from raw knowledge sources. We then integrate Ski and its variations with three knowledge injection techniques: Retrieval Augmented Generation (RAG), Supervised Fine-tuning (SFT), and Continual Pre-training (CPT) to inject and refine knowledge in language models. Extensive empirical experiments are conducted on various question-answering tasks spanning finance, biomedicine, and open-generation domains to demonstrate that Ski significantly outperforms baseline methods by facilitating effective knowledge injection. We believe that our work is an important step towards enhancing the factual accuracy of LLM outputs by refining knowledge representation and injection capabilities.",
            "link": "https://www.semanticscholar.org/paper/3f7983ee14ef0efb9b8fdf4a7593d1fc7ae28609",
            "authors": "Jiaxin Zhang, Wendi Cui, Yiran Huang, Kamalika Das, Sricharan Kumar",
            "EMNLP Paper ID": "2948",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "17227b3b1c3427c1f3ce9aad3b2b0192b8ecb8db",
            "title": "ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense Question Answering",
            "abstract": "Current Large Language Models (LLMs) have shown strong reasoning capabilities in commonsense question answering benchmarks, but the process underlying their success remains largely opaque. As a consequence, recent approaches have equipped LLMs with mechanisms for knowledge retrieval, reasoning and introspection, not only to improve their capabilities but also to enhance the interpretability of their outputs. However, these methods require additional training, hand-crafted templates or human-written explanations. To address these issues, we introduce ZEBRA, a zero-shot question answering framework that combines retrieval, case-based reasoning and introspection and dispenses with the need for additional training of the LLM. Given an input question, ZEBRA retrieves relevant question-knowledge pairs from a knowledge base and generates new knowledge by reasoning over the relationships in these pairs. This generated knowledge is then used to answer the input question, improving the model's performance and interpretability. We evaluate our approach across 8 well-established commonsense reasoning benchmarks, demonstrating that ZEBRA consistently outperforms strong LLMs and previous knowledge integration approaches, achieving an average accuracy improvement of up to 4.5 points.",
            "link": "https://www.semanticscholar.org/paper/17227b3b1c3427c1f3ce9aad3b2b0192b8ecb8db",
            "authors": "F. Molfese, Simone Conia, Riccardo Orlando, Roberto Navigli",
            "EMNLP Paper ID": "3226",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "3777748543f1ff1f3ed8b16f063435217173b865",
            "title": "Llama SLayer 8B: Shallow Layers Hold the Key to Knowledge Injection",
            "abstract": "As a manner to augment pre-trained large language models (LLM), knowledge injection is critical to develop vertical domain large models and has been widely studied. Although most current approaches, including parameter-efficient fine-tuning (PEFT) and block expansion methods, uniformly apply knowledge across all LLM layers, it raises the question: are all layers equally crucial for knowledge injection? We begin by evaluating the importance of each layer in finding the optimal layer range for knowledge injection. Intuitively, the more important layers should play a more critical role in knowledge injection and deserve a denser injection. We observe performance dips in question-answering benchmarks after the removal or expansion of the shallow layers, and the degradation shrinks as the layer gets deeper, indicating that the shallow layers hold the key to knowledge injection. This insight leads us to propose the S strategy, a post-pretraining strategy of selectively enhancing shallow layers while pruning the less effective deep ones. Based on this strategy, we introduce Llama Slayer-8B and Llama Slayer-8B-Instruct. We experimented on the corpus of code $\\&$ math and demonstrated the effectiveness of our strategy. Further experiments across different LLM, Mistral-7B, and a legal corpus confirmed the general applicability of the approach, underscoring its wide-ranging efficacy. Our code is available at: \\https://github.com/txchen-USTC/Llama-Slayer",
            "link": "https://www.semanticscholar.org/paper/3777748543f1ff1f3ed8b16f063435217173b865",
            "authors": "Tianxiang Chen, Zhentao Tan, Tao Gong, Yue Wu, Qi Chu, Bin Liu, Jieping Ye, Nenghai Yu",
            "matchScore": 321.98822,
            "original title": "Llama SLayer 8B: Shallow Layers Hold the Key to Knowledge Injection",
            "original authors": "Tianxiang Chen, Zhentao Tan, Tao Gong, Yue Wu, Qi Chu, Bin Liu, Jieping Ye, Nenghai Yu",
            "EMNLP Paper ID": "1225",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 12): 17:45\u201318:45 (Evening)",
            "Type": "Findings"
        },
        {
            "paperId": "113c435bb7db63b200f1c78cfff3bbdf6d01ee77",
            "title": "Learning to Plan for Retrieval-Augmented Large Language Models from Knowledge Graphs",
            "abstract": "Improving the performance of large language models (LLMs) in complex question-answering (QA) scenarios has always been a research focal point. Recent studies have attempted to enhance LLMs' performance by combining step-wise planning with external retrieval. While effective for advanced models like GPT-3.5, smaller LLMs face challenges in decomposing complex questions, necessitating supervised fine-tuning. Previous work has relied on manual annotation and knowledge distillation from teacher LLMs, which are time-consuming and not accurate enough. In this paper, we introduce a novel framework for enhancing LLMs' planning capabilities by using planning data derived from knowledge graphs (KGs). LLMs fine-tuned with this data have improved planning capabilities, better equipping them to handle complex QA tasks that involve retrieval. Evaluations on multiple datasets, including our newly proposed benchmark, highlight the effectiveness of our framework and the benefits of KG-derived planning data.",
            "link": "https://www.semanticscholar.org/paper/113c435bb7db63b200f1c78cfff3bbdf6d01ee77",
            "authors": "Junjie Wang, Mingyang Chen, Binbin Hu, Dan Yang, Ziqi Liu, Yue Shen, Peng Wei, Zhiqiang Zhang, Jinjie Gu, Jun Zhou, Jeff Z. Pan, Wen Zhang, Hua-zeng Chen",
            "matchScore": 227.86603,
            "original title": "Learning to Plan for Retrieval-Augmented Large Language Models from Knowledge Graphs",
            "original authors": "Junjie Wang, Mingyang Chen, Binbin Hu, Dan Yang, Ziqi Liu, YUE SHEN, Peng Wei, Zhiqiang Zhang, Jinjie GU, JUN ZHOU, Jeff Z. Pan, Wen Zhang, Huajun Chen",
            "EMNLP Paper ID": "1634",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "fdfcfbe1e2aaf5c6bb307d04d71dc54ba5159ebd",
            "title": "LINKED: Eliciting, Filtering and Integrating Knowledge in Large Language Model for Commonsense Reasoning",
            "abstract": "Large language models (LLMs) sometimes demonstrate poor performance on knowledge-intensive tasks, commonsense reasoning is one of them. Researchers typically address these issues by retrieving related knowledge from knowledge graphs or employing self-enhancement methods to elicit knowledge in LLMs. However, noisy knowledge and invalid reasoning issues hamper their ability to answer questions accurately. To this end, we propose a novel method named eliciting, filtering and integrating knowledge in large language model (LINKED). In it, we design a reward model to filter out the noisy knowledge and take the marginal consistent reasoning module to reduce invalid reasoning. With our comprehensive experiments on two complex commonsense reasoning benchmarks, our method outperforms SOTA baselines (up to 9.0% improvement of accuracy). Besides, to measure the positive and negative impact of the injected knowledge, we propose a new metric called effectiveness-preservation score for the knowledge enhancement works. Finally, through extensive experiments, we conduct an in-depth analysis and find many meaningful conclusions about LLMs in commonsense reasoning tasks.",
            "link": "https://www.semanticscholar.org/paper/fdfcfbe1e2aaf5c6bb307d04d71dc54ba5159ebd",
            "authors": "Jiachun Li, Pengfei Cao, Chenhao Wang, Zhuoran Jin, Yubo Chen, Kang Liu, Xiaojian Jiang, Jiexin Xu, Jun Zhao",
            "matchScore": 262.08423,
            "original title": "LINKED: Eliciting, Filtering and Integrating Knowledge in Large Language Model for Commonsense Reasoning",
            "original authors": "Jiachun Li, Pengfei Cao, Chenhao Wang, Zhuoran Jin, Yubo Chen, Kang Liu, Xiaojian Jiang, Jiexin Xu, Jun Zhao",
            "EMNLP Paper ID": "1858",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "d0bb89165d072090e53ecf0f607351666bc997cf",
            "title": "Question-guided Knowledge Graph Re-scoring and Injection for Knowledge Graph Question Answering",
            "abstract": "Knowledge graph question answering (KGQA) involves answering natural language questions by leveraging structured information stored in a knowledge graph. Typically, KGQA initially retrieve a targeted subgraph from a large-scale knowledge graph, which serves as the basis for reasoning models to address queries. However, the retrieved subgraph inevitably brings distraction information for knowledge utilization, impeding the model's ability to perform accurate reasoning. To address this issue, we propose a Question-guided Knowledge Graph Re-scoring method (Q-KGR) to eliminate noisy pathways for the input question, thereby focusing specifically on pertinent factual knowledge. Moreover, we introduce Knowformer, a parameter-efficient method for injecting the re-scored knowledge graph into large language models to enhance their ability to perform factual reasoning. Extensive experiments on multiple KGQA benchmarks demonstrate the superiority of our method over existing systems.",
            "link": "https://www.semanticscholar.org/paper/d0bb89165d072090e53ecf0f607351666bc997cf",
            "authors": "Yu Zhang, Kehai Chen, Xuefeng Bai, zhao kang, Quanjiang Guo, Min Zhang",
            "matchScore": 310.79532,
            "original title": "Question-guided Knowledge Graph Re-scoring and Injection for Knowledge Graph Question Answering",
            "original authors": "Yu Zhang, Kehai Chen, Xuefeng Bai, zhao kang, Quanjiang Guo, Min Zhang",
            "EMNLP Paper ID": "1878",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 12): 17:45\u201318:45 (Evening)",
            "Type": "Findings"
        },
        {
            "paperId": "2352114a493f180799bb40f4369c218e83a2a37d",
            "title": "$R^3$-NL2GQL: A Model Coordination and Knowledge Graph Alignment Approach for NL2GQL",
            "abstract": "While current tasks of converting natural language to SQL (NL2SQL) using Foundation Models have shown impressive achievements, adapting these approaches for converting natural language to Graph Query Language (NL2GQL) encounters hurdles due to the distinct nature of GQL compared to SQL, alongside the diverse forms of GQL. Moving away from traditional rule-based and slot-filling methodologies, we introduce a novel approach, $R^3$-NL2GQL, integrating both small and large Foundation Models for ranking, rewriting, and refining tasks. This method leverages the interpretative strengths of smaller models for initial ranking and rewriting stages, while capitalizing on the superior generalization and query generation prowess of larger models for the final transformation of natural language queries into GQL formats. Addressing the scarcity of datasets in this emerging field, we have developed a bilingual dataset, sourced from graph database manuals and selected open-source Knowledge Graphs (KGs). Our evaluation of this methodology on this dataset demonstrates its promising efficacy and robustness.",
            "link": "https://www.semanticscholar.org/paper/2352114a493f180799bb40f4369c218e83a2a37d",
            "authors": "Yuhang Zhou, Yu He, Siyu Tian, Yuchen Ni, Zhangyue Yin, Xiang Liu, Chuanjun Ji, Sen Liu, Xipeng Qiu, Guangnan Ye, Hongfeng Chai",
            "matchScore": 302.40424,
            "original title": "$R^3$-NL2GQL: A Model Coordination and Knowledge Graph Alignment Approach for NL2GQL",
            "original authors": "Yuhang Zhou, Yu He, Siyu Tian, Yuchen Ni, Zhangyue Yin, Xiang Liu, Chuanjun Ji, Sen Liu, Xipeng Qiu, Guangnan Ye, Hongfeng Chai",
            "EMNLP Paper ID": "2671",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "906401f84d2d026dfdc3f5e2e292a8b95f0ab64c",
            "title": "LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments",
            "abstract": "The rapid obsolescence of information in Large Language Models (LLMs) has driven the development of various techniques to incorporate new facts. However, existing methods for knowledge editing still face difficulties with multi-hop questions that require accurate fact identification and sequential logical reasoning, particularly among numerous fact updates. To tackle these challenges, this paper introduces Graph Memory-based Editing for Large Language Models (GMeLLo), a straitforward and effective method that merges the explicit knowledge representation of Knowledge Graphs (KGs) with the linguistic flexibility of LLMs. Beyond merely leveraging LLMs for question answering, GMeLLo employs these models to convert free-form language into structured queries and fact triples, facilitating seamless interaction with KGs for rapid updates and precise multi-hop reasoning. Our results show that GMeLLo significantly surpasses current state-of-the-art knowledge editing methods in the multi-hop question answering benchmark, MQuAKE, especially in scenarios with extensive knowledge edits.",
            "link": "https://www.semanticscholar.org/paper/906401f84d2d026dfdc3f5e2e292a8b95f0ab64c",
            "authors": "Ruirui Chen, Weifeng Jiang, Chengwei Qin, Ishaan Singh Rawal, Cheston Tan, Dongkyu Choi, Bo Xiong, Bo Ai",
            "matchScore": 266.51752,
            "original title": "LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments",
            "original authors": "Ruirui Chen, Weifeng Jiang, Chengwei Qin, Ishaan Singh Rawal, Cheston Tan, Dongkyu Choi, Bo Xiong, Bo Ai",
            "EMNLP Paper ID": "2790",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "417966078c16c2b098ab7c842ceb3689f67ce1ec",
            "title": "SPINACH: SPARQL-Based Information Navigation for Challenging Real-World Questions",
            "abstract": "Large Language Models (LLMs) have led to significant improvements in the Knowledge Base Question Answering (KBQA) task. However, datasets used in KBQA studies do not capture the true complexity of KBQA tasks. They either have simple questions, use synthetically generated logical forms, or are based on small knowledge base (KB) schemas. We introduce the SPINACH dataset, an expert-annotated KBQA dataset collected from discussions on Wikidata's\"Request a Query\"forum with 320 decontextualized question-SPARQL pairs. The complexity of these in-the-wild queries calls for a KBQA system that can dynamically explore large and often incomplete schemas and reason about them, as it is infeasible to create a comprehensive training dataset. We also introduce an in-context learning KBQA agent, also called SPINACH, that mimics how a human expert would write SPARQLs to handle challenging questions. SPINACH achieves a new state of the art on the QALD-7, QALD-9 Plus and QALD-10 datasets by 31.0%, 27.0%, and 10.0% in $F_1$, respectively, and coming within 1.6% of the fine-tuned LLaMA SOTA model on WikiWebQuestions. On our new SPINACH dataset, the SPINACH agent outperforms all baselines, including the best GPT-4-based KBQA agent, by at least 38.1% in $F_1$.",
            "link": "https://www.semanticscholar.org/paper/417966078c16c2b098ab7c842ceb3689f67ce1ec",
            "authors": "Shicheng Liu, Sina J. Semnani, Harold Triedman, Jialiang Xu, Isaac Dan Zhao, Monica S. Lam",
            "matchScore": 281.50568,
            "original title": "SPINACH: SPARQL-Based Information Navigation for Challenging Real-World Questions",
            "original authors": "Shicheng Liu, Sina Semnani, Harold Triedman, Jialiang Xu, Isaac Dan Zhao, Monica Lam",
            "EMNLP Paper ID": "3060",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "d8b92da52780452591c81135a779dc5a336210e5",
            "title": "Block-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph Embedding",
            "abstract": "The primary aim of Knowledge Graph embeddings (KGE) is to learn low-dimensional representations of entities and relations for predicting missing facts. While rotation-based methods like RotatE and QuatE perform well in KGE, they face two challenges: limited model flexibility requiring proportional increases in relation size with entity dimension, and difficulties in generalizing the model for higher-dimensional rotations. To address these issues, we introduce OrthogonalE, a novel KGE model employing matrices for entities and block-diagonal orthogonal matrices with Riemannian optimization for relations. This approach enhances the generality and flexibility of KGE models. The experimental results indicate that our new KGE model, OrthogonalE, is both general and flexible, significantly outperforming state-of-the-art KGE models while substantially reducing the number of relation parameters.",
            "link": "https://www.semanticscholar.org/paper/d8b92da52780452591c81135a779dc5a336210e5",
            "authors": "Yihua Zhu, Hidetoshi Shimodaira",
            "matchScore": 283.126,
            "original title": "Block-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph Embedding",
            "original authors": "Yihua Zhu, Hidetoshi Shimodaira",
            "EMNLP Paper ID": "3243",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "f0a3cc74df84803589b8e8850fa3434d06e0c8eb",
            "title": "DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's Disease Questions with Scientific Literature",
            "abstract": "Recent advancements in large language models (LLMs) have achieved promising performances across various applications. Nonetheless, the ongoing challenge of integrating long-tail knowledge continues to impede the seamless adoption of LLMs in specialized domains. In this work, we introduce DALK, a.k.a. Dynamic Co-Augmentation of LLMs and KG, to address this limitation and demonstrate its ability on studying Alzheimer's Disease (AD), a specialized sub-field in biomedicine and a global health priority. With a synergized framework of LLM and KG mutually enhancing each other, we first leverage LLM to construct an evolving AD-specific knowledge graph (KG) sourced from AD-related scientific literature, and then we utilize a coarse-to-fine sampling method with a novel self-aware knowledge retrieval approach to select appropriate knowledge from the KG to augment LLM inference capabilities. The experimental results, conducted on our constructed AD question answering (ADQA) benchmark, underscore the efficacy of DALK. Additionally, we perform a series of detailed analyses that can offer valuable insights and guidelines for the emerging topic of mutually enhancing KG and LLM. We will release the code and data at https://github.com/David-Li0406/DALK.",
            "link": "https://www.semanticscholar.org/paper/f0a3cc74df84803589b8e8850fa3434d06e0c8eb",
            "authors": "Dawei Li, Shu Yang, Zhen Tan, Jae Young Baik, Sunkwon Yun, Joseph Lee, Aaron Chacko, Bojian Hou, D. Duong-Tran, Ying Ding, Huan Liu, Li Shen, Tianlong Chen",
            "matchScore": 352.77023,
            "original title": "DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer\u2019s Disease Questions with Scientific Literature",
            "original authors": "Dawei Li, Shu Yang, Zhen Tan, Jae Young Baik, Sukwon Yun, Joseph Lee, Aaron Chacko, Bojian Hou, Duy Duong-Tran, Ying Ding, huan liu, Li Shen, Tianlong Chen",
            "EMNLP Paper ID": "432",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "99a6c1b99d8b5116eb7eaf5f6cfeb2d06a765360",
            "title": "Predictive Multiplicity of Knowledge Graph Embeddings in Link Prediction",
            "abstract": "Knowledge graph embedding (KGE) models are often used to predict missing links for knowledge graphs (KGs). However, multiple KG embeddings can perform almost equally well for link prediction yet give conflicting predictions for unseen queries. This phenomenon is termed \\textit{predictive multiplicity} in the literature. It poses substantial risks for KGE-based applications in high-stake domains but has been overlooked in KGE research. We define predictive multiplicity in link prediction, introduce evaluation metrics and measure predictive multiplicity for representative KGE methods on commonly used benchmark datasets. Our empirical study reveals significant predictive multiplicity in link prediction, with $8\\%$ to $39\\%$ testing queries exhibiting conflicting predictions. We address this issue by leveraging voting methods from social choice theory, significantly mitigating conflicts by $66\\%$ to $78\\%$ in our experiments.",
            "link": "https://www.semanticscholar.org/paper/99a6c1b99d8b5116eb7eaf5f6cfeb2d06a765360",
            "authors": "Yuqicheng Zhu, Nico Potyka, M. Nayyeri, Bo Xiong, Yunjie He, Evgeny Kharlamov, Steffen Staab",
            "matchScore": 213.04257,
            "original title": "Predictive Multiplicity of Knowledge Graph Embeddings in Link Prediction",
            "original authors": "Yuqicheng Zhu, Nico Potyka, Mojtaba Nayyeri, Bo Xiong, Yunjie He, Evgeny Kharlamov, Steffen Staab",
            "EMNLP Paper ID": "59",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "c027283ef0c96188166d9e585c884933a16186f0",
            "title": "Temporal Fact Reasoning over Hyper-Relational Knowledge Graphs",
            "abstract": "Stemming from traditional knowledge graphs (KGs), hyper-relational KGs (HKGs) provide additional key-value pairs (i.e., qualifiers) for each KG fact that help to better restrict the fact validity. In recent years, there has been an increasing interest in studying graph reasoning over HKGs. Meanwhile, as discussed in recent works that focus on temporal KGs (TKGs), world knowledge is ever-evolving, making it important to reason over temporal facts in KGs. Previous mainstream benchmark HKGs do not explicitly specify temporal information for each HKG fact. Therefore, almost all existing HKG reasoning approaches do not devise any module specifically for temporal reasoning. To better study temporal fact reasoning over HKGs, we propose a new type of data structure named hyper-relational TKG (HTKG). Every fact in an HTKG is coupled with a timestamp explicitly indicating its time validity. We develop two new benchmark HTKG datasets, i.e., Wiki-hy and YAGO-hy, and propose an HTKG reasoning model that efficiently models hyper-relational temporal facts. To support future research on this topic, we open-source our datasets and model.",
            "link": "https://www.semanticscholar.org/paper/c027283ef0c96188166d9e585c884933a16186f0",
            "authors": "Zifeng Ding, Jingcheng Wu, Jingpei Wu, Yan Xia, Volker Tresp",
            "matchScore": 271.8,
            "original title": "Temporal Fact Reasoning over Hyper-Relational Knowledge Graphs",
            "original authors": "Zifeng Ding, Jingcheng Wu, Jingpei Wu, Yan Xia, Bo Xiong, Volker Tresp",
            "EMNLP Paper ID": "60",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "3f7c241000e1add7e825f287492384708941172b",
            "title": "InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration",
            "abstract": "Though Large Language Models (LLMs) have shown remarkable open-generation capabilities across diverse domains, they struggle with knowledge-intensive tasks. To alleviate this issue, knowledge integration methods have been proposed to enhance LLMs with domain-specific knowledge graphs using external modules. However, they suffer from data inefficiency as they require both known and unknown knowledge for fine-tuning. Thus, we study a novel problem of integrating unknown knowledge into LLMs efficiently without unnecessary overlap of known knowledge. Injecting new knowledge poses the risk of forgetting previously acquired knowledge. To tackle this, we propose a novel Infuser-Guided Knowledge Integration (InfuserKI) framework that utilizes transformer internal states to determine whether to enhance the original LLM output with additional information, thereby effectively mitigating knowledge forgetting. Evaluations on the UMLS-2.5k and MetaQA domain knowledge graphs demonstrate that InfuserKI can effectively acquire new knowledge and outperform state-of-the-art baselines by 9% and 6%, respectively, in reducing knowledge forgetting.",
            "link": "https://www.semanticscholar.org/paper/3f7c241000e1add7e825f287492384708941172b",
            "authors": "Fali Wang, Runxue Bao, Suhang Wang, Wenchao Yu, Yanchi Liu, Wei Cheng, Haifeng Chen",
            "matchScore": 292.80444,
            "original title": "InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration",
            "original authors": "Fali Wang, Runxue Bao, Suhang Wang, Wenchao Yu, Yanchi Liu, Wei Cheng, Haifeng Chen",
            "EMNLP Paper ID": "738",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        }
    ],
    "Robustness, Security, and Contamination Challenges in Large Language Models": [
        {
            "paperId": "bb03245c2867c4252a4087a4e9a95794e7ed9025",
            "title": "Impeding LLM-assisted Cheating in Introductory Programming Assignments via Adversarial Perturbation",
            "abstract": "While Large language model (LLM)-based programming assistants such as CoPilot and ChatGPT can help improve the productivity of professional software developers, they can also facilitate cheating in introductory computer programming courses. Assuming instructors have limited control over the industrial-strength models, this paper investigates the baseline performance of 5 widely used LLMs on a collection of introductory programming problems, examines adversarial perturbations to degrade their performance, and describes the results of a user study aimed at understanding the efficacy of such perturbations in hindering actual code generation for introductory programming assignments. The user study suggests that i) perturbations combinedly reduced the average correctness score by 77%, ii) the drop in correctness caused by these perturbations was affected based on their detectability.",
            "link": "https://www.semanticscholar.org/paper/bb03245c2867c4252a4087a4e9a95794e7ed9025",
            "authors": "Saiful Islam Salim, Rubin Yuchan Yang, Alexander Cooper, Suryashree Ray, Saumya Debray, Sazzadur Rahaman",
            "EMNLP Paper ID": "57",
            "Oral/Poster": "Oral",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "0ce1e3ba7cb96dab6db15ee7b4add29cab5d0efb",
            "title": "Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection",
            "abstract": "Large Language Models (LLMs) have demonstrated exceptional proficiency in instruction-following, becoming increasingly crucial across various applications. However, this capability brings with it the risk of prompt injection attacks, where attackers inject instructions into LLMs' input to elicit undesirable actions or content. Understanding the robustness of LLMs against such attacks is vital for their safe implementation. In this work, we establish a benchmark to evaluate the robustness of instruction-following LLMs against prompt injection attacks. Our objective is to determine the extent to which LLMs can be influenced by injected instructions and their ability to differentiate between these injected and original target instructions. Through extensive experiments with leading instruction-following LLMs, we uncover significant vulnerabilities in their robustness to such attacks. Our results indicate that some models are overly tuned to follow any embedded instructions in the prompt, overly focusing on the latter parts of the prompt without fully grasping the entire context. By contrast, models with a better grasp of the context and instruction-following capabilities will potentially be more susceptible to compromise by injected instructions. This underscores the need to shift the focus from merely enhancing LLMs' instruction-following capabilities to improving their overall comprehension of prompts and discernment of instructions that are appropriate to follow. We hope our in-depth analysis offers insights into the underlying causes of these vulnerabilities, aiding in the development of future solutions. Code and data are available at https://github.com/Leezekun/instruction-following-robustness-eval",
            "link": "https://www.semanticscholar.org/paper/0ce1e3ba7cb96dab6db15ee7b4add29cab5d0efb",
            "authors": "Zekun Li, Baolin Peng, Pengcheng He, Xifeng Yan",
            "EMNLP Paper ID": "75",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "02ffeb8cb7a1d1336e0e135c7c1000610ae7b942",
            "title": "On Training Data Influence of GPT Models",
            "abstract": "Amidst the rapid advancements in generative language models, the investigation of how training data shapes the performance of GPT models is still emerging. This paper presents GPTfluence, a novel approach that leverages a featurized simulation to assess the impact of training examples on the training dynamics of GPT models. Our approach not only traces the influence of individual training instances on performance trajectories, such as loss and other key metrics, on targeted test points but also enables a comprehensive comparison with existing methods across various training scenarios in GPT models, ranging from 14 million to 2.8 billion parameters, across a range of downstream tasks. Contrary to earlier methods that struggle with generalization to new data, GPTfluence introduces a parameterized simulation of training dynamics, demonstrating robust generalization capabilities to unseen training data. This adaptability is evident across both fine-tuning and instruction-tuning scenarios, spanning tasks in natural language understanding and generation. We make our code and data publicly available at https://github.com/ernie-research/gptfluence.",
            "link": "https://www.semanticscholar.org/paper/02ffeb8cb7a1d1336e0e135c7c1000610ae7b942",
            "authors": "Qingyi Liu, Yekun Chai, Shuohuan Wang, Yu Sun, Keze Wang, Hua Wu",
            "EMNLP Paper ID": "351",
            "Oral/Poster": "Oral",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "beb2d4faee9f9c752a7993566a096cc3570a869f",
            "title": "Pretraining Data Detection for Large Language Models: A Divergence-based Calibration Method",
            "abstract": "As the scale of training corpora for large language models (LLMs) grows, model developers become increasingly reluctant to disclose details on their data. This lack of transparency poses challenges to scientific evaluation and ethical deployment. Recently, pretraining data detection approaches, which infer whether a given text was part of an LLM's training data through black-box access, have been explored. The Min-K\\% Prob method, which has achieved state-of-the-art results, assumes that a non-training example tends to contain a few outlier words with low token probabilities. However, the effectiveness may be limited as it tends to misclassify non-training texts that contain many common words with high probabilities predicted by LLMs. To address this issue, we introduce a divergence-based calibration method, inspired by the divergence-from-randomness concept, to calibrate token probabilities for pretraining data detection. We compute the cross-entropy (i.e., the divergence) between the token probability distribution and the token frequency distribution to derive a detection score. We have developed a Chinese-language benchmark, PatentMIA, to assess the performance of detection approaches for LLMs on Chinese text. Experimental results on English-language benchmarks and PatentMIA demonstrate that our proposed method significantly outperforms existing methods. Our code and PatentMIA benchmark are available at \\url{https://github.com/zhang-wei-chao/DC-PDD}.",
            "link": "https://www.semanticscholar.org/paper/beb2d4faee9f9c752a7993566a096cc3570a869f",
            "authors": "Weichao Zhang, Ruqing Zhang, Jiafeng Guo, M. D. Rijke, Yixing Fan, Xueqi Cheng",
            "EMNLP Paper ID": "577",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "e6ffa22c9f53c7dc97da365bfe868951ff7ddf42",
            "title": "Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models",
            "abstract": "The disconnect between tokenizer creation and model training in language models allows for specific inputs, such as the infamous SolidGoldMagikarp token, to induce unwanted model behaviour. Although such `glitch tokens', tokens present in the tokenizer vocabulary but that are nearly or entirely absent during model training, have been observed across various models, a reliable method to identify and address them has been missing. We present a comprehensive analysis of Large Language Model tokenizers, specifically targeting this issue of detecting under-trained tokens. Through a combination of tokenizer analysis, model weight-based indicators, and prompting techniques, we develop novel and effective methods for automatically detecting these problematic tokens. Our findings demonstrate the prevalence of such tokens across a diverse set of models and provide insights into improving the efficiency and safety of language models.",
            "link": "https://www.semanticscholar.org/paper/e6ffa22c9f53c7dc97da365bfe868951ff7ddf42",
            "authors": "Sander Land, Max Bartolo",
            "EMNLP Paper ID": "1353",
            "Oral/Poster": "Oral",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "e9ca46eeba79db1066d0c8ea81226777ce72c04f",
            "title": "Householder Pseudo-Rotation: A Novel Approach to Activation Editing in LLMs with Direction-Magnitude Perspective",
            "abstract": "Activation Editing, which involves directly editting the internal representations of large language models (LLMs) to alter their behaviors and achieve desired properties, has emerged as a promising area of research. Existing works primarily treat LLMs' activations as points in space and modify them by adding steering vectors. However, this approach is limited in its ability to achieve greater performance improvement while maintaining the necessary consistency of activation magnitudes. To overcome these issues, we propose a novel editing method that views activations in terms of their directions and magnitudes. Our method, named Householder Pseudo-Rotation (HPR), mimics the rotation transformation, thus preserving activation norms and resulting in an improved performance on various safety benchmarks.",
            "link": "https://www.semanticscholar.org/paper/e9ca46eeba79db1066d0c8ea81226777ce72c04f",
            "authors": "Van-Cuong Pham, Thien Huu Nguyen",
            "EMNLP Paper ID": "1584",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "dbcd51388bc622e7725782177c09cf8b5c1daf5d",
            "title": "Enhancing Training Data Attribution for Large Language Models with Fitting Error Consideration",
            "abstract": "The black-box nature of large language models (LLMs) poses challenges in interpreting results, impacting issues such as data intellectual property protection and hallucination tracing. Training data attribution (TDA) methods are considered effective solutions to address these challenges. Most recent TDA methods rely on influence functions, assuming the model achieves minimized empirical risk. However, achieving this criterion is difficult, and sourcing accuracy can be compromised by fitting errors during model training. In this paper, we introduce a novel TDA method called Debias and Denoise Attribution (DDA), which enhances influence functions by addressing fitting errors. Specifically, the debias strategy seeks to improve the performance of influence functions by eliminating the knowledge bias present in the base model before fine-tuning, while the denoise strategy aims to reduce discrepancies in influence scores arising from varying degrees of fitting during the training process through smoothing techniques. Experimental results demonstrate that our method significantly outperforms existing approaches, achieving an averaged AUC of 91.64%. Moreover, DDA exhibits strong generality and scalability across various sources and different-scale models like LLaMA2, QWEN2, and Mistral.",
            "link": "https://www.semanticscholar.org/paper/dbcd51388bc622e7725782177c09cf8b5c1daf5d",
            "authors": "Kangxi Wu, Liang Pang, Huawei Shen, Xueqi Cheng",
            "EMNLP Paper ID": "1633",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "8147e1594d4dfc468300666df149760610b993d6",
            "title": "SecCoder: Towards Generalizable and Robust Secure Code Generation",
            "abstract": "After large models (LMs) have gained widespread acceptance in code-related tasks, their superior generative capacity has greatly promoted the application of the code LM. Nevertheless, the security of the generated code has raised attention to its potential damage. Existing secure code generation methods have limited generalizability to unseen test cases and poor robustness against the attacked model, leading to safety failures in code generation. In this paper, we propose a generalizable and robust secure code generation method SecCoder by using in-context learning (ICL) and the safe demonstration. The dense retriever is also used to select the most helpful demonstration to maximize the improvement of the generated code's security. Experimental results show the superior generalizability of the proposed model SecCoder compared to the current secure code generation method, achieving a significant security improvement of an average of 7.20% on unseen test cases. The results also show the better robustness of SecCoder compared to the current attacked code LM, achieving a significant security improvement of an average of 7.74%. Our analysis indicates that SecCoder enhances the security of LMs in generating code, and it is more generalizable and robust.",
            "link": "https://www.semanticscholar.org/paper/8147e1594d4dfc468300666df149760610b993d6",
            "authors": "Boyu Zhang, Tianyu Du, Junkai Tong, Xuhong Zhang, Kingsum Chow, Sheng Cheng, Xun Wang, Jianwei Yin",
            "EMNLP Paper ID": "1677",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "43d5c7810e2c983582da3ee947034f8bcb8d219d",
            "title": "Data Contamination Can Cross Language Barriers",
            "abstract": "The opacity in developing large language models (LLMs) is raising growing concerns about the potential contamination of public benchmarks in the pre-training data. Existing contamination detection methods are typically based on the text overlap between training and evaluation data, which can be too superficial to reflect deeper forms of contamination. In this paper, we first present a cross-lingual form of contamination that inflates LLMs' performance while evading current detection methods, deliberately injected by overfitting LLMs on the translated versions of benchmark test sets. Then, we propose generalization-based approaches to unmask such deeply concealed contamination. Specifically, we examine the LLM's performance change after modifying the original benchmark by replacing the false answer choices with correct ones from other questions. Contaminated models can hardly generalize to such easier situations, where the false choices can be \\emph{not even wrong}, as all choices are correct in their memorization. Experimental results demonstrate that cross-lingual contamination can easily fool existing detection methods, but not ours. In addition, we discuss the potential utilization of cross-lingual contamination in interpreting LLMs' working mechanisms and in post-training LLMs for enhanced multilingual capabilities. The code and dataset we use can be obtained from \\url{https://github.com/ShangDataLab/Deep-Contam}.",
            "link": "https://www.semanticscholar.org/paper/43d5c7810e2c983582da3ee947034f8bcb8d219d",
            "authors": "Feng Yao, Yufan Zhuang, Zihao Sun, Sunan Xu, Animesh Kumar, Jingbo Shang",
            "EMNLP Paper ID": "2158",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "e80da784ce4f4de96a1f215b751bc7efe38f6406",
            "title": "Adapters Mixup: Mixing Parameter-Efficient Adapters to Enhance the Adversarial Robustness of Fine-tuned Pre-trained Text Classifiers",
            "abstract": "Existing works show that augmenting the training data of pre-trained language models (PLMs) for classification tasks fine-tuned via parameter-efficient fine-tuning methods (PEFT) using both clean and adversarial examples can enhance their robustness under adversarial attacks. However, this adversarial training paradigm often leads to performance degradation on clean inputs and requires frequent re-training on the entire data to account for new, unknown attacks. To overcome these challenges while still harnessing the benefits of adversarial training and the efficiency of PEFT, this work proposes a novel approach, called AdpMixup, that combines two paradigms: (1) fine-tuning through adapters and (2) adversarial augmentation via mixup to dynamically leverage existing knowledge from a set of pre-known attacks for robust inference. Intuitively, AdpMixup fine-tunes PLMs with multiple adapters with both clean and pre-known adversarial examples and intelligently mixes them up in different ratios during prediction. Our experiments show AdpMixup achieves the best trade-off between training efficiency and robustness under both pre-known and unknown attacks, compared to existing baselines on five downstream tasks across six varied black-box attacks and 2 PLMs. All source code will be available.",
            "link": "https://www.semanticscholar.org/paper/e80da784ce4f4de96a1f215b751bc7efe38f6406",
            "authors": "Tuc Nguyen, Thai Le",
            "EMNLP Paper ID": "2860",
            "Oral/Poster": "Oral",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "3f8332a84034eed88f46c9720df7f1260ce0d721",
            "title": "Evaluating Concurrent Robustness of Language Models Across Diverse Challenge Sets",
            "abstract": "Language models, characterized by their black-box nature, often hallucinate and display sensitivity to input perturbations, causing concerns about trust. To enhance trust, it is imperative to gain a comprehensive understanding of the model's failure modes and develop effective strategies to improve their performance. In this study, we introduce a methodology designed to examine how input perturbations affect language models across various scales, including pre-trained models and large language models (LLMs). Utilizing fine-tuning, we enhance the model's robustness to input perturbations. Additionally, we investigate whether exposure to one perturbation enhances or diminishes the model's performance with respect to other perturbations. To address robustness against multiple perturbations, we present three distinct fine-tuning strategies. Furthermore, we broaden the scope of our methodology to encompass large language models (LLMs) by leveraging a chain of thought (CoT) prompting approach augmented with exemplars. We employ the Tabular-NLI task to showcase how our proposed strategies adeptly train a robust model, enabling it to address diverse perturbations while maintaining accuracy on the original dataset.",
            "link": "https://www.semanticscholar.org/paper/3f8332a84034eed88f46c9720df7f1260ce0d721",
            "authors": "Vatsal Gupta, Pranshu Pandya, Tushar Kataria, Vivek Gupta, Dan Roth",
            "EMNLP Paper ID": "3130",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "f550b02f618cb774d93cad3b9f501514156221ec",
            "title": "CodeFort: Robust Training for Code Generation Models",
            "abstract": "Code generation models are not robust to small perturbations, which often lead to inconsistent and incorrect generations and significantly degrade the performance of these models. Improving the robustness of code generation models is crucial to better user experience when these models are deployed in real-world applications. However, existing efforts have not addressed this issue for code generation models. To fill this gap, we propose CodeFort, a framework to improve the robustness of code generation models, generalizing a large variety of code perturbations to enrich the training data and enabling various robust training strategies, mixing data augmentation, batch augmentation, adversarial logits pairing, and contrastive learning, all carefully designed to support high-throughput training. Extensive evaluations show that we improve the average robust pass rates of baseline CodeGen models from 14.79 to 21.74. Notably, the improvement in robustness against code-syntax perturbations is evidenced by a significant decrease in pass rate drop from 95.04% to 53.35%",
            "link": "https://www.semanticscholar.org/paper/f550b02f618cb774d93cad3b9f501514156221ec",
            "authors": "Yuhao Zhang, Shiqi Wang, Haifeng Qian, Zijian Wang, Mingyue Shang, Linbo Liu, Sanjay Krishna Gouda, Baishakhi Ray, M. K. Ramanathan, Xiaofei Ma, Anoop Deoras",
            "matchScore": 204.77798,
            "original title": "CodeFort: Robust Training for Code Generation Models",
            "original authors": "Yuhao Zhang, Shiqi Wang, Haifeng Qian, Zijian Wang, Mingyue Shang, Linbo Liu, Sanjay Krishna Gouda, Baishakhi Ray, Murali Krishna Ramanathan, Xiaofei Ma, Anoop Deoras",
            "EMNLP Paper ID": "1045",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "a1d29e4da609746b0c927ef141f31445e769ec3c",
            "title": "Inference-Time Decontamination: Reusing Leaked Benchmarks for Large Language Model Evaluation",
            "abstract": "The training process of large language models (LLMs) often involves varying degrees of test data contamination. Although current LLMs are achieving increasingly better performance on various benchmarks, their performance in practical applications does not always match their benchmark results. Leakage of benchmarks can prevent the accurate assessment of LLMs' true performance. However, constructing new benchmarks is costly, labor-intensive and still carries the risk of leakage. Therefore, in this paper, we ask the question, Can we reuse these leaked benchmarks for LLM evaluation? We propose Inference-Time Decontamination (ITD) to address this issue by detecting and rewriting leaked samples without altering their difficulties. ITD can mitigate performance inflation caused by memorizing leaked benchmarks. Our proof-of-concept experiments demonstrate that ITD reduces inflated accuracy by 22.9% on GSM8K and 19.0% on MMLU. On MMLU, using Inference-time Decontamination can lead to a decrease in the results of Phi3 and Mistral by 6.7% and 3.6% respectively. We hope that ITD can provide more truthful evaluation results for large language models.",
            "link": "https://www.semanticscholar.org/paper/a1d29e4da609746b0c927ef141f31445e769ec3c",
            "authors": "Qin Zhu, Qingyuan Cheng, Runyu Peng, Xiaonan Li, Tengxiao Liu, Runyu Peng, Xipeng Qiu, Xuanjing Huang",
            "matchScore": 305.80933,
            "original title": "Inference-Time Decontamination: Reusing Leaked Benchmarks for Large Language Model Evaluation",
            "original authors": "Qin Zhu, Qinyuan Cheng, Runyu Peng, Xiaonan Li, Ru Peng, Tengxiao Liu, Xipeng Qiu, Xuanjing Huang",
            "EMNLP Paper ID": "1906",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "9383e05fbeef79240721a51353f011618f80c142",
            "title": "Measuring the Robustness of NLP Models to Domain Shifts",
            "abstract": "Existing research on Domain Robustness (DR) suffers from disparate setups, limited task variety, and scarce research on recent capabilities such as in-context learning. Furthermore, the common practice of measuring DR might not be fully accurate. Current research focuses on challenge sets and relies solely on the Source Drop (SD): Using the source in-domain performance as a reference point for degradation. However, we argue that the Target Drop (TD), which measures degradation from the target in-domain performance, should be used as a complementary point of view. To address these issues, we first curated a DR benchmark comprised of 7 diverse NLP tasks, which enabled us to measure both the SD and the TD. We then conducted a comprehensive large-scale DR study involving over 14,000 domain shifts across 21 fine-tuned models and few-shot LLMs. We found that both model types suffer from drops upon domain shifts. While fine-tuned models excel in-domain, few-shot LLMs often surpass them cross-domain, showing better robustness. In addition, we found that a large SD can often be explained by shifting to a harder domain rather than by a genuine DR challenge, and this highlights the importance of TD as a complementary metric. We hope our study will shed light on the current DR state of NLP models and promote improved evaluation practices toward more robust models.",
            "link": "https://www.semanticscholar.org/paper/9383e05fbeef79240721a51353f011618f80c142",
            "authors": "Nitay Calderon, Naveh Porat, Eyal Ben-David, Zorik Gekhman, Nadav Oved, Roi Reichart",
            "matchScore": 188.25087,
            "original title": "Measuring the Robustness of NLP Models to Domain Shifts",
            "original authors": "Nitay Calderon, Naveh Porat, Eyal Ben-David, Alexander Chapanin, Zorik Gekhman, Nadav Oved, Vitaly Shalumov, Roi Reichart",
            "EMNLP Paper ID": "20",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "cb1454f820b8daf27031411e3f39ce2c3d304422",
            "title": "CERT-ED: Certifiably Robust Text Classification for Edit Distance",
            "abstract": "With the growing integration of AI in daily life, ensuring the robustness of systems to inference-time attacks is crucial. Among the approaches for certifying robustness to such adversarial examples, randomized smoothing has emerged as highly promising due to its nature as a wrapper around arbitrary black-box models. Previous work on randomized smoothing in natural language processing has primarily focused on specific subsets of edit distance operations, such as synonym substitution or word insertion, without exploring the certification of all edit operations. In this paper, we adapt Randomized Deletion (Huang et al., 2023) and propose, CERTified Edit Distance defense (CERT-ED) for natural language classification. Through comprehensive experiments, we demonstrate that CERT-ED outperforms the existing Hamming distance method RanMASK (Zeng et al., 2023) in 4 out of 5 datasets in terms of both accuracy and the cardinality of the certificate. By covering various threat models, including 5 direct and 5 transfer attacks, our method improves empirical robustness in 38 out of 50 settings.",
            "link": "https://www.semanticscholar.org/paper/cb1454f820b8daf27031411e3f39ce2c3d304422",
            "authors": "Zhuoqun Huang, Neil G. Marchant, Olga Ohrimenko, Benjamin I. P. Rubinstein",
            "matchScore": 264.4903,
            "original title": "CERT-ED: Certifiably Robust Text Classification for Edit Distance",
            "original authors": "Zhuoqun Huang, Neil G Marchant, Olga Ohrimenko, Benjamin I. P. Rubinstein",
            "EMNLP Paper ID": "2172",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "abd552a721666deb24938ba8f77731790a847ea0",
            "title": "Fighting Randomness with Randomness: Mitigating Optimisation Instability of Fine-Tuning using Delayed Ensemble and Noisy Interpolation",
            "abstract": "While fine-tuning of pre-trained language models generally helps to overcome the lack of labelled training samples, it also displays model performance instability. This instability mainly originates from randomness in initialisation or data shuffling. To address this, researchers either modify the training process or augment the available samples, which typically results in increased computational costs. We propose a new mitigation strategy, called Delayed Ensemble with Noisy Interpolation (DENI), that leverages the strengths of ensembling, noise regularisation and model interpolation, while retaining computational efficiency. We compare DENI with 9 representative mitigation strategies across 3 models, 4 tuning strategies and 7 text classification datasets. We show that: 1) DENI outperforms the best performing mitigation strategy (Ensemble), while using only a fraction of its cost; 2) the mitigation strategies are beneficial for parameter-efficient fine-tuning (PEFT) methods, outperforming full fine-tuning in specific cases; and 3) combining DENI with data augmentation often leads to even more effective instability mitigation.",
            "link": "https://www.semanticscholar.org/paper/abd552a721666deb24938ba8f77731790a847ea0",
            "authors": "Branislav Pecher, J\u00e1n Cegin, R\u00f3bert Belanec, Jakub Simko, Ivan Srba, M. Bielikov\u00e1",
            "matchScore": 363.9641,
            "original title": "Fighting Randomness with Randomness: Mitigating Optimisation Instability of Fine-Tuning using Delayed Ensemble and Noisy Interpolation",
            "original authors": "Branislav Pecher, Jan Cegin, Robert Belanec, Jakub Simko, Ivan Srba, Maria Bielikova",
            "EMNLP Paper ID": "2193",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "1d5a3c90e9a1d6309ef6b59296fb650826542219",
            "title": "Robust Text Classification: Analyzing Prototype-Based Networks",
            "abstract": "Downstream applications often require text classification models to be accurate and robust. While the accuracy of the state-of-the-art Language Models (LMs) approximates human performance, they often exhibit a drop in performance on noisy data found in the real world. This lack of robustness can be concerning, as even small perturbations in the text, irrelevant to the target task, can cause classifiers to incorrectly change their predictions. A potential solution can be the family of Prototype-Based Networks (PBNs) that classifies examples based on their similarity to prototypical examples of a class (prototypes) and has been shown to be robust to noise for computer vision tasks. In this paper, we study whether the robustness properties of PBNs transfer to text classification tasks under both targeted and static adversarial attack settings. Our results show that PBNs, as a mere architectural variation of vanilla LMs, offer more robustness compared to vanilla LMs under both targeted and static settings. We showcase how PBNs' interpretability can help us to understand PBNs' robustness properties. Finally, our ablation studies reveal the sensitivity of PBNs' robustness to how strictly clustering is done in the training phase, as tighter clustering results in less robust PBNs.",
            "link": "https://www.semanticscholar.org/paper/1d5a3c90e9a1d6309ef6b59296fb650826542219",
            "authors": "Zhivar Sourati, D. Deshpande, Filip Ilievski, Kiril Gashteovski, S. Saralajew",
            "matchScore": 219.15286,
            "original title": "Robust Text Classification: Analyzing Prototype-Based Networks",
            "original authors": "Zhivar Sourati, Darshan Girish Deshpande, Filip Ilievski, Kiril Gashteovski, Sascha Saralajew",
            "EMNLP Paper ID": "2492",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "12f04ecb1c9a76bed28656e1cb178c1b97eb7506",
            "title": "Out-of-Distribution Detection through Soft Clustering with Non-Negative Kernel Regression",
            "abstract": "As language models become more general purpose, increased attention needs to be paid to detecting out-of-distribution (OOD) instances, i.e., those not belonging to any of the distributions seen during training. Existing methods for detecting OOD data are computationally complex and storage-intensive. We propose a novel soft clustering approach for OOD detection based on non-negative kernel regression. Our approach greatly reduces computational and space complexities (up to 11x improvement in inference time and 87% reduction in storage requirements) and outperforms existing approaches by up to 4 AUROC points on four different benchmarks. We also introduce an entropy-constrained version of our algorithm, which leads to further reductions in storage requirements (up to 97% lower than comparable approaches) while retaining competitive performance. Our soft clustering approach for OOD detection highlights its potential for detecting tail-end phenomena in extreme-scale data settings.",
            "link": "https://www.semanticscholar.org/paper/12f04ecb1c9a76bed28656e1cb178c1b97eb7506",
            "authors": "Aryan Gulati, Xingjian Dong, Carlos Hurtado, Sarath Shekkizhar, Swabha Swayamdipta, Antonio Ortega",
            "matchScore": 247.82883,
            "original title": "Out-of-Distribution Detection through Soft Clustering with Non-Negative Kernel Regression",
            "original authors": "Aryan Gulati, Xingjian Dong, Carlos Hurtado, Sarath Shekkizhar, Swabha Swayamdipta, Antonio Ortega",
            "EMNLP Paper ID": "2534",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "3f37462afde6c573d78abfc5da73c93ec0998713",
            "title": "On Leakage of Code Generation Evaluation Datasets",
            "abstract": "In this paper, we consider contamination by code generation test sets, in particular in their use in modern large language models. We discuss three possible sources of such contamination and show findings supporting each of them: (i) direct data leakage, (ii) indirect data leakage through the use of synthetic data and (iii) overfitting to evaluation sets during model selection. To address this, we release Less Basic Python Problems (LBPP): an uncontaminated new benchmark of 161 prompts with their associated Python solutions. LBPP is released at https://huggingface.co/datasets/CohereForAI/lbpp .",
            "link": "https://www.semanticscholar.org/paper/3f37462afde6c573d78abfc5da73c93ec0998713",
            "authors": "Alexandre Matton, Tom Sherborne, Dennis Aumiller, Elena Tommasone, Milad Alizadeh, Jingyi He, Raymond Ma, Maxime Voisin, Ellen Gilsenan-McMahon, Matthias Gall'e",
            "matchScore": 191.8733,
            "original title": "On Leakage of Code Generation Evaluation Datasets",
            "original authors": "Alexandre Matton, Tom Sherborne, Dennis Aumiller, Elena Tommasone, Milad Alizadeh, Jingyi He, Raymond Ma, Maxime Voisin, Ellen Gilsenan-McMahon, Matthias Gall\u00e9",
            "EMNLP Paper ID": "2575",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "dfe926e52168827976a0b616b0f6aacd9de50267",
            "title": "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation",
            "abstract": "As large language models achieve impressive scores on traditional benchmarks, an increasing number of researchers are becoming concerned about benchmark data leakage during pre-training, commonly known as the data contamination problem. To ensure fair evaluation, recent benchmarks release only the training and validation sets, keeping the test set labels closed-source. They require anyone wishing to evaluate his language model to submit the model's predictions for centralized processing and then publish the model's result on their leaderboard. However, this submission process is inefficient and prevents effective error analysis. To address this issue, we propose to variabilize benchmarks and evaluate language models dynamically. Specifically, we extract variables from each test case and define a value range for each variable. For each evaluation, we sample new values from these value ranges to create unique test cases, thus ensuring a fresh evaluation each time. We applied this variable perturbation method to four datasets: GSM8K, ARC, CommonsenseQA, and TruthfulQA, which cover mathematical generation and multiple-choice tasks. Our experimental results demonstrate that this approach provides a more accurate assessment of the true capabilities of language models, effectively mitigating the contamination problem.",
            "link": "https://www.semanticscholar.org/paper/dfe926e52168827976a0b616b0f6aacd9de50267",
            "authors": "Kun Qian, Shunji Wan, Claudia Tang, Youzhi Wang, Xuanming Zhang, Maximillian Chen, Zhou Yu",
            "matchScore": 276.25568,
            "original title": "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation",
            "original authors": "Kun Qian, Shunji Wan, Claudia Tang, Youzhi Wang, Xuanming Zhang, Maximillian Chen, Zhou Yu",
            "EMNLP Paper ID": "3101",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "8bfbe322246f28d765a611781fe1a94289437104",
            "title": "Robust AI-Generated Text Detection by Restricted Embeddings",
            "abstract": "Growing amount and quality of AI-generated texts makes detecting such content more difficult. In most real-world scenarios, the domain (style and topic) of generated data and the generator model are not known in advance. In this work, we focus on the robustness of classifier-based detectors of AI-generated text, namely their ability to transfer to unseen generators or semantic domains. We investigate the geometry of the embedding space of Transformer-based text encoders and show that clearing out harmful linear subspaces helps to train a robust classifier, ignoring domain-specific spurious features. We investigate several subspace decomposition and feature selection strategies and achieve significant improvements over state of the art methods in cross-domain and cross-generator transfer. Our best approaches for head-wise and coordinate-based subspace removal increase the mean out-of-distribution (OOD) classification score by up to 9% and 14% in particular setups for RoBERTa and BERT embeddings respectively. We release our code and data: https://github.com/SilverSolver/RobustATD",
            "link": "https://www.semanticscholar.org/paper/8bfbe322246f28d765a611781fe1a94289437104",
            "authors": "Kristian Kuznetsov, Eduard Tulchinskii, Laida Kushnareva, German Magai, S. Barannikov, Sergey Nikolenko, Irina Piontkovskaya",
            "matchScore": 237.37129,
            "original title": "Robust AI-Generated Text Detection by Restricted Embeddings",
            "original authors": "Kristian Kuznetsov, Eduard Tulchinskii, Laida Kushnareva, German Magai, Serguei Barannikov, Sergey Nikolenko, Irina Piontkovskaya",
            "EMNLP Paper ID": "3267",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Findings"
        },
        {
            "paperId": "1e6edf2622ad0910f0e5aeb248f3c3ac88baa415",
            "title": "PaCoST: Paired Confidence Significance Testing for Benchmark Contamination Detection in Large Language Models",
            "abstract": "Large language models (LLMs) are known to be trained on vast amounts of data, which may unintentionally or intentionally include data from commonly used benchmarks. This inclusion can lead to cheatingly high scores on model leaderboards, yet result in disappointing performance in real-world applications. To address this benchmark contamination problem, we first propose a set of requirements that practical contamination detection methods should follow. Following these proposed requirements, we introduce PaCoST, a Paired Confidence Significance Testing to effectively detect benchmark contamination in LLMs. Our method constructs a counterpart for each piece of data with the same distribution, and performs statistical analysis of the corresponding confidence to test whether the model is significantly more confident under the original benchmark. We validate the effectiveness of PaCoST and apply it on popular open-source models and benchmarks. We find that almost all models and benchmarks we tested are suspected contaminated more or less. We finally call for new LLM evaluation methods.",
            "link": "https://www.semanticscholar.org/paper/1e6edf2622ad0910f0e5aeb248f3c3ac88baa415",
            "authors": "Huixuan Zhang, Yun Lin, Xiaojun Wan",
            "matchScore": 279.88748,
            "original title": "PaCoST: Paired Confidence Significance Testing for Benchmark Contamination Detection in Large Language Models",
            "original authors": "Huixuan Zhang, Yun Lin, Xiaojun Wan",
            "EMNLP Paper ID": "364",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "6ff083e9a005505d6cbce6fb28448b75d9981662",
            "title": "Towards Robust Extractive Question Answering Models: Rethinking the Training Methodology",
            "abstract": "This paper proposes a novel training method to improve the robustness of Extractive Question Answering (EQA) models. Previous research has shown that existing models, when trained on EQA datasets that include unanswerable questions, demonstrate a significant lack of robustness against distribution shifts and adversarial attacks. Despite this, the inclusion of unanswerable questions in EQA training datasets is essential for ensuring real-world reliability. Our proposed training method includes a novel loss function for the EQA problem and challenges an implicit assumption present in numerous EQA datasets. Models trained with our method maintain in-domain performance while achieving a notable improvement on out-of-domain datasets. This results in an overall F1 score improvement of 5.7 across all testing sets. Furthermore, our models exhibit significantly enhanced robustness against two types of adversarial attacks, with a performance decrease of only about a third compared to the default models.",
            "link": "https://www.semanticscholar.org/paper/6ff083e9a005505d6cbce6fb28448b75d9981662",
            "authors": "Son Quoc Tran, Matt Kretchmar",
            "matchScore": 277.78516,
            "original title": "Towards Robust Extractive Question Answering Models: Rethinking the Training Methodology",
            "original authors": "Son Quoc Tran, Matt Kretchmar",
            "EMNLP Paper ID": "439",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "c66028aaeca0a89f6fa24b6fb232e028dd250438",
            "title": "A Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial Scenarios",
            "abstract": "We evaluate the robustness of several large language models on multiple datasets. Robustness here refers to the relative insensitivity of the model's answers to meaning-preserving variants of their input. Benchmark datasets are constructed by introducing naturally-occurring, non-malicious perturbations, or by generating semantically equivalent paraphrases of input questions or statements. We further propose a novel metric for assessing a model robustness, and demonstrate its benefits in the non-adversarial scenario by empirical evaluation of several models on the created datasets.",
            "link": "https://www.semanticscholar.org/paper/c66028aaeca0a89f6fa24b6fb232e028dd250438",
            "authors": "Samuel Ackerman, Ella Rabinovich, E. Farchi, Ateret Anaby-Tavor",
            "matchScore": 259.01257,
            "original title": "A Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial Scenarios",
            "original authors": "Samuel Ackerman, Ella Rabinovich, Eitan Farchi, Ateret Anaby Tavor",
            "EMNLP Paper ID": "567",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "32e11cf020021c225782a3b77b5cfed4ae8e14a0",
            "title": "An Open Source Data Contamination Report for Large Language Models",
            "abstract": "Data contamination in model evaluation has become increasingly prevalent with the growing popularity of large language models. It allows models to\"cheat\"via memorisation instead of displaying true capabilities. Therefore, contamination analysis has become an crucial part of reliable model evaluation to validate results. However, existing contamination analysis is usually conducted internally by large language model developers and often lacks transparency and completeness. This paper presents an extensive data contamination report for over 15 popular large language models across six popular multiple-choice QA benchmarks. We also introduce an open-source pipeline that enables the community to perform contamination analysis on customised data and models. Our experiments reveal varying contamination levels ranging from 1\\% to 45\\% across benchmarks, with the contamination degree increasing rapidly over time. Performance analysis of large language models indicates that data contamination does not necessarily lead to increased model metrics: while significant accuracy boosts of up to 14\\% and 7\\% are observed on contaminated C-Eval and Hellaswag benchmarks, only a minimal increase is noted on contaminated MMLU. We also find larger models seem able to gain more advantages than smaller models on contaminated test sets.",
            "link": "https://www.semanticscholar.org/paper/32e11cf020021c225782a3b77b5cfed4ae8e14a0",
            "authors": "Yucheng Li",
            "matchScore": 217.01154,
            "original title": "An Open-Source Data Contamination Report for Large Language Models",
            "original authors": "YUCHENG LI, YUNHAO GUO, Frank Guerin, Chenghua Lin",
            "EMNLP Paper ID": "88",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        }
    ],
    "Enhancing Domain-Specific Information Retrieval Systems with Large Language Models": [
         {
            "paperId": "50e5be9c4d0e89d8d735e5852b9719c20f36e2bd",
            "title": "Knowledge Navigator: LLM-guided Browsing Framework for Exploratory Search in Scientific Literature",
            "abstract": "The exponential growth of scientific literature necessitates advanced tools for effective knowledge exploration. We present Knowledge Navigator, a system designed to enhance exploratory search abilities by organizing and structuring the retrieved documents from broad topical queries into a navigable, two-level hierarchy of named and descriptive scientific topics and subtopics. This structured organization provides an overall view of the research themes in a domain, while also enabling iterative search and deeper knowledge discovery within specific subtopics by allowing users to refine their focus and retrieve additional relevant documents. Knowledge Navigator combines LLM capabilities with cluster-based methods to enable an effective browsing method. We demonstrate our approach's effectiveness through automatic and manual evaluations on two novel benchmarks, CLUSTREC-COVID and SCITOC. Our code, prompts, and benchmarks are made publicly available.",
            "link": "https://www.semanticscholar.org/paper/50e5be9c4d0e89d8d735e5852b9719c20f36e2bd",
            "authors": "Uri Katz, Mosh Levy, Yoav Goldberg",
            "EMNLP Paper ID": "1841",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "9c79e9eee5c0abfecfa0190e01a0bf50e5842d11",
            "title": "ChatRetriever: Adapting Large Language Models for Generalized and Robust Conversational Dense Retrieval",
            "abstract": "Conversational search requires accurate interpretation of user intent from complex multi-turn contexts. This paper presents ChatRetriever, which inherits the strong generalization capability of large language models to robustly represent complex conversational sessions for dense retrieval. To achieve this, we propose a simple and effective dual-learning approach that adapts LLM for retrieval via contrastive learning while enhancing the complex session understanding through masked instruction tuning on high-quality conversational instruction tuning data. Extensive experiments on five conversational search benchmarks demonstrate that ChatRetriever substantially outperforms existing conversational dense retrievers, achieving state-of-the-art performance on par with LLM-based rewriting approaches. Furthermore, ChatRetriever exhibits superior robustness in handling diverse conversational contexts. Our work highlights the potential of adapting LLMs for retrieval with complex inputs like conversational search sessions and proposes an effective approach to advance this research direction.",
            "link": "https://www.semanticscholar.org/paper/9c79e9eee5c0abfecfa0190e01a0bf50e5842d11",
            "authors": "Kelong Mao, Chenlong Deng, Haonan Chen, Fengran Mo, Zheng Liu, Tetsuya Sakai, Zhicheng Dou",
            "EMNLP Paper ID": "146",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "77e5d0a68afcffb27191572590deced60feb9d5d",
            "title": "Retrieved Sequence Augmentation for Protein Representation Learning",
            "abstract": "The advancement of protein representation learning has been significantly influenced by the remarkable progress in language models. Accordingly, protein language models perform inference from individual sequences, thereby limiting their capacity to incorporate evolutionary knowledge present in sequence variations. Existing solutions, which rely on Multiple Sequence Alignments (MSA), suffer from substantial computational overhead and suboptimal generalization performance for de novo proteins. In light of these problems, we introduce a novel paradigm called Retrieved Sequence Augmentation (RSA) that enhances protein representation learning without necessitating additional alignment or preprocessing. RSA associates query protein sequences with a collection of structurally or functionally similar sequences in the database and integrates them for subsequent predictions. We demonstrate that protein language models benefit from retrieval enhancement in both structural and property prediction tasks, achieving a 5% improvement over MSA Transformer on average while being 373 times faster. Furthermore, our model exhibits superior transferability to new protein domains and outperforms MSA Transformer in de novo protein prediction. This study fills a much-encountered gap in protein prediction and brings us a step closer to demystifying the domain knowledge needed to understand protein sequences. Code is available at https://github.com/HKUNLP/RSA.",
            "link": "https://www.semanticscholar.org/paper/77e5d0a68afcffb27191572590deced60feb9d5d",
            "authors": "Chang Ma, Haiteng Zhao, Lin Zheng, Jiayi Xin, Qintong Li, Lijun Wu, Zhihong Deng, Yang Lu, Qi Liu, Lingpeng Kong",
            "EMNLP Paper ID": "199",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "38ca8bad77cb133576710acaf62be23da7f67258",
            "title": "CHIQ: Contextual History Enhancement for Improving Query Rewriting in Conversational Search",
            "abstract": "In this paper, we study how open-source large language models (LLMs) can be effectively deployed for improving query rewriting in conversational search, especially for ambiguous queries. We introduce CHIQ, a two-step method that leverages the capabilities of LLMs to resolve ambiguities in the conversation history before query rewriting. This approach contrasts with prior studies that predominantly use closed-source LLMs to directly generate search queries from conversation history. We demonstrate on five well-established benchmarks that CHIQ leads to state-of-the-art results across most settings, showing highly competitive performances with systems leveraging closed-source LLMs. Our study provides a first step towards leveraging open-source LLMs in conversational search, as a competitive alternative to the prevailing reliance on commercial LLMs. Data, models, and source code will be publicly available upon acceptance at https://github.com/fengranMark/CHIQ.",
            "link": "https://www.semanticscholar.org/paper/38ca8bad77cb133576710acaf62be23da7f67258",
            "authors": "Fengran Mo, Abbas Ghaddar, Kelong Mao, Mehdi Rezagholizadeh, Boxing Chen, Qun Liu, Jian-Yun Nie",
            "EMNLP Paper ID": "254",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "365f9c71eac3da18f42e6361de6746d17b30c081",
            "title": "Evaluating D-MERIT of Partial-annotation on Information Retrieval",
            "abstract": "Retrieval models are often evaluated on partially-annotated datasets. Each query is mapped to a few relevant texts and the remaining corpus is assumed to be irrelevant. As a result, models that successfully retrieve false negatives are punished in evaluation. Unfortunately, completely annotating all texts for every query is not resource efficient. In this work, we show that using partially-annotated datasets in evaluation can paint a distorted picture. We curate D-MERIT, a passage retrieval evaluation set from Wikipedia, aspiring to contain all relevant passages for each query. Queries describe a group (e.g.,\"journals about linguistics\") and relevant passages are evidence that entities belong to the group (e.g., a passage indicating that\"Language\"is a journal about linguistics). We show that evaluating on a dataset containing annotations for only a subset of the relevant passages might result in misleading ranking of the retrieval systems and that as more relevant texts are included in the evaluation set, the rankings converge. We propose our dataset as a resource for evaluation and our study as a recommendation for balance between resource-efficiency and reliable evaluation when annotating evaluation sets for text retrieval.",
            "link": "https://www.semanticscholar.org/paper/365f9c71eac3da18f42e6361de6746d17b30c081",
            "authors": "Royi Rassin, Yaron Fairstein, Oren Kalinsky, Guy Kushilevitz, Nachshon Cohen, Alexander Libov, Yoav Goldberg",
            "EMNLP Paper ID": "327",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "28f021fb7de72455788eb3455d0e9ab679cb67e6",
            "title": "Unifying Multimodal Retrieval via Document Screenshot Embedding",
            "abstract": "In the real world, documents are organized in different formats and varied modalities. Traditional retrieval pipelines require tailored document parsing techniques and content extraction modules to prepare input for indexing. This process is tedious, prone to errors, and has information loss. To this end, we propose Document Screenshot Embedding} (DSE), a novel retrieval paradigm that regards document screenshots as a unified input format, which does not require any content extraction preprocess and preserves all the information in a document (e.g., text, image and layout). DSE leverages a large vision-language model to directly encode document screenshots into dense representations for retrieval. To evaluate our method, we first craft the dataset of Wiki-SS, a 1.3M Wikipedia web page screenshots as the corpus to answer the questions from the Natural Questions dataset. In such a text-intensive document retrieval setting, DSE shows competitive effectiveness compared to other text retrieval methods relying on parsing. For example, DSE outperforms BM25 by 17 points in top-1 retrieval accuracy. Additionally, in a mixed-modality task of slide retrieval, DSE significantly outperforms OCR text retrieval methods by over 15 points in nDCG@10. These experiments show that DSE is an effective document retrieval paradigm for diverse types of documents. Model checkpoints, code, and Wiki-SS collection will be released.",
            "link": "https://www.semanticscholar.org/paper/28f021fb7de72455788eb3455d0e9ab679cb67e6",
            "authors": "Xueguang Ma, Sheng-Chieh Lin, Minghan Li, Wenhu Chen, Jimmy Lin",
            "EMNLP Paper ID": "728",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "984ec2da7e1544c9e4153503be0774ee66950d48",
            "title": "Taxonomy-guided Semantic Indexing for Academic Paper Search",
            "abstract": "Academic paper search is an essential task for efficient literature discovery and scientific advancement. While dense retrieval has advanced various ad-hoc searches, it often struggles to match the underlying academic concepts between queries and documents, which is critical for paper search. To enable effective academic concept matching for paper search, we propose Taxonomy-guided Semantic Indexing (TaxoIndex) framework. TaxoIndex extracts key concepts from papers and organizes them as a semantic index guided by an academic taxonomy, and then leverages this index as foundational knowledge to identify academic concepts and link queries and documents. As a plug-and-play framework, TaxoIndex can be flexibly employed to enhance existing dense retrievers. Extensive experiments show that TaxoIndex brings significant improvements, even with highly limited training data, and greatly enhances interpretability.",
            "link": "https://www.semanticscholar.org/paper/984ec2da7e1544c9e4153503be0774ee66950d48",
            "authors": "SeongKu Kang, Yunyi Zhang, Pengcheng Jiang, Dongha Lee, Jiawei Han, Hwanjo Yu",
            "EMNLP Paper ID": "801",
            "Oral/Poster": "Oral",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "b2f529919bb5c19c35c7c828516b03a258f2d3b0",
            "title": "AGRaME: Any-Granularity Ranking with Multi-Vector Embeddings",
            "abstract": "Ranking is a fundamental and popular problem in search. However, existing ranking algorithms usually restrict the granularity of ranking to full passages or require a specific dense index for each desired level of granularity. Such lack of flexibility in granularity negatively affects many applications that can benefit from more granular ranking, such as sentence-level ranking for open-domain question-answering, or proposition-level ranking for attribution. In this work, we introduce the idea of any-granularity ranking, which leverages multi-vector embeddings to rank at varying levels of granularity while maintaining encoding at a single (coarser) level of granularity. We propose a multi-granular contrastive loss for training multi-vector approaches, and validate its utility with both sentences and propositions as ranking units. Finally, we demonstrate the application of proposition-level ranking to post-hoc citation addition in retrieval-augmented generation, surpassing the performance of prompt-driven citation generation.",
            "link": "https://www.semanticscholar.org/paper/b2f529919bb5c19c35c7c828516b03a258f2d3b0",
            "authors": "R. Reddy, Omar Attia, Yunyao Li, Heng Ji, Saloni Potdar",
            "EMNLP Paper ID": "997",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "1741ba55146a8104b4dd19da25938f2fb6827b44",
            "title": "Assessing \"Implicit\" Retrieval Robustness of Large Language Models",
            "abstract": "Retrieval-augmented generation has gained popularity as a framework to enhance large language models with external knowledge. However, its effectiveness hinges on the retrieval robustness of the model. If the model lacks retrieval robustness, its performance is constrained by the accuracy of the retriever, resulting in significant compromises when the retrieved context is irrelevant. In this paper, we evaluate the\"implicit\"retrieval robustness of various large language models, instructing them to directly output the final answer without explicitly judging the relevance of the retrieved context. Our findings reveal that fine-tuning on a mix of gold and distracting context significantly enhances the model's robustness to retrieval inaccuracies, while still maintaining its ability to extract correct answers when retrieval is accurate. This suggests that large language models can implicitly handle relevant or irrelevant retrieved context by learning solely from the supervision of the final answer in an end-to-end manner. Introducing an additional process for explicit relevance judgment can be unnecessary and disrupts the end-to-end approach.",
            "link": "https://www.semanticscholar.org/paper/1741ba55146a8104b4dd19da25938f2fb6827b44",
            "authors": "Xiaoyu Shen, Rexhina Blloshmi, Dawei Zhu, Jiahuan Pei, Wei Zhang",
            "EMNLP Paper ID": "1024",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "03bd4cef88f02f7c0bdd87e36643b97fef4622f1",
            "title": "Enhancing Pre-Trained Generative Language Models with Question Attended Span Extraction on Machine Reading Comprehension",
            "abstract": "Machine Reading Comprehension (MRC) poses a significant challenge in the field of Natural Language Processing (NLP). While mainstream MRC methods predominantly leverage extractive strategies using encoder-only models such as BERT, generative approaches face the issue of out-of-control generation -- a critical problem where answers generated are often incorrect, irrelevant, or unfaithful to the source text. To address these limitations in generative models for MRC, we introduce the Question-Attended Span Extraction (QASE) module. Integrated during the fine-tuning phase of pre-trained generative language models (PLMs), QASE significantly enhances their performance, allowing them to surpass the extractive capabilities of advanced Large Language Models (LLMs) such as GPT-4 in few-shot settings. Notably, these gains in performance do not come with an increase in computational demands. The efficacy of the QASE module has been rigorously tested across various datasets, consistently achieving or even surpassing state-of-the-art (SOTA) results, thereby bridging the gap between generative and extractive models in extractive MRC tasks.",
            "link": "https://www.semanticscholar.org/paper/03bd4cef88f02f7c0bdd87e36643b97fef4622f1",
            "authors": "Lin Ai, Zheng Hui, Zizhou Liu, Julia Hirschberg",
            "EMNLP Paper ID": "1127",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "d1df556a0f0cf1090acf88b594e49e30a3cdd1f5",
            "title": "MixGR: Enhancing Retriever Generalization for Scientific Domain through Complementary Granularity",
            "abstract": "Recent studies show the growing significance of document retrieval in the generation of LLMs, i.e., RAG, within the scientific domain by bridging their knowledge gap. However, dense retrievers often struggle with domain-specific retrieval and complex query-document relationships, particularly when query segments correspond to various parts of a document. To alleviate such prevalent challenges, this paper introduces $\\texttt{MixGR}$, which improves dense retrievers' awareness of query-document matching across various levels of granularity in queries and documents using a zero-shot approach. $\\texttt{MixGR}$ fuses various metrics based on these granularities to a united score that reflects a comprehensive query-document similarity. Our experiments demonstrate that $\\texttt{MixGR}$ outperforms previous document retrieval by 24.7% and 9.8% on nDCG@5 with unsupervised and supervised retrievers, respectively, averaged on queries containing multiple subqueries from five scientific retrieval datasets. Moreover, the efficacy of two downstream scientific question-answering tasks highlights the advantage of $\\texttt{MixGR}$to boost the application of LLMs in the scientific domain.",
            "link": "https://www.semanticscholar.org/paper/d1df556a0f0cf1090acf88b594e49e30a3cdd1f5",
            "authors": "Fengyu Cai, Xinran Zhao, Tong Chen, Sihao Chen, Hongming Zhang, Iryna Gurevych, Heinz Koeppl",
            "EMNLP Paper ID": "1173",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "b7102e8534fec95fde5c6e5ceba278a75758b7cf",
            "title": "Improve Dense Passage Retrieval with Entailment Tuning",
            "abstract": "Retrieval module can be plugged into many downstream NLP tasks to improve their performance, such as open-domain question answering and retrieval-augmented generation. The key to a retrieval system is to calculate relevance scores to query and passage pairs. However, the definition of relevance is often ambiguous. We observed that a major class of relevance aligns with the concept of entailment in NLI tasks. Based on this observation, we designed a method called entailment tuning to improve the embedding of dense retrievers. Specifically, we unify the form of retrieval data and NLI data using existence claim as a bridge. Then, we train retrievers to predict the claims entailed in a passage with a variant task of masked prediction. Our method can be efficiently plugged into current dense retrieval methods, and experiments show the effectiveness of our method.",
            "link": "https://www.semanticscholar.org/paper/b7102e8534fec95fde5c6e5ceba278a75758b7cf",
            "authors": "Lu Dai, Hao Liu, Hui Xiong",
            "EMNLP Paper ID": "1327",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "fc080333b3a7e39d20dd362f2f9805014d531aaa",
            "title": "LitSearch: A Retrieval Benchmark for Scientific Literature Search",
            "abstract": "Literature search questions, such as\"Where can I find research on the evaluation of consistency in generated summaries?\"pose significant challenges for modern search engines and retrieval systems. These questions often require a deep understanding of research concepts and the ability to reason across entire articles. In this work, we introduce LitSearch, a retrieval benchmark comprising 597 realistic literature search queries about recent ML and NLP papers. LitSearch is constructed using a combination of (1) questions generated by GPT-4 based on paragraphs containing inline citations from research papers and (2) questions manually written by authors about their recently published papers. All LitSearch questions were manually examined or edited by experts to ensure high quality. We extensively benchmark state-of-the-art retrieval models and also evaluate two LLM-based reranking pipelines. We find a significant performance gap between BM25 and state-of-the-art dense retrievers, with a 24.8% absolute difference in recall@5. The LLM-based reranking strategies further improve the best-performing dense retriever by 4.4%. Additionally, commercial search engines and research tools like Google Search perform poorly on LitSearch, lagging behind the best dense retriever by up to 32 recall points. Taken together, these results show that LitSearch is an informative new testbed for retrieval systems while catering to a real-world use case.",
            "link": "https://www.semanticscholar.org/paper/fc080333b3a7e39d20dd362f2f9805014d531aaa",
            "authors": "Anirudh Ajith, Mengzhou Xia, Alexis Chevalier, Tanya Goyal, Danqi Chen, Tianyu Gao",
            "EMNLP Paper ID": "1753",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "cefc2ba177feb6f0d193b6026e5a29220fe7dbf5",
            "title": "Dense X Retrieval: What Retrieval Granularity Should We Use?",
            "abstract": "Dense retrieval has become a prominent method to obtain relevant context or world knowledge in open-domain NLP tasks. When we use a learned dense retriever on a retrieval corpus at inference time, an often-overlooked design choice is the retrieval unit in which the corpus is indexed, e.g. document, passage, or sentence. We discover that the retrieval unit choice significantly impacts the performance of both retrieval and downstream tasks. Distinct from the typical approach of using passages or sentences, we introduce a novel retrieval unit, proposition, for dense retrieval. Propositions are defined as atomic expressions within text, each encapsulating a distinct factoid and presented in a concise, self-contained natural language format. We conduct an empirical comparison of different retrieval granularity. Our experiments reveal that indexing a corpus by fine-grained units such as propositions significantly outperforms passage-level units in retrieval tasks. Moreover, constructing prompts with fine-grained retrieved units for retrieval-augmented language models improves the performance of downstream QA tasks given a specific computation budget.",
            "link": "https://www.semanticscholar.org/paper/cefc2ba177feb6f0d193b6026e5a29220fe7dbf5",
            "authors": "Tong Chen, Hongwei Wang, Sihao Chen, Wenhao Yu, Kaixin Ma, Xinran Zhao, Dong Yu, Hongming Zhang",
            "EMNLP Paper ID": "1766",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "cd4336a852099dceac7cf966521e711fdc3bfa1e",
            "title": "PairDistill: Pairwise Relevance Distillation for Dense Retrieval",
            "abstract": "Effective information retrieval (IR) from vast datasets relies on advanced techniques to extract relevant information in response to queries. Recent advancements in dense retrieval have showcased remarkable efficacy compared to traditional sparse retrieval methods. To further enhance retrieval performance, knowledge distillation techniques, often leveraging robust cross-encoder rerankers, have been extensively explored. However, existing approaches primarily distill knowledge from pointwise rerankers, which assign absolute relevance scores to documents, thus facing challenges related to inconsistent comparisons. This paper introduces Pairwise Relevance Distillation (PairDistill) to leverage pairwise reranking, offering fine-grained distinctions between similarly relevant documents to enrich the training of dense retrieval models. Our experiments demonstrate that PairDistill outperforms existing methods, achieving new state-of-the-art results across multiple benchmarks. This highlights the potential of PairDistill in advancing dense retrieval techniques effectively. Our source code and trained models are released at https://github.com/MiuLab/PairDistill",
            "link": "https://www.semanticscholar.org/paper/cd4336a852099dceac7cf966521e711fdc3bfa1e",
            "authors": "Chao-Wei Huang, Yun-Nung Chen",
            "EMNLP Paper ID": "2240",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "269fadb48a4a9908472779f810a1fa045ce3b912",
            "title": "Link, Synthesize, Retrieve: Universal Document Linking for Zero-Shot Information Retrieval",
            "abstract": "Despite the recent advancements in information retrieval (IR), zero-shot IR remains a significant challenge, especially when dealing with new domains, languages, and newly-released use cases that lack historical query traffic from existing users. For such cases, it is common to use query augmentations followed by fine-tuning pre-trained models on the document data paired with synthetic queries. In this work, we propose a novel Universal Document Linking (UDL) algorithm, which links similar documents to enhance synthetic query generation across multiple datasets with different characteristics. UDL leverages entropy for the choice of similarity models and named entity recognition (NER) for the link decision of documents using similarity scores. Our empirical studies demonstrate the effectiveness and universality of the UDL across diverse datasets and IR models, surpassing state-of-the-art methods in zero-shot cases. The developed code for reproducibility is included in https://github.com/eoduself/UDL",
            "link": "https://www.semanticscholar.org/paper/269fadb48a4a9908472779f810a1fa045ce3b912",
            "authors": "Dae Yon Hwang, Bilal Taha, Harshit Pande, Yaroslav Nechaev",
            "EMNLP Paper ID": "2384",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "89d06465e506314dcf4723ee9667be23d87e0ebe",
            "title": "CompAct: Compressing Retrieved Documents Actively for Question Answering",
            "abstract": "Retrieval-augmented generation supports language models to strengthen their factual groundings by providing external contexts. However, language models often face challenges when given extensive information, diminishing their effectiveness in solving questions. Context compression tackles this issue by filtering out irrelevant information, but current methods still struggle in realistic scenarios where crucial information cannot be captured with a single-step approach. To overcome this limitation, we introduce CompAct, a novel framework that employs an active strategy to condense extensive documents without losing key information. Our experiments demonstrate that CompAct brings significant improvements in both performance and compression rate on multi-hop question-answering benchmarks. CompAct flexibly operates as a cost-efficient plug-in module with various off-the-shelf retrievers or readers, achieving exceptionally high compression rates (47x).",
            "link": "https://www.semanticscholar.org/paper/89d06465e506314dcf4723ee9667be23d87e0ebe",
            "authors": "Chanwoong Yoon, Taewhoo Lee, Hyeon Hwang, Minbyul Jeong, Jaewoo Kang",
            "EMNLP Paper ID": "2945",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "d276a83b9bd5aaf75c6d921841959118e0d59870",
            "title": "BMRetriever: Tuning Large Language Models as Better Biomedical Text Retrievers",
            "abstract": "Developing effective biomedical retrieval models is important for excelling at knowledge-intensive biomedical tasks but still challenging due to the deficiency of sufficient publicly annotated biomedical data and computational resources. We present BMRetriever, a series of dense retrievers for enhancing biomedical retrieval via unsupervised pre-training on large biomedical corpora, followed by instruction fine-tuning on a combination of labeled datasets and synthetic pairs. Experiments on 5 biomedical tasks across 11 datasets verify BMRetriever's efficacy on various biomedical applications. BMRetriever also exhibits strong parameter efficiency, with the 410M variant outperforming baselines up to 11.7 times larger, and the 2B variant matching the performance of models with over 5B parameters. The training data and model checkpoints are released at \\url{https://huggingface.co/BMRetriever} to ensure transparency, reproducibility, and application to new domains.",
            "link": "https://www.semanticscholar.org/paper/d276a83b9bd5aaf75c6d921841959118e0d59870",
            "authors": "Ran Xu, Wenqi Shi, Yue Yu, Yuchen Zhuang, Yanqiao Zhu, M. D. Wang, Joyce C. Ho, Chao Zhang, Carl Yang",
            "EMNLP Paper ID": "3154",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "4bd090d1399c0ed872f1e41e963e43ac3f4872de",
            "title": "LumberChunker: Long-Form Narrative Document Segmentation",
            "abstract": "Modern NLP tasks increasingly rely on dense retrieval methods to access up-to-date and relevant contextual information. We are motivated by the premise that retrieval benefits from segments that can vary in size such that a content's semantic independence is better captured. We propose LumberChunker, a method leveraging an LLM to dynamically segment documents, which iteratively prompts the LLM to identify the point within a group of sequential passages where the content begins to shift. To evaluate our method, we introduce GutenQA, a benchmark with 3000\"needle in a haystack\"type of question-answer pairs derived from 100 public domain narrative books available on Project Gutenberg. Our experiments show that LumberChunker not only outperforms the most competitive baseline by 7.37% in retrieval performance (DCG@20) but also that, when integrated into a RAG pipeline, LumberChunker proves to be more effective than other chunking methods and competitive baselines, such as the Gemini 1.5M Pro. Our Code and Data are available at https://github.com/joaodsmarques/LumberChunker",
            "link": "https://www.semanticscholar.org/paper/4bd090d1399c0ed872f1e41e963e43ac3f4872de",
            "authors": "Andr\u00e9 V. Duarte, Joao Marques, Miguel Gra\u00e7a, M. Freire, Lei Li, Arlindo L. Oliveira",
            "matchScore": 225.44821,
            "original title": "LumberChunker: Long-Form Narrative Document Segmentation",
            "original authors": "Andr\u00e9 V. Duarte, Jo\u00e3o DS Marques, Miguel Gra\u00e7a, Miguel Freire, Lei Li, Arlindo L. Oliveira",
            "EMNLP Paper ID": "1320",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "bd0788f6908c0b3a5eb576e4f53999954391c9a7",
            "title": "Can't Remember Details in Long Documents? You Need Some R&R",
            "abstract": "Long-context large language models (LLMs) hold promise for tasks such as question-answering (QA) over long documents, but they tend to miss important information in the middle of context documents (arXiv:2307.03172v3). Here, we introduce $\\textit{R&R}$ -- a combination of two novel prompt-based methods called $\\textit{reprompting}$ and $\\textit{in-context retrieval}$ (ICR) -- to alleviate this effect in document-based QA. In reprompting, we repeat the prompt instructions periodically throughout the context document to remind the LLM of its original task. In ICR, rather than instructing the LLM to answer the question directly, we instruct it to retrieve the top $k$ passage numbers most relevant to the given question, which are then used as an abbreviated context in a second QA prompt. We test R&R with GPT-4 Turbo and Claude-2.1 on documents up to 80k tokens in length and observe a 16-point boost in QA accuracy on average. Our further analysis suggests that R&R improves performance on long document-based QA because it reduces the distance between relevant context and the instructions. Finally, we show that compared to short-context chunkwise methods, R&R enables the use of larger chunks that cost fewer LLM calls and output tokens, while minimizing the drop in accuracy.",
            "link": "https://www.semanticscholar.org/paper/bd0788f6908c0b3a5eb576e4f53999954391c9a7",
            "authors": "Devanshu Agrawal, Shang Gao, Martin Gajek",
            "matchScore": 289.86844,
            "original title": "Can\u2019t Remember Details in Long Documents? You Need Some R&R",
            "original authors": "Devanshu Agrawal, Shang Gao, Martin Gajek",
            "EMNLP Paper ID": "2487",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "3f925e9195fdc7ea99c5d8d957f41dd57159b867",
            "title": "Synthetic Multimodal Question Generation",
            "abstract": "Multimodal Retrieval Augmented Generation (MMRAG) is a powerful approach to question-answering over multimodal documents. A key challenge with evaluating MMRAG is the paucity of high-quality datasets matching the question styles and modalities of interest. In light of this, we propose SMMQG, a synthetic data generation framework. SMMQG leverages interplay between a retriever, large language model (LLM) and large multimodal model (LMM) to generate question and answer pairs directly from multimodal documents, with the questions conforming to specified styles and modalities. We use SMMQG to generate an MMRAG dataset of 1024 questions over Wikipedia documents and evaluate state-of-the-art models using it, revealing insights into model performance that are attainable only through style- and modality-specific evaluation data. Next, we measure the quality of data produced by SMMQG via a human study. We find that the quality of SMMQG-generated synthetic data is on par with the quality of the crowdsourced benchmark MMQA and that downstream evaluation results using both datasets strongly concur.",
            "link": "https://www.semanticscholar.org/paper/3f925e9195fdc7ea99c5d8d957f41dd57159b867",
            "authors": "Ian Wu, Sravan Jayanthi, Vijay Viswanathan, Simon Rosenberg, Sina Pakazad, Tongshuang Wu, Graham Neubig",
            "matchScore": 146.62613,
            "original title": "Synthetic Multimodal Question Generation",
            "original authors": "Ian Wu, Sravan Jayanthi, Vijay Viswanathan, Simon Rosenberg, Sina Khoshfetrat Pakazad, Tongshuang Wu, Graham Neubig",
            "EMNLP Paper ID": "2542",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "fe21b0db9233e921d2e4073fd80baeca124b265a",
            "title": "Dense Passage Retrieval: Is it Retrieving?",
            "abstract": "Dense passage retrieval (DPR) is the first step in the retrieval augmented generation (RAG) paradigm for improving the performance of large language models (LLM). DPR fine-tunes pre-trained networks to enhance the alignment of the embeddings between queries and relevant textual data. A deeper understanding of DPR fine-tuning will be required to fundamentally unlock the full potential of this approach. In this work, we explore DPR-trained models mechanistically by using a combination of probing, layer activation analysis, and model editing. Our experiments show that DPR training decentralizes how knowledge is stored in the network, creating multiple access pathways to the same information. We also uncover a limitation in this training style: the internal knowledge of the pre-trained model bounds what the retrieval model can retrieve. These findings suggest a few possible directions for dense retrieval: (1) expose the DPR training process to more knowledge so more can be decentralized, (2) inject facts as decentralized representations, (3) model and incorporate knowledge uncertainty in the retrieval process, and (4) directly map internal model knowledge to a knowledge base.",
            "link": "https://www.semanticscholar.org/paper/fe21b0db9233e921d2e4073fd80baeca124b265a",
            "authors": "Benjamin Z. Reichman, Larry Heck",
            "matchScore": 166.92432,
            "original title": "Dense Passage Retrieval: Is it Retrieving?",
            "original authors": "Benjamin Reichman, Larry Heck",
            "EMNLP Paper ID": "2629",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "6b810cc2c6c151882e3be65777567740efe6cf05",
            "title": "Exploring the Best Practices of Query Expansion with Large Language Models",
            "abstract": "Large Language Models (LLMs) are foundational in language technologies, particularly in information retrieval (IR). Previous studies have utilized LLMs for query expansion, achieving notable improvements in IR. In this paper, we thoroughly explore the best practice of leveraging LLMs for query expansion. To this end, we introduce a training-free, straightforward yet effective framework called Multi-Text Generation Integration (\\textsc{MuGI}). It leverages LLMs to generate multiple pseudo-references, integrating them with queries to enhance both sparse and dense retrievers. Our empirical findings reveal that: (1) Increasing the number of samples from LLMs benefits IR systems; (2) A balance between the query and pseudo-documents, and an effective integration strategy, is critical for high performance; (3) Contextual information from LLMs is essential, even boost a 23M model to outperform a 7B baseline model; (4) Pseudo relevance feedback can further calibrate queries for improved performance; and (5) Query expansion is widely applicable and versatile, consistently enhancing models ranging from 23M to 7B parameters. Our code and all generated references are made available at \\url{https://github.com/lezhang7/Retrieval_MuGI}",
            "link": "https://www.semanticscholar.org/paper/6b810cc2c6c151882e3be65777567740efe6cf05",
            "authors": "Le Zhang, Yihong Wu, Qian Yang, Jian-Yun Nie",
            "matchScore": 215.94872,
            "original title": "Exploring the Best Practices of Query Expansion with Large Language Models",
            "original authors": "Le Zhang, Yihong Wu, Qian Yang, Jian-Yun Nie",
            "EMNLP Paper ID": "375",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "1fbbaa475e93c2d851344c5181a0e39a99324aca",
            "title": "Multi-view Content-aware Indexing for Long Document Retrieval",
            "abstract": "Long document question answering (DocQA) aims to answer questions from long documents over 10k words. They usually contain content structures such as sections, sub-sections, and paragraph demarcations. However, the indexing methods of long documents remain under-explored, while existing systems generally employ fixed-length chunking. As they do not consider content structures, the resultant chunks can exclude vital information or include irrelevant content. Motivated by this, we propose the Multi-view Content-aware indexing (MC-indexing) for more effective long DocQA via (i) segment structured document into content chunks, and (ii) represent each content chunk in raw-text, keywords, and summary views. We highlight that MC-indexing requires neither training nor fine-tuning. Having plug-and-play capability, it can be seamlessly integrated with any retrievers to boost their performance. Besides, we propose a long DocQA dataset that includes not only question-answer pair, but also document structure and answer scope. When compared to state-of-art chunking schemes, MC-indexing has significantly increased the recall by 42.8%, 30.0%, 23.9%, and 16.3% via top k= 1.5, 3, 5, and 10 respectively. These improved scores are the average of 8 widely used retrievers (2 sparse and 6 dense) via extensive experiments.",
            "link": "https://www.semanticscholar.org/paper/1fbbaa475e93c2d851344c5181a0e39a99324aca",
            "authors": "Kuicai Dong, Derrick-Goh-Xin Deik, Yi Quan Lee, Hao Zhang, Xiangyang Li, Cong Zhang, Yong Liu",
            "matchScore": 234.93047,
            "original title": "Multi-view Content-aware Indexing for Long Document Retrieval",
            "original authors": "Kuicai Dong, Derrick Goh Xin Deik, Yi Quan Lee, Hao Zhang, Xiangyang Li, Cong Zhang, Yong Liu",
            "EMNLP Paper ID": "548",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "2611643edb8bd615ac4f5a9d84df822d1312b0ea",
            "title": "QRMeM: Unleash the Length Limitation through Question then Reflection Memory Mechanism",
            "abstract": "While large language models (LLMs) have made notable advancements in natural language processing, they continue to struggle with processing extensive text. Memory mechanism offers a flexible solution for managing long contexts, utilizing techniques such as compression, summarization, and structuring to facilitate nuanced and efficient handling of large volumes of text. However, existing techniques face challenges with static knowledge integration, leading to insufficient adaptation to task-specific needs and missing multi-segmentation relationships, which hinders the dynamic reorganization and logical combination of relevant segments during the response process. To address these issues, we introduce a novel strategy, Question then Reflection Memory Mechanism (QRMeM), incorporating a dual-structured memory pool. This pool synergizes static textual content with structured graph guidance, fostering a reflective trial-and-error approach for navigating and identifying relevant segments. Our evaluation across multiple-choice questions (MCQ) and multi-document question answering (Multi-doc QA) benchmarks showcases QRMeM enhanced performance compared to existing approaches.",
            "link": "https://www.semanticscholar.org/paper/2611643edb8bd615ac4f5a9d84df822d1312b0ea",
            "authors": "Bo Wang, Heyan Huang, Yixin Cao, Jiahao Ying, Wei Tang, Chong Feng",
            "matchScore": 312.62537,
            "original title": "QRMeM: Unleash the Length Limitation through Question then Reflection Memory Mechanism",
            "original authors": "Bo Wang, Heyan Huang, Yixin Cao, Jiahao Ying, Wei Tang, Chong Feng",
            "EMNLP Paper ID": "946",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "4afdd7b0dbd31781727dd318ba30b68e7bf153d0",
            "title": "Context-Driven Index Trimming: A Data Quality Perspective to Enhancing Precision of RALMs",
            "abstract": "Retrieval-Augmented Large Language Models (RALMs) have made significant strides in enhancing the accuracy of generated responses.However, existing research often overlooks the data quality issues within retrieval results, often caused by inaccurate existing vector-distance-based retrieval methods.We propose to boost the precision of RALMs' answers from a data quality perspective through the Context-Driven Index Trimming (CDIT) framework, where Context Matching Dependencies (CMDs) are employed as logical data quality rules to capture and regulate the consistency between retrieved contexts.Based on the semantic comprehension capabilities of Large Language Models (LLMs), CDIT can effectively identify and discard retrieval results that are inconsistent with the query context and further modify indexes in the database, thereby improving answer quality.Experiments demonstrate on challenging question-answering tasks.Also, the flexibility of CDIT is verified through its compatibility with various language models and indexing methods, which offers a promising approach to bolster RALMs' data quality and retrieval precision jointly.",
            "link": "https://www.semanticscholar.org/paper/4afdd7b0dbd31781727dd318ba30b68e7bf153d0",
            "authors": "Kexin Ma, Ruochun Jin, Xi Wang, Huan Chen, Jing Ren, Yuhua Tang",
            "matchScore": 295.47662,
            "original title": "Context-Driven Index Trimming: A Data Quality Perspective to Enhancing Precision of RALMs",
            "original authors": "Kexin Ma, Ruochun Jin, Wang Haotian, Wang Xi, Huan Chen, Yuhua Tang, Qian Wang",
            "EMNLP Paper ID": "958",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Findings"
        }
    ],
    "Privacy and Security in Large Language Models and NLP": [
        {
            "paperId": "acc79fcf42b64234c3aa48ba7eb4b21cd9271f51",
            "title": "GoldCoin: Grounding Large Language Models in Privacy Laws via Contextual Integrity Theory",
            "abstract": "Privacy issues arise prominently during the inappropriate transmission of information between entities. Existing research primarily studies privacy by exploring various privacy attacks, defenses, and evaluations within narrowly predefined patterns, while neglecting that privacy is not an isolated, context-free concept limited to traditionally sensitive data (e.g., social security numbers), but intertwined with intricate social contexts that complicate the identification and analysis of potential privacy violations. The advent of Large Language Models (LLMs) offers unprecedented opportunities for incorporating the nuanced scenarios outlined in privacy laws to tackle these complex privacy issues. However, the scarcity of open-source relevant case studies restricts the efficiency of LLMs in aligning with specific legal statutes. To address this challenge, we introduce a novel framework, GoldCoin, designed to efficiently ground LLMs in privacy laws for judicial assessing privacy violations. Our framework leverages the theory of contextual integrity as a bridge, creating numerous synthetic scenarios grounded in relevant privacy statutes (e.g., HIPAA), to assist LLMs in comprehending the complex contexts for identifying privacy risks in the real world. Extensive experimental results demonstrate that GoldCoin markedly enhances LLMs' capabilities in recognizing privacy risks across real court cases, surpassing the baselines on different judicial tasks.",
            "link": "https://www.semanticscholar.org/paper/acc79fcf42b64234c3aa48ba7eb4b21cd9271f51",
            "authors": "Wei Fan, Haoran Li, Zheye Deng, Weiqi Wang, Yangqiu Song",
            "EMNLP Paper ID": "381",
            "Oral/Poster": "Oral",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "1dc9b426c711a5085517555fa3b3ff937dd04b7b",
            "title": "C3PA: An Open Dataset of Expert-Annotated and Regulation-Aware Privacy Policies to Enable Scalable Regulatory Compliance Audits",
            "abstract": "The development of tools and techniques to analyze and extract organizations data habits from privacy policies are critical for scalable regulatory compliance audits. Unfortunately, these tools are becoming increasingly limited in their ability to identify compliance issues and fixes. After all, most were developed using regulation-agnostic datasets of annotated privacy policies obtained from a time before the introduction of landmark privacy regulations such as EUs GDPR and Californias CCPA. In this paper, we describe the first open regulation-aware dataset of expert-annotated privacy policies, C3PA (CCPA Privacy Policy Provision Annotations), aimed to address this challenge. C3PA contains over 48K expert-labeled privacy policy text segments associated with responses to CCPA-specific disclosure mandates from 411 unique organizations. We demonstrate that the C3PA dataset is uniquely suited for aiding automated audits of compliance with CCPA-related disclosure mandates.",
            "link": "https://www.semanticscholar.org/paper/1dc9b426c711a5085517555fa3b3ff937dd04b7b",
            "authors": "Maaz Bin Musa, Steven M. Winston, Garrison Allen, Jacob Schiller, Kevin Moore, Sean Quick, Johnathan Melvin, Padmini Srinivasan, Mihailis E. Diamantis, Rishab Nithyanand",
            "EMNLP Paper ID": "417",
            "Oral/Poster": "Oral",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "9db2805adb108c5d825f8479f3a2299e0385c53a",
            "title": "Private Language Models via Truncated Laplacian Mechanism",
            "abstract": "Deep learning models for NLP tasks are prone to variants of privacy attacks. To prevent privacy leakage, researchers have investigated word-level perturbations, relying on the formal guarantees of differential privacy (DP) in the embedding space. However, many existing approaches either achieve unsatisfactory performance in the high privacy regime when using the Laplacian or Gaussian mechanism, or resort to weaker relaxations of DP that are inferior to the canonical DP in terms of privacy strength. This raises the question of whether a new method for private word embedding can be designed to overcome these limitations. In this paper, we propose a novel private embedding method called the high dimensional truncated Laplacian mechanism. Specifically, we introduce a non-trivial extension of the truncated Laplacian mechanism, which was previously only investigated in one-dimensional space cases. Theoretically, we show that our method has a lower variance compared to the previous private word embedding methods. To further validate its effectiveness, we conduct comprehensive experiments on private embedding and downstream tasks using three datasets. Remarkably, even in the high privacy regime, our approach only incurs a slight decrease in utility compared to the non-private scenario.",
            "link": "https://www.semanticscholar.org/paper/9db2805adb108c5d825f8479f3a2299e0385c53a",
            "authors": "Tianhao Huang, Tao Yang, Ivan Habernal, Lijie Hu, Di Wang",
            "EMNLP Paper ID": "448",
            "Oral/Poster": "Oral",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "3b57a81af6865edbb8aea7919c667530946cb69c",
            "title": "Order of Magnitude Speedups for LLM Membership Inference",
            "abstract": "Large Language Models (LLMs) have the promise to revolutionize computing broadly, but their complexity and extensive training data also expose significant privacy vulnerabilities. One of the simplest privacy risks associated with LLMs is their susceptibility to membership inference attacks (MIAs), wherein an adversary aims to determine whether a specific data point was part of the model's training set. Although this is a known risk, state of the art methodologies for MIAs rely on training multiple computationally costly shadow models, making risk evaluation prohibitive for large models. Here we adapt a recent line of work which uses quantile regression to mount membership inference attacks; we extend this work by proposing a low-cost MIA that leverages an ensemble of small quantile regression models to determine if a document belongs to the model's training set or not. We demonstrate the effectiveness of this approach on fine-tuned LLMs of varying families (OPT, Pythia, Llama) and across multiple datasets. Across all scenarios we obtain comparable or improved accuracy compared to state of the art shadow model approaches, with as little as 6% of their computation budget. We demonstrate increased effectiveness across multi-epoch trained target models, and architecture miss-specification robustness, that is, we can mount an effective attack against a model using a different tokenizer and architecture, without requiring knowledge on the target model.",
            "link": "https://www.semanticscholar.org/paper/3b57a81af6865edbb8aea7919c667530946cb69c",
            "authors": "Rongting Zhang, Martin Bertran, Aaron Roth",
            "EMNLP Paper ID": "482",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "695553bab1369982f1849f96678b1f70498ba749",
            "title": "IDEAW: Robust Neural Audio Watermarking with Invertible Dual-Embedding",
            "abstract": "The audio watermarking technique embeds messages into audio and accurately extracts messages from the watermarked audio. Traditional methods develop algorithms based on expert experience to embed watermarks into the time-domain or transform-domain of signals. With the development of deep neural networks, deep learning-based neural audio watermarking has emerged. Compared to traditional algorithms, neural audio watermarking achieves better robustness by considering various attacks during training. However, current neural watermarking methods suffer from low capacity and unsatisfactory imperceptibility. Additionally, the issue of watermark locating, which is extremely important and even more pronounced in neural audio watermarking, has not been adequately studied. In this paper, we design a dual-embedding watermarking model for efficient locating. We also consider the impact of the attack layer on the invertible neural network in robustness training, improving the model to enhance both its reasonableness and stability. Experiments show that the proposed model, IDEAW, can withstand various attacks with higher capacity and more efficient locating ability compared to existing methods.",
            "link": "https://www.semanticscholar.org/paper/695553bab1369982f1849f96678b1f70498ba749",
            "authors": "Pengcheng Li, Xulong Zhang, Jing Xiao, Jianzong Wang",
            "EMNLP Paper ID": "488",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "7ad594b57f16d92bed211442275d846e38ee8735",
            "title": "Seeing the Forest through the Trees: Data Leakage from Partial Transformer Gradients",
            "abstract": "Recent studies have shown that distributed machine learning is vulnerable to gradient inversion attacks, where private training data can be reconstructed by analyzing the gradients of the models shared in training. Previous attacks established that such reconstructions are possible using gradients from all parameters in the entire models. However, we hypothesize that most of the involved modules, or even their sub-modules, are at risk of training data leakage, and we validate such vulnerabilities in various intermediate layers of language models. Our extensive experiments reveal that gradients from a single Transformer layer, or even a single linear component with 0.54% parameters, are susceptible to training data leakage. Additionally, we show that applying differential privacy on gradients during training offers limited protection against the novel vulnerability of data disclosure.",
            "link": "https://www.semanticscholar.org/paper/7ad594b57f16d92bed211442275d846e38ee8735",
            "authors": "Weijun Li, Qiongkai Xu, M. Dras",
            "EMNLP Paper ID": "524",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "8699ca2562722a986ffaea99cc6c94b39567f97b",
            "title": "Safely Learning with Private Data: A Federated Learning Framework for Large Language Model",
            "abstract": "Private data, being larger and quality-higher than public data, can greatly improve large language models (LLM). However, due to privacy concerns, this data is often dispersed in multiple silos, making its secure utilization for LLM training a challenge. Federated learning (FL) is an ideal solution for training models with distributed private data, but traditional frameworks like FedAvg are unsuitable for LLM due to their high computational demands on clients. An alternative, split learning, offloads most training parameters to the server while training embedding and output layers locally, making it more suitable for LLM. Nonetheless, it faces significant challenges in security and efficiency. Firstly, the gradients of embeddings are prone to attacks, leading to potential reverse engineering of private data. Furthermore, the server's limitation of handle only one client's training request at a time hinders parallel training, severely impacting training efficiency. In this paper, we propose a Federated Learning framework for LLM, named FL-GLM, which prevents data leakage caused by both server-side and peer-client attacks while improving training efficiency. Specifically, we first place the input block and output block on local client to prevent embedding gradient attacks from server. Secondly, we employ key-encryption during client-server communication to prevent reverse engineering attacks from peer-clients. Lastly, we employ optimization methods like client-batching or server-hierarchical, adopting different acceleration methods based on the actual computational capabilities of the server. Experimental results on NLU and generation tasks demonstrate that FL-GLM achieves comparable metrics to centralized chatGLM model, validating the effectiveness of our federated learning framework.",
            "link": "https://www.semanticscholar.org/paper/8699ca2562722a986ffaea99cc6c94b39567f97b",
            "authors": "JiaYing Zheng, Hainan Zhang, LingXiang Wang, Wangjie Qiu, Hongwei Zheng, Zhiming Zheng",
            "EMNLP Paper ID": "584",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "e0c199318c191bf6dbb46c2a9a425e99e9c977db",
            "title": "Thinking Outside of the Differential Privacy Box: A Case Study in Text Privatization with Language Model Prompting",
            "abstract": "The field of privacy-preserving Natural Language Processing has risen in popularity, particularly at a time when concerns about privacy grow with the proliferation of Large Language Models. One solution consistently appearing in recent literature has been the integration of Differential Privacy (DP) into NLP techniques. In this paper, we take these approaches into critical view, discussing the restrictions that DP integration imposes, as well as bring to light the challenges that such restrictions entail. To accomplish this, we focus on $\\textbf{DP-Prompt}$, a recent method for text privatization leveraging language models to rewrite texts. In particular, we explore this rewriting task in multiple scenarios, both with DP and without DP. To drive the discussion on the merits of DP in NLP, we conduct empirical utility and privacy experiments. Our results demonstrate the need for more discussion on the usability of DP in NLP and its benefits over non-DP approaches.",
            "link": "https://www.semanticscholar.org/paper/e0c199318c191bf6dbb46c2a9a425e99e9c977db",
            "authors": "Stephen Meisenbacher, Florian Matthes",
            "EMNLP Paper ID": "629",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "fea60f027ebb25372b2cf5dab2f2bd4bca96a75b",
            "title": "Reconstruct Your Previous Conversations! Comprehensively Investigating Privacy Leakage Risks in Conversations with GPT Models",
            "abstract": "Significant advancements have recently been made in large language models represented by GPT models. Users frequently have multi-round private conversations with cloud-hosted GPT models for task optimization. Yet, this operational paradigm introduces additional attack surfaces, particularly in custom GPTs and hijacked chat sessions. In this paper, we introduce a straightforward yet potent Conversation Reconstruction Attack. This attack targets the contents of previous conversations between GPT models and benign users, i.e., the benign users' input contents during their interaction with GPT models. The adversary could induce GPT models to leak such contents by querying them with designed malicious prompts. Our comprehensive examination of privacy risks during the interactions with GPT models under this attack reveals GPT-4's considerable resilience. We present two advanced attacks targeting improved reconstruction of past conversations, demonstrating significant privacy leakage across all models under these advanced techniques. Evaluating various defense mechanisms, we find them ineffective against these attacks. Our findings highlight the ease with which privacy can be compromised in interactions with GPT models, urging the community to safeguard against potential abuses of these models' capabilities.",
            "link": "https://www.semanticscholar.org/paper/fea60f027ebb25372b2cf5dab2f2bd4bca96a75b",
            "authors": "Junjie Chu, Zeyang Sha, Michael Backes, Yang Zhang",
            "EMNLP Paper ID": "734",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "8959b361ab4985de1116a9a271e3b6041e1efc02",
            "title": "Taylor Unswift: Secured Weight Release for Large Language Models via Taylor Expansion",
            "abstract": "Ensuring the security of released large language models (LLMs) poses a significant dilemma, as existing mechanisms either compromise ownership rights or raise data privacy concerns. To address this dilemma, we introduce TaylorMLP to protect the ownership of released LLMs and prevent their abuse. Specifically, TaylorMLP preserves the ownership of LLMs by transforming the weights of LLMs into parameters of Taylor-series. Instead of releasing the original weights, developers can release the Taylor-series parameters with users, thereby ensuring the security of LLMs. Moreover, TaylorMLP can prevent abuse of LLMs by adjusting the generation speed. It can induce low-speed token generation for the protected LLMs by increasing the terms in the Taylor-series. This intentional delay helps LLM developers prevent potential large-scale unauthorized uses of their models. Empirical experiments across five datasets and three LLM architectures demonstrate that TaylorMLP induces over 4x increase in latency, producing the tokens precisely matched with original LLMs. Subsequent defensive experiments further confirm that TaylorMLP effectively prevents users from reconstructing the weight values based on downstream datasets.",
            "link": "https://www.semanticscholar.org/paper/8959b361ab4985de1116a9a271e3b6041e1efc02",
            "authors": "Guanchu Wang, Yu-Neng Chuang, Ruixiang Tang, Shaochen Zhong, Jiayi Yuan, Hongye Jin, Zirui Liu, V. Chaudhary, Shuai Xu, James Caverlee, Xia Hu",
            "EMNLP Paper ID": "778",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "c06d80dbce7b8dc34e34f938aca0de0d5cb54fcf",
            "title": "KnowledgeSG: Privacy-Preserving Synthetic Text Generation with Knowledge Distillation from Server",
            "abstract": "The success of large language models (LLMs) facilitate many parties to fine-tune LLMs on their own private data. However, this practice raises privacy concerns due to the memorization of LLMs. Existing solutions, such as utilizing synthetic data for substitution, struggle to simultaneously improve performance and preserve privacy. They either rely on a local model for generation, resulting in a performance decline, or take advantage of APIs, directly exposing the data to API servers. To address this issue, we propose KnowledgeSG, a novel client-server framework which enhances synthetic data quality and improves model performance while ensuring privacy. We achieve this by learning local knowledge from the private data with differential privacy (DP) and distilling professional knowledge from the server. Additionally, inspired by federated learning, we transmit models rather than data between the client and server to prevent privacy leakage. Extensive experiments in medical and financial domains demonstrate the effectiveness of KnowledgeSG. Our code is now publicly available at https://github.com/wwh0411/KnowledgeSG.",
            "link": "https://www.semanticscholar.org/paper/c06d80dbce7b8dc34e34f938aca0de0d5cb54fcf",
            "authors": "Wenhao Wang, Xiaoyu Liang, Rui Ye, Jingyi Chai, Siheng Chen, Yanfeng Wang",
            "EMNLP Paper ID": "872",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "2237d4ef1aaaf411b4e3d9dc04a4d5ac1df4b11d",
            "title": "ReCaLL: Membership Inference via Relative Conditional Log-Likelihoods",
            "abstract": "The rapid scaling of large language models (LLMs) has raised concerns about the transparency and fair use of the pretraining data used for training them. Detecting such content is challenging due to the scale of the data and limited exposure of each instance during training. We propose ReCaLL (Relative Conditional Log-Likelihood), a novel membership inference attack (MIA) to detect LLMs' pretraining data by leveraging their conditional language modeling capabilities. ReCaLL examines the relative change in conditional log-likelihoods when prefixing target data points with non-member context. Our empirical findings show that conditioning member data on non-member prefixes induces a larger decrease in log-likelihood compared to non-member data. We conduct comprehensive experiments and show that ReCaLL achieves state-of-the-art performance on the WikiMIA dataset, even with random and synthetic prefixes, and can be further improved using an ensemble approach. Moreover, we conduct an in-depth analysis of LLMs' behavior with different membership contexts, providing insights into how LLMs leverage membership information for effective inference at both the sequence and token level.",
            "link": "https://www.semanticscholar.org/paper/2237d4ef1aaaf411b4e3d9dc04a4d5ac1df4b11d",
            "authors": "Roy Xie, Junlin Wang, Ruomin Huang, Minxing Zhang, Rong Ge, Jian Pei, Neil Zhenqiang Gong, Bhuwan Dhingra",
            "EMNLP Paper ID": "1001",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "b164a92dada02fce401f578f1c2442fcd46c8ebc",
            "title": "PostMark: A Robust Blackbox Watermark for Large Language Models",
            "abstract": "The most effective techniques to detect LLM-generated text rely on inserting a detectable signature -- or watermark -- during the model's decoding process. Most existing watermarking methods require access to the underlying LLM's logits, which LLM API providers are loath to share due to fears of model distillation. As such, these watermarks must be implemented independently by each LLM provider. In this paper, we develop PostMark, a modular post-hoc watermarking procedure in which an input-dependent set of words (determined via a semantic embedding) is inserted into the text after the decoding process has completed. Critically, PostMark does not require logit access, which means it can be implemented by a third party. We also show that PostMark is more robust to paraphrasing attacks than existing watermarking methods: our experiments cover eight baseline algorithms, five base LLMs, and three datasets. Finally, we evaluate the impact of PostMark on text quality using both automated and human assessments, highlighting the trade-off between quality and robustness to paraphrasing. We release our code, outputs, and annotations at https://github.com/lilakk/PostMark.",
            "link": "https://www.semanticscholar.org/paper/b164a92dada02fce401f578f1c2442fcd46c8ebc",
            "authors": "Yapei Chang, Kalpesh Krishna, Amir Houmansadr, J. Wieting, Mohit Iyyer",
            "EMNLP Paper ID": "1023",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "4882d6dd815226b830ad143be471555876be9df4",
            "title": "Where Am I From? Identifying Origin of LLM-generated Content",
            "abstract": "Generative models, particularly large language models (LLMs), have achieved remarkable success in producing natural and high-quality content. However, their widespread adoption raises concerns regarding copyright infringement, privacy violations, and security risks associated with AI-generated content. To address these concerns, we propose a novel digital forensics framework for LLMs, enabling the tracing of AI-generated content back to its source. This framework embeds a secret watermark directly into the generated output, eliminating the need for model retraining. To enhance traceability, especially for short outputs, we introduce a \"depth watermark\" that strengthens the link be-tween content and generator. Our approach ensures accurate tracing while maintaining the quality of the generated content. Extensive experiments across various settings and datasets validate the effectiveness and robustness of our proposed framework.",
            "link": "https://www.semanticscholar.org/paper/4882d6dd815226b830ad143be471555876be9df4",
            "authors": "Liying Li, Yihan Bai, Minhao Cheng",
            "EMNLP Paper ID": "1425",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "47ce51b8c156883bc2ec8cf6e68905eb5d081230",
            "title": "PrivacyMind: Large Language Models Can Be Contextual Privacy Protection Learners",
            "abstract": "The proliferation of Large Language Models (LLMs) has driven considerable interest in fine-tuning them with domain-specific data to create specialized language models. Nevertheless, such domain-specific fine-tuning data often contains contextually sensitive personally identifiable information (PII). Direct fine-tuning of LLMs on this data without privacy protection poses a risk of data leakage of sensitive PII during inference time. To address this challenge, we introduce Contextual Privacy Protection Language Models (PrivacyMind), a novel paradigm for fine-tuning LLMs that effectively injects domain-specific knowledge while safeguarding inference-time data privacy. Our work offers a theoretical analysis for model design and benchmarks various techniques such as corpus curation, penalty-based unlikelihood in training loss, instruction-based tuning, etc. Extensive experiments across diverse datasets and scenarios demonstrate the effectiveness of our approaches. In particular, instruction tuning with both positive and negative examples stands out as a promising method, effectively protecting private data while enhancing the model's knowledge. Our work underscores the potential for Large Language Models as robust contextual privacy protection learners. The complete code and data for the work can be found at https://github.com/Yijia-Xiao/PrivacyMind.",
            "link": "https://www.semanticscholar.org/paper/47ce51b8c156883bc2ec8cf6e68905eb5d081230",
            "authors": "Yijia Xiao, Yiqiao Jin, Yushi Bai, Yue Wu, Xianjun Yang, Xiao Luo, Wenchao Yu, Xujiang Zhao, Yanchi Liu, Haifeng Chen, Wei Wang, Wei Cheng",
            "EMNLP Paper ID": "1637",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "d8d99c00c82206aebd5cf9f752c63bcfa266b488",
            "title": "Granular Privacy Control for Geolocation with Vision Language Models",
            "abstract": "Vision Language Models (VLMs) are rapidly advancing in their capability to answer information-seeking questions. As these models are widely deployed in consumer applications, they could lead to new privacy risks due to emergent abilities to identify people in photos, geolocate images, etc. As we demonstrate, somewhat surprisingly, current open-source and proprietary VLMs are very capable image geolocators, making widespread geolocation with VLMs an immediate privacy risk, rather than merely a theoretical future concern. As a first step to address this challenge, we develop a new benchmark, GPTGeoChat, to test the ability of VLMs to moderate geolocation dialogues with users. We collect a set of 1,000 image geolocation conversations between in-house annotators and GPT-4v, which are annotated with the granularity of location information revealed at each turn. Using this new dataset, we evaluate the ability of various VLMs to moderate GPT-4v geolocation conversations by determining when too much location information has been revealed. We find that custom fine-tuned models perform on par with prompted API-based models when identifying leaked location information at the country or city level; however, fine-tuning on supervised data appears to be needed to accurately moderate finer granularities, such as the name of a restaurant or building.",
            "link": "https://www.semanticscholar.org/paper/d8d99c00c82206aebd5cf9f752c63bcfa266b488",
            "authors": "Ethan Mendes, Yang Chen, James Hays, Sauvik Das, Wei Xu, Alan Ritter",
            "EMNLP Paper ID": "2042",
            "Oral/Poster": "Oral",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "aa853d6ddf598266a4c48647dae7b0a8e8a8ff49",
            "title": "User Inference Attacks on Large Language Models",
            "abstract": "Fine-tuning is a common and effective method for tailoring large language models (LLMs) to specialized tasks and applications. In this paper, we study the privacy implications of fine-tuning LLMs on user data. To this end, we consider a realistic threat model, called user inference, wherein an attacker infers whether or not a user's data was used for fine-tuning. We design attacks for performing user inference that require only black-box access to the fine-tuned LLM and a few samples from a user which need not be from the fine-tuning dataset. We find that LLMs are susceptible to user inference across a variety of fine-tuning datasets, at times with near perfect attack success rates. Further, we theoretically and empirically investigate the properties that make users vulnerable to user inference, finding that outlier users, users with identifiable shared features between examples, and users that contribute a large fraction of the fine-tuning data are most susceptible to attack. Based on these findings, we identify several methods for mitigating user inference including training with example-level differential privacy, removing within-user duplicate examples, and reducing a user's contribution to the training data. While these techniques provide partial mitigation of user inference, we highlight the need to develop methods to fully protect fine-tuned LLMs against this privacy risk.",
            "link": "https://www.semanticscholar.org/paper/aa853d6ddf598266a4c48647dae7b0a8e8a8ff49",
            "authors": "Nikhil Kandpal, Krishna Pillutla, Alina Oprea, P. Kairouz, Christopher A. Choquette-Choo, Zheng Xu",
            "EMNLP Paper ID": "2253",
            "Oral/Poster": "Oral",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "b4e4a90601986a3fd13130aee961450328802164",
            "title": "Waterfall: Framework for Robust and Scalable Text Watermarking",
            "abstract": "Protecting intellectual property (IP) of text such as articles and code is increasingly important, especially as sophisticated attacks become possible, such as paraphrasing by large language models (LLMs) or even unauthorized training of LLMs on copyrighted text to infringe such IP. However, existing text watermarking methods are not robust enough against such attacks nor scalable to millions of users for practical implementation. In this paper, we propose Waterfall, the first training-free framework for robust and scalable text watermarking applicable across multiple text types (e.g., articles, code) and languages supportable by LLMs, for general text and LLM data provenance. Waterfall comprises several key innovations, such as being the first to use LLM as paraphrasers for watermarking along with a novel combination of techniques that are surprisingly effective in achieving robust verifiability and scalability. We empirically demonstrate that Waterfall achieves significantly better scalability, robust verifiability, and computational efficiency compared to SOTA article-text watermarking methods, and also showed how it could be directly applied to the watermarking of code.",
            "link": "https://www.semanticscholar.org/paper/b4e4a90601986a3fd13130aee961450328802164",
            "authors": "Gregory Kang Ruey Lau, Xinyuan Niu, Hieu Dao, Jiangwei Chen, Chuan-Sheng Foo, K. H. Low",
            "EMNLP Paper ID": "2672",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "de033ca3ba431f505207b3a435f0e707b5bd5945",
            "title": "Private prediction for large-scale synthetic text generation",
            "abstract": "We present an approach for generating differentially private synthetic text using large language models (LLMs), via private prediction. In the private prediction framework, we only require the output synthetic data to satisfy differential privacy guarantees. This is in contrast to approaches that train a generative model on potentially sensitive user-supplied source data and seek to ensure the model itself is safe to release. We prompt a pretrained LLM with source data, but ensure that next-token predictions are made with differential privacy guarantees. Previous work in this paradigm reported generating a small number of examples (<10) at reasonable privacy levels, an amount of data that is useful only for downstream in-context learning or prompting. In contrast, we make changes that allow us to generate thousands of high-quality synthetic data points, greatly expanding the set of potential applications. Our improvements come from an improved privacy analysis and a better private selection mechanism, which makes use of the equivalence between the softmax layer for sampling tokens in LLMs and the exponential mechanism. Furthermore, we introduce a novel use of public predictions via the sparse vector technique, in which we do not pay privacy costs for tokens that are predictable without sensitive data; we find this to be particularly effective for structured data.",
            "link": "https://www.semanticscholar.org/paper/de033ca3ba431f505207b3a435f0e707b5bd5945",
            "authors": "Kareem Amin, Alex Bie, Weiwei Kong, Alexey Kurakin, Natalia Ponomareva, Umar Syed, Andreas Terzis, Sergei Vassilvitskii",
            "matchScore": 215.31395,
            "original title": "Private prediction for large-scale synthetic text generation",
            "original authors": "Kareem Amin, Alex Bie, Weiwei Kong, Alexey Kurakin, Natalia Ponomareva, Umar Syed, Andreas Terzis, Sergei Vassilvitskii",
            "EMNLP Paper ID": "1477",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "21c62f2ef68323099480c80318e48baae8b9098f",
            "title": "GuardEmb: Dynamic Watermark for Safeguarding Large Language Model Embedding Service Against Model Stealing Attack",
            "abstract": "Large language model (LLM) companies provide Embedding as a Service (EaaS) to assist the individual in efficiently dealing with downstream tasks such as text classification and recommendation. However, recent works reveal the risk of the model stealing attack, posing a financial threat to EaaS providers. To protect the copyright of EaaS, we propose GuardEmb, a dynamic embedding watermarking method, striking a balance between enhancing watermark detectability and preserving embedding functionality. Our approach involves selecting special tokens and perturbing embeddings containing these tokens to inject watermarks. Simultaneously, we train a verifier to detect these watermarks. In the event of an attacker attempting to replicate our EaaS for profit, their model inherits our watermarks. For watermark verification, we construct verification texts to query the suspicious EaaS, and the verifier identifies our watermarks within the responses, effectively tracing copyright infringement. Extensive experiments across diverse datasets showcase the high detectability of our watermark method, even in out-of-distribution scenarios, without compromising embedding functionality. Our code is publicly available at https://github. com/Melodramass/Dynamic-Watermark .",
            "link": "https://www.semanticscholar.org/paper/21c62f2ef68323099480c80318e48baae8b9098f",
            "authors": "Liaoyaqi Wang, Minhao Cheng",
            "matchScore": 359.9115,
            "original title": "GuardEmb: Dynamic Watermark for Safeguarding Large Language Model Embedding Service Against Model Stealing Attack",
            "original authors": "Liaoyaqi Wang, Minhao Cheng",
            "EMNLP Paper ID": "1566",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "9e29379132eef9528ab340323f851726682992c6",
            "title": "Fine-Tuning Language Models with Differential Privacy through Adaptive Noise Allocation",
            "abstract": "Language models are capable of memorizing detailed patterns and information, leading to a double-edged effect: they achieve impressive modeling performance on downstream tasks with the stored knowledge but also raise significant privacy concerns. Traditional differential privacy based training approaches offer robust safeguards by employing a uniform noise distribution across all parameters. However, this overlooks the distinct sensitivities and contributions of individual parameters in privacy protection and often results in suboptimal models. To address these limitations, we propose ANADP, a novel algorithm that adaptively allocates additive noise based on the importance of model parameters. We demonstrate that ANADP narrows the performance gap between regular fine-tuning and traditional DP fine-tuning on a series of datasets while maintaining the required privacy constraints.",
            "link": "https://www.semanticscholar.org/paper/9e29379132eef9528ab340323f851726682992c6",
            "authors": "Xianzhi Li, Ran Zmigrod, Zhiqiang Ma, Xiaomo Liu, Xiaodan Zhu",
            "matchScore": 279.55414,
            "original title": "Fine-Tuning Language Models with Differential Privacy through Adaptive Noise Allocation",
            "original authors": "Xianzhi Li, Ran Zmigrod, Xiaodan Zhu, Zhiqiang Ma, Xiaomo Liu",
            "EMNLP Paper ID": "1771",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "3b1ed0508a24ef157cb4458ab95dead821b8c90a",
            "title": "CodeIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models of Code",
            "abstract": "Large Language Models (LLMs) have achieved remarkable progress in code generation. It now becomes crucial to identify whether the code is AI-generated and to determine the specific model used, particularly for purposes such as protecting Intellectual Property (IP) in industry and preventing cheating in programming exercises. To this end, several attempts have been made to insert watermarks into machine-generated code. However, existing approaches are limited to inserting only a single bit of information or overly depending on particular code patterns. In this paper, we introduce CodeIP, a novel multi-bit watermarking technique that embeds additional information to preserve crucial provenance details, such as the vendor ID of an LLM, thereby safeguarding the IPs of LLMs in code generation. Furthermore, to ensure the syntactical correctness of the generated code, we propose constraining the sampling process for predicting the next token by training a type predictor. Experiments conducted on a real-world dataset across five programming languages demonstrate the effectiveness of CodeIP in watermarking LLMs for code generation while maintaining the syntactical correctness of code.",
            "link": "https://www.semanticscholar.org/paper/3b1ed0508a24ef157cb4458ab95dead821b8c90a",
            "authors": "Batu Guan, Yao Wan, Zhangqian Bi, Zheng Wang, Hongyu Zhang, Pan Zhou, Lichao Sun",
            "matchScore": 279.3143,
            "original title": "CodeIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models of Code",
            "original authors": "Batu Guan, Yao Wan, Zhangqian Bi, Zheng Wang, Hongyu Zhang, Yulei Sui, Pan Zhou, Lichao Sun",
            "EMNLP Paper ID": "1929",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "7d75b26b835292750aa199230c4a88ffee339a28",
            "title": "Code Membership Inference for Detecting Unauthorized Data Use in Code Pre-trained Language Models",
            "abstract": "Code pre-trained language models (CPLMs) have received great attention since they can benefit various tasks that facilitate software development and maintenance. However, CPLMs are trained on massive open-source code, raising concerns about potential data infringement. This paper launches the first study of detecting unauthorized code use in CPLMs, i.e., Code Membership Inference (CMI) task. We design a framework Buzzer for different settings of CMI. Buzzer deploys several inference techniques, including distilling the target CPLM, ensemble inference, and unimodal and bimodal calibration. Extensive experiments show that CMI can be achieved with high accuracy using Buzzer. Hence, Buzzer can serve as a CMI tool and help protect intellectual property rights.",
            "link": "https://www.semanticscholar.org/paper/7d75b26b835292750aa199230c4a88ffee339a28",
            "authors": "Sheng Zhang, Hui Li",
            "matchScore": 301.87863,
            "original title": "Code Membership Inference for Detecting Unauthorized Data Use in Code Pre-trained Language Models",
            "original authors": "Sheng Zhang, Hui Li, Rongrong Ji",
            "EMNLP Paper ID": "2146",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "b209d160cb09dc47b0d716ef5e256304f5033ae5",
            "title": "Privacy Evaluation Benchmarks for NLP Models",
            "abstract": "By inducing privacy attacks on NLP models, attackers can obtain sensitive information such as training data and model parameters, etc. Although researchers have studied, in-depth, several kinds of attacks in NLP models, they are non-systematic analyses. It lacks a comprehensive understanding of the impact caused by the attacks. For example, we must consider which scenarios can apply to which attacks, what the common factors are that affect the performance of different attacks, the nature of the relationships between different attacks, and the influence of various datasets and models on the effectiveness of the attacks, etc. Therefore, we need a benchmark to holistically assess the privacy risks faced by NLP models. In this paper, we present a privacy attack and defense evaluation benchmark in the field of NLP, which includes the conventional/small models and large language models (LLMs). This benchmark supports a variety of models, datasets, and protocols, along with standardized modules for comprehensive evaluation of attacks and defense strategies. Based on the above framework, we present a study on the association between auxiliary data from different domains and the strength of privacy attacks. And we provide an improved attack method in this scenario with the help of Knowledge Distillation (KD). Furthermore, we propose a chained framework for privacy attacks. Allowing a practitioner to chain multiple attacks to achieve a higher-level attack objective. Based on this, we provide some defense and enhanced attack strategies. The code for reproducing the results can be found at https://github.com/user2311717757/nlp_doctor.",
            "link": "https://www.semanticscholar.org/paper/b209d160cb09dc47b0d716ef5e256304f5033ae5",
            "authors": "Wei Huang, Yinggui Wang, Cen Chen",
            "matchScore": 192.54785,
            "original title": "Privacy Evaluation Benchmarks for NLP Models",
            "original authors": "Wei Huang, Yinggui Wang, Cen Chen",
            "EMNLP Paper ID": "539",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "f76d146388975c226ed17667e1f3fc8f1bc8e3c6",
            "title": "Granularity is crucial when applying differential privacy to text: An investigation for neural machine translation",
            "abstract": "Applying differential privacy (DP) by means of the DP-SGD algorithm to protect individual data points during training is becoming increasingly popular in NLP. However, the choice of granularity at which DP is applied is often neglected. For example, neural machine translation (NMT) typically operates on the sentence-level granularity. From the perspective of DP, this setup assumes that each sentence belongs to a single person and any two sentences in the training dataset are independent. This assumption is however violated in many real-world NMT datasets, e.g., those including dialogues. For proper application of DP we thus must shift from sentences to entire documents. In this paper, we investigate NMT at both the sentence and document levels, analyzing the privacy/utility trade-off for both scenarios, and evaluating the risks of not using the appropriate privacy granularity in terms of leaking personally identifiable information (PII). Our findings indicate that the document-level NMT system is more resistant to membership inference attacks, emphasizing the significance of using the appropriate granularity when working with DP.",
            "link": "https://www.semanticscholar.org/paper/f76d146388975c226ed17667e1f3fc8f1bc8e3c6",
            "authors": "Doan Nam Long Vu, Timour Igamberdiev, Ivan Habernal",
            "matchScore": 190.41513,
            "original title": "Granularity is crucial when applying differential privacy to text",
            "original authors": "Doan Nam Long Vu, Timour Igamberdiev, Ivan Habernal",
            "EMNLP Paper ID": "87",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        }
    ],
    "Entity-Oriented Tasks in NLP: Matching, Recognition, and Extraction with Advanced Methods": [
        {
            "paperId": "417f2341b9152ca58f3e4f19c05e99ca70cab392",
            "title": "Learning from Natural Language Explanations for Generalizable Entity Matching",
            "abstract": "Entity matching is the task of linking records from different sources that refer to the same real-world entity. Past work has primarily treated entity linking as a standard supervised learning problem. However, supervised entity matching models often do not generalize well to new data, and collecting exhaustive labeled training data is often cost prohibitive. Further, recent efforts have adopted LLMs for this task in few/zero-shot settings, exploiting their general knowledge. But LLMs are prohibitively expensive for performing inference at scale for real-world entity matching tasks. As an efficient alternative, we re-cast entity matching as a conditional generation task as opposed to binary classification. This enables us to\"distill\"LLM reasoning into smaller entity matching models via natural language explanations. This approach achieves strong performance, especially on out-of-domain generalization tests (10.85% F-1) where standalone generative methods struggle. We perform ablations that highlight the importance of explanations, both for performance and model robustness.",
            "link": "https://www.semanticscholar.org/paper/417f2341b9152ca58f3e4f19c05e99ca70cab392",
            "authors": "Somin Wadhwa, Adit Krishnan, Runhui Wang, Byron C. Wallace, Chris Kong",
            "EMNLP Paper ID": "684",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "e9cc6a78e78118b02a76948cd88f7a3a4a215cfb",
            "title": "Contrastive Entity Coreference and Disambiguation for Historical Texts",
            "abstract": "Massive-scale historical document collections are crucial for social science research. Despite increasing digitization, these documents typically lack unique cross-document identifiers for individuals mentioned within the texts, as well as individual identifiers from external knowledgebases like Wikipedia/Wikidata. Existing entity disambiguation methods often fall short in accuracy for historical documents, which are replete with individuals not remembered in contemporary knowledgebases. This study makes three key contributions to improve cross-document coreference resolution and disambiguation in historical texts: a massive-scale training dataset replete with hard negatives - that sources over 190 million entity pairs from Wikipedia contexts and disambiguation pages - high-quality evaluation data from hand-labeled historical newswire articles, and trained models evaluated on this historical benchmark. We contrastively train bi-encoder models for coreferencing and disambiguating individuals in historical texts, achieving accurate, scalable performance that identifies out-of-knowledgebase individuals. Our approach significantly surpasses other entity disambiguation models on our historical newswire benchmark. Our models also demonstrate competitive performance on modern entity disambiguation benchmarks, particularly certain news disambiguation datasets.",
            "link": "https://www.semanticscholar.org/paper/e9cc6a78e78118b02a76948cd88f7a3a4a215cfb",
            "authors": "Abhishek Arora, Emily Silcock, Leander Heldring, Melissa Dell",
            "EMNLP Paper ID": "695",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "1639a841b69d7b3ae8f5638a995064d6a8405e07",
            "title": "Learning to Extract Structured Entities Using Language Models",
            "abstract": "Recent advances in machine learning have significantly impacted the field of information extraction, with Language Models (LMs) playing a pivotal role in extracting structured information from unstructured text. Prior works typically represent information extraction as triplet-centric and use classical metrics such as precision and recall for evaluation. We reformulate the task to be entity-centric, enabling the use of diverse metrics that can provide more insights from various perspectives. We contribute to the field by introducing Structured Entity Extraction and proposing the Approximate Entity Set OverlaP (AESOP) metric, designed to appropriately assess model performance. Later, we introduce a new Multistage Structured Entity Extraction (MuSEE) model that harnesses the power of LMs for enhanced effectiveness and efficiency by decomposing the extraction task into multiple stages. Quantitative and human side-by-side evaluations confirm that our model outperforms baselines, offering promising directions for future advancements in structured entity extraction. Our source code and datasets are available at https://github.com/microsoft/Structured-Entity-Extraction.",
            "link": "https://www.semanticscholar.org/paper/1639a841b69d7b3ae8f5638a995064d6a8405e07",
            "authors": "Haolun Wu, Ye Yuan, Liana Mikaelyan, Alexander Meulemans, Xue Liu, James Hensman, Bhaskar Mitra",
            "EMNLP Paper ID": "761",
            "Oral/Poster": "Oral",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "73aaf2eff06a98a83e533df1a9f7930813c83d38",
            "title": "ADELIE: Aligning Large Language Models on Information Extraction",
            "abstract": "Large language models (LLMs) usually fall short on information extraction (IE) tasks and struggle to follow the complex instructions of IE tasks. This primarily arises from LLMs not being aligned with humans, as mainstream alignment datasets typically do not include IE data. In this paper, we introduce ADELIE (Aligning large language moDELs on Information Extraction), an aligned LLM that effectively solves various IE tasks, including closed IE, open IE, and on-demand IE. We first collect and construct a high-quality alignment corpus IEInstruct for IE. Then we train ADELIE_SFT using instruction tuning on IEInstruct. We further train ADELIE_SFT with direct preference optimization (DPO) objective, resulting in ADELIE_DPO. Extensive experiments on various held-out IE datasets demonstrate that our models (ADELIE_SFT and ADELIE_DPO) achieve state-of-the-art (SoTA) performance among open-source models. We further explore the general capabilities of ADELIE, and experimental results reveal that their general capabilities do not exhibit a noticeable decline. We will release the code, data, and models to facilitate further research.",
            "link": "https://www.semanticscholar.org/paper/73aaf2eff06a98a83e533df1a9f7930813c83d38",
            "authors": "Y. Qi, Hao Peng, Xiaozhi Wang, Bin Xu, Lei Hou, Juanzi Li",
            "EMNLP Paper ID": "832",
            "Oral/Poster": "Oral",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "eecc71949047e5d69c0fdc41911fd4583b7e64b2",
            "title": "Major Entity Identification: A Generalizable Alternative to Coreference Resolution",
            "abstract": "The limited generalization of coreference resolution (CR) models has been a major bottleneck in the task's broad application. Prior work has identified annotation differences, especially for mention detection, as one of the main reasons for the generalization gap and proposed using additional annotated target domain data. Rather than relying on this additional annotation, we propose an alternative referential task, Major Entity Identification (MEI), where we: (a) assume the target entities to be specified in the input, and (b) limit the task to only the frequent entities. Through extensive experiments, we demonstrate that MEI models generalize well across domains on multiple datasets with supervised models and LLM-based few-shot prompting. Additionally, MEI fits the classification framework, which enables the use of robust and intuitive classification-based metrics. Finally, MEI is also of practical use as it allows a user to search for all mentions of a particular entity or a group of entities of interest.",
            "link": "https://www.semanticscholar.org/paper/eecc71949047e5d69c0fdc41911fd4583b7e64b2",
            "authors": "Kawshik Manikantan, Shubham Toshniwal, Makarand Tapaswi, Vineet Gandhi Cvit, Iiit Hyderabad, Nvidia",
            "EMNLP Paper ID": "1359",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "c3b82c397e3bc30942a214b8aca7d861d453a74f",
            "title": "NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data",
            "abstract": "Large Language Models (LLMs) have shown impressive abilities in data annotation, opening the way for new approaches to solve classic NLP problems. In this paper, we show how to use LLMs to create NuNER, a compact language representation model specialized in the Named Entity Recognition (NER) task. NuNER can be fine-tuned to solve downstream NER problems in a data-efficient way, outperforming similar-sized foundation models in the few-shot regime and competing with much larger LLMs. We find that the size and entity-type diversity of the pre-training dataset are key to achieving good performance. We view NuNER as a member of the broader family of task-specific foundation models, recently unlocked by LLMs.",
            "link": "https://www.semanticscholar.org/paper/c3b82c397e3bc30942a214b8aca7d861d453a74f",
            "authors": "Sergei Bogdanov, Alexandre Constantin, Timoth'ee Bernard, Benoit Crabb'e, Etienne Bernard",
            "EMNLP Paper ID": "1376",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "c300c7cadc1536c5208da952bad5fe07748cbda2",
            "title": "SciER: An Entity and Relation Extraction Dataset for Datasets, Methods, and Tasks in Scientific Documents",
            "abstract": "Scientific information extraction (SciIE) is critical for converting unstructured knowledge from scholarly articles into structured data (entities and relations). Several datasets have been proposed for training and validating SciIE models. However, due to the high complexity and cost of annotating scientific texts, those datasets restrict their annotations to specific parts of paper, such as abstracts, resulting in the loss of diverse entity mentions and relations in context. In this paper, we release a new entity and relation extraction dataset for entities related to datasets, methods, and tasks in scientific articles. Our dataset contains 106 manually annotated full-text scientific publications with over 24k entities and 12k relations. To capture the intricate use and interactions among entities in full texts, our dataset contains a fine-grained tag set for relations. Additionally, we provide an out-of-distribution test set to offer a more realistic evaluation. We conduct comprehensive experiments, including state-of-the-art supervised models and our proposed LLM-based baselines, and highlight the challenges presented by our dataset, encouraging the development of innovative models to further the field of SciIE.",
            "link": "https://www.semanticscholar.org/paper/c300c7cadc1536c5208da952bad5fe07748cbda2",
            "authors": "Qi Zhang, Zhijia Chen, Huitong Pan, Cornelia Caragea, Longin Jan Latecki, E. Dragut",
            "EMNLP Paper ID": "1513",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "bfc467453d258329c29e2d4dd3b322dbb29984bd",
            "title": "Grasping the Essentials: Tailoring Large Language Models for Zero-Shot Relation Extraction",
            "abstract": "Relation extraction (RE) aims to identify semantic relationships between entities within text. Despite considerable advancements, existing models predominantly require extensive annotated training data, which is both costly and labor-intensive to collect. Moreover, these models often struggle to adapt to new or unseen relations. Few-shot learning, aiming to lessen annotation demands, typically provides incomplete and biased supervision for target relations, leading to degraded and unstable performance. To accurately and explicitly describe relation semantics while minimizing annotation demands, we explore the definition only zero-shot RE setting where only relation definitions expressed in natural language are used to train a RE model. We introduce REPaL, comprising three stages: (1) We leverage large language models (LLMs) to generate initial seed instances from relation definitions and an unlabeled corpus. (2) We fine-tune a bidirectional Small Language Model (SLM) with initial seeds to learn relations for the target domain. (3) We expand pattern coverage and mitigate bias from initial seeds by integrating feedback from the SLM's predictions on the unlabeled corpus and the synthesis history. To accomplish this, we leverage the multi-turn conversation ability of LLMs to generate new instances in follow-up dialogues, informed by both the feedback and synthesis history. Studies reveal that definition-oriented seed synthesis enhances pattern coverage whereas indiscriminately increasing seed quantity leads to performance saturation. Experiments on two datasets show REPaL significantly improved cost-effective zero-shot performance by large margins.",
            "link": "https://www.semanticscholar.org/paper/bfc467453d258329c29e2d4dd3b322dbb29984bd",
            "authors": "Sizhe Zhou, Yu Meng, Bowen Jin, Jiawei Han",
            "EMNLP Paper ID": "1558",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "c5047737b65948282f925cbcb7b4b4e06717583e",
            "title": "OneNet: A Fine-Tuning Free Framework for Few-Shot Entity Linking via Large Language Model Prompting",
            "abstract": "Entity Linking (EL) is the process of associating ambiguous textual mentions to specific entities in a knowledge base. Traditional EL methods heavily rely on large datasets to enhance their performance, a dependency that becomes problematic in the context of few-shot entity linking, where only a limited number of examples are available for training. To address this challenge, we present OneNet, an innovative framework that utilizes the few-shot learning capabilities of Large Language Models (LLMs) without the need for fine-tuning. To the best of our knowledge, this marks a pioneering approach to applying LLMs to few-shot entity linking tasks. OneNet is structured around three key components prompted by LLMs: (1) an entity reduction processor that simplifies inputs by summarizing and filtering out irrelevant entities, (2) a dual-perspective entity linker that combines contextual cues and prior knowledge for precise entity linking, and (3) an entity consensus judger that employs a unique consistency algorithm to alleviate the hallucination in the entity linking reasoning. Comprehensive evaluations across seven benchmark datasets reveal that OneNet outperforms current state-of-the-art entity linking methods.",
            "link": "https://www.semanticscholar.org/paper/c5047737b65948282f925cbcb7b4b4e06717583e",
            "authors": "Xukai Liu, Ye Liu, Kai Zhang, Kehang Wang, Qi Liu, Enhong Chen",
            "EMNLP Paper ID": "1575",
            "Oral/Poster": "Oral",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "53cff7952ce991032d3ad5ed06cf8f52e91a78ca",
            "title": "Preserving Generalization of Language models in Few-shot Continual Relation Extraction",
            "abstract": "Few-shot Continual Relations Extraction (FCRE) is an emerging and dynamic area of study where models can sequentially integrate knowledge from new relations with limited labeled data while circumventing catastrophic forgetting and preserving prior knowledge from pre-trained backbones. In this work, we introduce a novel method that leverages often-discarded language model heads. By employing these components via a mutual information maximization strategy, our approach helps maintain prior knowledge from the pre-trained backbone and strategically aligns the primary classification head, thereby enhancing model performance. Furthermore, we explore the potential of Large Language Models (LLMs), renowned for their wealth of knowledge, in addressing FCRE challenges. Our comprehensive experimental results underscore the efficacy of the proposed method and offer valuable insights for future work.",
            "link": "https://www.semanticscholar.org/paper/53cff7952ce991032d3ad5ed06cf8f52e91a78ca",
            "authors": "Quyen Tran, Nguyen Xuan Thanh, Nguyen Hoang Anh, Nam Le Hai, Trung Le, L. Ngo, Thien Huu Nguyen",
            "EMNLP Paper ID": "1590",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "e3573b85eaa0dfbf609dd0aaa5eeb050f6763a96",
            "title": "Context-Aware Adapter Tuning for Few-Shot Relation Learning in Knowledge Graphs",
            "abstract": "Knowledge graphs (KGs) are instrumental in various real-world applications, yet they often suffer from incompleteness due to missing relations. To predict instances for novel relations with limited training examples, few-shot relation learning approaches have emerged, utilizing techniques such as meta-learning. However, the assumption is that novel relations in meta-testing and base relations in meta-training are independently and identically distributed, which may not hold in practice. To address the limitation, we propose RelAdapter, a context-aware adapter for few-shot relation learning in KGs designed to enhance the adaptation process in meta-learning. First, RelAdapter is equipped with a lightweight adapter module that facilitates relation-specific, tunable adaptation of meta-knowledge in a parameter-efficient manner. Second, RelAdapter is enriched with contextual information about the target relation, enabling enhanced adaptation to each distinct relation. Extensive experiments on three benchmark KGs validate the superiority of RelAdapter over state-of-the-art methods.",
            "link": "https://www.semanticscholar.org/paper/e3573b85eaa0dfbf609dd0aaa5eeb050f6763a96",
            "authors": "Ran Liu, Zhongzhou Liu, Xiaoli Li, Yuan Fang",
            "EMNLP Paper ID": "2092",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "fff665caf490d815b090f50bd6393bdff3a51829",
            "title": "Embedded Named Entity Recognition using Probing Classifiers",
            "abstract": "Streaming text generation has become a common way of increasing the responsiveness of language model powered applications, such as chat assistants. At the same time, extracting semantic information from generated text is a useful tool for applications such as automated fact checking or retrieval augmented generation. Currently, this requires either separate models during inference, which increases computational cost, or destructive fine-tuning of the language model. Instead, we propose an approach called EMBER which enables streaming named entity recognition in decoder-only language models without fine-tuning them and while incurring minimal additional computational cost at inference time. Specifically, our experiments show that EMBER maintains high token generation rates, with only a negligible decrease in speed of around 1% compared to a 43.64% slowdown measured for a baseline. We make our code and data available online, including a toolkit for training, testing, and deploying efficient token classification models optimized for streaming text generation.",
            "link": "https://www.semanticscholar.org/paper/fff665caf490d815b090f50bd6393bdff3a51829",
            "authors": "Nicholas Popovic, Michael F\u00e4rber",
            "EMNLP Paper ID": "2155",
            "Oral/Poster": "Oral",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "c064e74549f5c30b0f64e4935de58e6b06dcf30c",
            "title": "A Survey of Ontology Expansion for Conversational Understanding",
            "abstract": "In the rapidly evolving field of conversational AI, Ontology Expansion (OnExp) is crucial for enhancing the adaptability and robustness of conversational agents. Traditional models rely on static, predefined ontologies, limiting their ability to handle new and unforeseen user needs. This survey paper provides a comprehensive review of the state-of-the-art techniques in OnExp for conversational understanding. It categorizes the existing literature into three main areas: (1) New Intent Discovery, (2) New Slot-Value Discovery, and (3) Joint OnExp. By examining the methodologies, benchmarks, and challenges associated with these areas, we highlight several emerging frontiers in OnExp to improve agent performance in real-world scenarios and discuss their corresponding challenges. This survey aspires to be a foundational reference for researchers and practitioners, promoting further exploration and innovation in this crucial domain.",
            "link": "https://www.semanticscholar.org/paper/c064e74549f5c30b0f64e4935de58e6b06dcf30c",
            "authors": "Jinggui Liang, Yuxia Wu, Yuan Fang, Hao Fei, Lizi Liao",
            "EMNLP Paper ID": "2225",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "2cd20359882a0a3fc080743e5431a1863ce1a2a3",
            "title": "A fast and sound tagging method for discontinuous named-entity recognition",
            "abstract": "We introduce a novel tagging scheme for discontinuous named entity recognition based on an explicit description of the inner structure of discontinuous mentions. We rely on a weighted finite state automaton for both marginal and maximum a posteriori inference. As such, our method is sound in the sense that (1) well-formedness of predicted tag sequences is ensured via the automaton structure and (2) there is an unambiguous mapping between well-formed sequences of tags and (discontinuous) mentions. We evaluate our approach on three English datasets in the biomedical domain, and report comparable results to state-of-the-art while having a way simpler and faster model.",
            "link": "https://www.semanticscholar.org/paper/2cd20359882a0a3fc080743e5431a1863ce1a2a3",
            "authors": "Caio Corro",
            "EMNLP Paper ID": "2514",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 14): 13:00\u201314:00 (Afternoon)",
            "Type": "Main"
        },
        {
            "paperId": "db5d5cceac43f03202f4ddecc9090d7216041c38",
            "title": "Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia",
            "abstract": "Links are a fundamental part of information networks, turning isolated pieces of knowledge into a network of information that is much richer than the sum of its parts. However, adding a new link to the network is not trivial: it requires not only the identification of a suitable pair of source and target entities but also the understanding of the content of the source to locate a suitable position for the link in the text. The latter problem has not been addressed effectively, particularly in the absence of text spans in the source that could serve as anchors to insert a link to the target entity. To bridge this gap, we introduce and operationalize the task of entity insertion in information networks. Focusing on the case of Wikipedia, we empirically show that this problem is, both, relevant and challenging for editors. We compile a benchmark dataset in 105 languages and develop a framework for entity insertion called LocEI (Localized Entity Insertion) and its multilingual variant XLocEI. We show that XLocEI outperforms all baseline models (including state-of-the-art prompt-based ranking with LLMs such as GPT-4) and that it can be applied in a zero-shot manner on languages not seen during training with minimal performance drop. These findings are important for applying entity insertion models in practice, e.g., to support editors in adding links across the more than 300 language versions of Wikipedia.",
            "link": "https://www.semanticscholar.org/paper/db5d5cceac43f03202f4ddecc9090d7216041c38",
            "authors": "Tom'as Feith, Akhil Arora, Martin Gerlach, Debjit Paul, Robert West",
            "EMNLP Paper ID": "3309",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "79ff4eb495094e3b47468515846d507144135ae8",
            "title": "A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction",
            "abstract": "Large language models (LLMs) have demonstrated impressive abilities in generating unstructured natural language according to instructions. However, their performance can be inconsistent when tasked with producing text that adheres to specific structured formats, which is crucial in applications like named entity recognition (NER) or relation extraction (RE). To address this issue, this paper introduces an efficient method, G&O, to enhance their structured text generation capabilities. It breaks the generation into a two-step pipeline: initially, LLMs generate answers in natural language as intermediate responses. Subsequently, LLMs are asked to organize the output into the desired structure, using the intermediate responses as context. G&O effectively separates the generation of content from the structuring process, reducing the pressure of completing two orthogonal tasks simultaneously. Tested on zero-shot NER and RE, the results indicate a significant improvement in LLM performance with minimal additional efforts. This straightforward and adaptable prompting technique can also be combined with other strategies, like self-consistency, to further elevate LLM capabilities in various structured text generation tasks.",
            "link": "https://www.semanticscholar.org/paper/79ff4eb495094e3b47468515846d507144135ae8",
            "authors": "Yinghao Li, R. Ramprasad, Chao Zhang",
            "matchScore": 247.27582,
            "original title": "A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction",
            "original authors": "Yinghao Li, Rampi Ramprasad, Chao Zhang",
            "EMNLP Paper ID": "1016",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "79febdb86df982f27745cab2a4d3f42afb7e1d1d",
            "title": "Entity or Relation Embeddings? An Analysis of Encoding Strategies for Relation Extraction",
            "abstract": "Relation extraction is essentially a text classification problem, which can be tackled by fine-tuning a pre-trained language model (LM). However, a key challenge arises from the fact that relation extraction cannot straightforwardly be reduced to sequence or token classification. Existing approaches therefore solve the problem in an indirect way: they fine-tune an LM to learn embeddings of the head and tail entities, and then predict the relationship from these entity embeddings. Our hypothesis in this paper is that relation extraction models can be improved by capturing relationships in a more direct way. In particular, we experiment with appending a prompt with a [MASK] token, whose contextualised representation is treated as a relation embedding. While, on its own, this strategy significantly underperforms the aforementioned approach, we find that the resulting relation embeddings are highly complementary to what is captured by embeddings of the head and tail entity. By jointly considering both types of representations, we end up with a simple model that outperforms the state-of-the-art across several relation extraction benchmarks.",
            "link": "https://www.semanticscholar.org/paper/79febdb86df982f27745cab2a4d3f42afb7e1d1d",
            "authors": "Frank Mtumbuka, Steven Schockaert",
            "matchScore": 220.2807,
            "original title": "Entity or Relation Embeddings? An Analysis of Encoding Strategies for Relation Extraction",
            "original authors": "Frank Martin Mtumbuka, Steven Schockaert",
            "EMNLP Paper ID": "1229",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "ba03c6abfd8f4f258a9ae696a5ee88d08fa52a6b",
            "title": "Few-shot clinical entity recognition in English, French and Spanish: masked language models outperform generative model prompting",
            "abstract": "Large language models (LLMs) have become the preferred solution for many natural language processing tasks. In low-resource environments such as specialized domains, their few-shot capabilities are expected to deliver high performance. Named Entity Recognition (NER) is a critical task in information extraction that is not covered in recent LLM benchmarks. There is a need for better understanding the performance of LLMs for NER in a variety of settings including languages other than English. This study aims to evaluate generative LLMs, employed through prompt engineering, for few-shot clinical NER. %from the perspective of F1 performance and environmental impact. We compare 13 auto-regressive models using prompting and 16 masked models using fine-tuning on 14 NER datasets covering English, French and Spanish. While prompt-based auto-regressive models achieve competitive F1 for general NER, they are outperformed within the clinical domain by lighter biLSTM-CRF taggers based on masked models. Additionally, masked models exhibit lower environmental impact compared to auto-regressive models. Findings are consistent across the three languages studied, which suggests that LLM prompting is not yet suited for NER production in the clinical domain.",
            "link": "https://www.semanticscholar.org/paper/ba03c6abfd8f4f258a9ae696a5ee88d08fa52a6b",
            "authors": "Marco Naguib, Xavier Tannier, Aur'elie N'ev'eol",
            "matchScore": 390.02087,
            "original title": "Few-shot clinical entity recognition in English, French and Spanish: masked language models outperform generative model prompting",
            "original authors": "Marco Naguib, Xavier Tannier, Aur\u00e9lie N\u00e9v\u00e9ol",
            "EMNLP Paper ID": "1396",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "89d419482d339a81c059ee26aca4a611787f267c",
            "title": "A Survey on Open Information Extraction from Rule-based Model to Large Language Model",
            "abstract": "Open Information Extraction (OpenIE) represents a crucial NLP task aimed at deriving structured information from unstructured text, unrestricted by relation type or domain. This survey paper provides an overview of OpenIE technologies spanning from 2007 to 2024, emphasizing a chronological perspective absent in prior surveys. It examines the evolution of task settings in OpenIE to align with the advances in recent technologies. The paper categorizes OpenIE approaches into rule-based, neural, and pre-trained large language models, discussing each within a chronological framework. Additionally, it highlights prevalent datasets and evaluation metrics currently in use. Building on this extensive review, the paper outlines potential future directions in terms of datasets, information sources, output formats, methodologies, and evaluation metrics.",
            "link": "https://www.semanticscholar.org/paper/89d419482d339a81c059ee26aca4a611787f267c",
            "authors": "Pai Liu, Wenya Gao, Wen Dong, Songfang Huang, Yue Zhang",
            "matchScore": 232.10915,
            "original title": "A Survey on Open Information Extraction from Rule-based Model to Large Language Model",
            "original authors": "Liu Pai, Wenyang Gao, Wenjie Dong, Lin Ai, Ziwei Gong, Songfang Huang, Li Zongsheng, Ehsan Hoque, Julia Hirschberg, Yue Zhang",
            "EMNLP Paper ID": "1985",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "983adfc42734a7ec2595a6b62352de967ab9000a",
            "title": "Schema-Driven Information Extraction from Heterogeneous Tables",
            "abstract": "In this paper, we explore the question of whether large language models can support cost-efficient information extraction from tables. We introduce schema-driven information extraction, a new task that transforms tabular data into structured records following a human-authored schema. To assess various LLM's capabilities on this task, we present a benchmark comprised of tables from four diverse domains: machine learning papers, chemistry literature, material science journals, and webpages. We use this collection of annotated tables to evaluate the ability of open-source and API-based language models to extract information from tables covering diverse domains and data formats. Our experiments demonstrate that surprisingly competitive performance can be achieved without requiring task-specific pipelines or labels, achieving F1 scores ranging from 74.2 to 96.1, while maintaining cost efficiency. Moreover, through detailed ablation studies and analyses, we investigate the factors contributing to model success and validate the practicality of distilling compact models to reduce API reliance.",
            "link": "https://www.semanticscholar.org/paper/983adfc42734a7ec2595a6b62352de967ab9000a",
            "authors": "Fan Bai, Junmo Kang, Gabriel Stanovsky, Dayne Freitag, Alan Ritter",
            "matchScore": 216.00513,
            "original title": "Schema-Driven Information Extraction from Heterogeneous Tables",
            "original authors": "Fan Bai, Junmo Kang, Gabriel Stanovsky, Dayne Freitag, Mark Dredze, Alan Ritter",
            "EMNLP Paper ID": "2096",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "b34466cc6c387be3991ddb182e5749f0aca7612a",
            "title": "Is There a One-Model-Fits-All Approach to Information Extraction? Revisiting Task Definition Biases",
            "abstract": "Definition bias is a negative phenomenon that can mislead models. Definition bias in information extraction appears not only across datasets from different domains but also within datasets sharing the same domain. We identify two types of definition bias in IE: bias among information extraction datasets and bias between information extraction datasets and instruction tuning datasets. To systematically investigate definition bias, we conduct three probing experiments to quantitatively analyze it and discover the limitations of unified information extraction and large language models in solving definition bias. To mitigate definition bias in information extraction, we propose a multi-stage framework consisting of definition bias measurement, bias-aware fine-tuning, and task-specific bias mitigation. Experimental results demonstrate the effectiveness of our framework in addressing definition bias. Resources of this paper can be found at https://github.com/EZ-hwh/definition-bias",
            "link": "https://www.semanticscholar.org/paper/b34466cc6c387be3991ddb182e5749f0aca7612a",
            "authors": "Wenhao Huang, Qi He, Zhixu Li, Jiaqing Liang, Yanghua Xiao",
            "matchScore": 285.86548,
            "original title": "Is There a One-Model-Fits-All Approach to Information Extraction? Revisiting Task Definition Biases",
            "original authors": "Wenhao Huang, Qianyu He, Zhixu Li, Jiaqing Liang, Yanghua Xiao",
            "EMNLP Paper ID": "2097",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "8a2263e82b5519e49a906a37906f9b1d17e8eded",
            "title": "Consistent Document-Level Relation Extraction via Counterfactuals",
            "abstract": "Many datasets have been developed to train and evaluate document-level relation extraction (RE) models. Most of these are constructed using real-world data. It has been shown that RE models trained on real-world data suffer from factual biases. To evaluate and address this issue, we present CovEReD, a counterfactual data generation approach for document-level relation extraction datasets using entity replacement. We first demonstrate that models trained on factual data exhibit inconsistent behavior: while they accurately extract triples from factual data, they fail to extract the same triples after counterfactual modification. This inconsistency suggests that models trained on factual data rely on spurious signals such as specific entities and external knowledge $\\unicode{x2013}$ rather than on the input context $\\unicode{x2013}$ to extract triples. We show that by generating document-level counterfactual data with CovEReD and training models on them, consistency is maintained with minimal impact on RE performance. We release our CovEReD pipeline as well as Re-DocRED-CF, a dataset of counterfactual RE documents, to assist in evaluating and addressing inconsistency in document-level RE.",
            "link": "https://www.semanticscholar.org/paper/8a2263e82b5519e49a906a37906f9b1d17e8eded",
            "authors": "Ali Modarressi, Abdullatif K\u00f6ksal, Hinrich Schutze",
            "matchScore": 234.50287,
            "original title": "Consistent Document-level Relation Extraction via Counterfactuals",
            "original authors": "Ali Modarressi, Abdullatif K\u00f6ksal, Hinrich Schuetze",
            "EMNLP Paper ID": "2269",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "9a9f86047aa538e533501a0ff6b5703a4ba1738d",
            "title": "Unleashing the Power of Large Language Models in Zero-shot Relation Extraction via Self-Prompting",
            "abstract": "Recent research in zero-shot Relation Extraction (RE) has focused on using Large Language Models (LLMs) due to their impressive zero-shot capabilities. However, current methods often perform suboptimally, mainly due to a lack of detailed, context-specific prompts needed for understanding various sentences and relations. To address this, we introduce the Self-Prompting framework, a novel method designed to fully harness the embedded RE knowledge within LLMs. Specifically, our framework employs a three-stage diversity approach to prompt LLMs, generating multiple synthetic samples that encapsulate specific relations from scratch. These generated samples act as in-context learning samples, offering explicit and context-specific guidance to efficiently prompt LLMs for RE. Experimental evaluations on benchmark datasets show our approach outperforms existing LLM-based zero-shot RE methods. Additionally, our experiments confirm the effectiveness of our generation pipeline in producing high-quality synthetic data that enhances performance.",
            "link": "https://www.semanticscholar.org/paper/9a9f86047aa538e533501a0ff6b5703a4ba1738d",
            "authors": "Siyi Liu, Yang Li, Jiang Li, Shan Yang, Yunshi Lan",
            "matchScore": 289.01028,
            "original title": "Unleashing the Power of Large Language Models in Zero-shot Relation Extraction via Self-Prompting",
            "original authors": "Siyi Liu, Yang Li, Jiang Li, Shan Yang, Yunshi Lan",
            "EMNLP Paper ID": "2567",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Findings"
        },
        {
            "paperId": "07d5f44a5f472fa435af3db6af2e7daf951b5a30",
            "title": "Data-driven Coreference-based Ontology Building",
            "abstract": "While coreference resolution is traditionally used as a component in individual document understanding, in this work we take a more global view and explore what can we learn about a domain from the set of all document-level coreference relations that are present in a large corpus. We derive coreference chains from a corpus of 30 million biomedical abstracts and construct a graph based on the string phrases within these chains, establishing connections between phrases if they co-occur within the same coreference chain. We then use the graph structure and the betweeness centrality measure to distinguish between edges denoting hierarchy, identity and noise, assign directionality to edges denoting hierarchy, and split nodes (strings) that correspond to multiple distinct concepts. The result is a rich, data-driven ontology over concepts in the biomedical domain, parts of which overlaps significantly with human-authored ontologies. We release the coreference chains and resulting ontology under a creative-commons license, along with the code.",
            "link": "https://www.semanticscholar.org/paper/07d5f44a5f472fa435af3db6af2e7daf951b5a30",
            "authors": "Shir Ashury-Tahan, Amir David Nissan Cohen, Nadav Cohen, Y. Louzoun, Yoav Goldberg",
            "matchScore": 208.89581,
            "original title": "Data-driven Coreference-based Ontology Building",
            "original authors": "Shir Ashury Tahan, Amir David Nissan Cohen, Nadav Cohen, Yoram Louzoun, Yoav Goldberg",
            "EMNLP Paper ID": "2763",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "5c460010802450760beb5e182f48000d1eca9044",
            "title": "Granular Entity Mapper: Advancing Fine-grained Multimodal Named Entity Recognition and Grounding",
            "abstract": "Multimodal Named Entity Recognition and 001 Grounding (MNERG) aims to extract paired 002 textual and visual entities from texts and im-003 ages. It has been well explored through a two-004 step paradigm: initially identifying potential 005 visual entities using object detection methods 006 and then aligning the extracted textual entities 007 with their corresponding visual entities. How-008 ever, when it comes to fine-grained MNERG, 009 the long-tailed distribution of textual entity cat-010 egories and the performance of object detectors 011 limit the effectiveness of traditional methods. 012 Specifically, more detailed classification leads 013 to many low-frequency categories, and existing 014 object detection methods often fail to pinpoint 015 subtle regions within images. To address these 016 challenges, we propose the G ranular E ntity 017 M apper (GEM) framework. Firstly, we design 018 a multi-granularity entity recognition module, 019 followed by a reranking module based on the 020 Multimodal Large Language Model (MLLM) 021 to incorporate hierarchical information of en-022 tity categories, visual cues, and external tex-023 tual resources collectively for accurate fine-024 grained textual entity recognition. Then, we 025 utilize a pre-trained Large Visual Language 026 Model (LVLM) as an implicit visual entity 027 grounder that directly deduces relevant visual 028 entity regions from the entire image without the 029 need for bounding box training. Experimental 030 results on the GMNER and FMNERG datasets 031 demonstrate that our GEM framework achieves 032 state-of-the-art results on the fine-grained con-033 tent extraction task. 034",
            "link": "https://www.semanticscholar.org/paper/5c460010802450760beb5e182f48000d1eca9044",
            "authors": "Qi Zhang, Jinlan Fu, Xiaoyu Liu, Xuanjing Huang, Zhen Zhang, Yuhua Zhao, Hang Gao, Mengting Hu",
            "matchScore": 303.87653,
            "original title": "Granular Entity Mapper: Advancing Fine-grained Multimodal Named Entity Recognition and Grounding",
            "original authors": "ziqi wang, Chen Zhu, Zhi Zheng, Xinhang Li, Tong Xu, Yongyi He, Qi Liu, Ying Yu, Enhong Chen",
            "EMNLP Paper ID": "659",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        }
    ],
    "NLP Techniques and Evaluations in Multilingual and Cross-lingual Contexts": [
        {
            "paperId": "08306aab9eadc7f8eec7f0c53c389d9ae4aafb8a",
            "title": "Cross-lingual Transfer for Automatic Question Generation by Learning Interrogative Structures in Target Languages",
            "abstract": "Automatic question generation (QG) serves a wide range of purposes, such as augmenting question-answering (QA) corpora, enhancing chatbot systems, and developing educational materials. Despite its importance, most existing datasets predominantly focus on English, resulting in a considerable gap in data availability for other languages. Cross-lingual transfer for QG (XLT-QG) addresses this limitation by allowing models trained on high-resource language datasets to generate questions in low-resource languages. In this paper, we propose a simple and efficient XLT-QG method that operates without the need for monolingual, parallel, or labeled data in the target language, utilizing a small language model. Our model, trained solely on English QA datasets, learns interrogative structures from a limited set of question exemplars, which are then applied to generate questions in the target language. Experimental results show that our method outperforms several XLT-QG baselines and achieves performance comparable to GPT-3.5-turbo across different languages. Additionally, the synthetic data generated by our model proves beneficial for training multilingual QA models. With significantly fewer parameters than large language models and without requiring additional training for target languages, our approach offers an effective solution for QG and QA tasks across various languages.",
            "link": "https://www.semanticscholar.org/paper/08306aab9eadc7f8eec7f0c53c389d9ae4aafb8a",
            "authors": "Seonjeong Hwang, Yunsu Kim, Gary Geunbae Lee",
            "EMNLP Paper ID": "355",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "ea3df96325786f4d61c9f8d74e58c0083504eb7b",
            "title": "APPLS: Evaluating Evaluation Metrics for Plain Language Summarization",
            "abstract": "While there has been significant development of models for Plain Language Summarization (PLS), evaluation remains a challenge. PLS lacks a dedicated assessment metric, and the suitability of text generation evaluation metrics is unclear due to the unique transformations involved (e.g., adding background explanations, removing jargon). To address these questions, our study introduces a granular meta-evaluation testbed, APPLS, designed to evaluate metrics for PLS. We identify four PLS criteria from previous work -- informativeness, simplification, coherence, and faithfulness -- and define a set of perturbations corresponding to these criteria that sensitive metrics should be able to detect. We apply these perturbations to extractive hypotheses for two PLS datasets to form our testbed. Using APPLS, we assess performance of 14 metrics, including automated scores, lexical features, and LLM prompt-based evaluations. Our analysis reveals that while some current metrics show sensitivity to specific criteria, no single method captures all four criteria simultaneously. We therefore recommend a suite of automated metrics be used to capture PLS quality along all relevant criteria. This work contributes the first meta-evaluation testbed for PLS and a comprehensive evaluation of existing metrics. APPLS and our evaluation code is available at https://github.com/LinguisticAnomalies/APPLS.",
            "link": "https://www.semanticscholar.org/paper/ea3df96325786f4d61c9f8d74e58c0083504eb7b",
            "authors": "Yue Guo, Tal August, Gondy Leroy, T. Cohen, Lucy Lu Wang",
            "EMNLP Paper ID": "1046",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "b40d9ad792fddb69fa48cca6f0a9b68d3ead749c",
            "title": "Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems",
            "abstract": "LLMs and RAG systems are now capable of handling millions of input tokens or more. However, evaluating the output quality of such systems on long-context tasks remains challenging, as tasks like Needle-in-a-Haystack lack complexity. In this work, we argue that summarization can play a central role in such evaluation. We design a procedure to synthesize Haystacks of documents, ensuring that specific \\textit{insights} repeat across documents. The\"Summary of a Haystack\"(SummHay) task then requires a system to process the Haystack and generate, given a query, a summary that identifies the relevant insights and precisely cites the source documents. Since we have precise knowledge of what insights should appear in a haystack summary and what documents should be cited, we implement a highly reproducible automatic evaluation that can score summaries on two aspects - Coverage and Citation. We generate Haystacks in two domains (conversation, news), and perform a large-scale evaluation of 10 LLMs and corresponding 50 RAG systems. Our findings indicate that SummHay is an open challenge for current systems, as even systems provided with an Oracle signal of document relevance lag our estimate of human performance (56\\%) by 10+ points on a Joint Score. Without a retriever, long-context LLMs like GPT-4o and Claude 3 Opus score below 20% on SummHay. We show SummHay can also be used to study enterprise RAG systems and position bias in long-context models. We hope future systems can equal and surpass human performance on SummHay.",
            "link": "https://www.semanticscholar.org/paper/b40d9ad792fddb69fa48cca6f0a9b68d3ead749c",
            "authors": "Philippe Laban, A. R. Fabbri, Caiming Xiong, Chien-Sheng Wu",
            "EMNLP Paper ID": "1102",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "22dc690b1b4e186d3d6f4f61988c1d63d4c8951a",
            "title": "STORYSUMM: Evaluating Faithfulness in Story Summarization",
            "abstract": "Human evaluation has been the gold standard for checking faithfulness in abstractive summarization. However, with a challenging source domain like narrative, multiple annotators can agree a summary is faithful, while missing details that are obvious errors only once pointed out. We therefore introduce a new dataset, STORYSUMM, comprising LLM summaries of short stories with localized faithfulness labels and error explanations. This benchmark is for evaluation methods, testing whether a given method can detect challenging inconsistencies. Using this dataset, we first show that any one human annotation protocol is likely to miss inconsistencies, and we advocate for pursuing a range of methods when establishing ground truth for a summarization dataset. We finally test recent automatic metrics and find that none of them achieve more than 70% balanced accuracy on this task, demonstrating that it is a challenging benchmark for future work in faithfulness evaluation.",
            "link": "https://www.semanticscholar.org/paper/22dc690b1b4e186d3d6f4f61988c1d63d4c8951a",
            "authors": "Melanie Subbiah, Faisal Ladhak, Akankshya Mishra, Griffin Adams, Lydia B. Chilton, Kathleen McKeown",
            "EMNLP Paper ID": "1118",
            "Oral/Poster": "Oral",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "a7a452322c6a0f763f5624445a1fdf55539c8717",
            "title": "GlobeSumm: A Challenging Benchmark Towards Unifying Multi-lingual, Cross-lingual and Multi-document News Summarization",
            "abstract": "News summarization in today's global scene can be daunting with its flood of multilingual content and varied viewpoints from different sources. However, current studies often neglect such real-world scenarios as they tend to focus solely on either single-language or single-document tasks. To bridge this gap, we aim to unify Multi-lingual, Cross-lingual and Multi-document Summarization into a novel task, i.e., MCMS, which encapsulates the real-world requirements all-in-one. Nevertheless, the lack of a benchmark inhibits researchers from adequately studying this invaluable problem. To tackle this, we have meticulously constructed the GLOBESUMM dataset by first collecting a wealth of multilingual news reports and restructuring them into event-centric format. Additionally, we introduce the method of protocol-guided prompting for high-quality and cost-effective reference annotation. In MCMS, we also highlight the challenge of conflicts between news reports, in addition to the issues of redundancies and omissions, further enhancing the complexity of GLOBESUMM. Through extensive experimental analysis, we validate the quality of our dataset and elucidate the inherent challenges of the task. We firmly believe that GLOBESUMM, given its challenging nature, will greatly contribute to the multilingual communities and the evaluation of LLMs.",
            "link": "https://www.semanticscholar.org/paper/a7a452322c6a0f763f5624445a1fdf55539c8717",
            "authors": "Yangfan Ye, Xiachong Feng, Xiaocheng Feng, Weitao Ma, Libo Qin, Dongliang Xu, Qing Yang, Hongtao Liu, Bing Qin",
            "EMNLP Paper ID": "1231",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "a0456a40418827c336ebc3e771b91dd14b06840e",
            "title": "Unlocking Markets: A Multilingual Benchmark to Cross-Market Question Answering",
            "abstract": "Users post numerous product-related questions on e-commerce platforms, affecting their purchase decisions. Product-related question answering (PQA) entails utilizing product-related resources to provide precise responses to users. We propose a novel task of Multilingual Cross-market Product-based Question Answering (MCPQA) and define the task as providing answers to product-related questions in a main marketplace by utilizing information from another resource-rich auxiliary marketplace in a multilingual context. We introduce a large-scale dataset comprising over 7 million questions from 17 marketplaces across 11 languages. We then perform automatic translation on the Electronics category of our dataset, naming it as McMarket. We focus on two subtasks: review-based answer generation and product-related question ranking. For each subtask, we label a subset of McMarket using an LLM and further evaluate the quality of the annotations via human assessment. We then conduct experiments to benchmark our dataset, using models ranging from traditional lexical models to LLMs in both single-market and cross-market scenarios across McMarket and the corresponding LLM subset. Results show that incorporating cross-market information significantly enhances performance in both tasks.",
            "link": "https://www.semanticscholar.org/paper/a0456a40418827c336ebc3e771b91dd14b06840e",
            "authors": "Yifei Yuan, Yang Deng, Anders Sogaard, Mohammad Aliannejadi",
            "EMNLP Paper ID": "1292",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "ef00e8f46708f0345270e19b24dc03aee32c5a3e",
            "title": "DECOR: Improving Coherence in L2 English Writing with a Novel Benchmark for Incoherence Detection, Reasoning, and Rewriting",
            "abstract": "Coherence in writing, an aspect that second-language (L2) English learners often struggle with, is crucial in assessing L2 English writing. Existing automated writing evaluation systems primarily use basic surface linguistic features to detect coherence in writing. However, little effort has been made to correct the detected incoherence, which could significantly benefit L2 language learners seeking to improve their writing. To bridge this gap, we introduce DECOR, a novel benchmark that includes expert annotations for detecting incoherence in L2 English writing, identifying the underlying reasons, and rewriting the incoherent sentences. To our knowledge, DECOR is the first coherence assessment dataset specifically designed for improving L2 English writing, featuring pairs of original incoherent sentences alongside their expert-rewritten counterparts. Additionally, we fine-tuned models to automatically detect and rewrite incoherence in student essays. We find that incorporating specific reasons for incoherence during fine-tuning consistently improves the quality of the rewrites, achieving a result that is favored in both automatic and human evaluations.",
            "link": "https://www.semanticscholar.org/paper/ef00e8f46708f0345270e19b24dc03aee32c5a3e",
            "authors": "Xuanming Zhang, Anthony Diaz, Zixun Chen, Qingyang Wu, Kun Qian, Erik Voss, Zhou Yu",
            "EMNLP Paper ID": "1335",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "ec825db38600bc85eef445706de5e013d150feaf",
            "title": "QGEval: Benchmarking Multi-dimensional Evaluation for Question Generation",
            "abstract": "Automatically generated questions often suffer from problems such as unclear expression or factual inaccuracies, requiring a reliable and comprehensive evaluation of their quality. Human evaluation is widely used in the field of question generation (QG) and serves as the gold standard for automatic metrics. However, there is a lack of unified human evaluation criteria, which hampers consistent and reliable evaluations of both QG models and automatic metrics. To address this, we propose QGEval, a multi-dimensional Evaluation benchmark for Question Generation, which evaluates both generated questions and existing automatic metrics across 7 dimensions: fluency, clarity, conciseness, relevance, consistency, answerability, and answer consistency. We demonstrate the appropriateness of these dimensions by examining their correlations and distinctions. Through consistent evaluations of QG models and automatic metrics with QGEval, we find that 1) most QG models perform unsatisfactorily in terms of answerability and answer consistency, and 2) existing metrics fail to align well with human judgments when evaluating generated questions across the 7 dimensions. We expect this work to foster the development of both QG technologies and their evaluation.",
            "link": "https://www.semanticscholar.org/paper/ec825db38600bc85eef445706de5e013d150feaf",
            "authors": "Weiping Fu, Bifan Wei, Jianxiang Hu, Zhongmin Cai, Jun Liu",
            "EMNLP Paper ID": "1367",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "d7cc84e25f1fd8cfb0582be3856002ffd8ccfa54",
            "title": "ReadMe++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment",
            "abstract": "We present a comprehensive evaluation of large language models for multilingual readability assessment. Existing evaluation resources lack domain and language diversity, limiting the ability for cross-domain and cross-lingual analyses. This paper introduces ReadMe++, a multilingual multi-domain dataset with human annotations of 9757 sentences in Arabic, English, French, Hindi, and Russian, collected from 112 different data sources. This benchmark will encourage research on developing robust multilingual readability assessment methods. Using ReadMe++, we benchmark multilingual and monolingual language models in the supervised, unsupervised, and few-shot prompting settings. The domain and language diversity in ReadMe++ enable us to test more effective few-shot prompting, and identify shortcomings in state-of-the-art unsupervised methods. Our experiments also reveal exciting results of superior domain generalization and enhanced cross-lingual transfer capabilities by models trained on ReadMe++. We will make our data publicly available and release a python package tool for multilingual sentence readability prediction using our trained models at: https://github.com/tareknaous/readme",
            "link": "https://www.semanticscholar.org/paper/d7cc84e25f1fd8cfb0582be3856002ffd8ccfa54",
            "authors": "Tarek Naous, Michael Joseph Ryan, Mohit Chandra, Wei Xu",
            "EMNLP Paper ID": "1428",
            "Oral/Poster": "Oral",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "7d6052c672930172f27e61bbf494018cdcb3f4da",
            "title": "Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision",
            "abstract": "Cross-lingual open domain question answering (CLQA) is a complex problem, comprising cross-lingual retrieval from a multilingual knowledge base, followed by answer generation in the query language. Both steps are usually tackled by separate models, requiring substantial annotated datasets, and typically auxiliary resources, like machine translation systems to bridge between languages. In this paper, we show that CLQA can be addressed using a single encoder-decoder model. To effectively train this model, we propose a self-supervised method based on exploiting the cross-lingual link structure within Wikipedia. We demonstrate how linked Wikipedia pages can be used to synthesise supervisory signals for cross-lingual retrieval, through a form of cloze query, and generate more natural questions to supervise answer generation. Together, we show our approach, \\texttt{CLASS}, outperforms comparable methods on both supervised and zero-shot language adaptation settings, including those using machine translation.",
            "link": "https://www.semanticscholar.org/paper/7d6052c672930172f27e61bbf494018cdcb3f4da",
            "authors": "Fan Jiang, Tom Drummond, Trevor Cohn",
            "EMNLP Paper ID": "1601",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "76a70f8cfe54af7c79d5f2985a78ca9700b223c0",
            "title": "Can We Trust the Performance Evaluation of Uncertainty Estimation Methods in Text Summarization?",
            "abstract": "Text summarization, a key natural language generation (NLG) task, is vital in various domains. However, the high cost of inaccurate summaries in risk-critical applications, particularly those involving human-in-the-loop decision-making, raises concerns about the reliability of uncertainty estimation on text summarization (UE-TS) evaluation methods. This concern stems from the dependency of uncertainty model metrics on diverse and potentially conflicting NLG metrics. To address this issue, we introduce a comprehensive UE-TS benchmark incorporating 31 NLG metrics across four dimensions. The benchmark evaluates the uncertainty estimation capabilities of two large language models and one pre-trained language model on three datasets, with human-annotation analysis incorporated where applicable. We also assess the performance of 14 common uncertainty estimation methods within this benchmark. Our findings emphasize the importance of considering multiple uncorrelated NLG metrics and diverse uncertainty estimation methods to ensure reliable and efficient evaluation of UE-TS techniques. Our code and data are available https://github.com/he159ok/Benchmark-of-Uncertainty-Estimation-Methods-in-Text-Summarization.",
            "link": "https://www.semanticscholar.org/paper/76a70f8cfe54af7c79d5f2985a78ca9700b223c0",
            "authors": "Jianfeng He, Runing Yang, Linlin Yu, Changbin Li, Ruoxi Jia, Feng Chen, Ming Jin, Chang-Tien Lu",
            "EMNLP Paper ID": "1945",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 14): 13:00\u201314:00 (Afternoon)",
            "Type": "Main"
        },
        {
            "paperId": "ca862ab6bdfc5eee2f3764e028f9c6cb13683f21",
            "title": "Is It Really Long Context if All You Need Is Retrieval? Towards Genuinely Difficult Long Context NLP",
            "abstract": "Improvements in language models' capabilities have pushed their applications towards longer contexts, making long-context evaluation and development an active research area. However, many disparate use-cases are grouped together under the umbrella term of\"long-context\", defined simply by the total length of the model's input, including - for example - Needle-in-a-Haystack tasks, book summarization, and information aggregation. Given their varied difficulty, in this position paper we argue that conflating different tasks by their context length is unproductive. As a community, we require a more precise vocabulary to understand what makes long-context tasks similar or different. We propose to unpack the taxonomy of long-context based on the properties that make them more difficult with longer contexts. We propose two orthogonal axes of difficulty: (I) Diffusion: How hard is it to find the necessary information in the context? (II) Scope: How much necessary information is there to find? We survey the literature on long-context, provide justification for this taxonomy as an informative descriptor, and situate the literature with respect to it. We conclude that the most difficult and interesting settings, whose necessary information is very long and highly diffused within the input, is severely under-explored. By using a descriptive vocabulary and discussing the relevant properties of difficulty in long-context, we can implement more informed research in this area. We call for a careful design of tasks and benchmarks with distinctly long context, taking into account the characteristics that make it qualitatively different from shorter context.",
            "link": "https://www.semanticscholar.org/paper/ca862ab6bdfc5eee2f3764e028f9c6cb13683f21",
            "authors": "Omer Goldman, Alon Jacovi, Aviv Slobodkin, Aviya Maimon, Ido Dagan, Reut Tsarfaty",
            "EMNLP Paper ID": "1952",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "f951fd5fc315478d51b22dd40d0cae769b42489b",
            "title": "Mitigating the Impact of Reference Quality on Evaluation of Summarization Systems with Reference-Free Metrics",
            "abstract": "Automatic metrics are used as proxies to evaluate abstractive summarization systems when human annotations are too expensive. To be useful, these metrics should be fine-grained, show a high correlation with human annotations, and ideally be independent of reference quality; however, most standard evaluation metrics for summarization are reference-based, and existing reference-free metrics correlate poorly with relevance, especially on summaries of longer documents. In this paper, we introduce a reference-free metric that correlates well with human evaluated relevance, while being very cheap to compute. We show that this metric can also be used alongside reference-based metrics to improve their robustness in low quality reference settings.",
            "link": "https://www.semanticscholar.org/paper/f951fd5fc315478d51b22dd40d0cae769b42489b",
            "authors": "Th\u00e9o Gigant, Camille Guinaudeau, Marc D\u00e9combas, Fr'ed'eric Dufaux",
            "EMNLP Paper ID": "2460",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "7b98ec261e39b1fe7310243a9d7bfb76c566f2ca",
            "title": "Towards Enhancing Coherence in Extractive Summarization: Dataset and Experiments with LLMs",
            "abstract": "Extractive summarization plays a pivotal role in natural language processing due to its wide-range applications in summarizing diverse content efficiently, while also being faithful to the original content. Despite significant advancement achieved in extractive summarization by Large Language Models (LLMs), these summaries frequently exhibit incoherence. An important aspect of the coherent summary is its readability for intended users. Although there have been many datasets and benchmarks proposed for creating coherent extractive summaries, none of them currently incorporate user intent to improve coherence in extractive summarization. Motivated by this, we propose a systematically created human-annotated dataset consisting of coherent summaries for five publicly available datasets and natural language user feedback, offering valuable insights into how to improve coherence in extractive summaries. We utilize this dataset for aligning LLMs through supervised fine-tuning with natural language human feedback to enhance the coherence of their generated summaries. Preliminary experiments with Falcon-40B and Llama-2-13B show significant performance improvements (~10% Rouge-L) in terms of producing coherent summaries. We further utilize human feedback to benchmark results over instruction-tuned models such as FLAN-T5 which resulted in several interesting findings. Data and source code are available at https://github.com/Mihir3009/Extract-AI.",
            "link": "https://www.semanticscholar.org/paper/7b98ec261e39b1fe7310243a9d7bfb76c566f2ca",
            "authors": "Mihir Parmar, Hanieh Deilamsalehy, Franck Dernoncourt, Seunghyun Yoon, Ryan Rossi, Trung Bui",
            "EMNLP Paper ID": "2577",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "7c434f0243c1fc6f68150e4af0997636c3336084",
            "title": "M2QA: Multi-domain Multilingual Question Answering",
            "abstract": "Generalization and robustness to input variation are core desiderata of machine learning research. Language varies along several axes, most importantly, language instance (e.g. French) and domain (e.g. news). While adapting NLP models to new languages within a single domain, or to new domains within a single language, is widely studied, research in joint adaptation is hampered by the lack of evaluation datasets. This prevents the transfer of NLP systems from well-resourced languages and domains to non-dominant language-domain combinations. To address this gap, we introduce M2QA, a multi-domain multilingual question answering benchmark. M2QA includes 13,500 SQuAD 2.0-style question-answer instances in German, Turkish, and Chinese for the domains of product reviews, news, and creative writing. We use M2QA to explore cross-lingual cross-domain performance of fine-tuned models and state-of-the-art LLMs and investigate modular approaches to domain and language adaptation. We witness 1) considerable performance variations across domain-language combinations within model classes and 2) considerable performance drops between source and target language-domain combinations across all model sizes. We demonstrate that M2QA is far from solved, and new methods to effectively transfer both linguistic and domain-specific information are necessary. We make M2QA publicly available at https://github.com/UKPLab/m2qa.",
            "link": "https://www.semanticscholar.org/paper/7c434f0243c1fc6f68150e4af0997636c3336084",
            "authors": "Leon Arne Engl\u00e4nder, Hannah Sterz, Clifton A. Poth, Jonas Pfeiffer, Ilia Kuznetsov, Iryna Gurevych",
            "matchScore": 217.80702,
            "original title": "M2QA: Multi-domain Multilingual Question Answering",
            "original authors": "Leon Engl\u00e4nder, Hannah Sterz, Clifton A Poth, Jonas Pfeiffer, Ilia Kuznetsov, Iryna Gurevych",
            "EMNLP Paper ID": "1288",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "3654fbaf46a580aeae9ea44609370cdac9bf0cab",
            "title": "What's under the hood: Investigating Automatic Metrics on Meeting Summarization",
            "abstract": "Meeting summarization has become a critical task considering the increase in online interactions. While new techniques are introduced regularly, their evaluation uses metrics not designed to capture meeting-specific errors, undermining effective evaluation. This paper investigates what the frequently used automatic metrics capture and which errors they mask by correlating automatic metric scores with human evaluations across a broad error taxonomy. We commence with a comprehensive literature review on English meeting summarization to define key challenges like speaker dynamics and contextual turn-taking and error types such as missing information and linguistic inaccuracy, concepts previously loosely defined in the field. We examine the relationship between characteristic challenges and errors by using annotated transcripts and summaries from Transformer-based sequence-to-sequence and autoregressive models from the general summary QMSum dataset. Through experimental validation, we find that different model architectures respond variably to challenges in meeting transcripts, resulting in different pronounced links between challenges and errors. Current default-used metrics struggle to capture observable errors, showing weak to mid-correlations, while a third of the correlations show trends of error masking. Only a subset reacts accurately to specific errors, while most correlations show either unresponsiveness or failure to reflect the error's impact on summary quality.",
            "link": "https://www.semanticscholar.org/paper/3654fbaf46a580aeae9ea44609370cdac9bf0cab",
            "authors": "Frederic Kirstein, Jan Philip Wahle, Terry Ruas, Bela Gipp",
            "matchScore": 272.2985,
            "original title": "What\u2019s under the hood: Investigating Automatic Metrics on Meeting Summarization",
            "original authors": "Frederic Kirstein, Jan Philip Wahle, Terry Ruas, Bela Gipp",
            "EMNLP Paper ID": "1369",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "e514e19d3ab5937a88a4369adc1d45a9110f7816",
            "title": "TurkishMMLU: Measuring Massive Multitask Language Understanding in Turkish",
            "abstract": "Multiple choice question answering tasks evaluate the reasoning, comprehension, and mathematical abilities of Large Language Models (LLMs). While existing benchmarks employ automatic translation for multilingual evaluation, this approach is error-prone and potentially introduces culturally biased questions, especially in social sciences. We introduce the first multitask, multiple-choice Turkish QA benchmark, TurkishMMLU, to evaluate LLMs' understanding of the Turkish language. TurkishMMLU includes over 10,000 questions, covering 9 different subjects from Turkish high-school education curricula. These questions are written by curriculum experts, suitable for the high-school curricula in Turkey, covering subjects ranging from natural sciences and math questions to more culturally representative topics such as Turkish Literature and the history of the Turkish Republic. We evaluate over 20 LLMs, including multilingual open-source (e.g., Gemma, Llama, MT5), closed-source (GPT 4o, Claude, Gemini), and Turkish-adapted (e.g., Trendyol) models. We provide an extensive evaluation, including zero-shot and few-shot evaluation of LLMs, chain-of-thought reasoning, and question difficulty analysis along with model performance. We provide an in-depth analysis of the Turkish capabilities and limitations of current LLMs to provide insights for future LLMs for the Turkish language. We publicly release our code for the dataset and evaluation: https://github.com/ArdaYueksel/TurkishMMLU.",
            "link": "https://www.semanticscholar.org/paper/e514e19d3ab5937a88a4369adc1d45a9110f7816",
            "authors": "Arda Yuksel, Abdullatif K\u00f6ksal, Lutfi Kerem cSenel, Anna Korhonen, Hinrich Schutze",
            "matchScore": 264.3557,
            "original title": "TurkishMMLU: Measuring Massive Multitask Language Understanding in Turkish",
            "original authors": "Arda Y\u00fcksel, Abdullatif K\u00f6ksal, L\u00fctfi Kerem Senel, Anna Korhonen, Hinrich Schuetze",
            "EMNLP Paper ID": "1435",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "37d33bbf66dbcd5312538b344c829d11f9e95a67",
            "title": "Event-Keyed Summarization",
            "abstract": "We introduce event-keyed summarization (EKS), a novel task that marries traditional summarization and document-level event extraction, with the goal of generating a contextualized summary for a specific event, given a document and an extracted event structure. We introduce a dataset for this task, MUCSUM, consisting of summaries of all events in the classic MUC-4 dataset, along with a set of baselines that comprises both pretrained LM standards in the summarization literature, as well as larger frontier models. We show that ablations that reduce EKS to traditional summarization or structure-to-text yield inferior summaries of target events and that MUCSUM is a robust benchmark for this task. Lastly, we conduct a human evaluation of both reference and model summaries, and provide some detailed analysis of the results.",
            "link": "https://www.semanticscholar.org/paper/37d33bbf66dbcd5312538b344c829d11f9e95a67",
            "authors": "William Gantt Walden, Alexander Martin, Pavlo Kuchmiichuk, Aaron Steven White",
            "matchScore": 110.95131,
            "original title": "Event-Keyed Summarization",
            "original authors": "William Gantt, Alexander Martin, Pavlo Kuchmiichuk, Aaron Steven White",
            "EMNLP Paper ID": "1503",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "88faf932905250c11cdb52d62e91560958aba27d",
            "title": "Datasets for Multilingual Answer Sentence Selection",
            "abstract": "Answer Sentence Selection (AS2) is a critical task for designing effective retrieval-based Question Answering (QA) systems. Most advancements in AS2 focus on English due to the scarcity of annotated datasets for other languages. This lack of resources prevents the training of effective AS2 models in different languages, creating a performance gap between QA systems in English and other locales. In this paper, we introduce new high-quality datasets for AS2 in five European languages (French, German, Italian, Portuguese, and Spanish), obtained through supervised Automatic Machine Translation (AMT) of existing English AS2 datasets such as ASNQ, WikiQA, and TREC-QA using a Large Language Model (LLM). We evaluated our approach and the quality of the translated datasets through multiple experiments with different Transformer architectures. The results indicate that our datasets are pivotal in producing robust and powerful multilingual AS2 models, significantly contributing to closing the performance gap between English and other languages.",
            "link": "https://www.semanticscholar.org/paper/88faf932905250c11cdb52d62e91560958aba27d",
            "authors": "Matteo Gabburo, S. Campese, Federico Agostini, Alessandro Moschitti",
            "matchScore": 199.98068,
            "original title": "Datasets for Multilingual Answer Sentence Selection",
            "original authors": "Matteo Gabburo, Stefano Campese, Federico Agostini, Alessandro Moschitti",
            "EMNLP Paper ID": "1873",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "64fee1a3de0172e9044b90a87a52c689face9a1f",
            "title": "Multi-Target Cross-Lingual Summarization: a novel task and a language-neutral approach",
            "abstract": "Cross-lingual summarization aims to bridge language barriers by summarizing documents in different languages. However, ensuring semantic coherence across languages is an overlooked challenge and can be critical in several contexts. To fill this gap, we introduce multi-target cross-lingual summarization as the task of summarizing a document into multiple target languages while ensuring that the produced summaries are semantically similar. We propose a principled re-ranking approach to this problem and a multi-criteria evaluation protocol to assess semantic coherence across target languages, marking a first step that will hopefully stimulate further research on this problem.",
            "link": "https://www.semanticscholar.org/paper/64fee1a3de0172e9044b90a87a52c689face9a1f",
            "authors": "Diogo Pernes, Gonccalo M. Correia, Afonso Mendes",
            "matchScore": 272.6148,
            "original title": "Multi-Target Cross-Lingual Summarization: a novel task and a language-neutral approach",
            "original authors": "Diogo Pernes, Gon\u00e7alo M. Correia, Afonso Mendes",
            "EMNLP Paper ID": "2526",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "f59d7d9cdd1cb6d62227033074d27d466513b6ca",
            "title": "Multilingual Synopses of Movie Narratives: A Dataset for Story Understanding",
            "abstract": "Story video-text alignment, a core task in computational story understanding, aims to align video clips with corresponding sentences in their descriptions. However, progress on the task has been held back by the scarcity of manually annotated video-text correspondence and the heavy concentration on English narrations of Hollywood movies. To address these issues, in this paper, we construct a large-scale multi-lingual video story dataset named Multilingual Synopses of Movie Narratives (M-S Y M O N), containing 13,166 movie summary videos from 7 languages, as well as manual annotation of fine-grained video-text correspondences for 101.5 hours of video. Training on the human annotated data from M-S Y M O N outperforms the SOTA methods by 15.7 and 16.2 percentage points on Clip Accuracy and Sentence IoU scores, respectively, demonstrating the effectiveness of the annotations. As benchmarks for future research, we create 6 baseline approaches with different multilingual training strategies, compare their performance in both intra-lingual and cross-lingual setups, exemplifying the challenges of multilingual video-text alignment.",
            "link": "https://www.semanticscholar.org/paper/f59d7d9cdd1cb6d62227033074d27d466513b6ca",
            "authors": "Yidan Sun, Jianfei Yu, Boyang Li",
            "matchScore": 275.22006,
            "original title": "Multilingual Synopses of Movie Narratives: A Dataset for Story Understanding",
            "original authors": "Yidan Sun, Jianfei Yu, Boyang Li",
            "EMNLP Paper ID": "2618",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "74d6e2a3d2bc2c43fdd56c154c614edf26b3191b",
            "title": "Reference-based Metrics Disprove Themselves in Question Generation",
            "abstract": "Reference-based metrics such as BLEU and BERTScore are widely used to evaluate question generation (QG). In this study, on QG benchmarks such as SQuAD and HotpotQA, we find that using human-written references cannot guarantee the effectiveness of the reference-based metrics. Most QG benchmarks have only one reference; we replicate the annotation process and collect another reference. A good metric is expected to grade a human-validated question no worse than generated questions. However, the results of reference-based metrics on our newly collected reference disproved the metrics themselves. We propose a reference-free metric consisted of multi-dimensional criteria such as naturalness, answerability, and complexity, utilizing large language models. These criteria are not constrained to the syntactic or semantic of a single reference question, and the metric does not require a diverse set of references. Experiments reveal that our metric accurately distinguishes between high-quality questions and flawed ones, and achieves state-of-the-art alignment with human judgment.",
            "link": "https://www.semanticscholar.org/paper/74d6e2a3d2bc2c43fdd56c154c614edf26b3191b",
            "authors": "Bang Nguyen, Mengxia Yu, Yun Huang, Meng-Long Jiang",
            "matchScore": 256.34113,
            "original title": "Reference-based Metrics Disprove Themselves in Question Generation",
            "original authors": "Bang Nguyen, Mengxia Yu, Yun Huang, Meng Jiang",
            "EMNLP Paper ID": "2663",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Findings"
        },
        {
            "paperId": "7348810c869647f1cac03bec29089c2b40c4bff1",
            "title": "Disordered-DABS: A Benchmark for Dynamic Aspect-Based Summarization in Disordered Texts",
            "abstract": "Aspect-based summarization has seen significant advancements, especially in structured text. Yet, summarizing disordered, large-scale texts, like those found in social media and customer feedback, remains a significant challenge. Current research largely targets predefined aspects within structured texts, neglecting the complexities of dynamic and disordered environments. Addressing this gap, we introduce Disordered-DABS, a novel benchmark for dynamic aspect-based summarization tailored to unstructured text. Developed by adapting existing datasets for cost-efficiency and scalability, our comprehensive experiments and detailed human evaluations reveal that Disordered-DABS poses unique challenges to contemporary summarization models, including state-of-the-art language models such as GPT-3.5.",
            "link": "https://www.semanticscholar.org/paper/7348810c869647f1cac03bec29089c2b40c4bff1",
            "authors": "Xiaobo Guo, Soroush Vosoughi",
            "matchScore": 273.51984,
            "original title": "Disordered-DABS: A Benchmark for Dynamic Aspect-Based Summarization in Disordered Texts",
            "original authors": "Xiaobo Guo, Soroush Vosoughi",
            "EMNLP Paper ID": "73",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "a20381c4a44969054cf96deff2a413afe5e767f8",
            "title": "UniSumEval: Towards Unified, Fine-Grained, Multi-Dimensional Summarization Evaluation for LLMs",
            "abstract": "Existing benchmarks for summarization quality evaluation often lack diverse input scenarios, focus on narrowly defined dimensions (e.g., faithfulness), and struggle with subjective and coarse-grained annotation schemes. To address these shortcomings, we create UniSumEval benchmark, which extends the range of input context (e.g., domain, length) and provides fine-grained, multi-dimensional annotations. We use AI assistance in data creation, identifying potentially hallucinogenic input texts, and also helping human annotators reduce the difficulty of fine-grained annotation tasks. With UniSumEval, we benchmark nine latest language models as summarizers, offering insights into their performance across varying input contexts and evaluation dimensions. Furthermore, we conduct a thorough comparison of SOTA automated summary evaluators. Our benchmark data will be available at https://github.com/DISL-Lab/UniSumEval-v1.0.",
            "link": "https://www.semanticscholar.org/paper/a20381c4a44969054cf96deff2a413afe5e767f8",
            "authors": "Yuho Lee, Taewon Yun, Jason Cai, Hang Su, Hwanjun Song",
            "matchScore": 325.49036,
            "original title": "UniSumEval: Towards Unified, Fine-grained, Multi-dimensional Summarization Evaluation for LLMs",
            "original authors": "Yuho Lee, Taewon Yun, Jason Cai, Hang Su, Hwanjun Song",
            "EMNLP Paper ID": "786",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        }
    ],
    "Diverse Applications and Evaluations of Large Language Models (LLMs)": [
        {
            "paperId": "f503b95c0a64f6a84eb1d90e5ea1e094b1e1892b",
            "title": "Systematic Biases in LLM Simulations of Debates",
            "abstract": "The emergence of Large Language Models (LLMs), has opened exciting possibilities for constructing computational simulations designed to replicate human behavior accurately. Current research suggests that LLM-based agents become increasingly human-like in their performance, sparking interest in using these AI agents as substitutes for human participants in behavioral studies. However, LLMs are complex statistical learners without straightforward deductive rules, making them prone to unexpected behaviors. Hence, it is crucial to study and pinpoint the key behavioral distinctions between humans and LLM-based agents. In this study, we highlight the limitations of LLMs in simulating human interactions, particularly focusing on LLMs' ability to simulate political debates on topics that are important aspects of people's day-to-day lives and decision-making processes. Our findings indicate a tendency for LLM agents to conform to the model's inherent social biases despite being directed to debate from certain political perspectives. This tendency results in behavioral patterns that seem to deviate from well-established social dynamics among humans. We reinforce these observations using an automatic self-fine-tuning method, which enables us to manipulate the biases within the LLM and demonstrate that agents subsequently align with the altered biases. These results underscore the need for further research to develop methods that help agents overcome these biases, a critical step toward creating more realistic simulations.",
            "link": "https://www.semanticscholar.org/paper/f503b95c0a64f6a84eb1d90e5ea1e094b1e1892b",
            "authors": "Amir Taubenfeld, Yaniv Dover, Roi Reichart, Ariel Goldstein",
            "EMNLP Paper ID": "26",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "8aadf59ac62821683a5475a45cbc1a89e372027a",
            "title": "Tracking the perspectives of interacting language models",
            "abstract": "Large language models (LLMs) are capable of producing high quality information at unprecedented rates. As these models continue to entrench themselves in society, the content they produce will become increasingly pervasive in databases that are, in turn, incorporated into the pre-training data, fine-tuning data, retrieval data, etc. of other language models. In this paper we formalize the idea of a communication network of LLMs and introduce a method for representing the perspective of individual models within a collection of LLMs. Given these tools we systematically study information diffusion in the communication network of LLMs in various simulated settings.",
            "link": "https://www.semanticscholar.org/paper/8aadf59ac62821683a5475a45cbc1a89e372027a",
            "authors": "Hayden S. Helm, Brandon Duderstadt, Youngser Park, Carey E. Priebe",
            "EMNLP Paper ID": "179",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "4043201b9d7d232608b95d899e1a1e59bc2f0e56",
            "title": "I Need Help! Evaluating LLM's Ability to Ask for Users' Support: A Case Study on Text-to-SQL Generation",
            "abstract": "This study explores the proactive ability of LLMs to seek user support. We propose metrics to evaluate the trade-off between performance improvements and user burden, and investigate whether LLMs can determine when to request help under varying information availability. Our experiments show that without external feedback, many LLMs struggle to recognize their need for user support. The findings highlight the importance of external signals and provide insights for future research on improving support-seeking strategies. Source code: https://github.com/appier-research/i-need-help",
            "link": "https://www.semanticscholar.org/paper/4043201b9d7d232608b95d899e1a1e59bc2f0e56",
            "authors": "Cheng-Kuang Wu, Zhi Rui Tam, Chao-Chung Wu, Chieh-Yen Lin, Hung-yi Lee, Yun-Nung Chen",
            "EMNLP Paper ID": "247",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "ca01cb09c81af738b8108615115e7bfe96f44ec9",
            "title": "An Electoral Approach to Diversify LLM-based Multi-Agent Collective Decision-Making",
            "abstract": "Modern large language models (LLMs) have exhibited cooperative synergy on complex task-solving, and collective decision-making (CDM) is a pivotal component in LLM-based multi-agent collaboration frameworks. Our survey on 52 recent such systems uncovers a severe lack of diversity, with a heavy reliance on dictatorial and plurality voting for CDM. Through the lens of social choice theory, we scrutinize widely-adopted CDM methods and identify their limitations. To enrich current landscape of LLM-based CDM, we present GEDI, an electoral CDM module that incorporates various ordinal preferential voting mechanisms. Our empirical case study across three benchmarks shows that the integration of certain CDM methods can markedly improve the reasoning capabilities and robustness of some leading LLMs, all without requiring intricate system designs. Additionally, we find that some CDM mechanisms generate positive synergies even with as few as three agents. The voting-based methods also demonstrate robustness against single points of failure, as well as diversity in terms of hit-rate@k and subject-wise impacts.",
            "link": "https://www.semanticscholar.org/paper/ca01cb09c81af738b8108615115e7bfe96f44ec9",
            "authors": "Xiutian Zhao, Ke Wang, Wei Peng",
            "EMNLP Paper ID": "302",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "9c970ab6a3f8a5c3a5b3fcff19952edaaa79d24a",
            "title": "Understanding \"Democratization\" in NLP and ML Research",
            "abstract": "Recent improvements in natural language processing (NLP) and machine learning (ML) and increased mainstream adoption have led to researchers frequently discussing the\"democratization\"of artificial intelligence. In this paper, we seek to clarify how democratization is understood in NLP and ML publications, through large-scale mixed-methods analyses of papers using the keyword\"democra*\"published in NLP and adjacent venues. We find that democratization is most frequently used to convey (ease of) access to or use of technologies, without meaningfully engaging with theories of democratization, while research using other invocations of\"democra*\"tends to be grounded in theories of deliberation and debate. Based on our findings, we call for researchers to enrich their use of the term democratization with appropriate theory, towards democratic technologies beyond superficial access.",
            "link": "https://www.semanticscholar.org/paper/9c970ab6a3f8a5c3a5b3fcff19952edaaa79d24a",
            "authors": "Arjun Subramonian, Vagrant Gautam, Dietrich Klakow, Zeerak Talat",
            "EMNLP Paper ID": "352",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "cad94e9b1c8034c0a4bf74b601676f49f7697864",
            "title": "How Does the Disclosure of AI Assistance Affect the Perceptions of Writing?",
            "abstract": "Recent advances in generative AI technologies like large language models have boosted the incorporation of AI assistance in writing workflows, leading to the rise of a new paradigm of human-AI co-creation in writing. To understand how people perceive writings that are produced under this paradigm, in this paper, we conduct an experimental study to understand whether and how the disclosure of the level and type of AI assistance in the writing process would affect people's perceptions of the writing on various aspects, including their evaluation on the quality of the writing and their ranking of different writings. Our results suggest that disclosing the AI assistance in the writing process, especially if AI has provided assistance in generating new content, decreases the average quality ratings for both argumentative essays and creative stories. This decrease in the average quality ratings often comes with an increased level of variations in different individuals' quality evaluations of the same writing. Indeed, factors such as an individual's writing confidence and familiarity with AI writing assistants are shown to moderate the impact of AI assistance disclosure on their writing quality evaluations. We also find that disclosing the use of AI assistance may significantly reduce the proportion of writings produced with AI's content generation assistance among the top-ranked writings.",
            "link": "https://www.semanticscholar.org/paper/cad94e9b1c8034c0a4bf74b601676f49f7697864",
            "authors": "Zhuoyan Li, Chen Liang, Jing Peng, Ming Yin",
            "EMNLP Paper ID": "529",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "56e7bda25b83228f91962d3465fd587cfe8908e1",
            "title": "How Far Can We Extract Diverse Perspectives from Large Language Models? Criteria-Based Diversity Prompting!",
            "abstract": "Collecting diverse human opinions is costly and challenging. This leads to a recent trend in exploiting large language models (LLMs) for generating diverse data for potential scalable and efficient solutions. However, the extent to which LLMs can generate diverse perspectives on subjective topics is still unclear. In this study, we explore LLMs' capacity of generating diverse perspectives and rationales on subjective topics such as social norms and argumentative texts. We introduce the problem of extracting maximum diversity from LLMs. Motivated by how humans form opinions based on values, we propose a criteria-based prompting technique to ground diverse opinions. To see how far we can extract diverse perspectives from LLMs, or called diversity coverage, we employ a step-by-step recall prompting to generate more outputs from the model iteratively. Our methods, applied to various tasks, show that LLMs can indeed produce diverse opinions according to the degree of task subjectivity. We also find that LLM's performance of extracting maximum diversity is on par with human.",
            "link": "https://www.semanticscholar.org/paper/56e7bda25b83228f91962d3465fd587cfe8908e1",
            "authors": "Shirley Anugrah Hayati, Minhwa Lee, Dheeraj Rajagopal, Dongyeop Kang",
            "EMNLP Paper ID": "590",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "7e1303dd9296718f44a29577e9feb0d9562075e3",
            "title": "Investigating LLMs as Voting Assistants via Contextual Augmentation: A Case Study on the European Parliament Elections 2024",
            "abstract": "In light of the recent 2024 European Parliament elections, we are investigating if LLMs can be used as Voting Advice Applications (VAAs). We audit MISTRAL and MIXTRAL models and evaluate their accuracy in predicting the stance of political parties based on the latest\"EU and I\"voting assistance questionnaire. Furthermore, we explore alternatives to improve models' performance by augmenting the input context via Retrieval-Augmented Generation (RAG) relying on web search, and Self-Reflection using staged conversations that aim to re-collect relevant content from the model's internal memory. We find that MIXTRAL is highly accurate with an 82% accuracy on average with a significant performance disparity across different political groups (50-95%). Augmenting the input context with expert-curated information can lead to a significant boost of approx. 9%, which remains an open challenge for automated RAG approaches, even considering curated content.",
            "link": "https://www.semanticscholar.org/paper/7e1303dd9296718f44a29577e9feb0d9562075e3",
            "authors": "Ilias Chalkidis",
            "EMNLP Paper ID": "610",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 14): 13:00\u201314:00 (Afternoon)",
            "Type": "Main"
        },
        {
            "paperId": "fc5b8b891a4613a073c8ad14c3e2a465b799848f",
            "title": "Personas as a Way to Model Truthfulness in Language Models",
            "abstract": "Large language models (LLMs) are trained on vast amounts of text from the internet, which contains both factual and misleading information about the world. While unintuitive from a classic view of LMs, recent work has shown that the truth value of a statement can be elicited from the model's representations. This paper presents an explanation for why LMs appear to know the truth despite not being trained with truth labels. We hypothesize that the pretraining data is generated by groups of (un)truthful agents whose outputs share common features, and they form a (un)truthful persona. By training on this data, LMs can infer and represent the persona in its activation space. This allows the model to separate truth from falsehoods and controls the truthfulness of its generation. We show evidence for the persona hypothesis via two observations: (1) we can probe whether a model's answer will be truthful before it is generated; (2) finetuning a model on a set of facts improves its truthfulness on unseen topics. Next, using arithmetics as a synthetic environment, we show that structures of the pretraining data are crucial for the model to infer the truthful persona. Overall, our findings suggest that models can exploit hierarchical structures in the data to learn abstract concepts like truthfulness.",
            "link": "https://www.semanticscholar.org/paper/fc5b8b891a4613a073c8ad14c3e2a465b799848f",
            "authors": "Nitish Joshi, Javier Rando, Abulhair Saparov, Najoung Kim, He He",
            "EMNLP Paper ID": "714",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "bb0f51647fdd2083b21a698cb759f1593f4fba82",
            "title": "Diversity Over Size: On the Effect of Sample and Topic Sizes for Topic-Dependent Argument Mining Datasets",
            "abstract": "The task of Argument Mining, that is extracting and classifying argument components for a specific topic from large document sources, is an inherently difficult task for machine learning models and humans alike, as large Argument Mining datasets are rare and recognition of argument components requires expert knowledge. The task becomes even more difficult if it also involves stance detection of retrieved arguments. In this work, we investigate the effect of Argument Mining dataset composition in few- and zero-shot settings. Our findings show that, while fine-tuning is mandatory to achieve acceptable model performance, using carefully composed training samples and reducing the training sample size by up to almost 90% can still yield 95% of the maximum performance. This gain is consistent across three Argument Mining tasks on three different datasets. We also publish a new dataset for future benchmarking.",
            "link": "https://www.semanticscholar.org/paper/bb0f51647fdd2083b21a698cb759f1593f4fba82",
            "authors": "Benjamin Schiller, Johannes Daxenberger, Andreas Waldis, Iryna Gurevych",
            "EMNLP Paper ID": "1240",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "4ab03200801816b27d1363373e9c55c115c4b09b",
            "title": "LLM Task Interference: An Initial Study on the Impact of Task-Switch in Conversational History",
            "abstract": "With the recent emergence of powerful instruction-tuned large language models (LLMs), various helpful conversational Artificial Intelligence (AI) systems have been deployed across many applications. When prompted by users, these AI systems successfully perform a wide range of tasks as part of a conversation. To provide some sort of memory and context, such approaches typically condition their output on the entire conversational history. Although this sensitivity to the conversational history can often lead to improved performance on subsequent tasks, we find that performance can in fact also be negatively impacted, if there is a task-switch. To the best of our knowledge, our work makes the first attempt to formalize the study of such vulnerabilities and interference of tasks in conversational LLMs caused by task-switches in the conversational history. Our experiments across 5 datasets with 15 task switches using popular LLMs reveal that many of the task-switches can lead to significant performance degradation.",
            "link": "https://www.semanticscholar.org/paper/4ab03200801816b27d1363373e9c55c115c4b09b",
            "authors": "Akash Gupta, Ivaxi Sheth, Vyas Raina, Mark J. F. Gales, Mario Fritz",
            "EMNLP Paper ID": "1685",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "dc499f1df26d388428297c96c49a390a9264de30",
            "title": "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators",
            "abstract": "Large Language Models (LLMs) and AI assistants driven by these models are experiencing exponential growth in usage among both expert and amateur users. In this work, we focus on evaluating the reliability of current LLMs as science communicators. Unlike existing benchmarks, our approach emphasizes assessing these models on scientific questionanswering tasks that require a nuanced understanding and awareness of answerability. We introduce a novel dataset, SCiPS-QA, comprising 742 Yes/No queries embedded in complex scientific concepts, along with a benchmarking suite that evaluates LLMs for correctness and consistency across various criteria. We benchmark three proprietary LLMs from the OpenAI GPT family and 13 open-access LLMs from the Meta Llama-2, Llama-3, and Mistral families. While most open-access models significantly underperform compared to GPT-4 Turbo, our experiments identify Llama-3-70B as a strong competitor, often surpassing GPT-4 Turbo in various evaluation aspects. We also find that even the GPT models exhibit a general incompetence in reliably verifying LLM responses. Moreover, we observe an alarming trend where human evaluators are deceived by incorrect responses from GPT-4 Turbo.",
            "link": "https://www.semanticscholar.org/paper/dc499f1df26d388428297c96c49a390a9264de30",
            "authors": "Prasoon Bajpai, Niladri Chatterjee, Subhabrata Dutta, Tanmoy Chakraborty",
            "EMNLP Paper ID": "1866",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "adf985e8ef9ae5699444d9180d03d13a2b2ed525",
            "title": "AutoPersuade: A Framework for Evaluating and Explaining Persuasive Arguments",
            "abstract": "We introduce AutoPersuade, a three-part framework for constructing persuasive messages. First, we curate a large dataset of arguments with human evaluations. Next, we develop a novel topic model to identify argument features that influence persuasiveness. Finally, we use this model to predict the effectiveness of new arguments and assess the causal impact of different components to provide explanations. We validate AutoPersuade through an experimental study on arguments for veganism, demonstrating its effectiveness with human studies and out-of-sample predictions.",
            "link": "https://www.semanticscholar.org/paper/adf985e8ef9ae5699444d9180d03d13a2b2ed525",
            "authors": "Till Raphael Saenger, Musashi Hinck, Justin Grimmer, Brandon M. Stewart",
            "EMNLP Paper ID": "1925",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "4046475556d334a61612c63622a40676907c7b5d",
            "title": "The LLM Effect: Are Humans Truly Using LLMs, or Are They Being Influenced By Them Instead?",
            "abstract": "Large Language Models (LLMs) have shown capabilities close to human performance in various analytical tasks, leading researchers to use them for time and labor-intensive analyses. However, their capability to handle highly specialized and open-ended tasks in domains like policy studies remains in question. This paper investigates the efficiency and accuracy of LLMs in specialized tasks through a structured user study focusing on Human-LLM partnership. The study, conducted in two stages-Topic Discovery and Topic Assignment-integrates LLMs with expert annotators to observe the impact of LLM suggestions on what is usually human-only analysis. Results indicate that LLM-generated topic lists have significant overlap with human generated topic lists, with minor hiccups in missing document-specific topics. However, LLM suggestions may significantly improve task completion speed, but at the same time introduce anchoring bias, potentially affecting the depth and nuance of the analysis, raising a critical question about the trade-off between increased efficiency and the risk of biased analysis.",
            "link": "https://www.semanticscholar.org/paper/4046475556d334a61612c63622a40676907c7b5d",
            "authors": "Alexander S. Choi, Syeda Sabrina Akter, JP Singh, Antonios Anastasopoulos",
            "EMNLP Paper ID": "3080",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "af61c4f2c9e4ed58d1cd3daf8cd4dbf916f5f4e7",
            "title": "The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models",
            "abstract": "Recent advances in Large Language Models (LLMs) have sparked wide interest in validating and comprehending the human-like cognitive-behavioral traits LLMs may capture and convey. These cognitive-behavioral traits include typically Attitudes, Opinions, Values (AOVs). However, measuring AOVs embedded within LLMs remains opaque, and different evaluation methods may yield different results. This has led to a lack of clarity on how different studies are related to each other and how they can be interpreted. This paper aims to bridge this gap by providing a comprehensive overview of recent works on the evaluation of AOVs in LLMs. Moreover, we survey related approaches in different stages of the evaluation pipeline in these works. By doing so, we address the potential and challenges with respect to understanding the model, human-AI alignment, and downstream application in social sciences. Finally, we provide practical insights into evaluation methods, model enhancement, and interdisciplinary collaboration, thereby contributing to the evolving landscape of evaluating AOVs in LLMs.",
            "link": "https://www.semanticscholar.org/paper/af61c4f2c9e4ed58d1cd3daf8cd4dbf916f5f4e7",
            "authors": "Bolei Ma, Xinpeng Wang, Tiancheng Hu, Anna Haensch, Michael A. Hedderich, Barbara Plank, Frauke Kreuter",
            "matchScore": 224.02757,
            "original title": "The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models",
            "original authors": "Bolei Ma, Xinpeng Wang, Tiancheng Hu, Anna-Carolina Haensch, Michael A. Hedderich, Barbara Plank, Frauke Kreuter",
            "EMNLP Paper ID": "1835",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "edf260dee56a06d897547fb460a1e317d7eb571b",
            "title": "Can Language Models Recognize Convincing Arguments?",
            "abstract": "The capabilities of large language models (LLMs) have raised concerns about their potential to create and propagate convincing narratives. Here, we study their performance in detecting convincing arguments to gain insights into LLMs' persuasive capabilities without directly engaging in experimentation with humans. We extend a dataset by Durmus and Cardie (2018) with debates, votes, and user traits and propose tasks measuring LLMs' ability to (1) distinguish between strong and weak arguments, (2) predict stances based on beliefs and demographic characteristics, and (3) determine the appeal of an argument to an individual based on their traits. We show that LLMs perform on par with humans in these tasks and that combining predictions from different LLMs yields significant performance gains, surpassing human performance. The data and code released with this paper contribute to the crucial effort of continuously evaluating and monitoring LLMs' capabilities and potential impact. (https://go.epfl.ch/persuasion-llm)",
            "link": "https://www.semanticscholar.org/paper/edf260dee56a06d897547fb460a1e317d7eb571b",
            "authors": "Paula Rescala, Manoel Horta Ribeiro, Tiancheng Hu, Robert West",
            "matchScore": 220.4083,
            "original title": "Can Language Models Recognize Convincing Arguments?",
            "original authors": "Paula Rescala, Manoel Horta Ribeiro, Tiancheng Hu, Robert West",
            "EMNLP Paper ID": "1839",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "0012eb77f5627d4f960a0c8c1f4848033a59e52f",
            "title": "Can LLM be a Personalized Judge?",
            "abstract": "Ensuring that large language models (LLMs) reflect diverse user values and preferences is crucial as their user bases expand globally. It is therefore encouraging to see the growing interest in LLM personalization within the research community. However, current works often rely on the LLM-as-a-Judge approach for evaluation without thoroughly examining its validity. In this paper, we investigate the reliability of LLM-as-a-Personalized-Judge, asking LLMs to judge user preferences based on personas. Our findings suggest that directly applying LLM-as-a-Personalized-Judge is less reliable than previously assumed, showing low and inconsistent agreement with human ground truth. The personas typically used are often overly simplistic, resulting in low predictive power. To address these issues, we introduce verbal uncertainty estimation into the LLM-as-a-Personalized-Judge pipeline, allowing the model to express low confidence on uncertain judgments. This adjustment leads to much higher agreement (above 80%) on high-certainty samples for binary tasks. Through human evaluation, we find that the LLM-as-a-Personalized-Judge achieves comparable performance to third-party humans evaluation and even surpasses human performance on high-certainty samples. Our work indicates that certainty-enhanced LLM-as-a-Personalized-Judge offers a promising direction for developing more reliable and scalable methods for evaluating LLM personalization.",
            "link": "https://www.semanticscholar.org/paper/0012eb77f5627d4f960a0c8c1f4848033a59e52f",
            "authors": "Yijiang River Dong, Tiancheng Hu, Nigel Collier",
            "matchScore": 173.76468,
            "original title": "Can LLM be a Personalized Judge?",
            "original authors": "Yijiang River Dong, Tiancheng Hu, Nigel Collier",
            "EMNLP Paper ID": "2078",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "540937fd9669776b9a2234ea584435aa0f3bd163",
            "title": "Zero-shot Persuasive Chatbots with LLM-Generated Strategies and Information Retrieval",
            "abstract": "Persuasion plays a pivotal role in a wide range of applications from health intervention to the promotion of social good. Persuasive chatbots employed responsibly for social good can be an enabler of positive individual and social change. Existing methods rely on fine-tuning persuasive chatbots with task-specific training data which is costly, if not infeasible, to collect. Furthermore, they employ only a handful of pre-defined persuasion strategies. We propose PersuaBot, a zero-shot chatbot based on Large Language Models (LLMs) that is factual and more persuasive by leveraging many more nuanced strategies. PersuaBot uses an LLM to first generate natural responses, from which the strategies used are extracted. To combat hallucination of LLMs, Persuabot replace any unsubstantiated claims in the response with retrieved facts supporting the extracted strategies. We applied our chatbot, PersuaBot, to three significantly different domains needing persuasion skills: donation solicitation, recommendations, and health intervention. Our experiments on simulated and human conversations show that our zero-shot approach is more persuasive than prior work, while achieving factual accuracy surpassing state-of-the-art knowledge-oriented chatbots.",
            "link": "https://www.semanticscholar.org/paper/540937fd9669776b9a2234ea584435aa0f3bd163",
            "authors": "Kazuaki Furumai, Roberto Legaspi, Julio Vizcarra, Yudai Yamazaki, Yasutaka Nishimura, Sina J. Semnani, Kazushi Ikeda, Weiyan Shi, Monica S. Lam",
            "matchScore": 293.59332,
            "original title": "Zero-shot Persuasive Chatbots with LLM-Generated Strategies and Information Retrieval",
            "original authors": "Kazuaki Furumai, Roberto Legaspi, Julio Cesar Vizcarra Romero, Yudai Yamazaki, Yasutaka Nishimura, Sina Semnani, Kazushi Ikeda, Weiyan Shi, Monica Lam",
            "EMNLP Paper ID": "2227",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "4adc9c77af90257b21e9efd54b8935f8dbff979e",
            "title": "Data-Centric AI in the Age of Large Language Models",
            "abstract": "This position paper proposes a data-centric viewpoint of AI research, focusing on large language models (LLMs). We start by making the key observation that data is instrumental in the developmental (e.g., pretraining and fine-tuning) and inferential stages (e.g., in-context learning) of LLMs, and yet it receives disproportionally low attention from the research community. We identify four specific scenarios centered around data, covering data-centric benchmarks and data curation, data attribution, knowledge transfer, and inference contextualization. In each scenario, we underscore the importance of data, highlight promising research directions, and articulate the potential impacts on the research community and, where applicable, the society as a whole. For instance, we advocate for a suite of data-centric benchmarks tailored to the scale and complexity of data for LLMs. These benchmarks can be used to develop new data curation methods and document research efforts and results, which can help promote openness and transparency in AI and LLM research.",
            "link": "https://www.semanticscholar.org/paper/4adc9c77af90257b21e9efd54b8935f8dbff979e",
            "authors": "Xinyi Xu, Zhaoxuan Wu, Rui Qiao, Arun Verma, Yao Shu, Jingtan Wang, Xinyuan Niu, Zhenfeng He, Jiangwei Chen, Zijian Zhou, Gregory Kang Ruey Lau, Hieu Dao, Lucas Agussurja, Rachael Hwee Ling Sim, Xiaoqiang Lin, Wenyang Hu, Zhongxiang Dai, Pang Wei Koh, B. Low",
            "matchScore": 186.58847,
            "original title": "Position Paper: Data-Centric AI in the Age of Large Language Models",
            "original authors": "Xinyi Xu, Zhaoxuan Wu, Rui Qiao, Arun Verma, Yao Shu, Jingtan Wang, Xinyuan Niu, Zhenfeng He, Jiangwei Chen, Zijian Zhou, Gregory Kang Ruey Lau, Hieu Dao, Lucas Agussurja, Rachael Hwee Ling Sim, Xiaoqiang Lin, Wenyang Hu, Zhongxiang Dai, Pang Wei Koh, Bryan Kian Hsiang Low",
            "EMNLP Paper ID": "2338",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "156f1a58661814f60ab4abaf027f64e47d1c4fdf",
            "title": "Persuasiveness of Generated Free-Text Rationales in Subjective Decisions: A Case Study on Pairwise Argument Ranking",
            "abstract": "Generating free-text rationales is among the emergent capabilities of Large Language Models (LLMs). These rationales have been found to enhance LLM performance across various NLP tasks. Recently, there has been growing interest in using these rationales to provide insights for various important downstream tasks. In this paper, we analyze generated free-text rationales in tasks with subjective answers, emphasizing the importance of rationalization in such scenarios. We focus on pairwise argument ranking, a highly subjective task with significant potential for real-world applications, such as debate assistance. We evaluate the persuasiveness of rationales generated by nine LLMs to support their subjective choices. Our findings suggest that open-source LLMs, particularly Llama2-70B-chat, are capable of providing highly persuasive rationalizations, surpassing even GPT models. Additionally, our experiments show that rationale persuasiveness can be improved by controlling its parameters through prompting or through self-refinement.",
            "link": "https://www.semanticscholar.org/paper/156f1a58661814f60ab4abaf027f64e47d1c4fdf",
            "authors": "Mohamed S. Elaraby, Diane Litman, Xiang Lorraine Li, Ahmed Magooda",
            "matchScore": 326.41187,
            "original title": "Persuasiveness of Generated Free-Text Rationales in Subjective Decisions: A Case Study on Pairwise Argument Ranking",
            "original authors": "Mohamed Elaraby, Diane Litman, Xiang Lorraine Li, Ahmed Magooda",
            "EMNLP Paper ID": "2767",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "4b8525b580010336a36bf6339f9918ca61674ed2",
            "title": "Are Large Language Models (LLMs) Good Social Predictors?",
            "abstract": "The prediction has served as a crucial scientific method in modern social studies. With the recent advancement of Large Language Models (LLMs), efforts have been made to leverage LLMs to predict the human features in social life, such as presidential voting. These works suggest that LLMs are capable of generating human-like responses. However, we find that the promising performance achieved by previous studies is because of the existence of input shortcut features to the response. In fact, by removing these shortcuts, the performance is reduced dramatically. To further revisit the ability of LLMs, we introduce a novel social prediction task, Soc-PRF Prediction, which utilizes general features as input and simulates real-world social study settings. With the comprehensive investigations on various LLMs, we reveal that LLMs cannot work as expected on social prediction when given general input features without shortcuts. We further investigate possible reasons for this phenomenon that suggest potential ways to enhance LLMs for social prediction.",
            "link": "https://www.semanticscholar.org/paper/4b8525b580010336a36bf6339f9918ca61674ed2",
            "authors": "Kaiqi Yang, Hang Li, Hongzhi Wen, Tai-Quan Peng, Jiliang Tang, Hui Liu",
            "matchScore": 229.60803,
            "original title": "Are Large Language Models (LLMs) Good Social Predictors?",
            "original authors": "Kaiqi Yang, Hang Li, Hongzhi Wen, Tai-Quan Peng, Jiliang Tang, Hui Liu",
            "EMNLP Paper ID": "556",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "4c801cbd501841d4440e3670a9961938af9d5407",
            "title": "A Psycholinguistic Evaluation of Language Models' Sensitivity to Argument Roles",
            "abstract": "We present a systematic evaluation of large language models' sensitivity to argument roles, i.e., who did what to whom, by replicating psycholinguistic studies on human argument role processing. In three experiments, we find that language models are able to distinguish verbs that appear in plausible and implausible contexts, where plausibility is determined through the relation between the verb and its preceding arguments. However, none of the models capture the same selective patterns that human comprehenders exhibit during real-time verb prediction. This indicates that language models' capacity to detect verb plausibility does not arise from the same mechanism that underlies human real-time sentence processing.",
            "link": "https://www.semanticscholar.org/paper/4c801cbd501841d4440e3670a9961938af9d5407",
            "authors": "Eun-Kyoung Rosa Lee, Sathvik Nair, Naomi H. Feldman",
            "matchScore": 232.3392,
            "original title": "A Psycholinguistic Evaluation of Language Models\u2019 Sensitivity to Argument Roles",
            "original authors": "Eun-Kyoung Rosa Lee, Sathvik Nair, Naomi Feldman",
            "EMNLP Paper ID": "668",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Findings"
        },
        {
            "paperId": "097d26af3bd536537a9a679ca5e0156082e9ebf5",
            "title": "Can Large Language Models Identify Authorship?",
            "abstract": "The ability to accurately identify authorship is crucial for verifying content authenticity and mitigating misinformation. Large Language Models (LLMs) have demonstrated an exceptional capacity for reasoning and problem-solving. However, their potential in authorship analysis remains under-explored. Traditional studies have depended on hand-crafted stylistic features, whereas state-of-the-art approaches leverage text embeddings from pre-trained language models. These methods, which typically require fine-tuning on labeled data, often suffer from performance degradation in cross-domain applications and provide limited explainability. This work seeks to address three research questions: (1) Can LLMs perform zero-shot, end-to-end authorship verification effectively? (2) Are LLMs capable of accurately attributing authorship among multiple candidates authors (e.g., 10 and 20)? (3) Can LLMs provide explainability in authorship analysis, particularly through the role of linguistic features? Moreover, we investigate the integration of explicit linguistic features to guide LLMs in their reasoning processes. Our assessment demonstrates LLMs' proficiency in both tasks without the need for domain-specific fine-tuning, providing explanations into their decision making via a detailed analysis of linguistic features. This establishes a new benchmark for future research on LLM-based authorship analysis.",
            "link": "https://www.semanticscholar.org/paper/097d26af3bd536537a9a679ca5e0156082e9ebf5",
            "authors": "Baixiang Huang, Canyu Chen, Kai Shu",
            "matchScore": 186.02084,
            "original title": "Can Large Language Models Identify Authorship?",
            "original authors": "Baixiang Huang, Canyu Chen, Kai Shu",
            "EMNLP Paper ID": "82",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        }
    ],
    "Tokenization and Efficient Inference in LLMs": [
        {
            "paperId": "c74326259a24bbba3e5130f0ee42a546ea31301b",
            "title": "Tokenization Is More Than Compression",
            "abstract": "Tokenization is a foundational step in natural language processing (NLP) tasks, bridging raw text and language models. Existing tokenization approaches like Byte-Pair Encoding (BPE) originate from the field of data compression, and it has been suggested that the effectiveness of BPE stems from its ability to condense text into a relatively small number of tokens. We test the hypothesis that fewer tokens lead to better downstream performance by introducing PathPiece, a new tokenizer that segments a document's text into the minimum number of tokens for a given vocabulary. Through extensive experimentation we find this hypothesis not to be the case, casting doubt on the understanding of the reasons for effective tokenization. To examine which other factors play a role, we evaluate design decisions across all three phases of tokenization: pre-tokenization, vocabulary construction, and segmentation, offering new insights into the design of effective tokenizers. Specifically, we illustrate the importance of pre-tokenization and the benefits of using BPE to initialize vocabulary construction. We train 64 language models with varying tokenization, ranging in size from 350M to 2.4B parameters, all of which are made publicly available.",
            "link": "https://www.semanticscholar.org/paper/c74326259a24bbba3e5130f0ee42a546ea31301b",
            "authors": "Craig W. Schmidt, Varshini Reddy, Haoran Zhang, Alec Alameddine, Omri Uzan, Yuval Pinter, Chris Tanner",
            "EMNLP Paper ID": "90",
            "Oral/Poster": "Oral",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "c1ee5b660d297cdca90f220ce619ec6c0434ea50",
            "title": "CUTE: Measuring LLMs' Understanding of Their Tokens",
            "abstract": "Large Language Models (LLMs) show remarkable performance on a wide variety of tasks. Most LLMs split text into multi-character tokens and process them as atomic units without direct access to individual characters. This raises the question: To what extent can LLMs learn orthographic information? To answer this, we propose a new benchmark, CUTE, which features a collection of tasks designed to test the orthographic knowledge of LLMs. We evaluate popular LLMs on CUTE, finding that most of them seem to know the spelling of their tokens, yet fail to use this information effectively to manipulate text, calling into question how much of this knowledge is generalizable.",
            "link": "https://www.semanticscholar.org/paper/c1ee5b660d297cdca90f220ce619ec6c0434ea50",
            "authors": "Lukas Edman, Helmut Schmid, Alexander Fraser",
            "EMNLP Paper ID": "339",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "f4a6b2d967c727c0e777d2d9e4e08f490e32dd2a",
            "title": "Leading Whitespaces of Language Models' Subword Vocabulary Poses a Confound for Calculating Word Probabilities",
            "abstract": "Predictions of word-by-word conditional probabilities from Transformer-based language models are often evaluated to model the incremental processing difficulty of human readers. In this paper, we argue that there is a confound posed by the most common method of aggregating subword probabilities of such language models into word probabilities. This is due to the fact that tokens in the subword vocabulary of most language models have leading whitespaces and therefore do not naturally define stop probabilities of words. We first prove that this can result in distributions over word probabilities that sum to more than one, thereby violating the axiom that $\\mathsf{P}(\\Omega) = 1$. This property results in a misallocation of word-by-word surprisal, where the unacceptability of the end of the current word is incorrectly carried over to the next word. Additionally, this implicit prediction of word boundaries incorrectly models psycholinguistic experiments where human subjects directly observe upcoming word boundaries. We present a simple decoding technique to reaccount the probability of the trailing whitespace into that of the current word, which resolves this confound. Experiments show that this correction reveals lower estimates of garden-path effects in transitive/intransitive sentences and poorer fits to naturalistic reading times.",
            "link": "https://www.semanticscholar.org/paper/f4a6b2d967c727c0e777d2d9e4e08f490e32dd2a",
            "authors": "Byung-Doh Oh, William Schuler",
            "EMNLP Paper ID": "393",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "b0ec37343d0a4df1efbf991a6655e51c47a7103f",
            "title": "Where is the signal in tokenization space?",
            "abstract": "Large Language Models (LLMs) are typically shipped with tokenizers that deterministically encode text into so-called canonical token sequences, to which the LLMs assign probability values. One common assumption is that the probability of a piece of text is the probability of its canonical token sequence. However, the tokenization of a string is not unique: e.g., the Llama2 tokenizer encodes Tokens as [Tok,ens], but [Tok,en,s] also represents the same text. In this paper, we study non-canonical tokenizations. We prove that, given a string, it is computationally hard to find the most likely tokenization for an autoregressive LLM, as well as to compute the marginal probability over all possible tokenizations. We then show how the marginal is, in most cases, indistinguishable from the canonical probability. Surprisingly, we then empirically demonstrate the existence of a significant amount of signal hidden within tokenization space. Notably, by simply aggregating the probabilities of non-canonical tokenizations, we achieve improvements across a range of LLM evaluation benchmarks for a variety of architectures, including transformers and state space models.",
            "link": "https://www.semanticscholar.org/paper/b0ec37343d0a4df1efbf991a6655e51c47a7103f",
            "authors": "Renato Lui Geh, Honghua Zhang, Kareem Ahmed, Benjie Wang, Guy Van den Broeck",
            "EMNLP Paper ID": "445",
            "Oral/Poster": "Oral",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "1fb76e69ee4180204f9480853abc8c7bc5d4ddcf",
            "title": "Context-Aware Assistant Selection for Improved Inference Acceleration with Large Language Models",
            "abstract": "Despite their widespread adoption, large language models (LLMs) remain prohibitive to use under resource constraints, with their ever growing sizes only increasing the barrier for use. One noted issue is the high latency associated with auto-regressive generation, rendering large LLMs use dependent on advanced computing infrastructure. Assisted decoding, where a smaller draft model guides a larger target model's generation, has helped alleviate this, but remains dependent on alignment between the two models. Thus if the draft model is insufficiently capable on some domain relative to the target model, performance can degrade. Alternatively, one can leverage multiple draft models to better cover the expertise of the target, but when multiple black-box draft models are available, selecting an assistant without details about its construction can be difficult. To better understand this decision making problem, we observe it as a contextual bandit, where a policy must choose a draft model based on a context. We show that even without prior knowledge of the draft models, creating an offline dataset from only outputs of independent draft/target models and training a policy over the alignment of these outputs can accelerate performance on multiple domains provided the candidates are effective. Further results show this to hold on various settings with multiple assisted decoding candidates, highlighting its flexibility and the advantageous role that such decision making can play.",
            "link": "https://www.semanticscholar.org/paper/1fb76e69ee4180204f9480853abc8c7bc5d4ddcf",
            "authors": "Jerry Huang, Prasanna Parthasarathi, Mehdi Rezagholizadeh, Sarath Chandar",
            "EMNLP Paper ID": "649",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "d613e92a30f19029d842d8ef7d73f8ad4dbbd3ae",
            "title": "Optimized Speculative Sampling for GPU Hardware Accelerators",
            "abstract": "In this work, we optimize speculative sampling for parallel hardware accelerators to improve sampling speed. We notice that substantial portions of the intermediate matrices necessary for speculative sampling can be computed concurrently. This allows us to distribute the workload across multiple GPU threads, enabling simultaneous operations on matrix segments within thread blocks. This results in profiling time improvements ranging from 6% to 13% relative to the baseline implementation, without compromising accuracy. To further accelerate speculative sampling, probability distributions parameterized by softmax are approximated by sigmoid. This approximation approach results in significantly greater relative improvements in profiling time, ranging from 37% to 94%, with a minor decline in accuracy. We conduct extensive experiments on both automatic speech recognition and summarization tasks to validate the effectiveness of our optimization methods.",
            "link": "https://www.semanticscholar.org/paper/d613e92a30f19029d842d8ef7d73f8ad4dbbd3ae",
            "authors": "Dominik Wagner, Seanie Lee, Ilja Baumann, Philipp Seeberger, K. Riedhammer, T. Bocklet",
            "EMNLP Paper ID": "723",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "c06c8aff22a7e446e938ab3c2ba5e9bcfc1cefd3",
            "title": "Lexically Grounded Subword Segmentation",
            "abstract": "We present three innovations in tokenization and subword segmentation. First, we propose to use unsupervised morphological analysis with Morfessor as pre-tokenization. Second, we present an algebraic method for obtaining subword embeddings grounded in a word embedding space. Based on that, we design a novel subword segmentation algorithm that uses the embeddings, ensuring that the procedure considers lexical meaning. Third, we introduce an efficient segmentation algorithm based on a subword bigram model that can be initialized with the lexically aware segmentation method to avoid using Morfessor and large embedding tables at inference time. We evaluate the proposed approaches using two intrinsic metrics and measure their performance on two downstream tasks: part-of-speech tagging and machine translation. Our experiments show significant improvements in the morphological plausibility of the segmentation when evaluated using segmentation precision on morpheme boundaries and improved R\\'enyi efficiency in 8 languages. Although the proposed tokenization methods do not have a large impact on automatic translation quality, we observe consistent performance gains in the arguably more morphological task of part-of-speech tagging.",
            "link": "https://www.semanticscholar.org/paper/c06c8aff22a7e446e938ab3c2ba5e9bcfc1cefd3",
            "authors": "Jindvrich Libovick'y, Jindvrich Helcl",
            "EMNLP Paper ID": "835",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "cab58a0263d454604896dce6b8fbf4df1dd99ff0",
            "title": "EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees",
            "abstract": "Inference with modern Large Language Models (LLMs) is expensive and time-consuming, and speculative sampling has proven to be an effective solution. Most speculative sampling methods such as EAGLE use a static draft tree, implicitly assuming that the acceptance rate of draft tokens depends only on their position. Interestingly, we found that the acceptance rate of draft tokens is also context-dependent. In this paper, building upon EAGLE, we propose EAGLE-2, which introduces a new technique of context-aware dynamic draft tree into drafting modeling. This improvement leverages the fact that the draft model of EAGLE is well-calibrated: the confidence scores from the draft model approximate acceptance rates with small errors. We conducted extensive evaluations on three series of LLMs and six tasks, with EAGLE-2 achieving speedup ratios 3.05x-4.26x, which is 20%-40% faster than EAGLE-1. EAGLE-2 also ensures that the distribution of the generated text remains unchanged, making it a lossless acceleration algorithm.",
            "link": "https://www.semanticscholar.org/paper/cab58a0263d454604896dce6b8fbf4df1dd99ff0",
            "authors": "Yuhui Li, Fangyun Wei, Chao Zhang, Hongyang Zhang",
            "EMNLP Paper ID": "836",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "e7742c309e98795692809cae6f66e01d1b5a884e",
            "title": "Gold Panning in Vocabulary: An Adaptive Method for Vocabulary Expansion of Domain-Specific LLMs",
            "abstract": "While Large Language Models (LLMs) demonstrate impressive generation abilities, they frequently struggle when it comes to specialized domains due to their limited domain-specific knowledge. Studies on domain-specific LLMs resort to expanding the vocabulary before fine-tuning on domain-specific corpus, aiming to decrease the sequence length and enhance efficiency during decoding, without thoroughly investigating the results of vocabulary expansion to LLMs over different domains. Our pilot study reveals that expansion with only a subset of the entire vocabulary may lead to superior performance. Guided by the discovery, this paper explores how to identify a vocabulary subset to achieve the optimal results. We introduce VEGAD, an adaptive method that automatically identifies valuable words from a given domain vocabulary. Our method has been validated through experiments on three Chinese datasets, demonstrating its effectiveness. Additionally, we have undertaken comprehensive analyses of the method. The selection of a optimal subset for expansion has shown to enhance performance on both domain-specific tasks and general tasks, showcasing the potential of VEGAD.",
            "link": "https://www.semanticscholar.org/paper/e7742c309e98795692809cae6f66e01d1b5a884e",
            "authors": "Chengyuan Liu, Shihang Wang, Lizhi Qing, Kun Kuang, Yangyang Kang, Changlong Sun, Fei Wu",
            "EMNLP Paper ID": "839",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "0247c8299c1ead8fb2d4e5b82aeb9f1058048c87",
            "title": "A Thorough Examination of Decoding Methods in the Era of LLMs",
            "abstract": "Decoding methods play an indispensable role in converting language models from next-token predictors into practical task solvers. Prior research on decoding methods, primarily focusing on task-specific models, may not extend to the current era of general-purpose large language models (LLMs). Moreover, the recent influx of decoding strategies has further complicated this landscape. This paper provides a comprehensive and multifaceted analysis of various decoding methods within the context of LLMs, evaluating their performance, robustness to hyperparameter changes, and decoding speeds across a wide range of tasks, models, and deployment environments. Our findings reveal that decoding method performance is notably task-dependent and influenced by factors such as alignment, model size, and quantization. Intriguingly, sensitivity analysis exposes that certain methods achieve superior performance at the cost of extensive hyperparameter tuning, highlighting the trade-off between attaining optimal results and the practicality of implementation in varying contexts.",
            "link": "https://www.semanticscholar.org/paper/0247c8299c1ead8fb2d4e5b82aeb9f1058048c87",
            "authors": "Chufan Shi, Haoran Yang, Deng Cai, Zhisong Zhang, Yifan Wang, Yujiu Yang, Wai Lam",
            "EMNLP Paper ID": "996",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "e4f7aa7e40fd13a3930a75f3af5a43964509e21f",
            "title": "Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs",
            "abstract": "LLMs process text as sequences of tokens that roughly correspond to words, where less common words are represented by multiple tokens. However, individual tokens are often semantically unrelated to the meanings of the words/concepts they comprise. For example, Llama-2-7b's tokenizer splits the word\"northeastern\"into the tokens ['_n', 'ort', 'he', 'astern'], none of which correspond to semantically meaningful units like\"north\"or\"east.\"Similarly, the overall meanings of named entities like\"Neil Young\"and multi-word expressions like\"break a leg\"cannot be directly inferred from their constituent tokens. Mechanistically, how do LLMs convert such arbitrary groups of tokens into useful higher-level representations? In this work, we find that last token representations of named entities and multi-token words exhibit a pronounced\"erasure\"effect, where information about previous and current tokens is rapidly forgotten in early layers. Using this observation, we propose a method to\"read out\"the implicit vocabulary of an autoregressive LLM by examining differences in token representations across layers, and present results of this method for Llama-2-7b and Llama-3-8B. To our knowledge, this is the first attempt to probe the implicit vocabulary of an LLM.",
            "link": "https://www.semanticscholar.org/paper/e4f7aa7e40fd13a3930a75f3af5a43964509e21f",
            "authors": "Sheridan Feucht, David Atkinson, Byron C. Wallace, David Bau",
            "EMNLP Paper ID": "1082",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "36b169e1c44b83f0b8b39b62800bf3fdf80470d1",
            "title": "Distributional Properties of Subword Regularization",
            "abstract": "Subword regularization, used widely in NLP, improves model performance by reducing the dependency on exact tokenizations, augmenting the training corpus, and exposing the model to more unique contexts during training. BPE and MaxMatch, two popular subword tokenization schemes, have stochastic dropout regularization variants. However, there has not been an analysis of the distributions formed by them. We show that these stochastic variants are heavily biased towards a small set of tokenizations per word. If the benefits of subword regularization are as mentioned, we hypothesize that biasedness artificially limits the effectiveness of these schemes. Thus, we propose an algorithm to uniformly sample tokenizations that we use as a drop-in replacement for the stochastic aspects of existing tokenizers, and find that it improves machine translation quality.",
            "link": "https://www.semanticscholar.org/paper/36b169e1c44b83f0b8b39b62800bf3fdf80470d1",
            "authors": "Marco Cognetta, Vil\u00e9m Zouhar, Naoaki Okazaki",
            "EMNLP Paper ID": "1223",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "43fff8982ae0db83ebc001d9f36ff2787cb1d8a5",
            "title": "Towards Fast Multilingual LLM Inference: Speculative Decoding and Specialized Drafters",
            "abstract": "Large language models (LLMs) have revolutionized natural language processing and broadened their applicability across diverse commercial applications. However, the deployment of these models is constrained by high inference time in multilingual settings. To mitigate this challenge, this paper explores a training recipe of an assistant model in speculative decoding, which are leveraged to draft and-then its future tokens are verified by the target LLM. We show that language-specific draft models, optimized through a targeted pretrain-and-finetune strategy, substantially brings a speedup of inference time compared to the previous methods. We validate these models across various languages in inference time, out-of-domain speedup, and GPT-4o evaluation.",
            "link": "https://www.semanticscholar.org/paper/43fff8982ae0db83ebc001d9f36ff2787cb1d8a5",
            "authors": "Euiin Yi, Taehyeon Kim, Hongseok Jeung, Du-Seong Chang, SeYoung Yun",
            "EMNLP Paper ID": "1227",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "4985da3fa08a74644bfb244c1fe5f0c64330fade",
            "title": "Decoding with Limited Teacher Supervision Requires Understanding When to Trust the Teacher",
            "abstract": "How can small-scale large language models (LLMs) efficiently utilize the supervision of LLMs to improve their generative quality? This question has been well studied in scenarios where there is no restriction on the number of LLM supervisions one can use, giving birth to many decoding algorithms that utilize supervision without further training. However, it is still unclear what is an effective strategy under the $\\textit{limited supervision}$ scenario, where we assume that no more than a few tokens can be generated by LLMs. To this end, we develop an algorithm to effectively aggregate the small-scale LLM and LLM predictions on initial tokens so that the generated tokens can more accurately condition the subsequent token generation by small-scale LLM only. Critically, we find that it is essential to adaptively overtrust or disregard the LLM prediction based on the confidence of the small-scale LLM. Through our experiments on a wide range of models and datasets, we demonstrate that our method provides a consistent improvement over conventional decoding strategies. $\\small$ $\\textbf{Code:}$ https://github.com/HJ-Ok/DecLimSup",
            "link": "https://www.semanticscholar.org/paper/4985da3fa08a74644bfb244c1fe5f0c64330fade",
            "authors": "Hyunjong Ok, Jegwang Ryu, Jaeho Lee",
            "EMNLP Paper ID": "1455",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "a03cb5e9806e532966a13e4fab7cef9c8d81dec5",
            "title": "Make Some Noise: Unlocking Language Model Parallel Inference Capability through Noisy Training",
            "abstract": "Existing speculative decoding methods typically require additional model structure and training processes to assist the model for draft token generation. This makes the migration of acceleration methods to the new model more costly and more demanding on device memory. To address this problem, we propose the Make Some Noise (MSN) training framework as a replacement for the supervised fine-tuning stage of the large language model. The training method simply introduces some noise at the input for the model to learn the denoising task. It significantly enhances the parallel decoding capability of the model without affecting the original task capability. In addition, we propose a tree-based retrieval-augmented Jacobi (TR-Jacobi) decoding strategy to further improve the inference speed of MSN models. Experiments in both the general and code domains have shown that MSN can improve inference speed by 2.3-2.7x times without compromising model performance. The MSN model also achieves comparable acceleration ratios to the SOTA model with additional model structure on Spec-Bench.",
            "link": "https://www.semanticscholar.org/paper/a03cb5e9806e532966a13e4fab7cef9c8d81dec5",
            "authors": "Yixuan Wang, Xianzhen Luo, Fuxuan Wei, Yijun Liu, Qingfu Zhu, Xuanyu Zhang, Qing Yang, Dongliang Xu, Wanxiang Che",
            "EMNLP Paper ID": "1499",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "4623901ec1f8311b32e27bd627b2bce56161c6bb",
            "title": "Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding",
            "abstract": "Speculative decoding is a widely used method that accelerates the generation process of large language models (LLMs) with no compromise in model performance. It achieves this goal by using an existing smaller model for drafting and then employing the target LLM to verify the draft in a low-cost parallel manner. Under such a drafting-verification framework, drafting efficiency has become a bottleneck in the final speedup of speculative decoding. Therefore, generating longer drafts at less cost can lead to better decoding speedup. To achieve this, we introduce Ouroboros, which can generate draft phrases to parallelize the drafting process and meanwhile lengthen drafts in a training-free manner. The experimental results on various typical text generation tasks show that Ouroboros can achieve speedups of up to $2.8\\times$ over speculative decoding and $3.9\\times$ over vanilla decoding, without fine-tuning draft and target models. The source code of Ouroboros is available at https://github.com/thunlp/Ouroboros.",
            "link": "https://www.semanticscholar.org/paper/4623901ec1f8311b32e27bd627b2bce56161c6bb",
            "authors": "Weilin Zhao, Yuxiang Huang, Xu Han, Wang Xu, Chaojun Xiao, Xinrong Zhang, Yewei Fang, Kaihuo Zhang, Zhiyuan Liu, Maosong Sun",
            "EMNLP Paper ID": "1551",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "08df6e6286b22907601292a106c71ec1a602eb5e",
            "title": "BPE Gets Picky: Efficient Vocabulary Refinement During Tokenizer Training",
            "abstract": "Language models can largely benefit from efficient tokenization. However, they still mostly utilize the classical BPE algorithm, a simple and reliable method. This has been shown to cause such issues as under-trained tokens and sub-optimal compression that may affect the downstream performance. We introduce Picky BPE, a modified BPE algorithm that carries out vocabulary refinement during tokenizer training. Our method improves vocabulary efficiency, eliminates under-trained tokens, and does not compromise text compression. Our experiments show that our method does not reduce the downstream performance, and in several cases improves it.",
            "link": "https://www.semanticscholar.org/paper/08df6e6286b22907601292a106c71ec1a602eb5e",
            "authors": "Pavel Chizhov, Catherine Arnett, Elizaveta Korotkova, Ivan P. Yamshchikov",
            "EMNLP Paper ID": "1954",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "96c3a6156546d0447fa2b3327e55bc5973e01d57",
            "title": "FFN-SkipLLM: A Hidden Gem for Autoregressive Decoding with Adaptive Feed Forward Skipping",
            "abstract": "Autoregressive Large Language Models (e.g., LLaMa, GPTs) are omnipresent achieving remarkable success in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges for autoregressive token-by-token generation. To mitigate computation overload incurred during generation, several early-exit and layer-dropping strategies have been proposed. Despite some promising success due to the redundancy across LLMs layers on metrics like Rough-L/BLUE, our careful knowledge-intensive evaluation unveils issues such as generation collapse, hallucination of wrong facts, and noticeable performance drop even at the trivial exit ratio of 10-15% of layers. We attribute these errors primarily to ineffective handling of the KV cache through state copying during early-exit. In this work, we observed the saturation of computationally expensive feed-forward blocks of LLM layers and proposed FFN-SkipLLM, which is a novel fine-grained skip strategy of autoregressive LLMs. More specifically, FFN-SkipLLM is an input-adaptive feed-forward skipping strategy that can skip 25-30% of FFN blocks of LLMs with marginal change in performance on knowledge-intensive generation tasks without any requirement to handle KV cache. Our extensive experiments and ablation across benchmarks like MT-Bench, Factoid-QA, and variable-length text summarization illustrate how our simple and ease-at-use method can facilitate faster autoregressive decoding.",
            "link": "https://www.semanticscholar.org/paper/96c3a6156546d0447fa2b3327e55bc5973e01d57",
            "authors": "A. Jaiswal, Bodun Hu, Lu Yin, Yeonju Ro, Shiwei Liu, Tianlong Chen, Aditya Akella",
            "EMNLP Paper ID": "1993",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "94dbc06cd2c891f5cebe1f6af392c1a6b2b35a1b",
            "title": "How to Compute the Probability of a Word",
            "abstract": "Language models (LMs) estimate a probability distribution over strings in a natural language; these distributions are crucial for computing perplexity and surprisal in linguistics research. While we are usually concerned with measuring these values for words, most LMs operate over subwords. Despite seemingly straightforward, accurately computing probabilities over one unit given probabilities over the other requires care. Indeed, we show here that many recent linguistic studies have been incorrectly computing these values. This paper derives the correct methods for computing word probabilities, highlighting issues when relying on language models that use beginning-of-word (bow)-marking tokenisers, e.g., the GPT family. Empirically, we show that correcting the widespread bug in probability computations affects measured outcomes in sentence comprehension and lexical optimisation analyses.",
            "link": "https://www.semanticscholar.org/paper/94dbc06cd2c891f5cebe1f6af392c1a6b2b35a1b",
            "authors": "Tiago Pimentel, Clara Meister",
            "EMNLP Paper ID": "2284",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "aa00170f631ad7e732875d3ff13d2c2dda0419e5",
            "title": "Generation with Dynamic Vocabulary",
            "abstract": "We introduce a new dynamic vocabulary for language models. It can involve arbitrary text spans during generation. These text spans act as basic generation bricks, akin to tokens in the traditional static vocabularies. We show that, the ability to generate multi-tokens atomically improve both generation quality and efficiency (compared to the standard language model, the MAUVE metric is increased by 25%, the latency is decreased by 20%). The dynamic vocabulary can be deployed in a plug-and-play way, thus is attractive for various downstream applications. For example, we demonstrate that dynamic vocabulary can be applied to different domains in a training-free manner. It also helps to generate reliable citations in question answering tasks (substantially enhancing citation results without compromising answer accuracy).",
            "link": "https://www.semanticscholar.org/paper/aa00170f631ad7e732875d3ff13d2c2dda0419e5",
            "authors": "Yanting Liu, Tao Ji, Changzhi Sun, Yuanbin Wu, Xiaoling Wang",
            "EMNLP Paper ID": "2380",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "ef2ebc205bf8d7c4cea73ff8774689f65d426e71",
            "title": "Temperature-Centric Investigation of Speculative Decoding with Knowledge Distillation",
            "abstract": "Speculative decoding stands as a pivotal technique to expedite inference in autoregressive (large) language models. This method employs a smaller draft model to speculate a block of tokens, which the target model then evaluates for acceptance. Despite a wealth of studies aimed at increasing the efficiency of speculative decoding, the influence of generation configurations on the decoding process remains poorly understood, especially concerning decoding temperatures. This paper delves into the effects of decoding temperatures on speculative decoding's efficacy. Beginning with knowledge distillation (KD), we first highlight the challenge of decoding at higher temperatures, and demonstrate KD in a consistent temperature setting could be a remedy. We also investigate the effects of out-of-domain testing sets with out-of-range temperatures. Building upon these findings, we take an initial step to further the speedup for speculative decoding, particularly in a high-temperature generation setting. Our work offers new insights into how generation configurations drastically affect the performance of speculative decoding, and underscores the need for developing methods that focus on diverse decoding configurations. Code is publically available at https://github.com/ozyyshr/TempSpec.",
            "link": "https://www.semanticscholar.org/paper/ef2ebc205bf8d7c4cea73ff8774689f65d426e71",
            "authors": "Siru Ouyang, Shuohang Wang, Minhao Jiang, Ming Zhong, Dong Yu, Jiawei Han, Yelong Shen",
            "matchScore": 229.47882,
            "original title": "Temperature-Centric Investigation of Speculative Decoding with Knowledge Distillation",
            "original authors": "Siru Ouyang, Shuohang Wang, Minhao Jiang, Ming Zhong, Donghan Yu, Jiawei Han, yelong shen",
            "EMNLP Paper ID": "2561",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "36f438b56387d10da4f6d2b1c551fc685ae4ec8e",
            "title": "Adaptive BPE Tokenization for Enhanced Vocabulary Adaptation in Finetuning Pretrained Language Models",
            "abstract": "In this work, we show a fundamental limitation in vocabulary adaptation approaches that use Byte-Pair Encoding (BPE) tokenization scheme for fine-tuning pretrained language models (PLMs) to expert domains. Current approaches trivially append the target domain-specific vocabulary at the end of the PLM vocabulary. This approach leads to a lower priority score and causes sub-optimal tokenization in BPE that iteratively uses merge rules to tokenize a given text. To mitigate this issue, we propose AdaptBPE where the BPE tokenization initialization phase is modified to first perform the longest string matching on the added (target) vocabulary before tokenizing at the character level. We perform an extensive evaluation of AdaptBPE versus the standard BPE over various classification and summarization tasks; AdaptBPE improves by 3.57% (in terms of accuracy) and 1.87% (in terms of Rouge-L), respectively. AdaptBPE for MEDVOC works particularly well when reference summaries have high OOV concentration or are longer in length. We also conduct a human evaluation, revealing that AdaptBPE generates more relevant and more faithful summaries as compared to MEDVOC. We make our codebase publicly available at https://github.com/gb-kgp/adaptbpe.",
            "link": "https://www.semanticscholar.org/paper/36f438b56387d10da4f6d2b1c551fc685ae4ec8e",
            "authors": "Gunjan Balde, Soumyadeep Roy, Mainack Mondal, Niloy Ganguly",
            "matchScore": 298.96643,
            "original title": "Adaptive BPE Tokenization for Enhanced Vocabulary Adaptation in Finetuning Pretrained Language Models",
            "original authors": "Gunjan Balde, Soumyadeep Roy, Mainack Mondal, Niloy Ganguly",
            "EMNLP Paper ID": "2836",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "d4e25f1a01580d7b1d3be2ca500fb6a03b2042b6",
            "title": "Draft on the Fly: Adaptive Self-Speculative Decoding using Cosine Similarity",
            "abstract": "We present a simple on the fly method for faster inference of large language models. Unlike other (self-)speculative decoding techniques, our method does not require fine-tuning or black-box optimization to generate a fixed draft model, relying instead on simple rules to generate varying draft models adapted to the input context. We show empirically that our light-weight algorithm is competitive with the current SOTA for self-speculative decoding, while being a truly plug-and-play method.",
            "link": "https://www.semanticscholar.org/paper/d4e25f1a01580d7b1d3be2ca500fb6a03b2042b6",
            "authors": "Michael R. Metel, Peng Lu, Boxing Chen, Mehdi Rezagholizadeh, I. Kobyzev",
            "matchScore": 297.6864,
            "original title": "Draft on the Fly: Adaptive Self-Speculative Decoding using Cosine Similarity",
            "original authors": "Michael R. Metel, Peng Lu, Boxing Chen, Mehdi Rezagholizadeh, Ivan Kobyzev",
            "EMNLP Paper ID": "451",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        }
    ],
    "Multimodal Models for Visual Reasoning and Chart Interpretation": [
        {
            "paperId": "415e25e8e3c8c10255abbb704d545f9d02c54c76",
            "title": "GeoGPT4V: Towards Geometric Multi-modal Large Language Models with Geometric Image Generation",
            "abstract": "Large language models have seen widespread adoption in math problem-solving. However, in geometry problems that usually require visual aids for better understanding, even the most advanced multi-modal models currently still face challenges in effectively using image information. High-quality data is crucial for enhancing the geometric capabilities of multi-modal models, yet existing open-source datasets and related efforts are either too challenging for direct model learning or suffer from misalignment between text and images. To overcome this issue, we introduce a novel pipeline that leverages GPT-4 and GPT-4V to generate relatively basic geometry problems with aligned text and images, facilitating model learning. We have produced a dataset of 4.9K geometry problems and combined it with 19K open-source data to form our GeoGPT4V dataset. Experimental results demonstrate that the GeoGPT4V dataset significantly improves the geometry performance of various models on the MathVista and MathVision benchmarks. The code is available at https://github.com/Lanyu0303/GeoGPT4V_Project",
            "link": "https://www.semanticscholar.org/paper/415e25e8e3c8c10255abbb704d545f9d02c54c76",
            "authors": "Shihao Cai, Keqin Bao, Hangyu Guo, Jizhi Zhang, Jun Song, Bo Zheng",
            "EMNLP Paper ID": "98",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "7530ff872ad718828fe44fe4610d50a5c48e7063",
            "title": "TopViewRS: Vision-Language Models as Top-View Spatial Reasoners",
            "abstract": "Top-view perspective denotes a typical way in which humans read and reason over different types of maps, and it is vital for localization and navigation of humans as well as of `non-human' agents, such as the ones backed by large Vision-Language Models (VLMs). Nonetheless, spatial reasoning capabilities of modern VLMs remain unattested and underexplored. In this work, we thus study their capability to understand and reason over spatial relations from the top view. The focus on top view also enables controlled evaluations at different granularity of spatial reasoning; we clearly disentangle different abilities (e.g., recognizing particular objects versus understanding their relative positions). We introduce the TopViewRS (Top-View Reasoning in Space) dataset, consisting of 11,384 multiple-choice questions with either realistic or semantic top-view map as visual input. We then use it to study and evaluate VLMs across 4 perception and reasoning tasks with different levels of complexity. Evaluation of 10 representative open- and closed-source VLMs reveals the gap of more than 50% compared to average human performance, and it is even lower than the random baseline in some cases. Although additional experiments show that Chain-of-Thought reasoning can boost model capabilities by 5.82% on average, the overall performance of VLMs remains limited. Our findings underscore the critical need for enhanced model capability in top-view spatial reasoning and set a foundation for further research towards human-level proficiency of VLMs in real-world multimodal tasks.",
            "link": "https://www.semanticscholar.org/paper/7530ff872ad718828fe44fe4610d50a5c48e7063",
            "authors": "Chengzu Li, Caiqi Zhang, Han Zhou, Nigel Collier, Anna Korhonen, Ivan Vuli'c",
            "EMNLP Paper ID": "202",
            "Oral/Poster": "Oral",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "24354722e36c358b69893ab05220d6f2291989d1",
            "title": "TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning",
            "abstract": "Charts are important for presenting and explaining complex data relationships. Recently, multimodal large language models (MLLMs) have shown remarkable capabilities in various chart understanding tasks. However, the sheer size of these models in terms of parameters and computational requirements limits their use in resource-constrained environments. In this paper, we present TinyChart, an efficient MLLM for chart understanding with only 3B parameters. TinyChart overcomes two key challenges in efficient chart understanding: (1) reduce the burden of learning numerical computations through a Program-of-Thoughts (PoT) learning strategy, which trains the model to generate Python programs for numerical calculations, and (2) reduce lengthy vision feature sequences produced by the vision transformer for high-resolution images through a Vision Token Merging module, which gradually merges most similar vision tokens. Extensive experiments demonstrate that our 3B TinyChart achieves SOTA performance on a variety of chart understanding benchmarks including ChartQA, Chart-to-Text, Chart-to-Table, OpenCQA, and ChartX. It outperforms several chart understanding MLLM with up to 13B parameters such as ChartLlama and ChartAst, and close-sourced general-purpose MLLM GPT-4V on ChartQA. It also demonstrates its superior efficiency with higher throughput during inference due to a smaller model scale and more efficient vision encoding. Our code and model are available at https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/TinyChart.",
            "link": "https://www.semanticscholar.org/paper/24354722e36c358b69893ab05220d6f2291989d1",
            "authors": "Liang Zhang, Anwen Hu, Haiyang Xu, Mingshi Yan, Yichen Xu, Qin Jin, Ji Zhang, Fei Huang",
            "EMNLP Paper ID": "212",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "fc753026180f9d76f82a0bd292b2d801c264cb02",
            "title": "Enhancing Advanced Visual Reasoning Ability of Large Language Models",
            "abstract": "Recent advancements in Vision-Language (VL) research have sparked new benchmarks for complex visual reasoning, challenging models' advanced reasoning ability. Traditional Vision-Language Models (VLMs) perform well in visual perception tasks while struggling with complex reasoning scenarios. Conversely, Large Language Models (LLMs) demonstrate robust text reasoning capabilities; however, they lack visual acuity. To bridge this gap, we propose Complex Visual Reasoning Large Language Models (CVR-LLM), capitalizing on VLMs' visual perception proficiency and LLMs' extensive reasoning capability. Unlike recent multimodal large language models (MLLMs) that require a projection layer, our approach transforms images into detailed, context-aware descriptions using an iterative self-refinement loop and leverages LLMs' text knowledge for accurate predictions without extra training. We also introduce a novel multi-modal in-context learning (ICL) methodology to enhance LLMs' contextual understanding and reasoning. Additionally, we introduce Chain-of-Comparison (CoC), a step-by-step comparison technique enabling contrasting various aspects of predictions. Our CVR-LLM presents the first comprehensive study across a wide array of complex visual reasoning tasks and achieves SOTA performance among all.",
            "link": "https://www.semanticscholar.org/paper/fc753026180f9d76f82a0bd292b2d801c264cb02",
            "authors": "Zhiyuan Li, Dongnan Liu, Chaoyi Zhang, Heng Wang, Tengfei Xue, Weidong Cai",
            "EMNLP Paper ID": "215",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "318069651dada8a413fcf9441589fafefecbcad3",
            "title": "Selective Vision is the Challenge for Visual Reasoning: A Benchmark for Visual Argument Understanding",
            "abstract": "Visual arguments, often used in advertising or social causes, rely on images to persuade viewers to do or believe something. Understanding these arguments requires selective vision: only specific visual stimuli within an image are relevant to the argument, and relevance can only be understood within the context of a broader argumentative structure. While visual arguments are readily appreciated by human audiences, we ask: are today's AI capable of similar understanding? We present VisArgs, a dataset of 1,611 images annotated with 5,112 visual premises (with regions), 5,574 commonsense premises, and reasoning trees connecting them into structured arguments. We propose three tasks for evaluating visual argument understanding: premise localization, premise identification, and conclusion deduction. Experiments show that 1) machines struggle to capture visual cues: GPT-4-O achieved 78.5% accuracy, while humans reached 98.0%. Models also performed 19.5% worse when distinguishing between irrelevant objects within the image compared to external objects. 2) Providing relevant visual premises improved model performance significantly.",
            "link": "https://www.semanticscholar.org/paper/318069651dada8a413fcf9441589fafefecbcad3",
            "authors": "Jiwan Chung, Sungjae Lee, Minseo Kim, Seungju Han, Ashkan Yousefpour, Jack Hessel, Youngjae Yu",
            "EMNLP Paper ID": "271",
            "Oral/Poster": "Oral",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "9a8dddfd7e9e975a4d3a63174e9a4a07de389c14",
            "title": "From the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis",
            "abstract": "We explore multi-step reasoning in vision-language models (VLMs). The problem is challenging, as reasoning data consisting of multiple steps of visual and language processing are barely available. To overcome the challenge, we first introduce a least-to-most visual reasoning paradigm, which interleaves steps of decomposing a question into sub-questions and invoking external tools for resolving sub-questions. Based on the paradigm, we further propose a novel data synthesis approach that can automatically create questions and multi-step reasoning paths for an image in a bottom-up manner. Our approach divides the complex synthesis task into a few simple sub-tasks, and (almost entirely) relies on open-sourced models to accomplish the sub-tasks. Therefore, the entire synthesis process is reproducible and cost-efficient, and the synthesized data is quality guaranteed. With the approach, we construct $50$k visual reasoning examples. Then, we develop a visual reasoner through supervised fine-tuning, which is capable of generally enhancing the reasoning abilities of a wide range of existing VLMs in a plug-and-play fashion. Extensive experiments indicate that the visual reasoner can consistently and significantly improve four VLMs on four VQA benchmarks. Our code and dataset are available at https://github.com/steven-ccq/VisualReasoner.",
            "link": "https://www.semanticscholar.org/paper/9a8dddfd7e9e975a4d3a63174e9a4a07de389c14",
            "authors": "Chuanqi Cheng, Jian Guan, Wei Wu, Rui Yan",
            "EMNLP Paper ID": "541",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "538b6f1d0ce39e568aaf53a7f49812e693f1a0cb",
            "title": "Do Text-to-Vis Benchmarks Test Real Use of Visualisations?",
            "abstract": "Large language models are able to generate code for visualisations in response to simple user requests. This is a useful application and an appealing one for NLP research because plots of data provide grounding for language. However, there are relatively few benchmarks, and those that exist may not be representative of what users do in practice. This paper investigates whether benchmarks reflect real-world use through an empirical study comparing benchmark datasets with code from public repositories. Our findings reveal a substantial gap, with evaluations not testing the same distribution of chart types, attributes, and actions as real-world examples. One dataset is representative, but requires extensive modification to become a practical end-to-end benchmark. This shows that new benchmarks are needed to support the development of systems that truly address users' visualisation needs. These observations will guide future data creation, highlighting which features hold genuine significance for users.",
            "link": "https://www.semanticscholar.org/paper/538b6f1d0ce39e568aaf53a7f49812e693f1a0cb",
            "authors": "Hy Nguyen, Xuefei He, Andrew Reeson, Cecile Paris, Josiah Poon, Jonathan K. Kummerfeld",
            "EMNLP Paper ID": "838",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "50a436fed0a4a996c7dd4edc084343645e8d4dcd",
            "title": "Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens Grounding",
            "abstract": "Graphical User Interfaces (GUIs) are central to our interaction with digital devices and growing efforts have been made to build models for various GUI understanding tasks. However, these efforts largely overlook an important GUI-referring task: screen reading based on user-indicated points, which we name the Screen Point-and-Read (ScreenPR) task. Currently, this task is predominantly handled by rigid accessible screen reading tools, in great need of new models driven by advancements in Multimodal Large Language Models (MLLMs). In this paper, we propose a Tree-of-Lens (ToL) agent, utilizing a novel ToL grounding mechanism, to address the ScreenPR task. Based on the input point coordinate and the corresponding GUI screenshot, our ToL agent constructs a Hierarchical Layout Tree. Based on the tree, our ToL agent not only comprehends the content of the indicated area but also articulates the layout and spatial relationships between elements. Such layout information is crucial for accurately interpreting information on the screen, distinguishing our ToL agent from other screen reading tools. We also thoroughly evaluate the ToL agent against other baselines on a newly proposed ScreenPR benchmark, which includes GUIs from mobile, web, and operating systems. Last but not least, we test the ToL agent on mobile GUI navigation tasks, demonstrating its utility in identifying incorrect actions along the path of agent execution trajectories. Code and data: https://screen-point-and-read.github.io",
            "link": "https://www.semanticscholar.org/paper/50a436fed0a4a996c7dd4edc084343645e8d4dcd",
            "authors": "Yue Fan, Lei Ding, Ching-Chen Kuo, Shan Jiang, Yang Zhao, Xinze Guan, Jie Yang, Yi Zhang, Xin Eric Wang",
            "EMNLP Paper ID": "1064",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "fab9078698495dc2d5fdbce955a0970a34f7f7be",
            "title": "Text2Chart31: Instruction Tuning for Chart Generation with Automatic Feedback",
            "abstract": "Large language models (LLMs) have demonstrated strong capabilities across various language tasks, notably through instruction-tuning methods. However, LLMs face challenges in visualizing complex, real-world data through charts and plots. Firstly, existing datasets rarely cover a full range of chart types, such as 3D, volumetric, and gridded charts. Secondly, supervised fine-tuning methods do not fully leverage the intricate relationships within rich datasets, including text, code, and figures. To address these challenges, we propose a hierarchical pipeline and a new dataset for chart generation. Our dataset, Text2Chart31, includes 31 unique plot types referring to the Matplotlib library, with 11.1K tuples of descriptions, code, data tables, and plots. Moreover, we introduce a reinforcement learning-based instruction tuning technique for chart generation tasks without requiring human feedback. Our experiments show that this approach significantly enhances the model performance, enabling smaller models to outperform larger open-source models and be comparable to state-of-the-art proprietary models in data visualization tasks. We make the code and dataset available at https://github.com/fatemehpesaran310/Text2Chart31.",
            "link": "https://www.semanticscholar.org/paper/fab9078698495dc2d5fdbce955a0970a34f7f7be",
            "authors": "Fatemeh Pesaran zadeh, Juyeon Kim, Jin-Hwa Kim, Gunhee Kim",
            "EMNLP Paper ID": "1337",
            "Oral/Poster": "Oral",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "73307406e1d7fc49fd0c1944e362b207508b148b",
            "title": "CommVQA: Situating Visual Question Answering in Communicative Contexts",
            "abstract": "Current visual question answering (VQA) models tend to be trained and evaluated on image-question pairs in isolation. However, the questions people ask are dependent on their informational needs and prior knowledge about the image content. To evaluate how situating images within naturalistic contexts shapes visual questions, we introduce CommVQA, a VQA dataset consisting of images, image descriptions, real-world communicative scenarios where the image might appear (e.g., a travel website), and follow-up questions and answers conditioned on the scenario and description. CommVQA, which contains 1000 images and 8,949 question-answer pairs, poses a challenge for current models. Error analyses and a human-subjects study suggest that generated answers still contain high rates of hallucinations, fail to fittingly address unanswerable questions, and don't suitably reflect contextual information. Overall, we show that access to contextual information is essential for solving CommVQA, leading to the highest performing VQA model and highlighting the relevance of situating systems within communicative scenarios.",
            "link": "https://www.semanticscholar.org/paper/73307406e1d7fc49fd0c1944e362b207508b148b",
            "authors": "N. Naik, Christopher Potts, Elisa Kreiss",
            "EMNLP Paper ID": "1550",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "7b32e4e18dcd680ae2aee356542f022047e782ca",
            "title": "Reasoning Paths with Reference Objects Elicit Quantitative Spatial Reasoning in Large Vision-Language Models",
            "abstract": "Despite recent advances demonstrating vision-language models' (VLMs) abilities to describe complex relationships in images using natural language, their capability to quantitatively reason about object sizes and distances remains underexplored. In this work, we introduce a manually annotated benchmark, Q-Spatial Bench, with 271 questions across five categories designed for quantitative spatial reasoning and systematically investigate the performance of state-of-the-art VLMs on this task. Our analysis reveals that reasoning about distances between objects is particularly challenging for SoTA VLMs; however, some VLMs significantly outperform others, with an over 40-point gap between the two best performing models. We also make the surprising observation that the success rate of the top-performing VLM increases by 19 points when a reasoning path using a reference object emerges naturally in the response. Inspired by this observation, we develop a zero-shot prompting technique, SpatialPrompt, that encourages VLMs to answer quantitative spatial questions using reference objects as visual cues. By instructing VLMs to use reference objects in their reasoning paths via SpatialPrompt, Gemini 1.5 Pro, Gemini 1.5 Flash, and GPT-4V improve their success rates by over 40, 20, and 30 points, respectively. We emphasize that these significant improvements are obtained without needing more data, model architectural modifications, or fine-tuning.",
            "link": "https://www.semanticscholar.org/paper/7b32e4e18dcd680ae2aee356542f022047e782ca",
            "authors": "Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, David Acuna",
            "EMNLP Paper ID": "2007",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "507c4ae9196d68e18ec448c80403801581d63447",
            "title": "Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model",
            "abstract": "Although most current large multimodal models (LMMs) can already understand photos of natural scenes and portraits, their understanding of abstract images, e.g., charts, maps, or layouts, and visual reasoning capabilities remains quite rudimentary. They often struggle with simple daily tasks, such as reading time from a clock, understanding a flowchart, or planning a route using a road map. In light of this, we design a multi-modal self-instruct, utilizing large language models and their code capabilities to synthesize massive abstract images and visual reasoning instructions across daily scenarios. Our strategy effortlessly creates a multimodal benchmark with 11,193 instructions for eight visual scenarios: charts, tables, simulated maps, dashboards, flowcharts, relation graphs, floor plans, and visual puzzles. \\textbf{This benchmark, constructed with simple lines and geometric elements, exposes the shortcomings of most advanced LMMs} like Claude-3.5-Sonnet and GPT-4o in abstract image understanding, spatial relations reasoning, and visual element induction. Besides, to verify the quality of our synthetic data, we fine-tune an LMM using 62,476 synthetic chart, table and road map instructions. The results demonstrate improved chart understanding and map navigation performance, and also demonstrate potential benefits for other visual reasoning tasks. Our code is available at: \\url{https://github.com/zwq2018/Multi-modal-Self-instruct}.",
            "link": "https://www.semanticscholar.org/paper/507c4ae9196d68e18ec448c80403801581d63447",
            "authors": "Wenqi Zhang, Zhenglin Cheng, Yuanyu He, Mengna Wang, Yongliang Shen, Zeqi Tan, Guiyang Hou, Mingqian He, Yanna Ma, Weiming Lu, Yueting Zhuang",
            "EMNLP Paper ID": "2441",
            "Oral/Poster": "Oral",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "f3b13403a6d6608c987f5b23c5bf41cb17fe79d7",
            "title": "Losing Visual Needles in Image Haystacks: Vision Language Models are Easily Distracted in Short and Long Contexts",
            "abstract": "We present LoCoVQA, a dynamic benchmark generator for evaluating long-context extractive reasoning in vision language models (VLMs). LoCoVQA augments test examples for mathematical reasoning, VQA, and character recognition tasks with increasingly long visual contexts composed of both in-distribution and out-of-distribution distractor images. Across these tasks, a diverse set of VLMs rapidly lose performance as the visual context length grows, often exhibiting a striking logarithmic decay trend. This test assesses how well VLMs can ignore irrelevant information when answering queries -- a task that is quite easy for language models (LMs) in the text domain -- demonstrating that current state-of-the-art VLMs lack this essential capability for many long-context applications.",
            "link": "https://www.semanticscholar.org/paper/f3b13403a6d6608c987f5b23c5bf41cb17fe79d7",
            "authors": "Aditya Sharma, Michael Stephen Saxon, William Yang Wang, Haim-ing Bao, Mo Bavarian, Jeff Belgum, Ir-wan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brock-man, Tim Brooks, M. Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, B. Chess, Chester Cho, Hyung Casey Chu, Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Tarun Goel, Gabriel Gogineni, Rapha Goh, Jonathan Gontijo-Lopes, Morgan Gordon, Scott Grafstein, Ryan Gray, Joshua Greene, Shixiang Shane Gross, Yufei Gu, Chris Guo, Jesse Hallacy, Jeff Han, Harris Yuchen, Mike He, Johannes Heaton, C. Heidecke, Alan Hesse, Wade Hickey, Peter Hickey, Hoeschele Brandon, Kenny Houghton, Shengli Hsu, Xin Hu, Joost Hu, Shantanu Huizinga, Shawn Jain, Jain Joanne, Angela Jang, Roger Jiang, Haozhun Jiang, Denny Jin, Shino Jin, Billie Jomoto, Hee-woo Jonn, Tomer Jun, \u0141ukasz Kaftan, Ali Kaiser, Ingmar Ka-mali, Kanitscheider, Nitish Shirish, Keskar Tabarak, Logan Khan, J. Kilpatrick, Kim Christina, Yongjik Kim, Jan Hendrik Kim, Jamie Kirch-ner, Matt Kiros, Daniel Knight, Kokotajlo \u0141ukasz, A. Kondraciuk, Aris Kondrich, Kyle Kon-stantinidis, Gretchen Kosic, Vishal Krueger, Michael Kuo, Ikai Lampe, Teddy Lan, Jan Lee, Jade Leike, Daniel Leung, Chak Ming Levy, Li Rachel, Molly Lim, Stephanie Lin, Mateusz Lin, Theresa Litwin, Ryan Lopez, Patricia Lowe, Lue Anna, Kim Makanju, S. Malfacini, Todor Manning, Yaniv Markov, Bianca Markovski, Katie Martin, Andrew Mayer, Bob Mayne, Scott Mayer McGrew, Christine McKinney, Paul McLeavey, McMillan Jake, David McNeil, Aalok Medina, Jacob Mehta, Luke Menick, Andrey Metz, Pamela Mishchenko, Vinnie Mishkin, Evan Monaco, Daniel Morikawa, Tong Mossing, Mira Mu, Oleg Murati, David Murk, Ashvin M\u00e9ly, Reiichiro Nair, Rajeev Nakano, Nayak Arvind, Richard Neelakantan, Hyeonwoo Ngo, Noh Long, Cullen Ouyang, Jakub O\u2019Keefe, Alex Pachocki, J. Paino, Ashley Palermo, Giambat-tista Pantuliano, Joel Parascandolo, Emy Parish, Alex Parparita, Mikhail Passos, Andrew Pavlov, Adam Peng, Filipe Perel-man, de Avila Belbute, Michael Peres, Petrov Henrique, Pond\u00e9, Michael Oliveira Pinto, Michelle Pokrass, Vitchyr H. Pong, Tolly Pow-ell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack W. Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-der, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin D. Sokolowsky, Yang Song, Natalie Staudacher, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim-ing Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Tianhao Shengjia Zhao",
            "matchScore": 362.18054,
            "original title": "Losing Visual Needles in Image Haystacks: Vision Language Models are Easily Distracted in Short and Long Contexts",
            "original authors": "Aditya Sharma, Michael Saxon, William Yang Wang",
            "EMNLP Paper ID": "1088",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "a42d61ed9e478940d5d074c27e0eb65986dd2545",
            "title": "Difficult Task Yes but Simple Task No: Unveiling the Laziness in Multimodal LLMs",
            "abstract": "Multimodal Large Language Models (MLLMs) demonstrate a strong understanding of the real world and can even handle complex tasks. However, they still fail on some straightforward visual question-answering (VQA) problems. This paper dives deeper into this issue, revealing that models tend to err when answering easy questions (e.g. Yes/No questions) about an image, even though they can correctly describe it. We refer to this model behavior discrepancy between difficult and simple questions as model laziness. To systematically investigate model laziness, we manually construct LazyBench, a benchmark that includes Yes/No, multiple choice, short answer questions, and image description tasks that are related to the same subjects in the images. Based on LazyBench, we observe that laziness widely exists in current advanced MLLMs (e.g. GPT-4o, Gemini-1.5-pro, Claude 3 and LLaVA-v1.5-13B), and it is more pronounced on stronger models. We also analyze the VQA v2 (LLaVA-v1.5-13B) benchmark and find that about half of its failure cases are caused by model laziness, which further highlights the importance of ensuring that the model fully utilizes its capability. To this end, we conduct preliminary exploration on how to mitigate laziness and find that chain of thought (CoT) can effectively address this issue.",
            "link": "https://www.semanticscholar.org/paper/a42d61ed9e478940d5d074c27e0eb65986dd2545",
            "authors": "Sihang Zhao, Youliang Yuan, Xiaoying Tang, Pinjia He",
            "matchScore": 306.4611,
            "original title": "Difficult Task Yes but Simple Task No: Unveiling the Laziness in Multimodal LLMs",
            "original authors": "Sihang Zhao, Youliang Yuan, Xiaoying Tang, Pinjia He",
            "EMNLP Paper ID": "1568",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "e07ff6d26b2e85e5bc28727717a6bcd40998b593",
            "title": "MACAROON: Training Vision-Language Models To Be Your Engaged Partners",
            "abstract": "Large vision-language models (LVLMs), while proficient in following instructions and responding to diverse questions, invariably generate detailed responses even when questions are ambiguous or unanswerable, leading to hallucinations and bias issues. Thus, it is essential for LVLMs to proactively engage with humans to ask for clarifications or additional information for better responses. In this study, we aim to shift LVLMs from passive answer providers to proactive engaged partners. We begin by establishing a three-tiered hierarchy for questions of invalid, ambiguous, and personalizable nature to measure the proactive engagement capabilities of LVLMs. Utilizing this hierarchy, we create PIE, (ProactIve Engagement Evaluation) through GPT-4o and human annotators, consisting of 853 questions across six distinct, fine-grained question types that are verified by human annotators and accompanied with well-defined metrics. Our evaluations on \\benchmark indicate poor performance of existing LVLMs, with the best-performing open-weights model only achieving an Aggregate Align Rate (AAR) of 0.28. In response, we introduce MACAROON, self-iMaginAtion for ContrAstive pReference OptimizatiON, which instructs LVLMs to autonomously generate contrastive response pairs for unlabeled questions given the task description and human-crafted criteria. Then, the self-imagined data is formatted for conditional reinforcement learning. Experimental results show MACAROON effectively improves LVLMs' capabilities to be proactively engaged (0.84 AAR) while maintaining comparable performance on general tasks.",
            "link": "https://www.semanticscholar.org/paper/e07ff6d26b2e85e5bc28727717a6bcd40998b593",
            "authors": "Shujin Wu, Y. Fung, Sha Li, Yixin Wan, Kai-Wei Chang, Heng Ji",
            "matchScore": 237.72536,
            "original title": "MACAROON: Training Vision-Language Models To Be Your Engaged Partners",
            "original authors": "Shujin Wu, Yi Fung, Sha Li, Yixin Wan, Kai-Wei Chang, Heng Ji",
            "EMNLP Paper ID": "1614",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "3d71d64603f421a36ef88cd00794e9f94006ca75",
            "title": "Zero-shot Commonsense Reasoning over Machine Imagination",
            "abstract": "Recent approaches to zero-shot commonsense reasoning have enabled Pre-trained Language Models (PLMs) to learn a broad range of commonsense knowledge without being tailored to specific situations. However, they often suffer from human reporting bias inherent in textual commonsense knowledge, leading to discrepancies in understanding between PLMs and humans. In this work, we aim to bridge this gap by introducing an additional information channel to PLMs. We propose Imagine (Machine Imagination-based Reasoning), a novel zero-shot commonsense reasoning framework designed to complement textual inputs with visual signals derived from machine-generated images. To achieve this, we enhance PLMs with imagination capabilities by incorporating an image generator into the reasoning process. To guide PLMs in effectively leveraging machine imagination, we create a synthetic pre-training dataset that simulates visual question-answering. Our extensive experiments on diverse reasoning benchmarks and analysis show that Imagine outperforms existing methods by a large margin, highlighting the strength of machine imagination in mitigating reporting bias and enhancing generalization capabilities.",
            "link": "https://www.semanticscholar.org/paper/3d71d64603f421a36ef88cd00794e9f94006ca75",
            "authors": "Hyuntae Park, Yeachan Kim, Jun-Hyung Park, SangKeun Lee",
            "matchScore": 246.62509,
            "original title": "Zero-shot Commonsense Reasoning over Machine Imagination",
            "original authors": "Hyuntae Park, Yeachan Kim, Jun-Hyung Park, SangKeun Lee",
            "EMNLP Paper ID": "2258",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "dc97369d323bf7e6589fa4813b15413ea0acdb04",
            "title": "ChartInsights: Evaluating Multimodal Large Language Models for Low-Level Chart Question Answering",
            "abstract": "Chart question answering (ChartQA) tasks play a critical role in interpreting and extracting insights from visualization charts. While recent advancements in multimodal large language models (MLLMs) like GPT-4o have shown promise in high-level ChartQA tasks, such as chart captioning, their effectiveness in low-level ChartQA tasks (e.g., identifying correlations) remains underexplored. In this paper, we address this gap by evaluating MLLMs on low-level ChartQA using a newly curated dataset, ChartInsights, which consists of 22,347 (chart, task, query, answer) covering 10 data analysis tasks across 7 chart types. We systematically evaluate 19 advanced MLLMs, including 12 open-source and 7 closed-source models. The average accuracy rate across these models is 39.8%, with GPT-4o achieving the highest accuracy at 69.17%. To further explore the limitations of MLLMs in low-level ChartQA, we conduct experiments that alter visual elements of charts (e.g., changing color schemes, adding image noise) to assess their impact on the task effectiveness. Furthermore, we propose a new textual prompt strategy, Chain-of-Charts, tailored for low-level ChartQA tasks, which boosts performance by 14.41%, achieving an accuracy of 83.58%. Finally, incorporating a visual prompt strategy that directs attention to relevant visual elements further improves accuracy to 84.32%.",
            "link": "https://www.semanticscholar.org/paper/dc97369d323bf7e6589fa4813b15413ea0acdb04",
            "authors": "Yifan Wu, Lutao Yan, Leixian Shen, Yunhai Wang, Nan Tang, Yuyu Luo",
            "matchScore": 262.06293,
            "original title": "ChartInsights: Evaluating Multimodal Large Language Models for Low-Level Chart Question Answering",
            "original authors": "Yifan Wu, Lutao Yan, Leixian Shen, Yunhai Wang, Nan Tang, Yuyu Luo",
            "EMNLP Paper ID": "2393",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "15617761321cc2507636f830f3ad5948b0fedae5",
            "title": "VGA: Vision GUI Assistant - Minimizing Hallucinations through Image-Centric Fine-Tuning",
            "abstract": "Recent advances in Large Vision-Language Models (LVLMs) have significantly improve performance in image comprehension tasks, such as formatted charts and rich-content images. Yet, Graphical User Interface (GUI) pose a greater challenge due to their structured format and detailed textual information. Existing LVLMs often overly depend on internal knowledge and neglect image content, resulting in hallucinations and incorrect responses in GUI comprehension. To address these issues, we introduce VGA, a fine-tuned model designed for comprehensive GUI understanding. Our model aims to enhance the interpretation of visual data of GUI and reduce hallucinations. We first construct a Vision Question Answering (VQA) dataset of 63.8k high-quality examples with our propose Referent Method, which ensures the model's responses are highly depend on visual content within the image. We then design a two-stage fine-tuning method called Foundation and Advanced Comprehension (FAC) to enhance both the model's ability to extract information from image content and alignment with human intent. Experiments show that our approach enhances the model's ability to extract information from images and achieves state-of-the-art results in GUI understanding tasks. Our dataset and fine-tuning script will be released soon.",
            "link": "https://www.semanticscholar.org/paper/15617761321cc2507636f830f3ad5948b0fedae5",
            "authors": "Ziyang Meng, Yu Dai, Zezheng Gong, Shaoxiong Guo, Minglong Tang, Tongquan Wei",
            "matchScore": 351.6216,
            "original title": "VGA: Vision GUI Assistant - Minimizing Hallucinations through Image-Centric Fine-Tuning",
            "original authors": "Meng ziyang, Yu Dai, Zezheng Gong, ShaoxiongGuo, Minglong Tang, Tongquan Wei",
            "EMNLP Paper ID": "257",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "49d8165542eb630003150b2c9ea364b9b1995489",
            "title": "SciDoc2Diagrammer-MAF: Towards Generation of Scientific Diagrams from Documents guided by Multi-Aspect Feedback Refinement",
            "abstract": "Automating the creation of scientific diagrams from academic papers can significantly streamline the development of tutorials, presentations, and posters, thereby saving time and accelerating the process. Current text-to-image models struggle with generating accurate and visually appealing diagrams from long-context inputs. We propose SciDoc2Diagram, a task that extracts relevant information from scientific papers and generates diagrams, along with a benchmarking dataset, SciDoc2DiagramBench. We develop a multi-step pipeline SciDoc2Diagrammer that generates diagrams based on user intentions using intermediate code generation. We observed that initial diagram drafts were often incomplete or unfaithful to the source, leading us to develop SciDoc2Diagrammer-Multi-Aspect-Feedback (MAF), a refinement strategy that significantly enhances factual correctness and visual appeal and outperforms existing models on both automatic and human judgement.",
            "link": "https://www.semanticscholar.org/paper/49d8165542eb630003150b2c9ea364b9b1995489",
            "authors": "Ishani Mondal, Zongxia Li, Yufang Hou, Anandhavelu Natarajan, Aparna Garimella, Jordan Boyd-Graber",
            "matchScore": 357.92853,
            "original title": "SciDoc2Diagrammer-MAF: Towards Generation of Scientific Diagrams from Documents guided by Multi-Aspect Feedback Refinement",
            "original authors": "Ishani Mondal, Zongxia Li, Yufang Hou, Anandhavelu Natarajan, Aparna Garimella, Jordan Lee Boyd-Graber",
            "EMNLP Paper ID": "2600",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "a4d0d502b05d7600a475f2cbf1aba0e38b0c6cf9",
            "title": "TextLap: Customizing Language Models for Text-to-Layout Planning",
            "abstract": "Automatic generation of graphical layouts is crucial for many real-world applications, including designing posters, flyers, advertisements, and graphical user interfaces. Given the incredible ability of Large language models (LLMs) in both natural language understanding and generation, we believe that we could customize an LLM to help people create compelling graphical layouts starting with only text instructions from the user. We call our method TextLap (text-based layout planning). It uses a curated instruction-based layout planning dataset (InsLap) to customize LLMs as a graphic designer. We demonstrate the effectiveness of TextLap and show that it outperforms strong baselines, including GPT-4 based methods, for image generation and graphical design benchmarks.",
            "link": "https://www.semanticscholar.org/paper/a4d0d502b05d7600a475f2cbf1aba0e38b0c6cf9",
            "authors": "Jian Chen, Ruiyi Zhang, Yufan Zhou, Jennifer Healey, Jiuxiang Gu, Zhiqiang Xu, Changyou Chen",
            "matchScore": 175.49602,
            "original title": "Customizing Language Models for Text-to-Layout Planning",
            "original authors": "Jian Chen, Ruiyi Zhang, Yufan Zhou, Jennifer Healey, Jiuxiang Gu, Changyou Chen",
            "EMNLP Paper ID": "2758",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "97ff072aa38fd1b4ddf626fca00e353958585590",
            "title": "Unraveling the Truth: Do LLMs really Understand Charts? A Deep Dive into Consistency and Robustness",
            "abstract": "Chart question answering (CQA) is a crucial area of Visual Language Understanding. However, the robustness and consistency of current Visual Language Models (VLMs) in this field remain under-explored. This paper evaluates state-of-the-art VLMs on comprehensive datasets, developed specifically for this study, encompassing diverse question categories and chart formats. We investigate two key aspects: 1) the models' ability to handle varying levels of chart and question complexity, and 2) their robustness across different visual representations of the same underlying data. Our analysis reveals significant performance variations based on question and chart types, highlighting both strengths and weaknesses of current models. Additionally, we identify areas for improvement and propose future research directions to build more robust and reliable CQA systems. This study sheds light on the limitations of current models and paves the way for future advancements in the field.",
            "link": "https://www.semanticscholar.org/paper/97ff072aa38fd1b4ddf626fca00e353958585590",
            "authors": "Srija Mukhopadhyay, Adnan Qidwai, Aparna Garimella, Pritika Ramu, Vivek Gupta, Dan Roth",
            "matchScore": 332.98492,
            "original title": "Unraveling the Truth: Do LLMs really Understand Charts? A Deep Dive into Consistency and Robustness",
            "original authors": "Srija Mukhopadhyay, Adnan Qidwai, Aparna Garimella, Pritika Ramu, Vivek Gupta, Dan Roth",
            "EMNLP Paper ID": "3206",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "bd7c20539c5da9a498f36fdc00d41a1e63ed5bfa",
            "title": "Visual Question Decomposition on Multimodal Large Language Models",
            "abstract": "Question decomposition has emerged as an effective strategy for prompting Large Language Models (LLMs) to answer complex questions. However, while existing methods primarily focus on unimodal language models, the question decomposition capability of Multimodal Large Language Models (MLLMs) has yet to be explored. To this end, this paper explores visual question decomposition on MLLMs. Specifically, we introduce a systematic evaluation framework including a dataset and several evaluation criteria to assess the quality of the decomposed sub-questions, revealing that existing MLLMs struggle to produce high-quality sub-questions. To address this limitation, we propose a specific finetuning dataset, DecoVQA+, for enhancing the model's question decomposition capability. Aiming at enabling models to perform appropriate selective decomposition, we propose an efficient finetuning pipeline. The finetuning pipeline consists of our proposed dataset and a training objective for selective decomposition. Finetuned MLLMs demonstrate significant improvements in the quality of sub-questions and the policy of selective question decomposition. Additionally, the models also achieve higher accuracy with selective decomposition on VQA benchmark datasets.",
            "link": "https://www.semanticscholar.org/paper/bd7c20539c5da9a498f36fdc00d41a1e63ed5bfa",
            "authors": "Haowei Zhang, Jianzhe Liu, Zhen Han, Shuo Chen, Bailan He, Volker Tresp, Zhiqiang Xu, Jindong Gu",
            "matchScore": 217.35284,
            "original title": "Visual Question Decomposition on Multimodal Large Language Models",
            "original authors": "Haowei Zhang, Jianzhe Liu, Zhen Han, Shuo Chen, Bailan He, Volker Tresp, zhiqiang xu, Jindong Gu",
            "EMNLP Paper ID": "387",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "97ec0d508265f43d3d011a09ffbf599df9ea2b0d",
            "title": "Are Large Vision Language Models up to the Challenge of Chart Comprehension and Reasoning? An Extensive Investigation into the Capabilities and Limitations of LVLMs",
            "abstract": "Natural language is a powerful complementary modality of communication for data visualizations, such as bar and line charts. To facilitate chart-based reasoning using natural language, various downstream tasks have been introduced recently such as chart question answering, chart summarization, and fact-checking with charts. These tasks pose a unique challenge, demanding both vision-language reasoning and a nuanced understanding of chart data tables, visual encodings, and natural language prompts. Despite the recent success of Large Language Models (LLMs) across diverse NLP tasks, their abilities and limitations in the realm of data visualization remain under-explored, possibly due to their lack of multi-modal capabilities. To bridge the gap, this paper presents the first comprehensive evaluation of the recently developed large vision language models (LVLMs) for chart understanding and reasoning tasks. Our evaluation includes a comprehensive assessment of LVLMs, including GPT-4V and Gemini, across four major chart reasoning tasks. Furthermore, we perform a qualitative evaluation of LVLMs' performance on a diverse range of charts, aiming to provide a thorough analysis of their strengths and weaknesses. Our findings reveal that LVLMs demonstrate impressive abilities in generating fluent texts covering high-level data insights while also encountering common problems like hallucinations, factual errors, and data bias. We highlight the key strengths and limitations of chart comprehension tasks, offering insights for future research.",
            "link": "https://www.semanticscholar.org/paper/97ec0d508265f43d3d011a09ffbf599df9ea2b0d",
            "authors": "Mohammed Saidul Islam, Raian Rahman, Ahmed Masry, Md Tahmid Rahman Laskar, Mir Tafseer Nayeem, Enamul Hoque",
            "matchScore": 188.25919,
            "original title": "Are Large Vision Language Models up to the Challenge of Chart Comprehension and Reasoning",
            "original authors": "Mohammed Saidul Islam, Raian Rahman, Ahmed Masry, Md Tahmid Rahman Laskar, Mir Tafseer Nayeem, Enamul Hoque",
            "EMNLP Paper ID": "689",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        }
    ],
    "Analysis and Generation of Narrative and Behavioral Constructs using LLMs": [
        {
            "paperId": "6f38dc421b3f42eb737905262c867c6bcf6a77a4",
            "title": "HEART-felt Narratives: Tracing Empathy and Narrative Style in Personal Stories with LLMs",
            "abstract": "Empathy serves as a cornerstone in enabling prosocial behaviors, and can be evoked through sharing of personal experiences in stories. While empathy is influenced by narrative content, intuitively, people respond to the way a story is told as well, through narrative style. Yet the relationship between empathy and narrative style is not fully understood. In this work, we empirically examine and quantify this relationship between style and empathy using LLMs and large-scale crowdsourcing studies. We introduce a novel, theory-based taxonomy, HEART (Human Empathy and Narrative Taxonomy) that delineates elements of narrative style that can lead to empathy with the narrator of a story. We establish the performance of LLMs in extracting narrative elements from HEART, showing that prompting with our taxonomy leads to reasonable, human-level annotations beyond what prior lexicon-based methods can do. To show empirical use of our taxonomy, we collect a dataset of empathy judgments of stories via a large-scale crowdsourcing study with N=2,624 participants. We show that narrative elements extracted via LLMs, in particular, vividness of emotions and plot volume, can elucidate the pathways by which narrative style cultivates empathy towards personal stories. Our work suggests that such models can be used for narrative analyses that lead to human-centered social and behavioral insights.",
            "link": "https://www.semanticscholar.org/paper/6f38dc421b3f42eb737905262c867c6bcf6a77a4",
            "authors": "Jocelyn Shen, Joel Mire, Hae Won Park, C. Breazeal, Maarten Sap",
            "EMNLP Paper ID": "130",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "00ed15190582da13fe9e7d4edaf1be07097ab5fc",
            "title": "Revisiting the Reliability of Psychological Scales on Large Language Models",
            "abstract": "Recent research has focused on examining Large Language Models' (LLMs) characteristics from a psychological standpoint, acknowledging the necessity of understanding their behavioral characteristics. The administration of personality tests to LLMs has emerged as a noteworthy area in this context. However, the suitability of employing psychological scales, initially devised for humans, on LLMs is a matter of ongoing debate. Our study aims to determine the reliability of applying personality assessments to LLMs, explicitly investigating whether LLMs demonstrate consistent personality traits. Analysis of 2,500 settings per model, including GPT-3.5, GPT-4, Gemini-Pro, and LLaMA-3.1, reveals that various LLMs show consistency in responses to the Big Five Inventory, indicating a satisfactory level of reliability. Furthermore, our research explores the potential of GPT-3.5 to emulate diverse personalities and represent various groups-a capability increasingly sought after in social sciences for substituting human participants with LLMs to reduce costs. Our findings reveal that LLMs have the potential to represent different personalities with specific prompt instructions.",
            "link": "https://www.semanticscholar.org/paper/00ed15190582da13fe9e7d4edaf1be07097ab5fc",
            "authors": "Jen-tse Huang, Wenxuan Wang, Man Ho Lam, E. Li, Wenxiang Jiao, Michael R. Lyu",
            "EMNLP Paper ID": "694",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "3e033a81c97a00c73066394948f48b022da93c91",
            "title": "MirrorStories: Reflecting Diversity through Personalized Narrative Generation with Large Language Models",
            "abstract": "This study explores the effectiveness of Large Language Models (LLMs) in creating personalized\"mirror stories\"that reflect and resonate with individual readers' identities, addressing the significant lack of diversity in literature. We present MirrorStories, a corpus of 1,500 personalized short stories generated by integrating elements such as name, gender, age, ethnicity, reader interest, and story moral. We demonstrate that LLMs can effectively incorporate diverse identity elements into narratives, with human evaluators identifying personalized elements in the stories with high accuracy. Through a comprehensive evaluation involving 26 diverse human judges, we compare the effectiveness of MirrorStories against generic narratives. We find that personalized LLM-generated stories not only outscore generic human-written and LLM-generated ones across all metrics of engagement (with average ratings of 4.22 versus 3.37 on a 5-point scale), but also achieve higher textual diversity while preserving the intended moral. We also provide analyses that include bias assessments and a study on the potential for integrating images into personalized stories.",
            "link": "https://www.semanticscholar.org/paper/3e033a81c97a00c73066394948f48b022da93c91",
            "authors": "Sarfaroz Yunusov, Hamza Sidat, Ali Emami",
            "EMNLP Paper ID": "748",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "03c59f70b079c211bd9ff7a68826fa1f5e0fda02",
            "title": "MORPHEUS: Modeling Role from Personalized Dialogue History by Exploring and Utilizing Latent Space",
            "abstract": "Personalized Dialogue Generation (PDG) aims to create coherent responses according to roles or personas. Traditional PDG relies on external role data, which can be scarce and raise privacy concerns. Approaches address these issues by extracting role information from dialogue history, which often fail to generically model roles in continuous space. To overcome these limitations, we introduce a novel framework \\textbf{MO}dels \\textbf{R}oles from \\textbf{P}ersonalized Dialogue \\textbf{H}istory by \\textbf{E}xploring and \\textbf{U}tilizing Latent \\textbf{S}pace (MORPHEUS) through a three-stage training process. Specifically, we create a persona codebook to represent roles in latent space compactly, and this codebook is used to construct a posterior distribution of role information. This method enables the model to generalize across roles, allowing the generation of personalized dialogues even for unseen roles. Experiments on both Chinese and English datasets demonstrate that MORPHEUS enhances the extraction of role information, and improves response generation without external role data. Additionally, MORPHEUS can be considered an efficient fine-tuning for large language models.",
            "link": "https://www.semanticscholar.org/paper/03c59f70b079c211bd9ff7a68826fa1f5e0fda02",
            "authors": "Yihong Tang, Bo Wang, Dongming Zhao, Xiaojia Jin, Jijun Zhang, Ruifang He, Yuexian Hou",
            "EMNLP Paper ID": "871",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "7e2970084b55718344ffeb52731d0df8db4b5c89",
            "title": "Evaluating Character Understanding of Large Language Models via Character Profiling from Fictional Works",
            "abstract": "Large language models (LLMs) have demonstrated impressive performance and spurred numerous AI applications, in which role-playing agents (RPAs) are particularly popular, especially for fictional characters. The prerequisite for these RPAs lies in the capability of LLMs to understand characters from fictional works. Previous efforts have evaluated this capability via basic classification tasks or characteristic imitation, failing to capture the nuanced character understanding with LLMs. In this paper, we propose evaluating LLMs' character understanding capability via the character profiling task, i.e., summarizing character profiles from corresponding materials, a widely adopted yet understudied practice for RPA development. Specifically, we construct the CroSS dataset from literature experts and assess the generated profiles by comparing them with ground truth references and evaluating their applicability in downstream tasks. Our experiments, which cover various summarization methods and LLMs, have yielded promising results. These results strongly validate the character understanding capability of LLMs. Resources are available at https://github.com/Joanna0123/character_profiling.",
            "link": "https://www.semanticscholar.org/paper/7e2970084b55718344ffeb52731d0df8db4b5c89",
            "authors": "Xinfeng Yuan, Siyu Yuan, Yuhan Cui, Tianhe Lin, Xintao Wang, Rui Xu, Jiangjie Chen, Deqing Yang",
            "EMNLP Paper ID": "913",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "c7ef60df82534b63a3577203affc6c9e0feb101f",
            "title": "Kiss up, Kick down: Exploring Behavioral Changes in Multi-modal Large Language Models with Assigned Visual Personas",
            "abstract": "This study is the first to explore whether multi-modal large language models (LLMs) can align their behaviors with visual personas, addressing a significant gap in the literature that predominantly focuses on text-based personas. We developed a novel dataset of 5K fictional avatar images for assignment as visual personas to LLMs, and analyzed their negotiation behaviors based on the visual traits depicted in these images, with a particular focus on aggressiveness. The results indicate that LLMs assess the aggressiveness of images in a manner similar to humans and output more aggressive negotiation behaviors when prompted with an aggressive visual persona. Interestingly, the LLM exhibited more aggressive negotiation behaviors when the opponent's image appeared less aggressive than their own, and less aggressive behaviors when the opponents image appeared more aggressive.",
            "link": "https://www.semanticscholar.org/paper/c7ef60df82534b63a3577203affc6c9e0feb101f",
            "authors": "Seungjong Sun, Eungu Lee, Seo Yeon Baek, Seunghyun Hwang, Wonbyung Lee, Dongyan Nan, Bernard J. Jansen, Jang Hyun Kim",
            "EMNLP Paper ID": "1242",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "789c5dd71fb6085161931955339de2172539cdc8",
            "title": "Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent",
            "abstract": "Large Language Models (LLMs) have revolutionized open-domain dialogue agents but encounter challenges in multi-character role-playing (MCRP) scenarios. To address the issue, we present Neeko, an innovative framework designed for efficient multiple characters imitation. Unlike existing methods, Neeko employs a dynamic low-rank adapter (LoRA) strategy, enabling it to adapt seamlessly to diverse characters. Our framework breaks down the role-playing process into agent pre-training, multiple characters playing, and character incremental learning, effectively handling both seen and unseen roles. This dynamic approach, coupled with distinct LoRA blocks for each character, enhances Neeko's adaptability to unique attributes, personalities, and speaking patterns. As a result, Neeko demonstrates superior performance in MCRP over most existing methods, offering more engaging and versatile user interaction experiences. Code and data are available at https://github.com/weiyifan1023/Neeko.",
            "link": "https://www.semanticscholar.org/paper/789c5dd71fb6085161931955339de2172539cdc8",
            "authors": "Xiaoyan Yu, Tongxu Luo, Yifan Wei, Fangyu Lei, Yiming Huang, Peng Hao, Liehuang Zhu",
            "EMNLP Paper ID": "1460",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "de69c6b8b5eae28d45bbac31364175a9abfb826c",
            "title": "Measuring Psychological Depth in Language Models",
            "abstract": "Evaluations of creative stories generated by large language models (LLMs) often focus on objective properties of the text, such as its style, coherence, and diversity. While these metrics are indispensable, they do not speak to a story's subjective, psychological impact from a reader's perspective. We introduce the Psychological Depth Scale (PDS), a novel framework rooted in literary theory that measures an LLM's ability to produce authentic and narratively complex stories that provoke emotion, empathy, and engagement. We empirically validate our framework by showing that humans can consistently evaluate stories based on PDS (0.72 Krippendorff's alpha). We also explore techniques for automating the PDS to easily scale future analyses. GPT-4o, combined with a novel Mixture-of-Personas (MoP) prompting strategy, achieves an average Spearman correlation of 0.51 with human judgment while Llama-3-70B with constrained decoding scores as high as 0.68 for empathy. Finally, we compared the depth of stories authored by both humans and LLMs. Surprisingly, GPT-4 stories either surpassed or were statistically indistinguishable from highly-rated human-written stories sourced from Reddit. By shifting the focus from text to reader, the Psychological Depth Scale is a validated, automated, and systematic means of measuring the capacity of LLMs to connect with humans through the stories they tell.",
            "link": "https://www.semanticscholar.org/paper/de69c6b8b5eae28d45bbac31364175a9abfb826c",
            "authors": "Fabrice Harel-Canada, Hanyu Zhou, Sreya Mupalla, Zeynep Yildiz, Amit Sahai, Nanyun Peng",
            "EMNLP Paper ID": "2031",
            "Oral/Poster": "Oral",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "325143c178b6c303ff74631ed8d98f2f475ccc38",
            "title": "Are Large Language Models Capable of Generating Human-Level Narratives?",
            "abstract": "This paper investigates the capability of LLMs in storytelling, focusing on narrative development and plot progression. We introduce a novel computational framework to analyze narratives through three discourse-level aspects: i) story arcs, ii) turning points, and iii) affective dimensions, including arousal and valence. By leveraging expert and automatic annotations, we uncover significant discrepancies between the LLM- and human- written stories. While human-written stories are suspenseful, arousing, and diverse in narrative structures, LLM stories are homogeneously positive and lack tension. Next, we measure narrative reasoning skills as a precursor to generative capacities, concluding that most LLMs fall short of human abilities in discourse understanding. Finally, we show that explicit integration of aforementioned discourse features can enhance storytelling, as is demonstrated by over 40% improvement in neural storytelling in terms of diversity, suspense, and arousal.",
            "link": "https://www.semanticscholar.org/paper/325143c178b6c303ff74631ed8d98f2f475ccc38",
            "authors": "Yufei Tian, Tenghao Huang, Miri Liu, Derek Jiang, Alexander Spangher, Muhao Chen, Jonathan May, Nanyun Peng",
            "EMNLP Paper ID": "2117",
            "Oral/Poster": "Oral",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "db82caf8d2bab9decfc863e711735839807fa6b8",
            "title": "Collective Critics for Creative Story Generation",
            "abstract": "Generating a long story of several thousand words with narrative coherence using Large Language Models (LLMs) has been a challenging task. Previous research has addressed this challenge by proposing different frameworks that create a story plan and generate a long story based on that plan. However, these frameworks have been mainly focusing on maintaining narrative coherence in stories, often overlooking creativity in story planning and the expressiveness of the stories generated from those plans, which are desirable properties to captivate readers' interest. In this paper, we propose Collective Critics for Creative Story Generation framework (CritiCS), which is composed of plan refining stage (CrPlan) and story generation stage (CrText), to integrate a collective revision mechanism that promotes those properties into long-form story generation process. Specifically, in each stage, a group of LLM critics and one leader collaborate to incrementally refine drafts of plan and story throughout multiple rounds. Extensive human evaluation shows that the CritiCS can significantly enhance story creativity and reader engagement, while also maintaining narrative coherence. Furthermore, the design of the framework allows active participation from human writers in any role within the critique process, enabling interactive human-machine collaboration in story writing.",
            "link": "https://www.semanticscholar.org/paper/db82caf8d2bab9decfc863e711735839807fa6b8",
            "authors": "Minwook Bae, Hyounghun Kim",
            "EMNLP Paper ID": "2346",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "bf746159ec6008fa8e4d4134c848f8611066d62d",
            "title": "DataNarrative: Automated Data-Driven Storytelling with Visualizations and Texts",
            "abstract": "Data-driven storytelling is a powerful method for conveying insights by combining narrative techniques with visualizations and text. These stories integrate visual aids, such as highlighted bars and lines in charts, along with textual annotations explaining insights. However, creating such stories requires a deep understanding of the data and meticulous narrative planning, often necessitating human intervention, which can be time-consuming and mentally taxing. While Large Language Models (LLMs) excel in various NLP tasks, their ability to generate coherent and comprehensive data stories remains underexplored. In this work, we introduce a novel task for data story generation and a benchmark containing 1,449 stories from diverse sources. To address the challenges of crafting coherent data stories, we propose a multiagent framework employing two LLM agents designed to replicate the human storytelling process: one for understanding and describing the data (Reflection), generating the outline, and narration, and another for verification at each intermediary step. While our agentic framework generally outperforms non-agentic counterparts in both model-based and human evaluations, the results also reveal unique challenges in data story generation.",
            "link": "https://www.semanticscholar.org/paper/bf746159ec6008fa8e4d4134c848f8611066d62d",
            "authors": "Mohammed Saidul Islam, Enamul Hoque, Shafiq R. Joty, Md Tahmid Rahman Laskar, Md. Rizwan Parvez",
            "EMNLP Paper ID": "2444",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "187eefa5167705cf916751c135aa650c545a87a8",
            "title": "Pron vs Prompt: Can Large Language Models already Challenge a World-Class Fiction Author at Creative Text Writing?",
            "abstract": "It has become routine to report research results where Large Language Models (LLMs) outperform average humans in a wide range of language-related tasks, and creative text writing is no exception. It seems natural, then, to raise the bid: Are LLMs ready to compete in creative writing skills with a top (rather than average) novelist? To provide an initial answer for this question, we have carried out a contest between Patricio Pron (an awarded novelist, considered one of the best of his generation) and GPT-4 (one of the top performing LLMs), in the spirit of AI-human duels such as DeepBlue vs Kasparov and AlphaGo vs Lee Sidol. We asked Pron and GPT-4 to provide thirty titles each, and then to write short stories for both their titles and their opponent's. Then, we prepared an evaluation rubric inspired by Boden's definition of creativity, and we collected 5,400 manual assessments provided by literature critics and scholars. The results of our experimentation indicate that LLMs are still far from challenging a top human creative writer, and that reaching such level of autonomous creative writing skills probably cannot be reached simply with larger language models.",
            "link": "https://www.semanticscholar.org/paper/187eefa5167705cf916751c135aa650c545a87a8",
            "authors": "Guillermo Marco, Julio Gonzalo, Ram'on del Castillo, M. Girona",
            "EMNLP Paper ID": "2537",
            "Oral/Poster": "Oral",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "685b50b738e4597cfc9eb99b30691b0b40a1034b",
            "title": "Virtual Personas for Language Models via an Anthology of Backstories",
            "abstract": "Large language models (LLMs) are trained from vast repositories of text authored by millions of distinct authors, reflecting an enormous diversity of human traits. While these models bear the potential to be used as approximations of human subjects in behavioral studies, prior efforts have been limited in steering model responses to match individual human users. In this work, we introduce\"Anthology\", a method for conditioning LLMs to particular virtual personas by harnessing open-ended life narratives, which we refer to as\"backstories.\"We show that our methodology enhances the consistency and reliability of experimental outcomes while ensuring better representation of diverse sub-populations. Across three nationally representative human surveys conducted as part of Pew Research Center's American Trends Panel (ATP), we demonstrate that Anthology achieves up to 18% improvement in matching the response distributions of human respondents and 27% improvement in consistency metrics. Our code and generated backstories are available at https://github.com/CannyLab/anthology.",
            "link": "https://www.semanticscholar.org/paper/685b50b738e4597cfc9eb99b30691b0b40a1034b",
            "authors": "Suhong Moon, Marwa Abdulhai, Minwoo Kang, Joseph Suh, Widyadewi Soedarmadji, Eran Kohen Behar, David M. Chan",
            "EMNLP Paper ID": "2583",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "95e92effee730d10b4d638ae871da97dbb331a6b",
            "title": "Revealing Personality Traits: A New Benchmark Dataset for Explainable Personality Recognition on Dialogues",
            "abstract": "Personality recognition aims to identify the personality traits implied in user data such as dialogues and social media posts. Current research predominantly treats personality recognition as a classification task, failing to reveal the supporting evidence for the recognized personality. In this paper, we propose a novel task named Explainable Personality Recognition, aiming to reveal the reasoning process as supporting evidence of the personality trait. Inspired by personality theories, personality traits are made up of stable patterns of personality state, where the states are short-term characteristic patterns of thoughts, feelings, and behaviors in a concrete situation at a specific moment in time. We propose an explainable personality recognition framework called Chain-of-Personality-Evidence (CoPE), which involves a reasoning process from specific contexts to short-term personality states to long-term personality traits. Furthermore, based on the CoPE framework, we construct an explainable personality recognition dataset from dialogues, PersonalityEvd. We introduce two explainable personality state recognition and explainable personality trait recognition tasks, which require models to recognize the personality state and trait labels and their corresponding support evidence. Our extensive experiments based on Large Language Models on the two tasks show that revealing personality traits is very challenging and we present some insights for future research. Our data and code are available at https://github.com/Lei-Sun-RUC/PersonalityEvd.",
            "link": "https://www.semanticscholar.org/paper/95e92effee730d10b4d638ae871da97dbb331a6b",
            "authors": "Lei Sun, Jinming Zhao, Qin Jin",
            "EMNLP Paper ID": "2604",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "d4b8783a84888503ef3ad242197585258f8c218f",
            "title": "HoLLMwood: Unleashing the Creativity of Large Language Models in Screenwriting via Role Playing",
            "abstract": "Generative AI has demonstrated unprecedented creativity in the field of computer vision, yet such phenomena have not been observed in natural language processing. In particular, large language models (LLMs) can hardly produce written works at the level of human experts due to the extremely high complexity of literature writing. In this paper, we present HoLLMwood, an automated framework for unleashing the creativity of LLMs and exploring their potential in screenwriting, which is a highly demanding task. Mimicking the human creative process, we assign LLMs to different roles involved in the real-world scenario. In addition to the common practice of treating LLMs as ${Writer}$, we also apply LLMs as ${Editor}$, who is responsible for providing feedback and revision advice to ${Writer}$. Besides, to enrich the characters and deepen the plots, we introduce a role-playing mechanism and adopt LLMs as ${Actors}$ that can communicate and interact with each other. Evaluations on automatically generated screenplays show that HoLLMwood substantially outperforms strong baselines in terms of coherence, relevance, interestingness and overall quality.",
            "link": "https://www.semanticscholar.org/paper/d4b8783a84888503ef3ad242197585258f8c218f",
            "authors": "Jing Chen, Xinyu Zhu, Cheng Yang, Chufan Shi, Yadong Xi, Yuxiang Zhang, Junjie Wang, Jiashu Pu, Rongsheng Zhang, Yujiu Yang, Tian Feng",
            "matchScore": 289.13577,
            "original title": "HoLLMwood: Unleashing the Creativity of Large Language Models in Screenwriting via Role Playing",
            "original authors": "Jing Chen, Xinyu Zhu, Cheng Yang, Chufan Shi, Yadong Xi, Yuxiang Zhang, Junjie Wang, Jiashu Pu, Rongsheng Zhang, Yujiu Yang, Tian Feng",
            "EMNLP Paper ID": "1698",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "d155020d3c02741bcec71d1c5b909831a438bbbd",
            "title": "CHIRON: Rich Character Representations in Long-Form Narratives",
            "abstract": "Characters are integral to long-form narratives, but are poorly understood by existing story analysis and generation systems. While prior work has simplified characters via graph-based methods and brief character descriptions, we aim to better tackle the problem of representing complex characters by taking inspiration from advice given to professional writers. We propose CHIRON, a new `character sheet' based representation that organizes and filters textual information about characters. We construct CHIRON sheets in two steps: a Generation Module that prompts an LLM for character information via question-answering and a Validation Module that uses automated reasoning and a domain-specific entailment model to eliminate false facts about a character. We validate CHIRON via the downstream task of masked-character prediction, where our experiments show CHIRON is better and more flexible than comparable summary-based baselines. We also show that metrics derived from CHIRON can be used to automatically infer character-centricity in stories, and that these metrics align with human judgments.",
            "link": "https://www.semanticscholar.org/paper/d155020d3c02741bcec71d1c5b909831a438bbbd",
            "authors": "Alexander Gurung, Mirella Lapata",
            "matchScore": 252.1412,
            "original title": "CHIRON: Rich Character Representations in Long-Form Narratives",
            "original authors": "Alexander Gurung, Mirella Lapata",
            "EMNLP Paper ID": "1795",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "7d9e763511c7ed0054c299d481925a48e2973563",
            "title": "From Pixels to Personas: Investigating and Modeling Self-Anthropomorphism in Human-Robot Dialogues",
            "abstract": "Self-anthropomorphism in robots manifests itself through their display of human-like characteristics in dialogue, such as expressing preferences and emotions. Our study systematically analyzes self-anthropomorphic expression within various dialogue datasets, outlining the contrasts between self-anthropomorphic and non-self-anthropomorphic responses in dialogue systems. We show significant differences in these two types of responses and propose transitioning from one type to the other. We also introduce Pix2Persona, a novel dataset aimed at developing ethical and engaging AI systems in various embodiments. This dataset preserves the original dialogues from existing corpora and enhances them with paired responses: self-anthropomorphic and non-self-anthropomorphic for each original bot response. Our work not only uncovers a new category of bot responses that were previously under-explored but also lays the groundwork for future studies about dynamically adjusting self-anthropomorphism levels in AI systems to align with ethical standards and user expectations.",
            "link": "https://www.semanticscholar.org/paper/7d9e763511c7ed0054c299d481925a48e2973563",
            "authors": "Yu Li, Devamanyu Hazarika, Di Jin, Julia Hirschberg, Yang Liu",
            "matchScore": 282.93015,
            "original title": "From Pixels to Personas: Investigating and Modeling Self-Anthropomorphism in Human-Robot Dialogues",
            "original authors": "Yu Li, Devamanyu Hazarika, Di Jin, Julia Hirschberg, Yang Liu",
            "EMNLP Paper ID": "2006",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "2b2e0f5858dcf03dd4c87f3906a6fdd096761351",
            "title": "Not (yet) the whole story: Evaluating Visual Storytelling Requires More than Measuring Coherence, Grounding, and Repetition",
            "abstract": "Visual storytelling consists in generating a natural language story given a temporally ordered sequence of images. This task is not only challenging for models, but also very difficult to evaluate with automatic metrics since there is no consensus about what makes a story 'good'. In this paper, we introduce a novel method that measures story quality in terms of human likeness regarding three key aspects highlighted in previous work: visual grounding, coherence, and repetitiveness. We then use this method to evaluate the stories generated by several models, showing that the foundation model LLaVA obtains the best result, but only slightly so compared to TAPM, a 50-times smaller visual storytelling model. Upgrading the visual and language components of TAPM results in a model that yields competitive performance with a relatively low number of parameters. Finally, we carry out a human evaluation study, whose results suggest that a 'good' story may require more than a human-like level of visual grounding, coherence, and repetition.",
            "link": "https://www.semanticscholar.org/paper/2b2e0f5858dcf03dd4c87f3906a6fdd096761351",
            "authors": "Aditya K Surikuchi, R. Fern'andez, Sandro Pezzelle",
            "matchScore": 375.60406,
            "original title": "Not (yet) the whole story: Evaluating Visual Storytelling Requires More than Measuring Coherence, Grounding, and Repetition",
            "original authors": "Aditya Kaushik Surikuchi, Raquel Fern\u00e1ndez, Sandro Pezzelle",
            "EMNLP Paper ID": "2282",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "a31364b9a908c0634c76b276411405dcc73fa33e",
            "title": "Improving Quotation Attribution with Fictional Character Embeddings",
            "abstract": "Humans naturally attribute utterances of direct speech to their speaker in literary works. When attributing quotes, we process contextual information but also access mental representations of characters that we build and revise throughout the narrative. Recent methods to automatically attribute such utterances have explored simulating human logic with deterministic rules or learning new implicit rules with neural networks when processing contextual information. However, these systems inherently lack \\textit{character} representations, which often leads to errors in more challenging examples of attribution: anaphoric and implicit quotes. In this work, we propose to augment a popular quotation attribution system, BookNLP, with character embeddings that encode global stylistic information of characters derived from an off-the-shelf stylometric model, Universal Authorship Representation (UAR). We create DramaCV (Code and data can be found at https://github.com/deezer/character_embeddings_qa ), a corpus of English drama plays from the 15th to 20th century that we automatically annotate for Authorship Verification of fictional characters utterances, and release two versions of UAR trained on DramaCV, that are tailored for literary characters analysis. Then, through an extensive evaluation on 28 novels, we show that combining BookNLP's contextual information with our proposed global character embeddings improves the identification of speakers for anaphoric and implicit quotes, reaching state-of-the-art performance.",
            "link": "https://www.semanticscholar.org/paper/a31364b9a908c0634c76b276411405dcc73fa33e",
            "authors": "Gaspard Michel, Elena V. Epure, Romain Hennequin, Christophe Cerisara",
            "matchScore": 234.9436,
            "original title": "Improving Quotation Attribution with Fictional Character Embeddings",
            "original authors": "Gaspard Michel, Elena V. Epure, Romain Hennequin, Christophe Cerisara",
            "EMNLP Paper ID": "2491",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "0353331b666c24845120a79a022e3abddcdb2fdc",
            "title": "SWAG: Storytelling With Action Guidance",
            "abstract": "Automated long-form story generation typically employs long-context large language models (LLMs) for one-shot creation, which can produce cohesive but not necessarily engaging content. We introduce Storytelling With Action Guidance (SWAG), a novel approach to storytelling with LLMs. Our approach frames story writing as a search problem through a two-model feedback loop: one LLM generates story content, and another auxiliary LLM is used to choose the next best\"action\"to steer the story's future direction. Our results show that SWAG can substantially outperform previous end-to-end story generation techniques when evaluated by GPT-4 and through human evaluation. Our SWAG pipeline using only small open-source models surpasses GPT-3.5-Turbo.",
            "link": "https://www.semanticscholar.org/paper/0353331b666c24845120a79a022e3abddcdb2fdc",
            "authors": "Zeeshan Patel, Karim El-Refai, Jonathan Pei, Tianle Li",
            "matchScore": 169.0094,
            "original title": "SWAG: Storytelling With Action Guidance",
            "original authors": "Jonathan Pei, Zeeshan Patel, Karim El-Refai, Tianle Li",
            "EMNLP Paper ID": "2733",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Findings"
        },
        {
            "paperId": "38fd1616d4e1e2e94ed3b5713ecfd60a8442a3bf",
            "title": "Capturing Minds, Not Just Words: Enhancing Role-Playing Language Models with Personality-Indicative Data",
            "abstract": "Role-playing agents (RPA) have been a popular application area for large language models (LLMs), attracting significant interest from both industry and academia.While existing RPAs well portray the characters' knowledge and tones, they face challenges in capturing their minds, especially for small role-playing language models (RPLMs). In this paper, we propose to enhance RPLMs via personality-indicative data. Specifically, we leverage questions from psychological scales and distill advanced RPAs to generate dialogues that grasp the minds of characters. Experimental results validate that RPLMs trained with our dataset exhibit advanced role-playing capabilities for both general and personality-related evaluations. Code and data are available at \\href{https://github.com/alienet1109/RolePersonality}{this URL}.",
            "link": "https://www.semanticscholar.org/paper/38fd1616d4e1e2e94ed3b5713ecfd60a8442a3bf",
            "authors": "Yiting Ran, Xintao Wang, Rui Xu, Xinfeng Yuan, Jiaqing Liang, Yanghua Xiao, Deqing Yang",
            "matchScore": 311.23535,
            "original title": "Capturing Minds, Not Just Words: Enhancing Role-Playing Language Models with Personality-Indicative Data",
            "original authors": "Yiting Ran",
            "EMNLP Paper ID": "2804",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "41677446ad94ed115d65423b4f53760f42c102c5",
            "title": "Evaluating Language Model Character Traits",
            "abstract": "Language models (LMs) can exhibit human-like behaviour, but it is unclear how to describe this behaviour without undue anthropomorphism. We formalise a behaviourist view of LM character traits: qualities such as truthfulness, sycophancy, or coherent beliefs and intentions, which may manifest as consistent patterns of behaviour. Our theory is grounded in empirical demonstrations of LMs exhibiting different character traits, such as accurate and logically coherent beliefs, and helpful and harmless intentions. We find that the consistency with which LMs exhibit certain character traits varies with model size, fine-tuning, and prompting. In addition to characterising LM character traits, we evaluate how these traits develop over the course of an interaction. We find that traits such as truthfulness and harmfulness can be stationary, i.e., consistent over an interaction, in certain contexts, but may be reflective in different contexts, meaning they mirror the LM's behavior in the preceding interaction. Our formalism enables us to describe LM behaviour precisely in intuitive language, without undue anthropomorphism.",
            "link": "https://www.semanticscholar.org/paper/41677446ad94ed115d65423b4f53760f42c102c5",
            "authors": "Francis Rhys Ward, Zejia Yang, Alex Jackson, Randy Brown, Chandler Smith, Grace Colverd, Louis Thomson, Raymond Douglas, Patrik Bartak, Andrew Rowan",
            "matchScore": 164.1839,
            "original title": "Evaluating Language Model Character Traits",
            "original authors": "Francis Rhys Ward, Zejia Yang, Alex Jackson, Randy Brown, Chandler Smith, Grace Beaney Colverd, Louis Alexander Thomson, Raymond Douglas, Patrik Bartak, Andrew Rowan",
            "EMNLP Paper ID": "287",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "9174a12377b1c2e8053f164f8bd22d7595e53ce6",
            "title": "BookWorm: A Dataset for Character Description and Analysis",
            "abstract": "Characters are at the heart of every story, driving the plot and engaging readers. In this study, we explore the understanding of characters in full-length books, which contain complex narratives and numerous interacting characters. We define two tasks: character description, which generates a brief factual profile, and character analysis, which offers an in-depth interpretation, including character development, personality, and social context. We introduce the BookWorm dataset, pairing books from the Gutenberg Project with human-written descriptions and analyses. Using this dataset, we evaluate state-of-the-art long-context models in zero-shot and fine-tuning settings, utilizing both retrieval-based and hierarchical processing for book-length inputs. Our findings show that retrieval-based approaches outperform hierarchical ones in both tasks. Additionally, fine-tuned models using coreference-based retrieval produce the most factual descriptions, as measured by fact- and entailment-based metrics. We hope our dataset, experiments, and analysis will inspire further research in character-based narrative understanding.",
            "link": "https://www.semanticscholar.org/paper/9174a12377b1c2e8053f164f8bd22d7595e53ce6",
            "authors": "Argyrios Papoudakis, Mirella Lapata, Frank Keller",
            "matchScore": 209.10703,
            "original title": "BookWorm: A Dataset for Character Description and Analysis",
            "original authors": "Argyrios Papoudakis, Mirella Lapata, Frank Keller",
            "EMNLP Paper ID": "893",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        }
    ],
    "Fact-Checking and Factual Consistency in Large Language Models": [
        {
            "paperId": "f7451a0ae5919157aa26f4dd35acd896e070f7d8",
            "title": "FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document",
            "abstract": "Through the advent of pre-trained language models, there have been notable advancements in abstractive summarization systems. Simultaneously, a considerable number of novel methods for evaluating factual consistency in abstractive summarization systems has been developed. But these evaluation approaches incorporate substantial limitations, especially on refinement and interpretability. In this work, we propose highly effective and interpretable factual inconsistency detection method metric Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document for abstractive summarization systems that is based on fine-grained atomic facts decomposition. Moreover, we align atomic facts decomposed from the summary with the source document through adaptive granularity expansion. These atomic facts represent a more fine-grained unit of information, facilitating detailed understanding and interpretability of the summary's factual inconsistency. Experimental results demonstrate that our proposed factual consistency checking system significantly outperforms existing systems.",
            "link": "https://www.semanticscholar.org/paper/f7451a0ae5919157aa26f4dd35acd896e070f7d8",
            "authors": "Joonho Yang, Seunghyun Yoon, Byeongjeong Kim, Hwanhee Lee",
            "EMNLP Paper ID": "4",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "99f53c35aaeec3c7c4a02eb3ca30a303a3d8291b",
            "title": "Do We Need Language-Specific Fact-Checking Models? The Case of Chinese",
            "abstract": "This paper investigates the potential benefits of language-specific fact-checking models, focusing on the case of Chinese. We first demonstrate the limitations of translation-based methods and multilingual large language models (e.g., GPT-4), highlighting the need for language-specific systems. We further propose a Chinese fact-checking system that can better retrieve evidence from a document by incorporating context information. To better analyze token-level biases in different systems, we construct an adversarial dataset based on the CHEF dataset, where each instance has large word overlap with the original one but holds the opposite veracity label. Experimental results on the CHEF dataset and our adversarial dataset show that our proposed method outperforms translation-based methods and multilingual LLMs and is more robust toward biases, while there is still large room for improvement, emphasizing the importance of language-specific fact-checking systems.",
            "link": "https://www.semanticscholar.org/paper/99f53c35aaeec3c7c4a02eb3ca30a303a3d8291b",
            "authors": "Caiqi Zhang, Zhijiang Guo, Andreas Vlachos",
            "EMNLP Paper ID": "214",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "3ea087a9a409bf16e49e0b7f8d444e7d91870f2d",
            "title": "An Analysis of Multilingual FActScore",
            "abstract": "FActScore has gained popularity as a metric to estimate the factuality of long-form texts generated by Large Language Models (LLMs) in English. However, there has not been any work in studying the behavior of FActScore in other languages. This paper studies the limitations of each component in the four-component pipeline of FActScore in the multilingual setting. We introduce a new dataset for FActScore on texts generated by strong multilingual LLMs. Our evaluation shows that LLMs exhibit distinct behaviors in both fact extraction and fact scoring tasks. No LLM produces consistent and reliable FActScore across languages with varying levels of resources. We also find that the knowledge source plays an important role in the quality of the estimated FActScore. Using Wikipedia as the knowledge source may hinder the true FActScore of long-form text due to its limited coverage in medium- and low-resource languages. We also incorporate three mitigations to our knowledge source that ultimately improve FActScore estimation across all languages.",
            "link": "https://www.semanticscholar.org/paper/3ea087a9a409bf16e49e0b7f8d444e7d91870f2d",
            "authors": "Kim Trong Vu, Michael Krumdick, Varshini Reddy, Franck Dernoncourt, V. Lai",
            "EMNLP Paper ID": "475",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "6de6414baf2a187b05981376abefd6a53275baa3",
            "title": "\"Image, Tell me your story!\" Predicting the original meta-context of visual misinformation",
            "abstract": "To assist human fact-checkers, researchers have developed automated approaches for visual misinformation detection. These methods assign veracity scores by identifying inconsistencies between the image and its caption, or by detecting forgeries in the image. However, they neglect a crucial point of the human fact-checking process: identifying the original meta-context of the image. By explaining what is actually true about the image, fact-checkers can better detect misinformation, focus their efforts on check-worthy visual content, engage in counter-messaging before misinformation spreads widely, and make their explanation more convincing. Here, we fill this gap by introducing the task of automated image contextualization. We create 5Pils, a dataset of 1,676 fact-checked images with question-answer pairs about their original meta-context. Annotations are based on the 5 Pillars fact-checking framework. We implement a first baseline that grounds the image in its original meta-context using the content of the image and textual evidence retrieved from the open web. Our experiments show promising results while highlighting several open challenges in retrieval and reasoning. We make our code and data publicly available.",
            "link": "https://www.semanticscholar.org/paper/6de6414baf2a187b05981376abefd6a53275baa3",
            "authors": "Jonathan Tonglet, M. Moens, Iryna Gurevych",
            "EMNLP Paper ID": "896",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "69e2aa1723b46e4df0445e8ecb838636b0911cd7",
            "title": "Towards Verifiable Text Generation with Evolving Memory and Self-Reflection",
            "abstract": "Despite the remarkable ability of large language models (LLMs) in language comprehension and generation, they often suffer from producing factually incorrect information, also known as hallucination. A promising solution to this issue is verifiable text generation, which prompts LLMs to generate content with citations for accuracy verification. However, verifiable text generation is non-trivial due to the focus-shifting phenomenon, the intricate reasoning needed to align the claim with correct citations, and the dilemma between the precision and breadth of retrieved documents. In this paper, we present VTG, an innovative framework for Verifiable Text Generation with evolving memory and self-reflection. VTG introduces evolving long short-term memory to retain both valuable documents and recent documents. A two-tier verifier equipped with an evidence finder is proposed to rethink and reflect on the relationship between the claim and citations. Furthermore, active retrieval and diverse query generation are utilized to enhance both the precision and breadth of the retrieved documents. We conduct extensive experiments on five datasets across three knowledge-intensive tasks and the results reveal that VTG significantly outperforms baselines.",
            "link": "https://www.semanticscholar.org/paper/69e2aa1723b46e4df0445e8ecb838636b0911cd7",
            "authors": "Hao Sun, Hengyi Cai, Bo Wang, Yingyan Hou, Xiaochi Wei, Shuaiqiang Wang, Yan Zhang, Dawei Yin",
            "EMNLP Paper ID": "954",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "d0489fd7c5ff833e08969341ae6e5fa1d0e6dd2a",
            "title": "MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents",
            "abstract": "Recognizing if LLM output can be grounded in evidence is central to many tasks in NLP: retrieval-augmented generation, summarization, document-grounded dialogue, and more. Current approaches to this kind of fact-checking are based on verifying each piece of a model generation against potential evidence using an LLM. However, this process can be very computationally expensive, requiring many calls to a model to check a single response. In this work, we show how to build small fact-checking models that have GPT-4-level performance but for 400x lower cost. We do this by constructing synthetic training data with GPT-4, which involves creating realistic yet challenging instances of factual errors via a structured generation procedure. Training on this data teaches models to check each fact in the claim and recognize synthesis of information across sentences. For evaluation, we unify datasets from recent work on fact-checking and grounding LLM generations into a new benchmark, LLM-AggreFact. Our best system MiniCheck-FT5 (770M parameters) outperforms all systems of comparable size and reaches GPT-4 accuracy. We release LLM-AggreFact, code for data synthesis, and models.",
            "link": "https://www.semanticscholar.org/paper/d0489fd7c5ff833e08969341ae6e5fa1d0e6dd2a",
            "authors": "Liyan Tang, Philippe Laban, Greg Durrett",
            "EMNLP Paper ID": "1009",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "3f77b6c2def038cf28a8bf81f32d7a26ff3fee5a",
            "title": "Temporally Consistent Factuality Probing for Large Language Models",
            "abstract": "The prolific use of Large Language Models (LLMs) as an alternate knowledge base requires them to be factually consistent, necessitating both correctness and consistency traits for paraphrased queries. Recently, significant attempts have been made to benchmark datasets and metrics to evaluate LLMs for these traits. However, structural simplicity (subject-relation-object) and contemporary association in their query formulation limit the broader definition of factuality and consistency. In this study, we introduce TeCFaP, a novel Temporally Consistent Factuality Probe task to expand the consistent factuality probe in the temporal dimension. To this end, we propose TEMP-COFAC, a high-quality dataset of prefix-style English query paraphrases. Subsequently, we extend the definitions of existing metrics to represent consistent factuality across temporal dimension. We experiment with a diverse set of LLMs and find most of them performing poorly on TeCFaP. Next, we propose a novel solution CoTSeLF (Consistent-Time-Sensitive Learning Framework) combining multi-task instruction tuning (MT-IT) with consistent-time-sensitive reinforcement learning (CTSRL) to improve temporally consistent factuality in LLMs. Our experiments demonstrate the efficacy of CoTSeLF over several baselines.",
            "link": "https://www.semanticscholar.org/paper/3f77b6c2def038cf28a8bf81f32d7a26ff3fee5a",
            "authors": "Ashutosh Bajpai, Aaryan Goyal, Atif Anwer, Tanmoy Chakraborty",
            "EMNLP Paper ID": "1862",
            "Oral/Poster": "Oral",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "85e4bb98ccc48cdd765e5b95781d70441b6d3032",
            "title": "On the Universal Truthfulness Hyperplane Inside LLMs",
            "abstract": "While large language models (LLMs) have demonstrated remarkable abilities across various fields, hallucination remains a significant challenge. Recent studies have explored hallucinations through the lens of internal representations, proposing mechanisms to decipher LLMs' adherence to facts. However, these approaches often fail to generalize to out-of-distribution data, leading to concerns about whether internal representation patterns reflect fundamental factual awareness, or only overfit spurious correlations on the specific datasets. In this work, we investigate whether a universal truthfulness hyperplane that distinguishes the model's factually correct and incorrect outputs exists within the model. To this end, we scale up the number of training datasets and conduct an extensive evaluation -- we train the truthfulness hyperplane on a diverse collection of over 40 datasets and examine its cross-task, cross-domain, and in-domain generalization. Our results indicate that increasing the diversity of the training datasets significantly enhances the performance in all scenarios, while the volume of data samples plays a less critical role. This finding supports the optimistic hypothesis that a universal truthfulness hyperplane may indeed exist within the model, offering promising directions for future research.",
            "link": "https://www.semanticscholar.org/paper/85e4bb98ccc48cdd765e5b95781d70441b6d3032",
            "authors": "Junteng Liu, Shiqi Chen, Yu Cheng, Junxian He",
            "EMNLP Paper ID": "2239",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "2fb593ca4b6d2631832d6424e238c32db3db5434",
            "title": "Factuality of Large Language Models in the Year 2024",
            "abstract": "Large language models (LLMs), especially when instruction-tuned for chat, have become part of our daily lives, freeing people from the process of searching, extracting, and integrating information from multiple sources by offering a straightforward answer to a variety of questions in a single place. Unfortunately, in many cases, LLM responses are factually incorrect, which limits their applicability in real-world scenarios. As a result, research on evaluating and improving the factuality of LLMs has attracted a lot of research attention recently. In this survey, we critically analyze existing work with the aim to identify the major challenges and their associated causes, pointing out to potential solutions for improving the factuality of LLMs, and analyzing the obstacles to automated factuality evaluation for open-ended text generation. We further offer an outlook on where future research should go.",
            "link": "https://www.semanticscholar.org/paper/2fb593ca4b6d2631832d6424e238c32db3db5434",
            "authors": "Yuxia Wang, Minghan Wang, Muhammad Arslan Manzoor, Fei Liu, Georgi Georgiev, Rocktim Jyoti Das, Preslav Nakov",
            "EMNLP Paper ID": "2516",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "6b15183fcf3516fb03cb40f4d29b9ed202b129c3",
            "title": "SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization",
            "abstract": "Large Language Models (LLMs) such as GPT&Llama have demonstrated significant achievements in summarization tasks but struggle with factual inaccuracies, a critical issue in clinical NLP applications where errors could lead to serious consequences. To counter the high costs and limited availability of expert-annotated data for factual alignment, this study introduces an innovative pipeline that utilizes>100B parameter GPT variants like GPT-3.5&GPT-4 to act as synthetic experts to generate high-quality synthetics feedback aimed at enhancing factual consistency in clinical note summarization. Our research primarily focuses on edit feedback generated by these synthetic feedback experts without additional human annotations, mirroring and optimizing the practical scenario in which medical professionals refine AI system outputs. Although such 100B+ parameter GPT variants have proven to demonstrate expertise in various clinical NLP tasks, such as the Medical Licensing Examination, there is scant research on their capacity to act as synthetic feedback experts and deliver expert-level edit feedback for improving the generation quality of weaker (<10B parameter) LLMs like GPT-2 (1.5B)&Llama 2 (7B) in clinical domain. So in this work, we leverage 100B+ GPT variants to act as synthetic feedback experts offering expert-level edit feedback, that is used to reduce hallucinations and align weaker (<10B parameter) LLMs with medical facts using two distinct alignment algorithms (DPO&SALT), endeavoring to narrow the divide between AI-generated content and factual accuracy. This highlights the substantial potential of LLM-based synthetic edits in enhancing the alignment of clinical factuality.",
            "link": "https://www.semanticscholar.org/paper/6b15183fcf3516fb03cb40f4d29b9ed202b129c3",
            "authors": "Prakamya Mishra, Zonghai Yao, Parth Vashisht, Feiyun Ouyang, Beining Wang, Vidhi Mody, Hong Yu",
            "EMNLP Paper ID": "2616",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "32d8df3dddfeb5d04326a17c0d506b1304aa8dc1",
            "title": "VERISCORE: Evaluating the factuality of verifiable claims in long-form text generation",
            "abstract": "Existing metrics for evaluating the factuality of long-form text, such as FACTSCORE (Min et al., 2023) and SAFE (Wei et al., 2024), decompose an input text into\"atomic claims\"and verify each against a knowledge base like Wikipedia. These metrics are not suitable for most generation tasks because they assume that every claim is verifiable (i.e., can plausibly be proven true or false). We address this issue with VERISCORE, a metric for diverse long-form generation tasks that contain both verifiable and unverifiable content. VERISCORE can be effectively implemented with either closed or fine-tuned open-weight language models, and human evaluation confirms that VERISCORE's extracted claims are more sensible than those from competing methods across eight different long-form tasks. We use VERISCORE to evaluate generations from 16 different models across multiple long-form tasks and find that while GPT-4o is the best-performing model overall, open-weight models such as Mixtral-8x22 are closing the gap. We show that an LM's VERISCORE on one task (e.g., biography generation) does not necessarily correlate to its VERISCORE on a different task (e.g., long-form QA), highlighting the need for expanding factuality evaluation across tasks with varying fact density.",
            "link": "https://www.semanticscholar.org/paper/32d8df3dddfeb5d04326a17c0d506b1304aa8dc1",
            "authors": "Yixiao Song, Yekyung Kim, Mohit Iyyer",
            "matchScore": 273.8802,
            "original title": "VeriScore: Evaluating the factuality of verifiable claims in long-form text generation",
            "original authors": "Yixiao Song, Yekyung Kim, Mohit Iyyer",
            "EMNLP Paper ID": "1971",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "e1089255323a4e2012eec8d1364ef4c93c7f058a",
            "title": "How Entangled is Factuality and Deception in German?",
            "abstract": "The statement\"The earth is flat\"is factually inaccurate, but if someone truly believes and argues in its favor, it is not deceptive. Research on deception detection and fact checking often conflates factual accuracy with the truthfulness of statements. This assumption makes it difficult to (a) study subtle distinctions and interactions between the two and (b) gauge their effects on downstream tasks. The belief-based deception framework disentangles these properties by defining texts as deceptive when there is a mismatch between what people say and what they truly believe. In this study, we assess if presumed patterns of deception generalize to German language texts. We test the effectiveness of computational models in detecting deception using an established corpus of belief-based argumentation. Finally, we gauge the impact of deception on the downstream task of fact checking and explore if this property confounds verification models. Surprisingly, our analysis finds no correlation with established cues of deception. Previous work claimed that computational models can outperform humans in deception detection accuracy, however, our experiments show that both traditional and state-of-the-art models struggle with the task, performing no better than random guessing. For fact checking, we find that Natural Language Inference-based verification performs worse on non-factual and deceptive content, while prompting Large Language Models for the same task is less sensitive to these properties.",
            "link": "https://www.semanticscholar.org/paper/e1089255323a4e2012eec8d1364ef4c93c7f058a",
            "authors": "Aswathy Velutharambath, Amelie W\u00fchrl, Roman Klinger",
            "matchScore": 236.14911,
            "original title": "How Entangled is Factuality and Deception in German?",
            "original authors": "Aswathy Velutharambath, Amelie Wuehrl, Roman Klinger",
            "EMNLP Paper ID": "1977",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "747f0aaa9c296cf0a54ec0f6b29318d397d64d4f",
            "title": "MAVEN-Fact: A Large-scale Event Factuality Detection Dataset",
            "abstract": "Event Factuality Detection (EFD) task determines the factuality of textual events, i.e., classifying whether an event is a fact, possibility, or impossibility, which is essential for faithfully understanding and utilizing event knowledge. However, due to the lack of high-quality large-scale data, event factuality detection is under-explored in event understanding research, which limits the development of EFD community. To address these issues and provide faithful event understanding, we introduce MAVEN-Fact, a large-scale and high-quality EFD dataset based on the MAVEN dataset. MAVEN-Fact includes factuality annotations of 112,276 events, making it the largest EFD dataset. Extensive experiments demonstrate that MAVEN-Fact is challenging for both conventional fine-tuned models and large language models (LLMs). Thanks to the comprehensive annotations of event arguments and relations in MAVEN, MAVEN-Fact also supports some further analyses and we find that adopting event arguments and relations helps in event factuality detection for fine-tuned models but does not benefit LLMs. Furthermore, we preliminarily study an application case of event factuality detection and find it helps in mitigating event-related hallucination in LLMs. Our dataset and codes can be obtained from \\url{https://github.com/lcy2723/MAVEN-FACT}",
            "link": "https://www.semanticscholar.org/paper/747f0aaa9c296cf0a54ec0f6b29318d397d64d4f",
            "authors": "Chunyang Li, Hao Peng, Xiaozhi Wang, Y. Qi, Lei Hou, Bin Xu, Juanzi Li",
            "matchScore": 250.82149,
            "original title": "MAVEN-FACT: A Large-scale Event Factuality Detection Dataset",
            "original authors": "Chunyang Li, Hao Peng, Xiaozhi Wang, Yunjia Qi, Lei Hou, Bin Xu, Juanzi Li",
            "EMNLP Paper ID": "2214",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "c5533c8be71d749d330d02fba9a4f1cf3bc43914",
            "title": "How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models",
            "abstract": "Given the growing influx of misinformation across news and social media, there is a critical need for systems that can provide effective real-time verification of news claims. Large language or multimodal model based verification has been proposed to scale up online policing mechanisms for mitigating spread of false and harmful content. While these can potentially reduce burden on human fact-checkers, such efforts may be hampered by foundation model training data becoming outdated. In this work, we test the limits of improving foundation model performance without continual updating through an initial study of knowledge transfer using either existing intra- and inter- domain benchmarks or explanations generated from large language models (LLMs). We evaluate on 12 public benchmarks for fact-checking and misinformation detection as well as two other tasks relevant to content moderation -- toxicity and stance detection. Our results on two recent multi-modal fact-checking benchmarks, Mocheg and Fakeddit, indicate that knowledge transfer strategies can improve Fakeddit performance over the state-of-the-art by up to 1.7% and Mocheg performance by up to 2.9%.",
            "link": "https://www.semanticscholar.org/paper/c5533c8be71d749d330d02fba9a4f1cf3bc43914",
            "authors": "Jaeyoung Lee, Ximing Lu, Jack Hessel, Faeze Brahman, Youngjae Yu, Yonatan Bisk, Yejin Choi, Saadia Gabriel",
            "matchScore": 271.01495,
            "original title": "How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models",
            "original authors": "Jaeyoung Lee, Ximing Lu, Jack Hessel, Faeze Brahman, Youngjae Yu, Yonatan Bisk, Yejin Choi, Saadia Gabriel",
            "EMNLP Paper ID": "2555",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "c29c2653d5d4bcbb7cb72f4618c2c058968d8cd5",
            "title": "ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs",
            "abstract": "In the midst of widespread misinformation and disinformation through social media and the proliferation of AI-generated texts, it has become increasingly difficult for people to validate and trust information they encounter. Many fact-checking approaches and tools have been developed, but they often lack appropriate explainability or granularity to be useful in various contexts. A text validation method that is easy to use, accessible, and can perform fine-grained evidence attribution has become crucial. More importantly, building user trust in such a method requires presenting the rationale behind each prediction, as research shows this significantly influences people's belief in automated systems. Localizing and bringing users' attention to the specific problematic content is also paramount, instead of providing simple blanket labels. In this paper, we present ClaimVer, a human-centric framework tailored to meet users' informational and verification needs by generating rich annotations and thereby reducing cognitive load. Designed to deliver comprehensive evaluations of texts, it highlights each claim, verifies it against a trusted knowledge graph (KG), presents the evidence, and provides succinct, clear explanations for each claim prediction. Finally, our framework introduces an attribution score, enhancing applicability across a wide range of downstream tasks.",
            "link": "https://www.semanticscholar.org/paper/c29c2653d5d4bcbb7cb72f4618c2c058968d8cd5",
            "authors": "Preetam Prabhu Srikar Dammu, Himanshu Naidu, Mouly Dewan, YoungMin Kim, Tanya Roosta, Aman Chadha, Chirag Shah",
            "matchScore": 302.19092,
            "original title": "ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs",
            "original authors": "Preetam Prabhu Srikar Dammu, Himanshu Naidu, Mouly Dewan, YoungMin Kim, Tanya Roosta, Aman Chadha, Chirag Shah",
            "EMNLP Paper ID": "2642",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "72c62b3a2280e66499d5918fadc3c31474425768",
            "title": "Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-checkers",
            "abstract": "The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs. In this work, we present a holistic end-to-end solution for annotating the factuality of LLM-generated responses, which encompasses a multi-stage annotation scheme designed to yield detailed labels concerning the verifiability and factual inconsistencies found in LLM outputs. We further construct an open-domain document-level factuality benchmark in three-level granularity: claim, sentence and document, aiming to facilitate the evaluation of automatic fact-checking systems. Preliminary experiments show that FacTool, FactScore and Perplexity.ai are struggling to identify false claims, with the best F1=0.63 by this annotation solution based on GPT-4. Annotation tool, benchmark and code are available at https://github.com/yuxiaw/Factcheck-GPT.",
            "link": "https://www.semanticscholar.org/paper/72c62b3a2280e66499d5918fadc3c31474425768",
            "authors": "Yuxia Wang, Revanth Gangi Reddy, Zain Muhammad Mujahid, Arnav Arora, Aleksandr Rubashevskii, Jiahui Geng, Osama Mohammed Afzal, Liangming Pan, Nadav Borenstein, Aditya Pillai, Isabelle Augenstein, Iryna Gurevych, Preslav Nakov",
            "matchScore": 297.20197,
            "original title": "Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-checkers",
            "original authors": "Yuxia Wang, Revanth Gangi Reddy, Zain Muhammad Mujahid, Arnav Arora, Aleksandr Rubashevskii, Jiahui Geng, OSAMA MOHAMMED AFZAL, Liangming Pan, Nadav Borenstein, Aditya Pillai, Isabelle Augenstein, Iryna Gurevych, Preslav Nakov",
            "EMNLP Paper ID": "2745",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "60be3610c60cbdb0f786e0ec2c6b0af635f61722",
            "title": "Identifying Factual Inconsistencies in Summaries: Grounding LLM Inference via Task Taxonomy",
            "abstract": "Factual inconsistencies pose a significant hurdle for the faithful summarization by generative models. While a major direction to enhance inconsistency detection is to derive stronger Natural Language Inference (NLI) models, we propose an orthogonal aspect that underscores the importance of incorporating task-specific taxonomy into the inference. To this end, we consolidate key error types of inconsistent facts in summaries, and incorporate them to facilitate both the zero-shot and supervised paradigms of LLMs. Extensive experiments on ten datasets of five distinct domains suggest that, zero-shot LLM inference could benefit from the explicit solution space depicted by the error type taxonomy, and achieves state-of-the-art performance overall, surpassing specialized non-LLM baselines, as well as recent LLM baselines. We further distill models that fuse the taxonomy into parameters through our designed prompt completions and supervised training strategies, efficiently substituting state-of-the-art zero-shot inference with much larger LLMs.",
            "link": "https://www.semanticscholar.org/paper/60be3610c60cbdb0f786e0ec2c6b0af635f61722",
            "authors": "Liyan Xu, Zhenlin Su, Mo Yu, Jin Xu, Jinho D. Choi, Jie Zhou, Fei Liu",
            "matchScore": 288.38593,
            "original title": "Identifying Factual Inconsistencies in Summaries: Grounding Model Inference via Task Taxonomy",
            "original authors": "Liyan Xu, Zhenlin Su, Mo Yu, Jin Xu, Jinho D. Choi, Jie Zhou, Fei Liu",
            "EMNLP Paper ID": "2826",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "4d710532fff7aec8187f68fb2ca90079c40e7004",
            "title": "FactAlign: Long-form Factuality Alignment of Large Language Models",
            "abstract": "Large language models have demonstrated significant potential as the next-generation information access engines. However, their reliability is hindered by issues of hallucination and generating non-factual content. This is particularly problematic in long-form responses, where assessing and ensuring factual accuracy is complex. In this paper, we address this gap by proposing FactAlign, a novel alignment framework designed to enhance the factuality of LLMs' long-form responses while maintaining their helpfulness. We introduce fKTO, a fine-grained, sentence-level alignment algorithm that extends the Kahneman-Tversky Optimization (KTO) alignment method. Leveraging recent advances in automatic factuality evaluation, FactAlign utilizes fine-grained factuality assessments to guide the alignment process. Our experiments on open-domain prompts and information-seeking questions demonstrate that FactAlign significantly improves the factual accuracy of LLM responses while also improving their helpfulness. Further analyses identify that FactAlign is capable of training LLMs to provide more information without losing factual precision, thus improving the factual F1 score. Our source code, datasets, and trained models are publicly available at https://github.com/MiuLab/FactAlign",
            "link": "https://www.semanticscholar.org/paper/4d710532fff7aec8187f68fb2ca90079c40e7004",
            "authors": "Chao-Wei Huang, Yun-Nung Chen",
            "matchScore": 244.77191,
            "original title": "FactAlign: Long-form Factuality Alignment of Large Language Models",
            "original authors": "Chao-Wei Huang, Yun-Nung Chen",
            "EMNLP Paper ID": "3135",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "d2d3960bd1e4e84cbb0051945ea1419005e75e07",
            "title": "Dial BeInfo for Faithfulness: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning",
            "abstract": "Factuality is a crucial requirement in information seeking dialogue: the system should respond to the user's queries so that the responses are meaningful and aligned with the knowledge provided to the system. However, most modern large language models suffer from hallucinations, that is, they generate responses not supported by or contradicting the knowledge source. To mitigate the issue and increase faithfulness of information-seeking dialogue systems, we introduce BeInfo, a simple yet effective method that applies behavioural tuning to aid information-seeking dialogue. Relying on three standard datasets, we show that models tuned with BeInfo} become considerably more faithful to the knowledge source both for datasets and domains seen during BeInfo-tuning, as well as on unseen domains, when applied in a zero-shot manner. In addition, we show that the models with 3B parameters (e.g., Flan-T5) tuned with BeInfo demonstrate strong performance on data from real `production' conversations and outperform GPT4 when tuned on a limited amount of such realistic in-domain dialogues.",
            "link": "https://www.semanticscholar.org/paper/d2d3960bd1e4e84cbb0051945ea1419005e75e07",
            "authors": "E. Razumovskaia, Ivan Vuli'c, Pavle Markovi'c, Tomasz Cichy, Qian Zheng, Tsung-Hsien Wen, Pawe\u0142 Budzianowski",
            "matchScore": 357.49942,
            "original title": "Dial BeInfo for Faithfulness: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning",
            "original authors": "Evgeniia Razumovskaia, Ivan Vuli\u0107, Pavle Markovi\u0107, Tomasz Cichy, Qian Zheng, Tsung-Hsien Wen, Pawe\u0142 Budzianowski",
            "EMNLP Paper ID": "3288",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "c80fb69f26fe186504a182c4b7f5ab186beeb019",
            "title": "Molecular Facts: Desiderata for Decontextualization in LLM Fact Verification",
            "abstract": "Automatic factuality verification of large language model (LLM) generations is becoming more and more widely used to combat hallucinations. A major point of tension in the literature is the granularity of this fact-checking: larger chunks of text are hard to fact-check, but more atomic facts like propositions may lack context to interpret correctly. In this work, we assess the role of context in these atomic facts. We argue that fully atomic facts are not the right representation, and define two criteria for molecular facts: decontextuality, or how well they can stand alone, and minimality, or how little extra information is added to achieve decontexuality. We quantify the impact of decontextualization on minimality, then present a baseline methodology for generating molecular facts automatically, aiming to add the right amount of information. We compare against various methods of decontextualization and find that molecular facts balance minimality with fact verification accuracy in ambiguous settings.",
            "link": "https://www.semanticscholar.org/paper/c80fb69f26fe186504a182c4b7f5ab186beeb019",
            "authors": "Anisha Gunjal, Greg Durrett",
            "matchScore": 310.58936,
            "original title": "Molecular Facts: Desiderata for Decontextualization in LLM Fact Verification",
            "original authors": "Anisha Gunjal, Greg Durrett",
            "EMNLP Paper ID": "755",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "ba18ac45bbcd8605e8737447b618b57821b2dfc6",
            "title": "MoleculeQA: A Dataset to Evaluate Factual Accuracy in Molecular Comprehension",
            "abstract": "Large language models are playing an increasingly significant role in molecular research, yet existing models often generate erroneous information, posing challenges to accurate molecular comprehension. Traditional evaluation metrics for generated content fail to assess a model's accuracy in molecular understanding. To rectify the absence of factual evaluation, we present MoleculeQA, a novel question answering (QA) dataset which possesses 62K QA pairs over 23K molecules. Each QA pair, composed of a manual question, a positive option and three negative options, has consistent semantics with a molecular description from authoritative molecular corpus. MoleculeQA is not only the first benchmark for molecular factual bias evaluation but also the largest QA dataset for molecular research. A comprehensive evaluation on MoleculeQA for existing molecular LLMs exposes their deficiencies in specific areas and pinpoints several particularly crucial factors for molecular understanding.",
            "link": "https://www.semanticscholar.org/paper/ba18ac45bbcd8605e8737447b618b57821b2dfc6",
            "authors": "Xingyu Lu, He Cao, Zijing Liu, Shengyuan Bai, Leqing Chen, Yuan Yao, Hai-Tao Zheng, Yu Li",
            "matchScore": 268.7099,
            "original title": "MoleculeQA: A Dataset to Evaluate Factual Accuracy in Molecular Comprehension",
            "original authors": "Xingyu Lu, He CAO, Zijing Liu, Shengyuan Bai, leqingchen, Yuan Yao, Hai-Tao Zheng, Yu Li",
            "EMNLP Paper ID": "757",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Findings"
        },
        {
            "paperId": "920675cac5887094f2d750b70da8ef402bfe871a",
            "title": "Generating Media Background Checks for Automated Source Critical Reasoning",
            "abstract": "Not everything on the internet is true. This unfortunate fact requires both humans and models to perform complex reasoning about credibility when working with retrieved information. In NLP, this problem has seen little attention. Indeed, retrieval-augmented models are not typically expected to distrust retrieved documents. Human experts overcome the challenge by gathering signals about the context, reliability, and tendency of source documents - that is, they perform source criticism. We propose a novel NLP task focused on finding and summarising such signals. We introduce a new dataset of 6,709\"media background checks\"derived from Media Bias / Fact Check, a volunteer-run website documenting media bias. We test open-source and closed-source LLM baselines with and without retrieval on this dataset, finding that retrieval greatly improves performance. We furthermore carry out human evaluation, demonstrating that 1) media background checks are helpful for humans, and 2) media background checks are helpful for retrieval-augmented models.",
            "link": "https://www.semanticscholar.org/paper/920675cac5887094f2d750b70da8ef402bfe871a",
            "authors": "Michael Schlichtkrull",
            "matchScore": 257.81213,
            "original title": "Generating Media Background Checks for Automated Source Critical Reasoning",
            "original authors": "Michael Sejr Schlichtkrull",
            "EMNLP Paper ID": "964",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        }
    ],
    "Improving Human Preference Alignment in Large Language Models": [
        {
            "paperId": "ace05d37f23750f16896976c0db17d48e52c4478",
            "title": "Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments",
            "abstract": "Large language models (LLMs) have shown promising abilities as cost-effective and reference-free evaluators for assessing language generation quality. In particular, pairwise LLM evaluators, which compare two generated texts and determine the preferred one, have been employed in a wide range of applications. However, LLMs exhibit preference biases and worrying sensitivity to prompt designs. In this work, we first reveal that the predictive preference of LLMs can be highly brittle and skewed, even with semantically equivalent instructions. We find that fairer predictive preferences from LLMs consistently lead to judgments that are better aligned with humans. Motivated by this phenomenon, we propose an automatic Zero-shot Evaluation-oriented Prompt Optimization framework, ZEPO, which aims to produce fairer preference decisions and improve the alignment of LLM evaluators with human judgments. To this end, we propose a zero-shot learning objective based on the preference decision fairness. ZEPO demonstrates substantial performance improvements over state-of-the-art LLM evaluators, without requiring labeled data, on representative meta-evaluation benchmarks. Our findings underscore the critical correlation between preference fairness and human alignment, positioning ZEPO as an efficient prompt optimizer for bridging the gap between LLM evaluators and human judgments.",
            "link": "https://www.semanticscholar.org/paper/ace05d37f23750f16896976c0db17d48e52c4478",
            "authors": "Han Zhou, Xingchen Wan, Yinhong Liu, Nigel Collier, Ivan Vuli'c, Anna Korhonen",
            "EMNLP Paper ID": "147",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "feab5206295c4f7203ea4b83c6f3e106f6dc1b19",
            "title": "AlignCap: Aligning Speech Emotion Captioning to Human Preferences",
            "abstract": "Speech Emotion Captioning (SEC) has gradually become an active research task. The emotional content conveyed through human speech are often complex, and classifying them into fixed categories may not be enough to fully capture speech emotions. Describing speech emotions through natural language may be a more effective approach. However, existing SEC methods often produce hallucinations and lose generalization on unseen speech. To overcome these problems, we propose AlignCap, which Aligning Speech Emotion Captioning to Human Preferences based on large language model (LLM) with two properties: 1) Speech-Text Alignment, which minimizing the divergence between the LLM's response prediction distributions for speech and text inputs using knowledge distillation (KD) Regularization. 2) Human Preference Alignment, where we design Preference Optimization (PO) Regularization to eliminate factuality and faithfulness hallucinations. We also extract emotional clues as a prompt for enriching fine-grained information under KD-Regularization. Experiments demonstrate that AlignCap presents stronger performance to other state-of-the-art methods on Zero-shot SEC task.",
            "link": "https://www.semanticscholar.org/paper/feab5206295c4f7203ea4b83c6f3e106f6dc1b19",
            "authors": "Ziqi Liang, Haoxiang Shi, Hanhui Chen",
            "EMNLP Paper ID": "438",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "1b5811eafe28b50c08640189483e9b3ab46a10de",
            "title": "Modular Pluralism: Pluralistic Alignment via Multi-LLM Collaboration",
            "abstract": "While existing alignment paradigms have been integral in developing large language models (LLMs), LLMs often learn an averaged human preference and struggle to model diverse preferences across cultures, demographics, and communities. We propose Modular Pluralism, a modular framework based on multi-LLM collaboration for pluralistic alignment: it\"plugs into\"a base LLM a pool of smaller but specialized community LMs, where models collaborate in distinct modes to flexibility support three modes of pluralism: Overton, steerable, and distributional. Modular Pluralism is uniquely compatible with black-box LLMs and offers the modular control of adding new community LMs for previously underrepresented communities. We evaluate Modular Pluralism with six tasks and four datasets featuring questions/instructions with value-laden and perspective-informed responses. Extensive experiments demonstrate that Modular Pluralism advances the three pluralism objectives across six black-box and open-source LLMs. Further analysis reveals that LLMs are generally faithful to the inputs from smaller community LLMs, allowing seamless patching by adding a new community LM to better cover previously underrepresented communities.",
            "link": "https://www.semanticscholar.org/paper/1b5811eafe28b50c08640189483e9b3ab46a10de",
            "authors": "Shangbin Feng, Taylor Sorensen, Yuhan Liu, Jillian R. Fisher, Chan Young Park, Yejin Choi, Yulia Tsvetkov",
            "EMNLP Paper ID": "461",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "05840877433670c00566af4ed6946080685b0075",
            "title": "Formality Favored: Unraveling the Learning Preferences of Large Language Models on Data with Conflicting Knowledge",
            "abstract": "Having been trained on massive pretraining 001 data, large language models have shown excel-002 lent performance on many knowledge-intensive 003 tasks. However, pretraining data tends to con-004 tain misleading and even conflicting informa-005 tion, and it is intriguing to understand how 006 LLMs handle these noisy data during train-007 ing. In this study, we systematically analyze 008 LLMs\u2019 learning preferences for data with con-009 flicting knowledge. We find that pretrained 010 LLMs establish learning preferences similar to 011 humans, i.e., preferences towards formal texts 012 and texts with fewer spelling errors, resulting 013 in faster learning and more favorable treatment 014 of knowledge in data with such features when 015 facing conflicts. This finding is generalizable 016 across models and languages and is more ev-017 ident in larger models. An in-depth analysis 018 reveals that LLMs tend to trust data with fea-019 tures that signify consistency with the majority 020 of data, and it is possible to instill new prefer-021 ences and erase old ones by manipulating the 022 degree of consistency with the majority data. 023",
            "link": "https://www.semanticscholar.org/paper/05840877433670c00566af4ed6946080685b0075",
            "authors": "Josh Achiam, Steven Adler, Lama Sandhini Agarwal, Ilge Akkaya, Florencia Leoni Aleman, Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Stella Biderman, Hailey Schoelkopf, Quentin Gregory, Herbie Anthony, Kyle Bradley, Eric O\u2019Brien, Hal-586 Mohammad Aflah, Shivanshu Khan, Purohit, Jonathan H Choi, Kristin E Hickman, Amy Monahan, Damai Dai, Li Dong, Y. Hao, Zhifang Sui, Baobao, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, Alexandre Frechette, Charlotte Smith, Laura Culp, Lev Proleev, Yingqi Luan, Xi Chen, James Lottes, Nathan Schucher, Federico Lebron, Alban Rrustemi, Natalie Clay, Phil Crone, Tom\u00e1s Kocisk\u00fd, Jeffrey Zhao, Bartek Perz, Dian Yu, Heidi Howard, Adam Bloniarz, J. Rae, Han Lu, L. Sifre, M. Maggioni, Fred Alcober, Daniel H Garrette, Megan Barnes, S. Thakoor, Jacob Austin, Gabriel Barth-Maron, William Wong, Rishabh Joshi, Rahma Chaabouni, Deeni Fatiha, Arun Ahuja, Ruibo Liu, Yunxuan Li, Sarah Cogan, Jeremy Chen, Chao Jia, Chenjie Gu, Qiao Zhang, Ale Jordan Grimstad, Jakse Hartman, Martin Chad-669, Gaurav Singh Tomar, Xavier Garcia, Emanuel Senter, Thanumalayan Taropa, Jacob Sankara-671 narayana Pillai, Michael Devlin, Laskin Las, Dasha Casas, Connie Valter, Tao, Lorenzo, Adri\u00e0 Blanco, David Puigdom\u00e8nech Badia, Reitter, Abhi Rao, Stephanie Winkler, Emilio Parisotto, Yiming Gu, Kate Olszewska, Yujing Zhang, Antoine Miech, Annie Louis, Laurent El, Denis Teplyashin, Geoff Brown, Elliot Catt, Nithya Attaluri, Jan Balaguer, Jackie Xiang, Pidong Wang, Zoe C. Ashwood, Anton Briukhov, A. bert, San-jay Webson, Smit Ganapathy, Sanghavi, Ajay Kannan, Ming-Wei Chang, Axel Stjerngren, Sina Samangooei, Bogdan Damoc, Alex Kaskasoli, S'ebastien M. R. Arnold, Vijay Vasudevan, Shubham, Jason Agrawal, Dmitry Riesa, Richard Lepikhin, S. Srinivasan, Hyeontaek Lim, Pranav Hodkinson, Johan Shyam, Steven Ferret, Ankush Garg, T. Paine, Jian Li, Yu-695 jia Li, Minh Giang, Alexander Neitz, Zaheer Abbas, Luk\u00e1s Zilka, Flavien Prost, Luheng He, Gaurav Monteiro, Chris Mishra, Josh Welty, Newlan, Ruizhe Zhao, Kevin Villela, Wenhao Luyu Wang, Matthew Rahtz, Mai Gim\u00e9nez, Legg Yeung, Hanzhao Lin, James Keeling, Petko Georgiev, Diana Mincu, Boxi Wu, Salem Haykal, Rachel Sapu-713, Kiran Vodrahalli, James Qin, Zeynep Cankara, Paul Chang, Ross Komarek, Mario McIlroy, Lu\u02c7ci\u00b4c, Paul Natsev, Paul Michel, Yong Cheng, Siyuan Bansal, Kris Qiao, Siamak Cao, Shakeri, Christina Butterfield, Justin Chung, Paul Kishan, Shivani Rubenstein, Arthur Agrawal, Mensch, Kedar, Karel Soparkar, Tim-othy Lenc, Aedan Chung, Pope, Lorenzo Maggiore, Jackie Kay, Priya Jhakra, Shibo, Joshua Wang, Mary Maynez, Taylor Phuong, Tobin, A. Tacchetti, Maja Trebacz, Kevin Robinson, Yash Katariya, Sebastian Riedel, Paige Bailey, Keyan Xiao, Nimesh Ghelani, Lora Aroyo, Neil Slone, Xuehan Houlsby, Zhen Xiong, Yang, E. Gribovskaya, Jonas Adler, Mateo Wirth, Music Li Lisa Lee, Thais Kagohara, Jay Pavagadhi, Sophie Bridgers, Anna Bortsova, Sanjay Ghemawat, Zafarali Ahmed, Tianqi Liu, Richard Powell, Vijay Bolina, Mariko Iinuma, Polina Zablotskaia, James Besley, D. Chung, T. Dozat, R. Comanescu, Xiance Si, Jeremy Greer, Guolong Su, M. Polacek, Raphael Lopez Kaufman, Simon Tokumine, Hexiang Hu, Elena Buchatskaya, Ying-Qi Miao, Mohamed Elhawaty, Aditya Siddhant, Nenad Toma\u0161ev, Jinwei Xing, Christina Greer, Helen Miller, Shereen Ashraf, Aurko Roy, Zizhao Zhang, Ada Ma, Angelos Filos, Milos Besta, Rory Blevins, Ted Kli-744 menko, Chih-Kuan Yeh, Soravit Changpinyo, Jiaqi Mu, Oscar Chang, Mantas Pajarskas, Carrie Muir, Vered Cohen, Charline Le Lan, Krishna Haridasan, Chang Lan, Jiepu Jiang, Justin Chiu, Jaime Alonso Lorenzo, Lars Lowe Sj\u00f6sund, S'ebastien Cevey, Zach Gleicher, Thi Avrahami, Anudhyan Boral, Hansa Srinivasan, Vittorio Selo, Rhys May, Konstantinos Aisopos, L'eonard Hussenot, Livio Baldini, Kate Soares, Michael B Baumli, Adri\u00e0 Chang, Ben Caine, A. Pritzel, Filip Pavetic, Fabio Pardo, Anita Gergely, Justin Frye, V. Ramasesh, Dan Horgan, Kartikeya Badola, hishek Jindal, S. Vikram, Zhitao Gong, Sergi Caelles, Ross Hemsley, Gregory Thornton, Fangxiaoyu Feng, Wojciech Stokowiec, Ce Zheng, Phoebe Thacker, \u00c7a\u02d8glar \u00dcnl\u00fc, Zhishuai Zhang, Mohammad Saleh, James Svensson, Maxwell Bileschi, Piyush Patil, Ankesh Anand, Roman Ring, Katerina Tsihlas, Arpi Vezer, Marco Selvi, Toby Shevlane, Mikel Ro-768 driguez, T. Kwiatkowski, Samira Daruki, Keran Rong, Allan Dafoe, Nicholas FitzGerald, Keren Gu-Lemberg, Mina Khan, Lisa Anne Hendricks, Marie Pellat, Vladimir Feinberg, James Cobon-772 Kerr, Tara N. Sainath, Maribeth Rauh, Sayed Hadi, Richard Hashemi, Yana Ives, YaGuang Hasson, Eric Li, Yuan Noland, N. Cao, Le Byrd, Qingze Hou, Thibault Wang, Michela Sottiaux, Paganini Jean-Baptiste, Alexandre Lespiau, Samer Moufarek, Kaushik Hassan, Joost Shivakumar, Amol van Amers-778 foort, Pratik Mandhane, Anirudh Joshi, Matthew Goyal, Andrew Tung, Hannah Brock, Vedant Shea-780 han, Cheng Misra, Nemanja Li, Raki\u00b4cevi\u00b4c Mostafa, Fangyu Dehghani, Sid Liu, Junhyuk Mittal, Seb Oh, Eren Noury, Fantine Sezener, Matthew Huot, Nicola Lamm, Charlie De Cao, Gamaleldin Chen, Ed Elsayed, Mahdis Chi, Ian Mahdieh, Nan Tenney, Ivan Hua, Patrick Petrychenko, Dylan Kane, Rishub Scand-786 inaro, Jonathan Jain, Romina Uesato, Datta Adam, Oskar Sadovsky, Dominik Bunyan, Rabiej Shimu, John Wu, Gautam Zhang, Edouard Vasudevan, Mahmoud Leurent, Ionut Alnahlawi, Nan Georgescu, Ivy Wei, Betty Zheng, Pam G Chan, Rabinovitch Piotr, Ye Stanczyk, David Zhang, Subhajit Steiner, Michael Naskar, Matthew Azzam, Adam Johnson, Chung-Cheng Paszke, Jaume Chiu, Sanchez Elias, Afroz Mohiuddin, Faizan Muhammad, Jin Miao, Andrew Lee, Nino Vieillard, Sahitya Potluri, Elnaz Park, Jiageng Davoodi, Jeff Zhang, Stanway, William Isaac, Zhe Chen, Johnson Jia, Zhenkai Levskaya, Chris Zhu, Gorgolewski, Peter, Yu Grabowski, Alberto Mao, Kaisheng Magni, Yao, Javier Snaider, Norman Casagrande, Paul Sugan-803, Evan Palmer, Geoffrey Irving, E. Loper, Manaal Faruqui, Isha Arkatkar, Izhak Nanxin Chen, Michael Fink, Alfonso Casta\u00f1o, Irene Gian-806, Wooyeol Kim, Miko\u0142aj Rybi\u00b4nski, Ashwin, Jennifer Sreevatsa, David Prendki, Adrian Soergel, Willi Goedeckemeyer, Mohsen Gierke, Jafari, Meenu, J. Gaba, Diana Wiesner, Yawen Gage Wright, Harsha Vashisht, Yana Kulizhskaya, Jay Hoover, Maigo Le, Lu Li, Chimezie Iwuanyanwu, Lu Liu, Kevin Ramirez, A. Ya. Khorlin, Albert Cui, Tian, Marin Lin, Marcus Georgiev, Ricardo Wu, Aguilar, Rama Lui, N. Pasumarthi, Lintz, Anitha Vi-819, L. Jayakumar, Daniel Nguyen Thiet, Pedro Andor, Cosmin Valenzuela, Daiyi Paduraru, Kather-821 ine Peng, Shuyuan Lee, Somer Zhang, Duc Greene, Dung, Paula Nguyen, Sarmishta Kurylowicz, Se-823 Velury, Cassidy bastian Krause, Lucas Hardin, Lili Dixon, Kiam Janzer, Ziqiang Choo, Biao Feng, Zhang, Quoc Le, Elena Allica Abellan, Dayou Du, Dan McK-827, Natasha Antropova, Tolga Bolukbasi, Orgad, David Keller, Daniel Reid, Maria Finchelstein, Abi, Remi Raad, Peter Crocker, Robert Hawkins, Dadashi, Colin Gaffney, Sid Lall, Ken Franko, Egor Filonov, Anna Bulanova, R\u00e9mi Leblond, Vikas Yadav, Harry Askham, Luis C. Cobo, Kelvin Xu, Felix Fischer, Jun Xu, Christina Sorokin, Chris Al-834, Chu-Cheng Lin",
            "EMNLP Paper ID": "585",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "0c40ebee64178279d96fb3bbac549c775706574d",
            "title": "Predicting Rewards Alongside Tokens: Non-disruptive Parameter Insertion for Efficient Inference Intervention in Large Language Model",
            "abstract": "Transformer-based large language models (LLMs) exhibit limitations such as generating unsafe responses, unreliable reasoning, etc. Existing inference intervention approaches attempt to mitigate these issues by finetuning additional models to produce calibration signals (such as rewards) that guide the LLM's decoding process. However, this solution introduces substantial time and space overhead due to the separate models required. This work proposes Non-disruptive parameters insertion (Otter), inserting extra parameters into the transformer architecture to predict calibration signals along with the original LLM output. Otter offers state-of-the-art performance on multiple demanding tasks while saving up to 86.5\\% extra space and 98.5\\% extra time. Furthermore, Otter seamlessly integrates with existing inference engines, requiring only a one-line code change, and the original model response remains accessible after the parameter insertion. Our code is publicly available at \\url{https://github.com/chenhan97/Otter}",
            "link": "https://www.semanticscholar.org/paper/0c40ebee64178279d96fb3bbac549c775706574d",
            "authors": "Chenhan Yuan, Fei Huang, Ru Peng, Keming Lu, Bowen Yu, Chang Zhou, Jingren Zhou",
            "EMNLP Paper ID": "618",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 12): 17:45\u201318:45 (Evening)",
            "Type": "Main"
        },
        {
            "paperId": "0bce2795374e0070138e89c84917bcba9ce9dee3",
            "title": "Data Advisor: Dynamic Data Curation for Safety Alignment of Large Language Models",
            "abstract": "Data is a crucial element in large language model (LLM) alignment. Recent studies have explored using LLMs for efficient data collection. However, LLM-generated data often suffers from quality issues, with underrepresented or absent aspects and low-quality datapoints. To address these problems, we propose Data Advisor, an enhanced LLM-based method for generating data that takes into account the characteristics of the desired dataset. Starting from a set of pre-defined principles in hand, Data Advisor monitors the status of the generated data, identifies weaknesses in the current dataset, and advises the next iteration of data generation accordingly. Data Advisor can be easily integrated into existing data generation methods to enhance data quality and coverage. Experiments on safety alignment of three representative LLMs (i.e., Mistral, Llama2, and Falcon) demonstrate the effectiveness of Data Advisor in enhancing model safety against various fine-grained safety issues without sacrificing model utility.",
            "link": "https://www.semanticscholar.org/paper/0bce2795374e0070138e89c84917bcba9ce9dee3",
            "authors": "Fei Wang, Ninareh Mehrabi, Palash Goyal, Rahul Gupta, Kai-Wei Chang, A. Galstyan",
            "EMNLP Paper ID": "928",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "2057869daa0b3f88b2b4ee3e7b5413d880357f35",
            "title": "LIONs: An Empirically Optimized Approach to Align Language Models",
            "abstract": "Alignment is a crucial step to enhance the instruction-following and conversational abilities of language models. Despite many recent work proposing new algorithms, datasets, and training pipelines, there is a lack of comprehensive studies measuring the impact of various design choices throughout the whole training process. We first conduct a rigorous analysis over a three-stage training pipeline consisting of supervised fine-tuning, offline preference learning, and online preference learning. We have found that using techniques like sequence packing, loss masking in SFT, increasing the preference dataset size in DPO, and online DPO training can significantly improve the performance of language models. We then train from Gemma-2b-base and LLama-3-8b-base, and find that our best models exceed the performance of the official instruct models tuned with closed-source data and algorithms. Our code and models can be found at \\url{https://github.com/Columbia-NLP-Lab/LionAlignment}.",
            "link": "https://www.semanticscholar.org/paper/2057869daa0b3f88b2b4ee3e7b5413d880357f35",
            "authors": "Xiao Yu, Qingyang Wu, Yu Li, Zhou Yu",
            "EMNLP Paper ID": "1005",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "7ed992b818e4a0490f6b5ef9ac249da60531075d",
            "title": "Encourage or Inhibit Monosemanticity? Revisit Monosemanticity from a Feature Decorrelation Perspective",
            "abstract": "To better interpret the intrinsic mechanism of large language models (LLMs), recent studies focus on monosemanticity on its basic units. A monosemantic neuron is dedicated to a single and specific concept, which forms a one-to-one correlation between neurons and concepts. Despite extensive research in monosemanticity probing, it remains unclear whether monosemanticity is beneficial or harmful to model capacity. To explore this question, we revisit monosemanticity from the feature decorrelation perspective and advocate for its encouragement. We experimentally observe that the current conclusion by wang2024learning, which suggests that decreasing monosemanticity enhances model performance, does not hold when the model changes. Instead, we demonstrate that monosemanticity consistently exhibits a positive correlation with model capacity, in the preference alignment process. Consequently, we apply feature correlation as a proxy for monosemanticity and incorporate a feature decorrelation regularizer into the dynamic preference optimization process. The experiments show that our method not only enhances representation diversity and activation sparsity but also improves preference alignment performance.",
            "link": "https://www.semanticscholar.org/paper/7ed992b818e4a0490f6b5ef9ac249da60531075d",
            "authors": "Hanqi Yan, Yanzheng Xiang, Guangyi Chen, Yifei Wang, Lin Gui, Yulan He",
            "EMNLP Paper ID": "1183",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "11e3007bb19dbf331b5e44f1b5e1f7ab95056c07",
            "title": "The Multilingual Alignment Prism: Aligning Global and Local Preferences to Reduce Harm",
            "abstract": "A key concern with the concept of\"alignment\"is the implicit question of\"alignment to what?\". AI systems are increasingly used across the world, yet safety alignment is often focused on homogeneous monolingual settings. Additionally, preference training and safety measures often overfit to harms common in Western-centric datasets. Here, we explore the viability of different alignment approaches when balancing dual objectives: addressing and optimizing for a non-homogeneous set of languages and cultural preferences while minimizing both global and local harms. We collect the first set of human annotated red-teaming prompts in different languages distinguishing between global and local harm, which serve as a laboratory for understanding the reliability of alignment techniques when faced with preference distributions that are non-stationary across geographies and languages. While this setting is seldom covered by the literature to date, which primarily centers on English harm mitigation, it captures real-world interactions with AI systems around the world. We establish a new precedent for state-of-the-art alignment techniques across 6 languages with minimal degradation in general performance. Our work provides important insights into cross-lingual transfer and novel optimization approaches to safeguard AI systems designed to serve global populations.",
            "link": "https://www.semanticscholar.org/paper/11e3007bb19dbf331b5e44f1b5e1f7ab95056c07",
            "authors": "Aakanksha, Arash Ahmadian, B. Ermi\u015f, Seraphina Goldfarb-Tarrant, Julia Kreutzer, Marzieh Fadaee, Sara Hooker",
            "EMNLP Paper ID": "1404",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "16342fd115398bb1bcf2f16baf94f9d4d122d480",
            "title": "Let Me Teach You: Pedagogical Foundations of Feedback for Language Models",
            "abstract": "Natural Language Feedback (NLF) is an increasingly popular mechanism for aligning Large Language Models (LLMs) to human preferences. Despite the diversity of the information it can convey, NLF methods are often hand-designed and arbitrary, with little systematic grounding. At the same time, research in learning sciences has long established several effective feedback models. In this opinion piece, we compile ideas from pedagogy to introduce FELT, a feedback framework for LLMs that outlines various characteristics of the feedback space, and a feedback content taxonomy based on these variables, providing a general mapping of the feedback space. In addition to streamlining NLF designs, FELT also brings out new, unexplored directions for research in NLF. We make our taxonomy available to the community, providing guides and examples for mapping our categorizations to future research.",
            "link": "https://www.semanticscholar.org/paper/16342fd115398bb1bcf2f16baf94f9d4d122d480",
            "authors": "Beatriz Borges, Niket Tandon, Tanja Kaser, Antoine Bosselut",
            "EMNLP Paper ID": "1409",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "c81fd487a418268d42dce8613236297a9dc127fc",
            "title": "Learning Personalized Alignment for Evaluating Open-ended Text Generation",
            "abstract": "Recent research has increasingly focused on evaluating large language models' (LLMs) alignment with diverse human values and preferences, particularly for open-ended tasks like story generation. Traditional evaluation metrics rely heavily on lexical similarity with human-written references, often showing poor correlation with human judgments and failing to account for alignment with the diversity of human preferences. To address these challenges, we introduce PerSE, an interpretable evaluation framework designed to assess alignment with specific human preferences. It is tuned to infer specific preferences from an in-context personal profile and evaluate the alignment between the generated content and personal preferences. PerSE enhances interpretability by providing detailed comments and fine-grained scoring, facilitating more personalized content generation. Our 13B LLaMA-2-based PerSE shows a 15.8% increase in Kendall correlation and a 13.7% rise in accuracy with zero-shot reviewers compared to GPT-4. It also outperforms GPT-4 by 46.01% in Kendall correlation on new domains, indicating its transferability.",
            "link": "https://www.semanticscholar.org/paper/c81fd487a418268d42dce8613236297a9dc127fc",
            "authors": "Danqing Wang, Kevin Yang, Hanlin Zhu, Xiaomeng Yang, Andrew Cohen, Lei Li, Yuandong Tian",
            "EMNLP Paper ID": "1542",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "3d5462cb9ce4654bd2b33e567401893ce0c1316e",
            "title": "Not Everything is All You Need: Toward Low-Redundant Optimization for Large Language Model Alignment",
            "abstract": "Large language models (LLMs) are still struggling in aligning with human preference in complex tasks and scenarios. They are prone to overfit into the unexpected patterns or superficial styles in the training data. We conduct an empirical study that only selects the top-10\\% most updated parameters in LLMs for alignment training, and see improvements in the convergence process and final performance. It indicates the existence of redundant neurons in LLMs for alignment training. To reduce its influence, we propose a low-redundant alignment method named \\textbf{ALLO}, focusing on optimizing the most related neurons with the most useful supervised signals. Concretely, we first identify the neurons that are related to the human preference data by a gradient-based strategy, then identify the alignment-related key tokens by reward models for computing loss. Besides, we also decompose the alignment process into the forgetting and learning stages, where we first forget the tokens with unaligned knowledge and then learn aligned knowledge, by updating different ratios of neurons, respectively. Experimental results on 10 datasets have shown the effectiveness of ALLO. Our code and data are available at \\url{https://github.com/RUCAIBox/ALLO}.",
            "link": "https://www.semanticscholar.org/paper/3d5462cb9ce4654bd2b33e567401893ce0c1316e",
            "authors": "Zhipeng Chen, Kun Zhou, Wayne Xin Zhao, Jingyuan Wang, Ji-Rong Wen",
            "EMNLP Paper ID": "1786",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "70dcd5bda31413c00696a95a769ea55635c32b70",
            "title": "Towards Aligning Language Models with Textual Feedback",
            "abstract": "We present ALT (ALignment with Textual feedback), an approach that aligns language models with user preferences expressed in text. We argue that text offers greater expressiveness, enabling users to provide richer feedback than simple comparative preferences and this richer feedback can lead to more efficient and effective alignment. ALT aligns the model by conditioning its generation on the textual feedback. Our method relies solely on language modeling techniques and requires minimal hyper-parameter tuning, though it still presents the main benefits of RL-based alignment algorithms and can effectively learn from textual feedback. We explore the efficacy and efficiency of textual feedback across different tasks such as toxicity reduction, summarization, and dialog response generation. We find that ALT outperforms PPO for the task of toxicity reduction while being able to match its performance on summarization with only 20% of the samples. We also explore how ALT can be used with feedback provided by an existing LLM where we explore an LLM providing constrained and unconstrained textual feedback. We also outline future directions to align models with natural language feedback.",
            "link": "https://www.semanticscholar.org/paper/70dcd5bda31413c00696a95a769ea55635c32b70",
            "authors": "Sauc Abadal Lloret, S. Dhuliawala, K. Murugesan, Mrinmaya Sachan",
            "EMNLP Paper ID": "2643",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "a7d047dd9f41d5f3e7eaa39e5ba8c97cccc7276d",
            "title": "Reformatted Alignment",
            "abstract": "The quality of finetuning data is crucial for aligning large language models (LLMs) with human values. Current methods to improve data quality are either labor-intensive or prone to factual errors caused by LLM hallucinations. This paper explores elevating the quality of existing instruction data to better align with human values, introducing a simple and effective approach named ReAlign, which reformats the responses of instruction data into a format that better aligns with pre-established criteria and the collated evidence. This approach minimizes human annotation, hallucination, and the difficulty in scaling, remaining orthogonal to existing alignment techniques. Experimentally, ReAlign significantly boosts the general alignment ability, math reasoning, factuality, and readability of the LLMs. Encouragingly, without introducing any additional data or advanced training techniques, and merely by reformatting the response, LLaMA-2-13B's mathematical reasoning ability on GSM8K can be improved from 46.77% to 56.63% in accuracy. Additionally, a mere 5% of ReAlign data yields a 67% boost in general alignment ability measured by the Alpaca dataset. This work highlights the need for further research into the science and mechanistic interpretability of LLMs. We have made the associated code and data publicly accessible to support future studies at https://github.com/GAIR-NLP/ReAlign.",
            "link": "https://www.semanticscholar.org/paper/a7d047dd9f41d5f3e7eaa39e5ba8c97cccc7276d",
            "authors": "Run-Ze Fan, Xuefeng Li, Haoyang Zou, Junlong Li, Shwai He, Ethan Chern, Jiewen Hu, Pengfei Liu",
            "matchScore": 74.026855,
            "original title": "Reformatted Alignment",
            "original authors": "Run-Ze Fan, Xuefeng Li, Haoyang Zou, Junlong Li, Shwai He, Ethan Chern, Jiewen Hu, Pengfei Liu",
            "EMNLP Paper ID": "100",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "c6811c3874d3b0e85755b86ff47ec1cdb40866f8",
            "title": "AlignSum: Data Pyramid Hierarchical Fine-tuning for Aligning with Human Summarization Preference",
            "abstract": "Text summarization tasks commonly employ Pre-trained Language Models (PLMs) to fit diverse standard datasets. While these PLMs excel in automatic evaluations, they frequently underperform in human evaluations, indicating a deviation between their generated summaries and human summarization preferences. This discrepancy is likely due to the low quality of fine-tuning datasets and the limited availability of high-quality human-annotated data that reflect true human preference. To address this challenge, we introduce a novel human summarization preference alignment framework AlignSum. This framework consists of three parts: Firstly, we construct a Data Pymarid with extractive, abstractive, and human-annotated summary data. Secondly, we conduct the Gaussian Resampling to remove summaries with extreme lengths. Finally, we implement the two-stage hierarchical fine-tuning with Data Pymarid after Gaussian Resampling. We apply AlignSum to PLMs on the human-annotated CNN/DailyMail and BBC XSum datasets. Experiments show that with AlignSum, PLMs like BART-Large surpass 175B GPT-3 in both automatic and human evaluations. This demonstrates that AlignSum significantly enhances the alignment of language models with human summarization preferences.",
            "link": "https://www.semanticscholar.org/paper/c6811c3874d3b0e85755b86ff47ec1cdb40866f8",
            "authors": "Yang Han, Yiming Wang, Rui Wang, Lu Chen, Kai Yu",
            "matchScore": 302.41278,
            "original title": "AlignSum: Data Pyramid Hierarchical Fine-tuning for Aligning with Human Summarization Preference",
            "original authors": "Yang Han, Yiming Wang, Rui Wang, Lu Chen, Kai Yu",
            "EMNLP Paper ID": "1792",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "200a19739ef76cb91c490be72d409f0fb0468901",
            "title": "TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models",
            "abstract": "Mainstream approaches to aligning large language models (LLMs) heavily rely on human preference data, particularly when models require periodic updates. The standard process for iterative alignment of LLMs involves collecting new human feedback for each update. However, the data collection process is costly and challenging to scale. To address this issue, we introduce the\"TS-Align\"framework, which fine-tunes a policy model using pairwise feedback data automatically mined from its outputs. This automatic mining process is efficiently accomplished through the collaboration between a large-scale teacher model and a small-scale student model. The policy fine-tuning process can be iteratively repeated using on-policy generations within our proposed teacher-student collaborative framework. Through extensive experiments, we demonstrate that our final aligned policy outperforms the base policy model with an average win rate of 69.7% across seven conversational or instruction-following datasets. Furthermore, we show that the ranking capability of the teacher is effectively distilled into the student through our pipeline, resulting in a small-scale yet effective reward model for policy model alignment.",
            "link": "https://www.semanticscholar.org/paper/200a19739ef76cb91c490be72d409f0fb0468901",
            "authors": "Chen Zhang, Chengguang Tang, Dading Chong, Ke Shi, Guohua Tang, Feng Jiang, Haizhou Li",
            "matchScore": 320.03088,
            "original title": "TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models",
            "original authors": "Chen Zhang, chengguang tang, Dading Chong, Ke Shi, Guohua Tang, Feng Jiang, Haizhou Li",
            "EMNLP Paper ID": "1863",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Findings"
        },
        {
            "paperId": "b5f0116aa898854aacc6f7ec6cb8867c9ed99fab",
            "title": "Compare without Despair: Reliable Preference Evaluation with Generation Separability",
            "abstract": "Human evaluation of generated language through pairwise preference judgments is pervasive. However, under common scenarios, such as when generations from a model pair are very similar, or when stochastic decoding results in large variations in generations, it results in inconsistent preference ratings. We address these challenges by introducing a meta-evaluation measure, separability, which estimates how suitable a test instance is for pairwise preference evaluation. For a candidate test instance, separability samples multiple generations from a pair of models, and measures how distinguishable the two sets of generations are. Our experiments show that instances with high separability values yield more consistent preference ratings from both human- and auto-raters. Further, the distribution of separability allows insights into which test benchmarks are more valuable for comparing models. Finally, we incorporate separability into ELO ratings, accounting for how suitable each test instance might be for reliably ranking LLMs. Overall, separability has implications for consistent, efficient and robust preference evaluation of LLMs with both human- and auto-raters.",
            "link": "https://www.semanticscholar.org/paper/b5f0116aa898854aacc6f7ec6cb8867c9ed99fab",
            "authors": "Sayan Ghosh, Tejas Srinivasan, Swabha Swayamdipta",
            "matchScore": 264.13153,
            "original title": "Compare without Despair: Reliable Preference Evaluation with Generation Separability",
            "original authors": "Sayan Ghosh, Tejas Srinivasan, Swabha Swayamdipta",
            "EMNLP Paper ID": "2502",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Findings"
        },
        {
            "paperId": "b925a290a35a3c11371fea0ff71b56371783d572",
            "title": "Pedagogical Alignment of Large Language Models",
            "abstract": "Large Language Models (LLMs), when used in educational settings without pedagogical fine-tuning, often provide immediate answers rather than guiding students through the problem-solving process. This approach falls short of pedagogically best practices and limits their effectiveness as educational tools. We term the objective of training LLMs to emulate effective teaching strategies as `pedagogical alignment.' In this paper, we investigate Learning from Human Preferences (LHP) algorithms to achieve this alignment objective. A key challenge in this process is the scarcity of high-quality preference datasets to guide the alignment. To address this, we propose a novel approach for constructing a large-scale dataset using synthetic data generation techniques, eliminating the need for time-consuming and costly manual annotation. Leveraging this dataset, our experiments with Llama and Mistral models demonstrate that LHP methods outperform standard supervised fine-tuning (SFT), improving pedagogical alignment accuracy by 13.1% and 8.7% respectively. Existing evaluation methods also lack quantitative metrics to adequately measure the pedagogical alignment of LLMs. To address this gap, we propose novel perplexity-based metrics that quantify LLMs' tendency to provide scaffolded guidance versus direct answers, offering a robust measure of pedagogical alignment. Our analysis provides compelling evidence for the superiority of LHP methods over SFT in optimizing LLMs' behavior, underscoring the potential of LHP methods in better aligning LLMs with educational objectives and fostering effective learning experiences. Code and models are available \\href{https://github.com/luffycodes/Tutorbot-Spock}{here}.",
            "link": "https://www.semanticscholar.org/paper/b925a290a35a3c11371fea0ff71b56371783d572",
            "authors": "Shashank Sonkar, Kangqi Ni, Sapana Chaudhary, R. Baraniuk",
            "matchScore": 154.78555,
            "original title": "Pedagogical Alignment of Large Language Models",
            "original authors": "Shashank Sonkar, Kangqi Ni, Sapana Chaudhary, Richard Baraniuk",
            "EMNLP Paper ID": "2660",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "577def35ce131165ea9f228466ed5fd2664f2e5f",
            "title": "Aligners: Decoupling LLMs and Alignment",
            "abstract": "Large Language Models (LLMs) need to be aligned with human expectations to ensure their safety and utility in most applications. Alignment is challenging, costly, and needs to be repeated for every LLM and alignment criterion. We propose to decouple LLMs and alignment by training aligner models that can be used to align any LLM for a given criteria on an as-needed basis, thus also reducing the potential negative impacts of alignment on performance. Our recipe for training the aligner models solely relies on synthetic data generated with a (prompted) LLM and can be easily adjusted for a variety of alignment criteria. We use the same synthetic data to train inspectors, binary miss-alignment classification models to guide a\"squad\"of multiple aligners. Our empirical results demonstrate consistent improvements when applying aligner squad to various LLMs, including chat-aligned models, across several instruction-following and red-teaming datasets.",
            "link": "https://www.semanticscholar.org/paper/577def35ce131165ea9f228466ed5fd2664f2e5f",
            "authors": "Lilian Ngweta, Mayank Agarwal, Subha Maity, Alex Gittens, Yuekai Sun, M. Yurochkin",
            "matchScore": 178.8872,
            "original title": "Aligners: Decoupling LLMs and Alignment",
            "original authors": "Lilian Ngweta, Mayank Agarwal, Subha Maity, Alex Gittens, Yuekai Sun, Mikhail Yurochkin",
            "EMNLP Paper ID": "2689",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "ec9203f6c25a353325dd23ed38e5036b79d9e79b",
            "title": "LongAlign: A Recipe for Long Context Alignment of Large Language Models",
            "abstract": "Extending large language models to effectively handle long contexts requires instruction fine-tuning on input sequences of similar length. To address this, we present LongAlign -- a recipe of the instruction data, training, and evaluation for long context alignment. First, we construct a long instruction-following dataset using Self-Instruct. To ensure the data diversity, it covers a broad range of tasks from various long context sources. Second, we adopt the packing and sorted batching strategies to speed up supervised fine-tuning on data with varied length distributions. Additionally, we develop a loss weighting method to balance the contribution to the loss across different sequences during packing training. Third, we introduce the LongBench-Chat benchmark for evaluating instruction-following capabilities on queries of 10k-100k in length. Experiments show that LongAlign outperforms existing recipes for LLMs in long context tasks by up to 30\\%, while also maintaining their proficiency in handling short, generic tasks. The code, data, and long-aligned models are open-sourced at https://github.com/THUDM/LongAlign.",
            "link": "https://www.semanticscholar.org/paper/ec9203f6c25a353325dd23ed38e5036b79d9e79b",
            "authors": "Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, Juanzi Li",
            "matchScore": 261.23334,
            "original title": "LongAlign: A Recipe for Long Context Alignment of Large Language Models",
            "original authors": "Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, Juanzi Li",
            "EMNLP Paper ID": "276",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "b6d9982b019039e1424abec6639492d77f3eb0bc",
            "title": "Exploring Multilingual Concepts of Human Value in Large Language Models: Is Value Alignment Consistent, Transferable and Controllable across Languages?",
            "abstract": "Prior research has revealed that certain abstract concepts are linearly represented as directions in the representation space of LLMs, predominantly centered around English. In this paper, we extend this investigation to a multilingual context, with a specific focus on human values-related concepts (i.e., value concepts) due to their significance for AI safety. Through our comprehensive exploration covering 7 types of human values, 16 languages and 3 LLM series with distinct multilinguality (e.g., monolingual, bilingual and multilingual), we first empirically confirm the presence of value concepts within LLMs in a multilingual format. Further analysis on the cross-lingual characteristics of these concepts reveals 3 traits arising from language resource disparities: cross-lingual inconsistency, distorted linguistic relationships, and unidirectional cross-lingual transfer between high- and low-resource languages, all in terms of value concepts. Moreover, we validate the feasibility of cross-lingual control over value alignment capabilities of LLMs, leveraging the dominant language as a source language. Ultimately, recognizing the significant impact of LLMs' multilinguality on our results, we consolidate our findings and provide prudent suggestions on the composition of multilingual data for LLMs pre-training.",
            "link": "https://www.semanticscholar.org/paper/b6d9982b019039e1424abec6639492d77f3eb0bc",
            "authors": "Shaoyang Xu, Weilong Dong, Zishan Guo, Xinwei Wu, Deyi Xiong",
            "matchScore": 328.45776,
            "original title": "Exploring Multilingual Concepts of Human Values in Large Language Models: Is Value Alignment Consistent, Transferable and Controllable across Languages?",
            "original authors": "Shaoyang Xu, Weilong Dong, Zishan Guo, Xinwei Wu, Deyi Xiong",
            "EMNLP Paper ID": "362",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "e87604639b3ec9bde72d88fce3f762b28a1c39eb",
            "title": "Inference-Time Language Model Alignment via Integrated Value Guidance",
            "abstract": "Large language models are typically fine-tuned to align with human preferences, but tuning large models is computationally intensive and complex. In this work, we introduce $\\textit{Integrated Value Guidance}$ (IVG), a method that uses implicit and explicit value functions to guide language model decoding at token and chunk-level respectively, efficiently aligning large language models purely at inference time. This approach circumvents the complexities of direct fine-tuning and outperforms traditional methods. Empirically, we demonstrate the versatility of IVG across various tasks. In controlled sentiment generation and summarization tasks, our method significantly improves the alignment of large models using inference-time guidance from $\\texttt{gpt2}$-based value functions. Moreover, in a more challenging instruction-following benchmark AlpacaEval 2.0, we show that both specifically tuned and off-the-shelf value functions greatly improve the length-controlled win rates of large models against $\\texttt{gpt-4-turbo}$ (e.g., $19.51\\% \\rightarrow 26.51\\%$ for $\\texttt{Mistral-7B-Instruct-v0.2}$ and $25.58\\% \\rightarrow 33.75\\%$ for $\\texttt{Mixtral-8x7B-Instruct-v0.1}$ with Tulu guidance).",
            "link": "https://www.semanticscholar.org/paper/e87604639b3ec9bde72d88fce3f762b28a1c39eb",
            "authors": "Zhixuan Liu, Zhanhui Zhou, Yuanfu Wang, Chao Yang, Yu Qiao",
            "matchScore": 262.8704,
            "original title": "Inference-Time Language Model Alignment via Integrated Value Guidance",
            "original authors": "Zhixuan Liu, Zhanhui Zhou, Yuanfu Wang, Chao Yang, Yu Qiao",
            "EMNLP Paper ID": "837",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Findings"
        }
    ],
    "Confidence Calibration and Uncertainty Quantification in Large Language Models": [
        {
            "paperId": "ba63e1ab5b6e9d849982ae293ac0483053badaff",
            "title": "Uncertainty in Language Models: Assessment through Rank-Calibration",
            "abstract": "Language Models (LMs) have shown promising performance in natural language generation. However, as LMs often generate incorrect or hallucinated responses, it is crucial to correctly quantify their uncertainty in responding to given inputs. In addition to verbalized confidence elicited via prompting, many uncertainty measures ($e.g.$, semantic entropy and affinity-graph-based measures) have been proposed. However, these measures can differ greatly, and it is unclear how to compare them, partly because they take values over different ranges ($e.g.$, $[0,\\infty)$ or $[0,1]$). In this work, we address this issue by developing a novel and practical framework, termed $Rank$-$Calibration$, to assess uncertainty and confidence measures for LMs. Our key tenet is that higher uncertainty (or lower confidence) should imply lower generation quality, on average. Rank-calibration quantifies deviations from this ideal relationship in a principled manner, without requiring ad hoc binary thresholding of the correctness score ($e.g.$, ROUGE or METEOR). The broad applicability and the granular interpretability of our methods are demonstrated empirically.",
            "link": "https://www.semanticscholar.org/paper/ba63e1ab5b6e9d849982ae293ac0483053badaff",
            "authors": "Xinmeng Huang, Shuo Li, Mengxin Yu, Matteo Sesia, Hamed Hassani, Insup Lee, O. Bastani, Edgar Dobriban",
            "EMNLP Paper ID": "30",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "036e96ed196a7f4bb812380f3b76ac75d4a648e4",
            "title": "Calibrating the Confidence of Large Language Models by Eliciting Fidelity",
            "abstract": "Large language models optimized with techniques like RLHF have achieved good alignment in being helpful and harmless. However, post-alignment, these language models often exhibit overconfidence, where the expressed confidence does not accurately calibrate with their correctness rate. In this paper, we decompose the language model confidence into the \\textit{Uncertainty} about the question and the \\textit{Fidelity} to the answer generated by language models. Then, we propose a plug-and-play method to estimate the confidence of language models. Our method has shown good calibration performance by conducting experiments with 6 RLHF-LMs on four MCQA datasets. Moreover, we propose two novel metrics, IPR and CE, to evaluate the calibration of the model, and we have conducted a detailed discussion on \\textit{Truly Well-Calibrated Confidence}. Our method could serve as a strong baseline, and we hope that this work will provide some insights into the model confidence calibration.",
            "link": "https://www.semanticscholar.org/paper/036e96ed196a7f4bb812380f3b76ac75d4a648e4",
            "authors": "Mozhi Zhang, Mianqiu Huang, Rundong Shi, Linsen Guo, Chong Peng, Peng Yan, Yaqian Zhou, Xipeng Qiu",
            "EMNLP Paper ID": "331",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "e7119a3366d4724c11f041306b3f1b9d4b9080f4",
            "title": "LUQ: Long-text Uncertainty Quantification for LLMs",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable capability in a variety of NLP tasks. However, LLMs are also prone to generate nonfactual content. Uncertainty Quantification (UQ) is pivotal in enhancing our understanding of a model's confidence on its generation, thereby aiding in the mitigation of nonfactual outputs. Existing research on UQ predominantly targets short text generation, typically yielding brief, word-limited responses. However, real-world applications frequently necessitate much longer responses. Our study first highlights the limitations of current UQ methods in handling long text generation. We then introduce \\textsc{Luq} and its two variations, a series of novel sampling-based UQ approaches specifically designed for long text. Our findings reveal that \\textsc{Luq} outperforms existing baseline methods in correlating with the model's factuality scores (negative coefficient of -0.85 observed for Gemini Pro). To further improve the factuality of LLM responses, we propose \\textsc{Luq-Ensemble}, a method that ensembles responses from multiple models and selects the response with the lowest uncertainty. The ensembling method greatly improves the response factuality upon the best standalone LLM.",
            "link": "https://www.semanticscholar.org/paper/e7119a3366d4724c11f041306b3f1b9d4b9080f4",
            "authors": "Caiqi Zhang, Fangyu Liu, Marco Basaldella, Nigel Collier",
            "EMNLP Paper ID": "575",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "df38798cb99338e1aac9c4dda154c78787f89df3",
            "title": "SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales",
            "abstract": "Large language models (LLMs) often generate inaccurate or fabricated information and generally fail to indicate their confidence, which limits their broader applications. Previous work elicits confidence from LLMs by direct or self-consistency prompting, or constructing specific datasets for supervised finetuning. The prompting-based approaches have inferior performance, and the training-based approaches are limited to binary or inaccurate group-level confidence estimates. In this work, we present the advanced SaySelf, a training framework that teaches LLMs to express more accurate fine-grained confidence estimates. In addition, beyond the confidence scores, SaySelf initiates the process of directing LLMs to produce self-reflective rationales that clearly identify gaps in their parametric knowledge and explain their uncertainty. This is achieved by using an LLM to automatically summarize the uncertainties in specific knowledge via natural language. The summarization is based on the analysis of the inconsistency in multiple sampled reasoning chains, and the resulting data is utilized for supervised fine-tuning. Moreover, we utilize reinforcement learning with a meticulously crafted reward function to calibrate the confidence estimates, motivating LLMs to deliver accurate, high-confidence predictions and to penalize overconfidence in erroneous outputs. Experimental results in both in-distribution and out-of-distribution datasets demonstrate the effectiveness of SaySelf in reducing the confidence calibration error and maintaining the task performance. We show that the generated self-reflective rationales are reasonable and can further contribute to the calibration. The code is made public at https://github.com/xu1868/SaySelf.",
            "link": "https://www.semanticscholar.org/paper/df38798cb99338e1aac9c4dda154c78787f89df3",
            "authors": "Tianyang Xu, Shujin Wu, Shizhe Diao, Xiaoze Liu, Xingyao Wang, Yangyi Chen, Jing Gao",
            "EMNLP Paper ID": "667",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "719ef75fb6f58b6097d95990b30a0747d35b56c2",
            "title": "Statistical Uncertainty in Word Embeddings: GloVe-V",
            "abstract": "Static word embeddings are ubiquitous in computational social science applications and contribute to practical decision-making in a variety of fields including law and healthcare. However, assessing the statistical uncertainty in downstream conclusions drawn from word embedding statistics has remained challenging. When using only point estimates for embeddings, researchers have no streamlined way of assessing the degree to which their model selection criteria or scientific conclusions are subject to noise due to sparsity in the underlying data used to generate the embeddings. We introduce a method to obtain approximate, easy-to-use, and scalable reconstruction error variance estimates for GloVe (Pennington et al., 2014), one of the most widely used word embedding models, using an analytical approximation to a multivariate normal model. To demonstrate the value of embeddings with variance (GloVe-V), we illustrate how our approach enables principled hypothesis testing in core word embedding tasks, such as comparing the similarity between different word pairs in vector space, assessing the performance of different models, and analyzing the relative degree of ethnic or gender bias in a corpus using different word lists.",
            "link": "https://www.semanticscholar.org/paper/719ef75fb6f58b6097d95990b30a0747d35b56c2",
            "authors": "Andrea Vallebueno, Cassandra Handan-Nader, Christopher D. Manning, Daniel E. Ho",
            "EMNLP Paper ID": "1030",
            "Oral/Poster": "Oral",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "656b36f00a2f6ef98a8032a0beb02c9504a292be",
            "title": "Precise Model Benchmarking with Only a Few Observations",
            "abstract": "How can we precisely estimate a large language model's (LLM) accuracy on questions belonging to a specific topic within a larger question-answering dataset? The standard direct estimator, which averages the model's accuracy on the questions in each subgroup, may exhibit high variance for subgroups (topics) with small sample sizes. Synthetic regression modeling, which leverages the model's accuracy on questions about other topics, may yield biased estimates that are too unreliable for large subgroups. We prescribe a simple yet effective solution: an empirical Bayes (EB) estimator that balances direct and regression estimates for each subgroup separately, improving the precision of subgroup-level estimates of model performance. Our experiments on multiple datasets show that this approach consistently provides more precise estimates of the LLM performance compared to the direct and regression approaches, achieving substantial reductions in the mean squared error. Confidence intervals for EB estimates also have near-nominal coverage and are narrower compared to those for the direct estimator. Additional experiments on tabular and vision data validate the benefits of this EB approach.",
            "link": "https://www.semanticscholar.org/paper/656b36f00a2f6ef98a8032a0beb02c9504a292be",
            "authors": "Riccardo Fogliato, Pratik Patil, Nil-Jana Akpinar, Mathew Monfort",
            "EMNLP Paper ID": "1068",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "b1b6948ea473605e7272970c77cbbcabd4569577",
            "title": "Contextualized Sequence Likelihood: Enhanced Confidence Scores for Natural Language Generation",
            "abstract": "The advent of large language models (LLMs) has dramatically advanced the state-of-the-art in numerous natural language generation tasks. For LLMs to be applied reliably, it is essential to have an accurate measure of their confidence. Currently, the most commonly used confidence score function is the likelihood of the generated sequence, which, however, conflates semantic and syntactic components. For instance, in question-answering (QA) tasks, an awkward phrasing of the correct answer might result in a lower probability prediction. Additionally, different tokens should be weighted differently depending on the context. In this work, we propose enhancing the predicted sequence probability by assigning different weights to various tokens using attention values elicited from the base LLM. By employing a validation set, we can identify the relevant attention heads, thereby significantly improving the reliability of the vanilla sequence probability confidence measure. We refer to this new score as the Contextualized Sequence Likelihood (CSL). CSL is easy to implement, fast to compute, and offers considerable potential for further improvement with task-specific prompts. Across several QA datasets and a diverse array of LLMs, CSL has demonstrated significantly higher reliability than state-of-the-art baselines in predicting generation quality, as measured by the AUROC or AUARC.",
            "link": "https://www.semanticscholar.org/paper/b1b6948ea473605e7272970c77cbbcabd4569577",
            "authors": "Zhen Lin, Shubhendu Trivedi, Jimeng Sun",
            "EMNLP Paper ID": "1170",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "332c86a4621500c3b8a7e70d132c38ce003c20f8",
            "title": "Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding",
            "abstract": "Calibrating language models (LMs) aligns their generation confidence with the actual likelihood of answer correctness, which can inform users about LMs' reliability and mitigate hallucinated content. However, prior calibration methods, such as self-consistency-based and logit-based approaches, are either limited in inference-time efficiency or fall short of providing informative signals. Moreover, simply filtering out low-confidence responses reduces the LM's helpfulness when the answers are correct. Therefore, effectively using calibration techniques to enhance an LM's factuality remains an unsolved challenge. In this paper, we first propose an activation-based calibration method, ActCab, which trains a linear layer on top of the LM's last-layer activations that can better capture the representations of knowledge. Built on top of ActCab, we further propose CoDec, a confidence-guided decoding strategy to elicit truthful answers with high confidence from LMs. By evaluating on five popular QA benchmarks, ActCab achieves superior calibration performance than all competitive baselines, e.g., by reducing the average expected calibration error (ECE) score by up to 39%. Further experiments on CoDec show consistent improvements in several LMs' factuality on challenging QA datasets, such as TruthfulQA, highlighting the value of confidence signals in enhancing factuality.",
            "link": "https://www.semanticscholar.org/paper/332c86a4621500c3b8a7e70d132c38ce003c20f8",
            "authors": "Xin Liu, Farima Fatahi Bayat, Lu Wang",
            "EMNLP Paper ID": "1184",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "7fccc31defc8833ee9900cfdbfc744ea3e1a04f5",
            "title": "FIRST: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation",
            "abstract": "Large language models (LLMs) have become increasingly prevalent in our daily lives, leading to an expectation for LLMs to be trustworthy -- - both accurate and well-calibrated (the prediction confidence should align with its ground truth correctness likelihood). Nowadays, fine-tuning has become the most popular method for adapting a model to practical usage by significantly increasing accuracy on downstream tasks. Despite the great accuracy it achieves, we found fine-tuning is still far away from satisfactory trustworthiness due to\"tuning-induced mis-calibration\". In this paper, we delve deeply into why and how mis-calibration exists in fine-tuned models, and how distillation can alleviate the issue. Then we further propose a brand new method named Efficient Trustworthy Distillation (FIRST), which utilizes a small portion of teacher's knowledge to obtain a reliable language model in a cost-efficient way. Specifically, we identify the\"concentrated knowledge\"phenomenon during distillation, which can significantly reduce the computational burden. Then we apply a\"trustworthy maximization\"process to optimize the utilization of this small portion of concentrated knowledge before transferring it to the student. Experimental results demonstrate the effectiveness of our method, where better accuracy (+2.3%) and less mis-calibration (-10%) are achieved on average across both in-domain and out-of-domain scenarios, indicating better trustworthiness.",
            "link": "https://www.semanticscholar.org/paper/7fccc31defc8833ee9900cfdbfc744ea3e1a04f5",
            "authors": "Kashun Shum, Minrui Xu, Jianshu Zhang, Zixin Chen, Shizhe Diao, Hanze Dong, Jipeng Zhang, Muhammad Omer Raza",
            "EMNLP Paper ID": "1468",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "54a02c13084e594533d2b56f1ffaf9101f93dc77",
            "title": "Fill In The Gaps: Model Calibration and Generalization with Synthetic Data",
            "abstract": "As machine learning models continue to swiftly advance, calibrating their performance has become a major concern prior to practical and widespread implementation. Most existing calibration methods often negatively impact model accuracy due to the lack of diversity of validation data, resulting in reduced generalizability. To address this, we propose a calibration method that incorporates synthetic data without compromising accuracy. We derive the expected calibration error (ECE) bound using the Probably Approximately Correct (PAC) learning framework. Large language models (LLMs), known for their ability to mimic real data and generate text with mixed class labels, are utilized as a synthetic data generation strategy to lower the ECE bound and improve model accuracy on real test data. Additionally, we propose data generation mechanisms for efficient calibration. Testing our method on four different natural language processing tasks, we observed an average up to 34\\% increase in accuracy and 33\\% decrease in ECE.",
            "link": "https://www.semanticscholar.org/paper/54a02c13084e594533d2b56f1ffaf9101f93dc77",
            "authors": "Yang Ba, M. Mancenido, Rong Pan",
            "EMNLP Paper ID": "2038",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "19800548837a32bacd4113a8d69b0e9e122be097",
            "title": "Calibrating Language Models with Adaptive Temperature Scaling",
            "abstract": "The effectiveness of large language models (LLMs) is not only measured by their ability to generate accurate outputs but also by their calibration-how well their confidence scores reflect the probability of their outputs being correct. While unsupervised pre-training has been shown to yield LLMs with well-calibrated conditional probabilities, recent studies have shown that after fine-tuning with reinforcement learning from human feedback (RLHF), the calibration of these models degrades significantly. In this work, we introduce Adaptive Temperature Scaling (ATS), a post-hoc calibration method that predicts a temperature scaling parameter for each token prediction. The predicted temperature values adapt based on token-level features and are fit over a standard supervised fine-tuning (SFT) dataset. The adaptive nature of ATS addresses the varying degrees of calibration shift that can occur after RLHF fine-tuning. ATS improves calibration by over 10-50% across three downstream natural language evaluation benchmarks compared to prior calibration methods and does not impede performance improvements from RLHF.",
            "link": "https://www.semanticscholar.org/paper/19800548837a32bacd4113a8d69b0e9e122be097",
            "authors": "Johnathan Xie, Annie S. Chen, Yoonho Lee, Eric Mitchell, Chelsea Finn",
            "EMNLP Paper ID": "2226",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "19061228228e48b7c43b8ac8fd646102ba63e7c3",
            "title": "Are Data Augmentation Methods in Named Entity Recognition Applicable for Uncertainty Estimation?",
            "abstract": "This work investigates the impact of data augmentation on confidence calibration and uncertainty estimation in Named Entity Recognition (NER) tasks. For the future advance of NER in safety-critical fields like healthcare and finance, it is essential to achieve accurate predictions with calibrated confidence when applying Deep Neural Networks (DNNs), including Pre-trained Language Models (PLMs), as a real-world application. However, DNNs are prone to miscalibration, which limits their applicability. Moreover, existing methods for calibration and uncertainty estimation are computational expensive. Our investigation in NER found that data augmentation improves calibration and uncertainty in cross-genre and cross-lingual setting, especially in-domain setting. Furthermore, we showed that the calibration for NER tends to be more effective when the perplexity of the sentences generated by data augmentation is lower, and that increasing the size of the augmentation further improves calibration and uncertainty.",
            "link": "https://www.semanticscholar.org/paper/19061228228e48b7c43b8ac8fd646102ba63e7c3",
            "authors": "Wataru Hashimoto, Hidetaka Kamigaito, Taro Watanabe",
            "EMNLP Paper ID": "2361",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "073d5e2a7da5b8991dc1030ea813f6a94a731777",
            "title": "Self-Consistency Boosts Calibration for Math Reasoning",
            "abstract": "Calibration, which establishes the correlation between accuracy and model confidence, is important for LLM development. We design three off-the-shelf calibration methods based on self-consistency (Wang et al., 2022) for math reasoning tasks. Evaluation on two popular benchmarks (GSM8K and MathQA) using strong open-source LLMs (Mistral and LLaMA2), our methods better bridge model confidence and accuracy than existing methods based on p(True) (Kadavath et al., 2022) or logit (Kadavath et al., 2022).",
            "link": "https://www.semanticscholar.org/paper/073d5e2a7da5b8991dc1030ea813f6a94a731777",
            "authors": "Ante Wang, Linfeng Song, Ye Tian, Baolin Peng, Lifeng Jin, Haitao Mi, Jinsong Su, Dong Yu",
            "matchScore": 221.24702,
            "original title": "Self-Consistency Boosts Calibration for Math Reasoning",
            "original authors": "Ante Wang, Linfeng Song, Ye Tian, Baolin Peng, Lifeng Jin, Haitao Mi, Jinsong Su, Dong Yu",
            "EMNLP Paper ID": "1238",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "bbc8eb04cbfa9f221dcd63d45ffd460b88a0ac01",
            "title": "ConU: Conformal Uncertainty in Large Language Models with Correctness Coverage Guarantees",
            "abstract": "Uncertainty quantification (UQ) in natural language generation (NLG) tasks remains an open challenge, exacerbated by the closed-source nature of the latest large language models (LLMs). This study investigates applying conformal prediction (CP), which can transform any heuristic uncertainty notion into rigorous prediction sets, to black-box LLMs in open-ended NLG tasks. We introduce a novel uncertainty measure based on self-consistency theory, and then develop a conformal uncertainty criterion by integrating the uncertainty condition aligned with correctness into the CP algorithm. Empirical evaluations indicate that our uncertainty measure outperforms prior state-of-the-art methods. Furthermore, we achieve strict control over the correctness coverage rate utilizing 7 popular LLMs on 4 free-form NLG datasets, spanning general-purpose and medical scenarios. Additionally, the calibrated prediction sets with small size further highlights the efficiency of our method in providing trustworthy guarantees for practical open-ended NLG applications.",
            "link": "https://www.semanticscholar.org/paper/bbc8eb04cbfa9f221dcd63d45ffd460b88a0ac01",
            "authors": "Zhiyuan Wang, Jinhao Duan, Lu Cheng, Yue Zhang, Qingni Wang, Hengtao Shen, Xiaofeng Zhu, Xiaoshuang Shi, Kaidi Xu",
            "matchScore": 280.92963,
            "original title": "ConU: Conformal Uncertainty in Large Language Models with Correctness Coverage Guarantees",
            "original authors": "Zhiyuan Wang, Jinhao Duan, Lu Cheng, Yue Zhang, Qingni Wang, Xiaoshuang Shi, Kaidi Xu, Heng Tao Shen, Xiaofeng Zhu",
            "EMNLP Paper ID": "1412",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 14): 13:00\u201314:00 (Afternoon)",
            "Type": "Findings"
        },
        {
            "paperId": "c9b6d0db6c2bebe0bcf593d95bea0e62b2443ef3",
            "title": "The Effect of Sampling Temperature on Problem Solving in Large Language Models",
            "abstract": "In this research study, we empirically investigate the effect of sampling temperature on the performance of Large Language Models (LLMs) on various problem-solving tasks. We created a multiple-choice question-and-answer (MCQA) exam by randomly sampling problems from standard LLM benchmarks. Then, we used nine popular LLMs with five prompt-engineering techniques to solve the MCQA problems while increasing the sampling temperature from 0.0 to 1.6. Despite anecdotal reports to the contrary, our empirical results indicate that changes in temperature from 0.0 to 1.0 do not have a statistically significant impact on LLM performance for problem-solving tasks. In addition, these results appear to generalize across LLMs, prompt-engineering techniques, and problem domains. All code, data, and supplemental materials are available on GitHub at: https://github.com/matthewrenze/jhu-llm-temperature",
            "link": "https://www.semanticscholar.org/paper/c9b6d0db6c2bebe0bcf593d95bea0e62b2443ef3",
            "authors": "Matthew Renze, Erhan Guven",
            "matchScore": 180.53967,
            "original title": "The Effect of Sampling Temperature on Problem Solving in Large Language Models",
            "original authors": "Matthew Renze",
            "EMNLP Paper ID": "1509",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Findings"
        },
        {
            "paperId": "56a4fb8bf5bac348e2efd5f8628d52a409102100",
            "title": "API Is Enough: Conformal Prediction for Large Language Models Without Logit-Access",
            "abstract": "This study aims to address the pervasive challenge of quantifying uncertainty in large language models (LLMs) without logit-access. Conformal Prediction (CP), known for its model-agnostic and distribution-free features, is a desired approach for various LLMs and data distributions. However, existing CP methods for LLMs typically assume access to the logits, which are unavailable for some API-only LLMs. In addition, logits are known to be miscalibrated, potentially leading to degraded CP performance. To tackle these challenges, we introduce a novel CP method that (1) is tailored for API-only LLMs without logit-access; (2) minimizes the size of prediction sets; and (3) ensures a statistical guarantee of the user-defined coverage. The core idea of this approach is to formulate nonconformity measures using both coarse-grained (i.e., sample frequency) and fine-grained uncertainty notions (e.g., semantic similarity). Experimental results on both close-ended and open-ended Question Answering tasks show our approach can mostly outperform the logit-based CP baselines.",
            "link": "https://www.semanticscholar.org/paper/56a4fb8bf5bac348e2efd5f8628d52a409102100",
            "authors": "Jiayuan Su, Jing Luo, Hongwei Wang, Lu Cheng",
            "matchScore": 265.90735,
            "original title": "API Is Enough: Conformal Prediction for Large Language Models Without Logit-Access",
            "original authors": "Jiayuan Su, Jing Luo, Hongwei Wang, Lu Cheng",
            "EMNLP Paper ID": "204",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "4929d63472be98cf5509b24c5986573b48f62770",
            "title": "Generalized Measures of Anticipation and Responsivity in Online Language Processing",
            "abstract": "We introduce a generalization of classic information-theoretic measures of predictive uncertainty in online language processing, based on the simulation of expected continuations of incremental linguistic contexts. Our framework provides a formal definition of anticipatory and responsive measures, and it equips experimenters with the tools to define new, more expressive measures beyond standard next-symbol entropy and surprisal. While extracting these standard quantities from language models is convenient, we demonstrate that using Monte Carlo simulation to estimate alternative responsive and anticipatory measures pays off empirically: New special cases of our generalized formula exhibit enhanced predictive power compared to surprisal for human cloze completion probability as well as ELAN, LAN, and N400 amplitudes, and greater complementarity with surprisal in predicting reading times.",
            "link": "https://www.semanticscholar.org/paper/4929d63472be98cf5509b24c5986573b48f62770",
            "authors": "Mario Giulianelli, Andreas Opedal, Ryan Cotterell",
            "matchScore": 233.0683,
            "original title": "Generalized Measures of Anticipation and Responsivity in Online Language Processing",
            "original authors": "Mario Giulianelli, Andreas Opedal, Ryan Cotterell",
            "EMNLP Paper ID": "2290",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Findings"
        },
        {
            "paperId": "c38738d33c6e74b79f286f0b345e51da71c71375",
            "title": "Predicting generalization performance with correctness discriminators",
            "abstract": "The ability to predict an NLP model's accuracy on unseen, potentially out-of-distribution data is a prerequisite for trustworthiness. We present a novel model that establishes upper and lower bounds on the accuracy, without requiring gold labels for the unseen data. We achieve this by training a discriminator which predicts whether the output of a given sequence-to-sequence model is correct or not. We show across a variety of tagging, parsing, and semantic parsing tasks that the gold accuracy is reliably between the predicted upper and lower bounds, and that these bounds are remarkably close together.",
            "link": "https://www.semanticscholar.org/paper/c38738d33c6e74b79f286f0b345e51da71c71375",
            "authors": "Yuekun Yao, Alexander Koller",
            "matchScore": 189.15137,
            "original title": "Predicting generalization performance with correctness discriminators",
            "original authors": "Yuekun Yao, Alexander Koller",
            "EMNLP Paper ID": "2300",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "14d0489047a1390434e7ea454e7e5165d9721ae3",
            "title": "Calibrating Long-form Generations from Large Language Models",
            "abstract": "To enhance Large Language Models' (LLMs) reliability, calibration is essential -- the model's assessed confidence scores should align with the actual likelihood of its responses being correct. However, current confidence elicitation methods and calibration metrics typically rely on a binary true/false assessment of response correctness. This approach does not apply to long-form generation, where an answer can be partially correct. Addressing this gap, we introduce a unified calibration framework, in which both the correctness of the LLMs' responses and their associated confidence levels are treated as distributions across a range of scores. Within this framework, we develop three metrics to precisely evaluate LLM calibration and further propose two confidence elicitation methods based on self-consistency and self-evaluation. Our experiments, which include long-form QA and summarization tasks, demonstrate that larger models don't necessarily guarantee better calibration, that calibration performance is found to be metric-dependent, and that self-consistency methods excel in factoid datasets. We also find that calibration can be enhanced through techniques such as fine-tuning, integrating relevant source documents, scaling the temperature, and combining self-consistency with self-evaluation. Lastly, we showcase a practical application of our system: selecting and cascading open-source models and ChatGPT to optimize correctness given a limited API budget. This research not only challenges existing notions of LLM calibration but also offers practical methodologies for improving trustworthiness in long-form generation.",
            "link": "https://www.semanticscholar.org/paper/14d0489047a1390434e7ea454e7e5165d9721ae3",
            "authors": "Yukun Huang, Yixin Liu, Raghuveer Thirukovalluru, Arman Cohan, Bhuwan Dhingra",
            "matchScore": 221.78293,
            "original title": "Calibrating Long-form Generations From Large Language Models",
            "original authors": "Yukun Huang, Yixin Liu, Raghuveer Thirukovalluru, Arman Cohan, Bhuwan Dhingra",
            "EMNLP Paper ID": "2612",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "754f9c903754d909cb754364f4d6416ada0ab2b5",
            "title": "Reconfidencing LLMs from the Grouping Loss Perspective",
            "abstract": "Large Language Models (LLMs), including ChatGPT and LLaMA, are susceptible to generating hallucinated answers in a confident tone. While efforts to elicit and calibrate confidence scores have proven useful, recent findings show that controlling uncertainty must go beyond calibration: predicted scores may deviate significantly from the actual posterior probabilities due to the impact of grouping loss. In this work, we construct a new evaluation dataset derived from a knowledge base to assess confidence scores given to answers of Mistral and LLaMA. Experiments show that they tend to be overconfident. Further, we show that they are more overconfident on some answers than others, \\emph{eg} depending on the nationality of the person in the query. In uncertainty-quantification theory, this is grouping loss. To address this, we propose a solution to reconfidence LLMs, canceling not only calibration but also grouping loss. The LLMs, after the reconfidencing process, indicate improved confidence alignment with the accuracy of their responses.",
            "link": "https://www.semanticscholar.org/paper/754f9c903754d909cb754364f4d6416ada0ab2b5",
            "authors": "Lihu Chen, Alexandre Perez-Lebel, Fabian M. Suchanek, G. Varoquaux",
            "matchScore": 222.09618,
            "original title": "Reconfidencing LLMs from the Grouping Loss Perspective",
            "original authors": "Lihu Chen, Alexandre Perez-Lebel, Fabian M. Suchanek, Gael Varoquaux",
            "EMNLP Paper ID": "328",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "8d978805f4f6b8d209b5db592d9f780c69d20196",
            "title": "Enhancing Healthcare LLM Trust with Atypical Presentations Recalibration",
            "abstract": "Black-box large language models (LLMs) are increasingly deployed in various environments, making it essential for these models to effectively convey their confidence and uncertainty, especially in high-stakes settings. However, these models often exhibit overconfidence, leading to potential risks and misjudgments. Existing techniques for eliciting and calibrating LLM confidence have primarily focused on general reasoning datasets, yielding only modest improvements. Accurate calibration is crucial for informed decision-making and preventing adverse outcomes but remains challenging due to the complexity and variability of tasks these models perform. In this work, we investigate the miscalibration behavior of black-box LLMs within the healthcare setting. We propose a novel method, \\textit{Atypical Presentations Recalibration}, which leverages atypical presentations to adjust the model's confidence estimates. Our approach significantly improves calibration, reducing calibration errors by approximately 60\\% on three medical question answering datasets and outperforming existing methods such as vanilla verbalized confidence, CoT verbalized confidence and others. Additionally, we provide an in-depth analysis of the role of atypicality within the recalibration framework.",
            "link": "https://www.semanticscholar.org/paper/8d978805f4f6b8d209b5db592d9f780c69d20196",
            "authors": "Jeremy Qin, Bang Liu, Quoc Dinh Nguyen",
            "matchScore": 264.58652,
            "original title": "Enhancing Healthcare LLM Trust with Atypical Presentations Recalibration",
            "original authors": "Jeremy Qin, Bang Liu, Quoc Dinh Nguyen",
            "EMNLP Paper ID": "526",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        }
    ],
    "Advanced Methods in Event Causality Identification and Analysis": [
        {
            "paperId": "d55e5a902760395c64ccfbef9d49e4d64a8de651",
            "title": "In-context Contrastive Learning for Event Causality Identification",
            "abstract": "Event Causality Identification (ECI) aims at determining the existence of a causal relation between two events. Although recent prompt learning-based approaches have shown promising improvements on the ECI task, their performance are often subject to the delicate design of multiple prompts and the positive correlations between the main task and derivate tasks. The in-context learning paradigm provides explicit guidance for label prediction in the prompt learning paradigm, alleviating its reliance on complex prompts and derivative tasks. However, it does not distinguish between positive and negative demonstrations for analogy learning. Motivated from such considerations, this paper proposes an In-Context Contrastive Learning (ICCL) model that utilizes contrastive learning to enhance the effectiveness of both positive and negative demonstrations. Additionally, we apply contrastive learning to event pairs to better facilitate event causality identification. Our ICCL is evaluated on the widely used corpora, including the EventStoryLine and Causal-TimeBank, and results show significant performance improvements over the state-of-the-art algorithms.",
            "link": "https://www.semanticscholar.org/paper/d55e5a902760395c64ccfbef9d49e4d64a8de651",
            "authors": "Chao Liang, Wei Xiang, Bang Wang",
            "EMNLP Paper ID": "112",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "ad4cf49104fe86d7219838345e09153202a37430",
            "title": "Advancing Event Causality Identification via Heuristic Semantic Dependency Inquiry Network",
            "abstract": "Event Causality Identification (ECI) focuses on extracting causal relations between events in texts. Existing methods for ECI primarily rely on causal features and external knowledge. However, these approaches fall short in two dimensions: (1) causal features between events in a text often lack explicit clues, and (2) external knowledge may introduce bias, while specific problems require tailored analyses. To address these issues, we propose SemDI - a simple and effective Semantic Dependency Inquiry Network for ECI. SemDI captures semantic dependencies within the context using a unified encoder. Then, it utilizes a Cloze Analyzer to generate a fill-in token based on comprehensive context understanding. Finally, this fill-in token is used to inquire about the causal relation between two events. Extensive experiments demonstrate the effectiveness of SemDI, surpassing state-of-the-art methods on three widely used benchmarks. Code is available at https://github.com/hrlics/SemDI.",
            "link": "https://www.semanticscholar.org/paper/ad4cf49104fe86d7219838345e09153202a37430",
            "authors": "Haoran Li, Qiang Gao, Hongmei Wu, Li Huang",
            "EMNLP Paper ID": "174",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "4fb825a8454e6a7f69f987424fd799b65ae1c6a1",
            "title": "Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text",
            "abstract": "Although Large Language Models (LLMs) excel at addressing straightforward reasoning tasks, they frequently struggle with difficulties when confronted by more complex multi-step reasoning due to a range of factors. Firstly, natural language often encompasses complex relationships among entities, making it challenging to maintain a clear reasoning chain over longer spans. Secondly, the abundance of linguistic diversity means that the same entities and relationships can be expressed using different terminologies and structures, complicating the task of identifying and establishing connections between multiple pieces of information. Graphs provide an effective solution to represent data rich in relational information and capture long-term dependencies among entities. To harness the potential of graphs, our paper introduces Structure Guided Prompt, an innovative three-stage task-agnostic prompting framework designed to improve the multi-step reasoning capabilities of LLMs in a zero-shot setting. This framework explicitly converts unstructured text into a graph via LLMs and instructs them to navigate this graph using task-specific strategies to formulate responses. By effectively organizing information and guiding navigation, it enables LLMs to provide more accurate and context-aware responses. Our experiments show that this framework significantly enhances the reasoning capabilities of LLMs, enabling them to excel in a broader spectrum of natural language scenarios.",
            "link": "https://www.semanticscholar.org/paper/4fb825a8454e6a7f69f987424fd799b65ae1c6a1",
            "authors": "Kewei Cheng, Nesreen K. Ahmed, T. Willke, Yizhou Sun",
            "EMNLP Paper ID": "1057",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "6a9132f5882264eab03b46fadb74982e11d2a590",
            "title": "Dependency Graph Parsing as Sequence Labeling",
            "abstract": "Various linearizations have been proposed to cast syntactic dependency parsing as sequence labeling. However, these approaches do not support more complex graph-based representations, such as semantic dependencies or enhanced universal dependencies, as they cannot handle reentrancy or cycles. By extending them, we define a range of unbounded and bounded linearizations that can be used to cast graph parsing as a tagging task, enlarging the toolbox of problems that can be solved under this paradigm. Experimental results on semantic dependency and enhanced UD parsing show that with a good choice of encoding, sequence-labeling dependency graph parsers combine high efficiency with accuracies close to the state of the art, in spite of their simplicity.",
            "link": "https://www.semanticscholar.org/paper/6a9132f5882264eab03b46fadb74982e11d2a590",
            "authors": "Ana Ezquerro, David Vilares, Carlos G'omez-Rodr'iguez",
            "EMNLP Paper ID": "1372",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "a9810b3aa237c61ebf84eefdfbd48c28d658c89b",
            "title": "Explicit, Implicit, and Scattered: Revisiting Event Extraction to Capture Complex Arguments",
            "abstract": "Prior works formulate the extraction of event-specific arguments as a span extraction problem, where event arguments are explicit -- i.e. assumed to be contiguous spans of text in a document. In this study, we revisit this definition of Event Extraction (EE) by introducing two key argument types that cannot be modeled by existing EE frameworks. First, implicit arguments are event arguments which are not explicitly mentioned in the text, but can be inferred through context. Second, scattered arguments are event arguments that are composed of information scattered throughout the text. These two argument types are crucial to elicit the full breadth of information required for proper event modeling. To support the extraction of explicit, implicit, and scattered arguments, we develop a novel dataset, DiscourseEE, which includes 7,464 argument annotations from online health discourse. Notably, 51.2% of the arguments are implicit, and 17.4% are scattered, making DiscourseEE a unique corpus for complex event extraction. Additionally, we formulate argument extraction as a text generation problem to facilitate the extraction of complex argument types. We provide a comprehensive evaluation of state-of-the-art models and highlight critical open challenges in generative event extraction. Our data and codebase are available at https://omar-sharif03.github.io/DiscourseEE.",
            "link": "https://www.semanticscholar.org/paper/a9810b3aa237c61ebf84eefdfbd48c28d658c89b",
            "authors": "Omar Sharif, Joseph Gatto, Madhusudan Basak, S. Preum",
            "EMNLP Paper ID": "1407",
            "Oral/Poster": "Oral",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "6b41e39d73e3724152d8d861e3a90c62214b5397",
            "title": "Weak Reward Model Transforms Generative Models into Robust Causal Event Extraction Systems",
            "abstract": "The inherent ambiguity of cause and effect boundaries poses a challenge in evaluating causal event extraction tasks. Traditional metrics like Exact Match and BertScore poorly reflect model performance, so we trained evaluation models to approximate human evaluation, achieving high agreement. We used them to perform Reinforcement Learning with extraction models to align them with human preference, prioritising semantic understanding. We successfully explored our approach through multiple datasets, including transferring an evaluator trained on one dataset to another as a way to decrease the reliance on human-annotated data. In that vein, we also propose a weak-to-strong supervision method that uses a fraction of the annotated data to train an evaluation model while still achieving high performance in training an RL model. Our code is available at https://github.com/oyarsa/event_extraction/tree/causal-event-extraction.",
            "link": "https://www.semanticscholar.org/paper/6b41e39d73e3724152d8d861e3a90c62214b5397",
            "authors": "Italo Luis da Silva, Hanqi Yan, Lin Gui, Yulan He",
            "EMNLP Paper ID": "1694",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "4a7550c486ba19ddcb23c2df38ceaaba8d69cacc",
            "title": "Bridging Local Details and Global Context in Text-Attributed Graphs",
            "abstract": "Representation learning on text-attributed graphs (TAGs) is vital for real-world applications, as they combine semantic textual and contextual structural information. Research in this field generally consist of two main perspectives: local-level encoding and global-level aggregating, respectively refer to textual node information unification (e.g., using Language Models) and structure-augmented modeling (e.g., using Graph Neural Networks). Most existing works focus on combining different information levels but overlook the interconnections, i.e., the contextual textual information among nodes, which provides semantic insights to bridge local and global levels. In this paper, we propose GraphBridge, a multi-granularity integration framework that bridges local and global perspectives by leveraging contextual textual information, enhancing fine-grained understanding of TAGs. Besides, to tackle scalability and efficiency challenges, we introduce a graphaware token reduction module. Extensive experiments across various models and datasets show that our method achieves state-of-theart performance, while our graph-aware token reduction module significantly enhances efficiency and solves scalability issues.",
            "link": "https://www.semanticscholar.org/paper/4a7550c486ba19ddcb23c2df38ceaaba8d69cacc",
            "authors": "Yaoke Wang, Yun Zhu, Wenqiao Zhang, Yueting Zhuang, Yunfei Li, Siliang Tang",
            "EMNLP Paper ID": "1707",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "27c9ffc1eba118e6f9df07bb5a83ef1806421a9c",
            "title": "Unleashing the Power of Emojis in Texts via Self-supervised Graph Pre-Training",
            "abstract": "Emojis have gained immense popularity on social platforms, serving as a common means to supplement or replace text. However, existing data mining approaches generally either completely ignore or simply treat emojis as ordinary Unicode characters, which may limit the model's ability to grasp the rich semantic information in emojis and the interaction between emojis and texts. Thus, it is necessary to release the emoji's power in social media data mining. To this end, we first construct a heterogeneous graph consisting of three types of nodes, i.e. post, word and emoji nodes to improve the representation of different elements in posts. The edges are also well-defined to model how these three elements interact with each other. To facilitate the sharing of information among post, word and emoji nodes, we propose a graph pre-train framework for text and emoji co-modeling, which contains two graph pre-training tasks: node-level graph contrastive learning and edge-level link reconstruction learning. Extensive experiments on the Xiaohongshu and Twitter datasets with two types of downstream tasks demonstrate that our approach proves significant improvement over previous strong baseline methods.",
            "link": "https://www.semanticscholar.org/paper/27c9ffc1eba118e6f9df07bb5a83ef1806421a9c",
            "authors": "Zhou Zhang, Dongzeng Tan, Jiaan Wang, Yilong Chen, Jiarong Xu",
            "EMNLP Paper ID": "2157",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "1e463b37c55df14c0d6c406e174439df7f19060a",
            "title": "Scope-enhanced Compositional Semantic Parsing for DRT",
            "abstract": "Discourse Representation Theory (DRT) distinguishes itself from other semantic representation frameworks by its ability to model complex semantic and discourse phenomena through structural nesting and variable binding. While seq2seq models hold the state of the art on DRT parsing, their accuracy degrades with the complexity of the sentence, and they sometimes struggle to produce well-formed DRT representations. We introduce the AMS parser, a compositional, neurosymbolic semantic parser for DRT. It rests on a novel mechanism for predicting quantifier scope. We show that the AMS parser reliably produces well-formed outputs and performs well on DRT parsing, especially on complex sentences.",
            "link": "https://www.semanticscholar.org/paper/1e463b37c55df14c0d6c406e174439df7f19060a",
            "authors": "Xiulin Yang, Jonas Groschwitz, Alexander Koller, Johan Bos",
            "EMNLP Paper ID": "2533",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "c11cd989fe4b3890875efd2a5ffa3b52e6707c03",
            "title": "LLaMIPa: An Incremental Discourse Parser",
            "abstract": "This paper provides the first discourse parsing experiments with a large language model(LLM) finetuned on corpora annotated in the style of SDRT (Segmented Discourse Representation Theory Asher, 1993; Asher and Lascarides, 2003). The result is a discourse parser, Llamipa (Llama Incremental Parser), that leverages discourse context, leading to substantial performance gains over approaches that use encoder-only models to provide local, context-sensitive representations of discourse units. Furthermore, it can process discourse data incrementally, which is essential for the eventual use of discourse information in downstream tasks.",
            "link": "https://www.semanticscholar.org/paper/c11cd989fe4b3890875efd2a5ffa3b52e6707c03",
            "authors": "Kate Thompson, Akshay Chaturvedi, Julie Hunter, Nicholas Asher",
            "matchScore": 188.4382,
            "original title": "LLaMIPa: An Incremental Discourse Parser",
            "original authors": "Kate Thompson, Akshay Chaturvedi, Julie Hunter, Nicholas Asher",
            "EMNLP Paper ID": "1310",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "c52112f53d261f22020f8ab735abf2fd78fd8892",
            "title": "What Would Happen Next? Predicting Consequences from An Event Causality Graph",
            "abstract": "Existing script event prediction task forcasts the subsequent event based on an event script chain. However, the evolution of historical events are more complicated in real world scenarios and the limited information provided by the event script chain also make it difficult to accurately predict subsequent events. This paper introduces a Causality Graph Event Prediction(CGEP) task that forecasting consequential event based on an Event Causality Graph (ECG). We propose a Semantic Enhanced Distance-sensitive Graph Prompt Learning (SeDGPL) Model for the CGEP task. In SeDGPL, (1) we design a Distance-sensitive Graph Linearization (DsGL) module to reformulate the ECG into a graph prompt template as the input of a PLM; (2) propose an Event-Enriched Causality Encoding (EeCE) module to integrate both event contextual semantic and graph schema information; (3) propose a Semantic Contrast Event Prediction (ScEP) module to enhance the event representation among numerous candidate events and predict consequential event following prompt learning paradigm. %We construct two CGEP datasets based on existing MAVEN-ERE and ESC corpus for experiments. Experiment results validate our argument our proposed SeDGPL model outperforms the advanced competitors for the CGEP task.",
            "link": "https://www.semanticscholar.org/paper/c52112f53d261f22020f8ab735abf2fd78fd8892",
            "authors": "Chuanhong Zhan, Wei Xiang, Chao Liang, Bang Wang",
            "matchScore": 315.31927,
            "original title": "What Would Happen Next? Predicting Consequences from An Event Causality Graph",
            "original authors": "Chuanhong Zhan, Wei Xiang, \u6881\u8d85, Bang Wang",
            "EMNLP Paper ID": "161",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "d7415efc4552a1bbf5fd31837230c494a183f2fc",
            "title": "PE: A Poincare Explanation Method for Fast Text Hierarchy Generation",
            "abstract": "The black-box nature of deep learning models in NLP hinders their widespread application. The research focus has shifted to Hierarchical Attribution (HA) for its ability to model feature interactions. Recent works model non-contiguous combinations with a time-costly greedy search in Eculidean spaces, neglecting underlying linguistic information in feature representations. In this work, we introduce a novel method, namely Poincare Explanation (PE), for modeling feature interactions with hyperbolic spaces in a time efficient manner. Specifically, we take building text hierarchies as finding spanning trees in hyperbolic spaces. First we project the embeddings into hyperbolic spaces to elicit inherit semantic and syntax hierarchical structures. Then we propose a simple yet effective strategy to calculate Shapley score. Finally we build the the hierarchy with proving the constructing process in the projected space could be viewed as building a minimum spanning tree and introduce a time efficient building algorithm. Experimental results demonstrate the effectiveness of our approach.",
            "link": "https://www.semanticscholar.org/paper/d7415efc4552a1bbf5fd31837230c494a183f2fc",
            "authors": "Qian Chen, Xiaofeng He, Hongzhao Li, Hongyu Yi",
            "matchScore": 274.4816,
            "original title": "PE: A Poincare Explanation Method for Fast Text Hierarchy Generation",
            "original authors": "Qian Chen, Dongyang Li, Xiaofeng He, Hongzhao Li, Hongyu Yi",
            "EMNLP Paper ID": "1650",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "7710fe5287a50ce6b0f4f12ed617f9b5f057738e",
            "title": "Causal Discovery Inspired Unsupervised Domain Adaptation for Emotion-Cause Pair Extraction",
            "abstract": "This paper tackles the task of emotion-cause pair extraction in the unsupervised domain adaptation setting. The problem is challenging as the distributions of the events causing emotions in target domains are dramatically different than those in source domains, despite the distributions of emotional expressions between domains are overlapped. Inspired by causal discovery, we propose a novel deep latent model in the variational autoencoder (VAE) framework, which not only captures the underlying latent structures of data but also utilizes the easily transferable knowledge of emotions as the bridge to link the distributions of events in different domains. To facilitate knowledge transfer across domains, we also propose a novel variational posterior regularization technique to disentangle the latent representations of emotions from those of events in order to mitigate the damage caused by the spurious correlations related to the events in source domains. Through extensive experiments, we demonstrate that our model outperforms the strongest baseline by approximately 11.05% on a Chinese benchmark and 2.45% on a English benchmark in terms of weighted-average F1 score. The source code will be publicly available upon acceptance.",
            "link": "https://www.semanticscholar.org/paper/7710fe5287a50ce6b0f4f12ed617f9b5f057738e",
            "authors": "Yuncheng Hua, Yujin Huang, Shuo Huang, Tao Feng, Lizhen Qu, Chris Bain, R. Bassed, Gholamreza Haffari",
            "matchScore": 312.69345,
            "original title": "Causal Discovery Inspired Unsupervised Domain Adaptation for Emotion-Cause Pair Extraction",
            "original authors": "YUNCHENG HUA, Yujin Huang, Shuo Huang, Tao Feng, Lizhen Qu, Christopher Bain, Richard Bassed, Reza Haf",
            "EMNLP Paper ID": "1704",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "b5c0c18ec3b6199f0f7a6d6b4756d9c0b2788492",
            "title": "HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs",
            "abstract": "Hypergraphs are characterized by complex topological structure, representing higher-order interactions among multiple entities through hyperedges. Lately, hypergraph-based deep learning methods to learn informative data representations for the problem of node classification on text-attributed hypergraphs have garnered increasing research attention. However, existing methods struggle to simultaneously capture the full extent of hypergraph structural information and the rich linguistic attributes inherent in the nodes attributes, which largely hampers their effectiveness and generalizability. To overcome these challenges, we explore ways to further augment a pretrained BERT model with specialized hypergraph-aware layers for the task of node classification. Such layers introduce higher-order structural inductive bias into the language model, thus improving the model's capacity to harness both higher-order context information from the hypergraph structure and semantic information present in text. In this paper, we propose a new architecture, HyperBERT, a mixed text-hypergraph model which simultaneously models hypergraph relational structure while maintaining the high-quality text encoding capabilities of a pre-trained BERT. Notably, HyperBERT presents results that achieve a new state-of-the-art on five challenging text-attributed hypergraph node classification benchmarks.",
            "link": "https://www.semanticscholar.org/paper/b5c0c18ec3b6199f0f7a6d6b4756d9c0b2788492",
            "authors": "A. Bazaga, Pietro Lio, G. Micklem",
            "matchScore": 328.57806,
            "original title": "HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs",
            "original authors": "Adri\u00e1n Bazaga, Pietro Lio, Gos Micklem",
            "EMNLP Paper ID": "1919",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 14): 13:00\u201314:00 (Afternoon)",
            "Type": "Findings"
        },
        {
            "paperId": "dbfb49de5ae1b351991d6c55e8179ff4640ed60e",
            "title": "GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models",
            "abstract": "Long-context capabilities are essential for large language models (LLMs) to tackle complex and long-input tasks. Despite numerous efforts made to optimize LLMs for long contexts, challenges persist in robustly processing long inputs. In this paper, we introduce GraphReader, a graph-based agent system designed to handle long texts by structuring them into a graph and employing an agent to explore this graph autonomously. Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan. It then invokes a set of predefined functions to read node content and neighbors, facilitating a coarse-to-fine exploration of the graph. Throughout the exploration, the agent continuously records new insights and reflects on current circumstances to optimize the process until it has gathered sufficient information to generate an answer. Experimental results on the LV-Eval dataset reveal that GraphReader, using a 4k context window, consistently outperforms GPT-4-128k across context lengths from 16k to 256k by a large margin. Additionally, our approach demonstrates superior performance on four challenging single-hop and multi-hop benchmarks.",
            "link": "https://www.semanticscholar.org/paper/dbfb49de5ae1b351991d6c55e8179ff4640ed60e",
            "authors": "Shilong Li, Yancheng He, Hangyu Guo, Xingyuan Bu, Ge Bai, Jie Liu, Jiaheng Liu, Xingwei Qu, Yangguang Li, Wanli Ouyang, Wenbo Su, Bo Zheng",
            "matchScore": 296.9182,
            "original title": "GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models",
            "original authors": "Shilong Li, Yancheng He, Hangyu Guo, Xingyuan Bu, Ge Bai, Jie Liu, Jiaheng Liu, Xingwei Qu, Yangguang Li, Wanli Ouyang, Wenbo Su, Bo Zheng",
            "EMNLP Paper ID": "2496",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "a318c186b325d9aac3f611668da764c364ee8e31",
            "title": "Let's Ask GNN: Empowering Large Language Model for Graph In-Context Learning",
            "abstract": "Textual Attributed Graphs (TAGs) are crucial for modeling complex real-world systems, yet leveraging large language models (LLMs) for TAGs presents unique challenges due to the gap between sequential text processing and graph-structured data. We introduce AskGNN, a novel approach that bridges this gap by leveraging In-Context Learning (ICL) to integrate graph data and task-specific information into LLMs. AskGNN employs a Graph Neural Network (GNN)-powered structure-enhanced retriever to select labeled nodes across graphs, incorporating complex graph structures and their supervision signals. Our learning-to-retrieve algorithm optimizes the retriever to select example nodes that maximize LLM performance on graph. Experiments across three tasks and seven LLMs demonstrate AskGNN's superior effectiveness in graph task performance, opening new avenues for applying LLMs to graph-structured data without extensive fine-tuning.",
            "link": "https://www.semanticscholar.org/paper/a318c186b325d9aac3f611668da764c364ee8e31",
            "authors": "Zhengyu Hu, Yichuan Li, Zhengyu Chen, Jingang Wang, Han Liu, Kyumin Lee, Kaize Ding",
            "matchScore": 260.9662,
            "original title": "Let\u2019s Ask GNN: Empowering Large Language Model for Graph In-Context Learning",
            "original authors": "Yichuan Li, Zhengyu Hu, Zhengyu Chen, Jingang Wang, Han Liu, Kyumin Lee, Kaize Ding",
            "EMNLP Paper ID": "278",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "104bdcf7d8e92d54484b77e979125aab2c48bda8",
            "title": "Presentations are not always linear! GNN meets LLM for Document-to-Presentation Transformation with Attribution",
            "abstract": "Automatically generating a presentation from the text of a long document is a challenging and useful problem. In contrast to a flat summary, a presentation needs to have a better and non-linear narrative, i.e., the content of a slide can come from different and non-contiguous parts of the given document. However, it is difficult to incorporate such non-linear mapping of content to slides and ensure that the content is faithful to the document. LLMs are prone to hallucination and their performance degrades with the length of the input document. Towards this, we propose a novel graph based solution where we learn a graph from the input document and use a combination of graph neural network and LLM to generate a presentation with attribution of content for each slide. We conduct thorough experiments to show the merit of our approach compared to directly using LLMs for this task.",
            "link": "https://www.semanticscholar.org/paper/104bdcf7d8e92d54484b77e979125aab2c48bda8",
            "authors": "Himanshu Maheshwari, Sambaran Bandyopadhyay, Aparna Garimella, Anandhavelu Natarajan",
            "matchScore": 333.79205,
            "original title": "Presentations are not always linear! GNN meets LLM for Document-to-Presentation Transformation with Attribution",
            "original authors": "Himanshu Maheshwari, Sambaran Bandyopadhyay, Aparna Garimella, Anandhavelu Natarajan",
            "EMNLP Paper ID": "3058",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "5c394a324fd9303490d8bbb40e4d65f00ba3f7b6",
            "title": "InsertGNN: A Hierarchical Graph Neural Network for the TOEFL Sentence Insertion Problem",
            "abstract": "Sentence insertion is an interesting NLP problem but received insuf\ufb01cient attention. Existing approaches in sentence ordering, text coherence, and question answering are neither suitable nor good enough at solving it. To bridge this gap, we propose InsertGNN, a simple yet effective model that represents the problem as a graph and adopts a hierarchical graph neural network (GNN) to learn the connection between sentences. We evaluate our method in our newly collected TOEFL dataset and further verify its effectiveness on the larger arXiv dataset using cross-domain learning. Extensive experiments demonstrate that InsertGNN outperforms all baselines by a large margin with an accuracy of 70%, rivaling the average human test scores.",
            "link": "https://www.semanticscholar.org/paper/5c394a324fd9303490d8bbb40e4d65f00ba3f7b6",
            "authors": "Fang Wu, Stan Z. Li",
            "matchScore": 304.7567,
            "original title": "InsertGNN: A Hierarchical Graph Neural Network for the TOEFL Sentence Insertion Problem",
            "original authors": "Fang Wu, Stan Z. Li",
            "EMNLP Paper ID": "31",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "c568decc196679e98826dca827bd0814ba59037d",
            "title": "Document-level Causal Relation Extraction with Knowledge-guided Binary Question Answering",
            "abstract": "As an essential task in information extraction (IE), Event-Event Causal Relation Extraction (ECRE) aims to identify and classify the causal relationships between event mentions in natural language texts. However, existing research on ECRE has highlighted two critical challenges, including the lack of document-level modeling and causal hallucinations. In this paper, we propose a Knowledge-guided binary Question Answering (KnowQA) method with event structures for ECRE, consisting of two stages: Event Structure Construction and Binary Question Answering. We conduct extensive experiments under both zero-shot and fine-tuning settings with large language models (LLMs) on the MECI and MAVEN-ERE datasets. Experimental results demonstrate the usefulness of event structures on document-level ECRE and the effectiveness of KnowQA by achieving state-of-the-art on the MECI dataset. We observe not only the effectiveness but also the high generalizability and low inconsistency of our method, particularly when with complete event structures after fine-tuning the models.",
            "link": "https://www.semanticscholar.org/paper/c568decc196679e98826dca827bd0814ba59037d",
            "authors": "Zimu Wang, Lei Xia, Wei Wang, Xinya Du",
            "matchScore": 304.21646,
            "original title": "Document-level Causal Relation Extraction with Knowledge-guided Binary Question Answering",
            "original authors": "Zimu Wang, Lei Xia, Wei Wang, Xinya Du",
            "EMNLP Paper ID": "3241",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "914a5521aa1ccd1813695e651c31a64be59b263a",
            "title": "Are LLMs Good Annotators for Discourse-level Event Relation Extraction?",
            "abstract": "Large Language Models (LLMs) have demonstrated proficiency in a wide array of natural language processing tasks. However, its effectiveness over discourse-level event relation extraction (ERE) tasks remains unexplored. In this paper, we assess the effectiveness of LLMs in addressing discourse-level ERE tasks characterized by lengthy documents and intricate relations encompassing coreference, temporal, causal, and subevent types. Evaluation is conducted using an commercial model, GPT-3.5, and an open-source model, LLaMA-2. Our study reveals a notable underperformance of LLMs compared to the baseline established through supervised learning. Although Supervised Fine-Tuning (SFT) can improve LLMs performance, it does not scale well compared to the smaller supervised baseline model. Our quantitative and qualitative analysis shows that LLMs have several weaknesses when applied for extracting event relations, including a tendency to fabricate event mentions, and failures to capture transitivity rules among relations, detect long distance relations, or comprehend contexts with dense event mentions.",
            "link": "https://www.semanticscholar.org/paper/914a5521aa1ccd1813695e651c31a64be59b263a",
            "authors": "Kangda Wei, Aayush Gautam, Ruihong Huang",
            "matchScore": 286.29752,
            "original title": "Are LLMs Good Annotators for Discourse-level Event Relation Extraction?",
            "original authors": "Kangda Wei, Aayush Gautam, Ruihong Huang",
            "EMNLP Paper ID": "8",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "9bba53be0a1694574b28c0e337062d5b1e033334",
            "title": "Graph-tree Fusion Model with Bidirectional Information Propagation for Long Document Classification",
            "abstract": "Long document classification presents challenges in capturing both local and global dependencies due to their extensive content and complex structure. Existing methods often struggle with token limits and fail to adequately model hierarchical relationships within documents. To address these constraints, we propose a novel model leveraging a graph-tree structure. Our approach integrates syntax trees for sentence encodings and document graphs for document encodings, which capture fine-grained syntactic relationships and broader document contexts, respectively. We use Tree Transformers to generate sentence encodings, while a graph attention network models inter- and intra-sentence dependencies. During training, we implement bidirectional information propagation from word-to-sentence-to-document and vice versa, which enriches the contextual representation. Our proposed method enables a comprehensive understanding of content at all hierarchical levels and effectively handles arbitrarily long contexts without token limit constraints. Experimental results demonstrate the effectiveness of our approach in all types of long document classification tasks.",
            "link": "https://www.semanticscholar.org/paper/9bba53be0a1694574b28c0e337062d5b1e033334",
            "authors": "Sudipta Singha Roy, Xindi Wang, Robert E. Mercer, Frank Rudzicz",
            "matchScore": 264.07285,
            "original title": "Graph-tree Fusion Model with Bidirectional Information Propagation for Long Document Classification",
            "original authors": "Sudipta Singha Roy, Xindi Wang, Robert Mercer, Frank Rudzicz",
            "EMNLP Paper ID": "891",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        }
    ],
    "LLM-based Multi-Agent Systems and Collaborations": [
        {
            "paperId": "ff406e2ab8fdcce6b051cad1ead794c928440f77",
            "title": "LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay",
            "abstract": "This paper explores the open research problem of understanding the social behaviors of LLM-based agents. Using Avalon as a testbed, we employ system prompts to guide LLM agents in gameplay. While previous studies have touched on gameplay with LLM agents, research on their social behaviors is lacking. We propose a novel framework, tailored for Avalon, features a multi-agent system facilitating efficient communication and interaction. We evaluate its performance based on game success and analyze LLM agents' social behaviors. Results affirm the framework's effectiveness in creating adaptive agents and suggest LLM-based agents' potential in navigating dynamic social interactions. By examining collaboration and confrontation behaviors, we offer insights into this field's research and applications. Our code is publicly available at https://github.com/3DAgentWorld/LLM-Game-Agent.",
            "link": "https://www.semanticscholar.org/paper/ff406e2ab8fdcce6b051cad1ead794c928440f77",
            "authors": "Yihuai Lan, Zhiqiang Hu, Lei Wang, Yang Wang, De-Yong Ye, Peilin Zhao, Ee-Peng Lim, Hui Xiong, Hao Wang",
            "EMNLP Paper ID": "9",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "9348b7b95982d0a675a767e92c23647aa6915a94",
            "title": "AgentReview: Exploring Peer Review Dynamics with LLM Agents",
            "abstract": "Peer review is fundamental to the integrity and advancement of scientific publication. Traditional methods of peer review analyses often rely on exploration and statistics of existing peer review data, which do not adequately address the multivariate nature of the process, account for the latent variables, and are further constrained by privacy concerns due to the sensitive nature of the data. We introduce AgentReview, the first large language model (LLM) based peer review simulation framework, which effectively disentangles the impacts of multiple latent factors and addresses the privacy issue. Our study reveals significant insights, including a notable 37.1% variation in paper decisions due to reviewers' biases, supported by sociological theories such as the social influence theory, altruism fatigue, and authority bias. We believe that this study could offer valuable insights to improve the design of peer review mechanisms. Our code is available at https://github.com/Ahren09/AgentReview.",
            "link": "https://www.semanticscholar.org/paper/9348b7b95982d0a675a767e92c23647aa6915a94",
            "authors": "Yiqiao Jin, Qinlin Zhao, Yiyang Wang, Hao Chen, Kaijie Zhu, Yijia Xiao, Jindong Wang",
            "EMNLP Paper ID": "145",
            "Oral/Poster": "Oral",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "72273f7a050529fc71c7d45c0256d2b9754f56bb",
            "title": "MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration",
            "abstract": "Large Language Models (LLMs) have marked a significant advancement in the field of natural language processing, demonstrating exceptional capabilities in reasoning, tool usage, and memory. As their applications extend into multi-agent environments, a need has arisen for a comprehensive evaluation framework that captures their abilities in reasoning, planning, collaboration, and more. This work introduces a novel benchmarking framework specifically tailored to assess LLMs within multi-agent settings, providing quantitative metrics to evaluate their judgment, reasoning, deception, self-awareness, cooperation, coordination, and rationality. We utilize games such as Chameleon and Undercover, alongside game theory scenarios like Cost Sharing, Multi-player Prisoner's Dilemma, and Public Good, to create diverse testing environments. Our framework is fortified with the Probabilistic Graphical Modeling (PGM) method, enhancing the LLMs' capabilities in navigating complex social and cognitive dimensions. The benchmark evaluates seven multi-agent systems powered by different LLMs, quantitatively highlighting a significant capability gap over threefold between the strongest, GPT-4, and the weakest, Llama-2-70B. It also confirms that our PGM enhancement boosts the inherent abilities of all selected models by 50% on average. Our codes are released here https://github.com/cathyxl/MAgIC.",
            "link": "https://www.semanticscholar.org/paper/72273f7a050529fc71c7d45c0256d2b9754f56bb",
            "authors": "Lin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren, Zhen Dong, Kurt Keutzer, See-Kiong Ng, Jiashi Feng",
            "EMNLP Paper ID": "828",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "2284fce145226b295b58157beca8edde6302018d",
            "title": "AdaSwitch: Adaptive Switching between Small and Large Agents for Effective Cloud-Local Collaborative Learning",
            "abstract": "Recent advancements in large language models (LLMs) have been remarkable. Users face a choice between using cloud-based LLMs for generation quality and deploying local-based LLMs for lower computational cost. The former option is typically costly and inefficient, while the latter usually fails to deliver satisfactory performance for reasoning steps requiring deliberate thought processes. In this work, we propose a novel LLM utilization paradigm that facilitates the collaborative operation of large cloud-based LLMs and smaller local-deployed LLMs. Our framework comprises two primary modules: the local agent instantiated with a relatively smaller LLM, handling less complex reasoning steps, and the cloud agent equipped with a larger LLM, managing more intricate reasoning steps. This collaborative processing is enabled through an adaptive mechanism where the local agent introspectively identifies errors and proactively seeks assistance from the cloud agent, thereby effectively integrating the strengths of both locally-deployed and cloud-based LLMs, resulting in significant enhancements in task completion performance and efficiency. We evaluate AdaSwitch across 7 benchmarks, ranging from mathematical reasoning and complex question answering, using various types of LLMs to instantiate the local and cloud agents. The empirical results show that AdaSwitch effectively improves the performance of the local agent, and sometimes achieves competitive results compared to the cloud agent while utilizing much less computational overhead.",
            "link": "https://www.semanticscholar.org/paper/2284fce145226b295b58157beca8edde6302018d",
            "authors": "Hao Sun, Jiayi Wu, Hengyi Cai, Xiaochi Wei, Yue Feng, Bo Wang, Shuaiqiang Wang, Yan Zhang, Dawei Yin",
            "EMNLP Paper ID": "917",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "5d3471ba45ce5585c7bc494c1b70e5c1f041b013",
            "title": "CodeAgent: Autonomous Communicative Agents for Code Review",
            "abstract": "Code review, which aims at ensuring the overall quality and reliability of software, is a cornerstone of software development. Unfortunately, while crucial, Code review is a labor-intensive process that the research community is looking to automate. Existing automated methods rely on single input-output generative models and thus generally struggle to emulate the collaborative nature of code review. This work introduces \\tool{}, a novel multi-agent Large Language Model (LLM) system for code review automation. CodeAgent incorporates a supervisory agent, QA-Checker, to ensure that all the agents' contributions address the initial review question. We evaluated CodeAgent on critical code review tasks: (1) detect inconsistencies between code changes and commit messages, (2) identify vulnerability introductions, (3) validate code style adherence, and (4) suggest code revision. The results demonstrate CodeAgent's effectiveness, contributing to a new state-of-the-art in code review automation. Our data and code are publicly available (\\url{https://github.com/Code4Agent/codeagent}).",
            "link": "https://www.semanticscholar.org/paper/5d3471ba45ce5585c7bc494c1b70e5c1f041b013",
            "authors": "Xunzhu Tang, Kisub Kim, Yewei Song, Cedric Lothritz, Bei Li, Saad Ezzini, Haoye Tian, Jacques Klein, T\u00e9gawend\u00e9 F. Bissyand\u00e9",
            "EMNLP Paper ID": "1318",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "6e0f888dbe861f775ce5d4fca2f30ddf90d93ed8",
            "title": "Red Teaming Language Models for Processing Contradictory Dialogues",
            "abstract": "Most language models currently available are prone to self-contradiction during dialogues. To mitigate this issue, this study explores a novel contradictory dialogue processing task that aims to detect and modify contradictory statements in a conversation. This task is inspired by research on context faithfulness and dialogue comprehension, which have demonstrated that the detection and understanding of contradictions often necessitate detailed explanations. We develop a dataset comprising contradictory dialogues, in which one side of the conversation contradicts itself. Each dialogue is accompanied by an explanatory label that highlights the location and details of the contradiction. With this dataset, we present a Red Teaming framework for contradictory dialogue processing. The framework detects and attempts to explain the dialogue, then modifies the existing contradictory content using the explanation. Our experiments demonstrate that the framework improves the ability to detect contradictory dialogues and provides valid explanations. Additionally, it showcases distinct capabilities for modifying such dialogues. Our study highlights the importance of the logical inconsistency problem in conversational AI.",
            "link": "https://www.semanticscholar.org/paper/6e0f888dbe861f775ce5d4fca2f30ddf90d93ed8",
            "authors": "Xiaofei Wen, Bangzheng Li, Tenghao Huang, Muhao Chen",
            "EMNLP Paper ID": "1352",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "385c74957858e7d6856d48e72b5a902b4c1aa28c",
            "title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
            "abstract": "Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of\"tit for tat\"and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of\"tit for tat\"state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents. Code is available at https://github.com/Skytliang/Multi-Agents-Debate.",
            "link": "https://www.semanticscholar.org/paper/385c74957858e7d6856d48e72b5a902b4c1aa28c",
            "authors": "Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi",
            "EMNLP Paper ID": "2170",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "b8abf36d107bc3369d0d7ede97cd35c4715b34d6",
            "title": "Communicating with Speakers and Listeners of Different Pragmatic Levels",
            "abstract": "This paper explores the impact of variable pragmatic competence on communicative success through simulating language learning and conversing between speakers and listeners with different levels of reasoning abilities. Through studying this interaction, we hypothesize that matching levels of reasoning between communication partners would create a more beneficial environment for communicative success and language learning. Our research findings indicate that learning from more explicit, literal language is advantageous, irrespective of the learner's level of pragmatic competence. Furthermore, we find that integrating pragmatic reasoning during language learning, not just during evaluation, significantly enhances overall communication performance. This paper provides key insights into the importance of aligning reasoning levels and incorporating pragmatic reasoning in optimizing communicative interactions.",
            "link": "https://www.semanticscholar.org/paper/b8abf36d107bc3369d0d7ede97cd35c4715b34d6",
            "authors": "Kata Nasz\u00e1di, F. Oliehoek, C. Monz",
            "EMNLP Paper ID": "3014",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "180f51787e4205de6ee4c0ae722529cc5e06c712",
            "title": "Assessing and Verifying Task Utility in LLM-Powered Applications",
            "abstract": "The rapid development of Large Language Models (LLMs) has led to a surge in applications that facilitate collaboration among multiple agents, assisting humans in their daily tasks. However, a significant gap remains in assessing to what extent LLM-powered applications genuinely enhance user experience and task execution efficiency. This highlights the need to verify utility of LLM-powered applications, particularly by ensuring alignment between the application's functionality and end-user needs. We introduce AgentEval, a novel framework designed to simplify the utility verification process by automatically proposing a set of criteria tailored to the unique purpose of any given application. This allows for a comprehensive assessment, quantifying the utility of an application against the suggested criteria. We present a comprehensive analysis of the effectiveness and robustness of AgentEval for two open source datasets including Math Problem solving and ALFWorld House-hold related tasks. For reproducibility purposes, we make the data, code and all the logs publicly available at https://bit.ly/3w3yKcS .",
            "link": "https://www.semanticscholar.org/paper/180f51787e4205de6ee4c0ae722529cc5e06c712",
            "authors": "Negar Arabzadeh, Siging Huo, Nikhil Mehta, Qinqyun Wu, Chi Wang, Ahmed Awadallah, Charles L. A. Clarke, J. Kiseleva",
            "EMNLP Paper ID": "3043",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "11d4478587c4d2ecb195fe911809946928767657",
            "title": "Shall We Team Up: Exploring Spontaneous Cooperation of Competing LLM Agents",
            "abstract": "Large Language Models (LLMs) have increasingly been utilized in social simulations, where they are often guided by carefully crafted instructions to stably exhibit human-like behaviors during simulations. Nevertheless, we doubt the necessity of shaping agents' behaviors for accurate social simulations. Instead, this paper emphasizes the importance of spontaneous phenomena, wherein agents deeply engage in contexts and make adaptive decisions without explicit directions. We explored spontaneous cooperation across three competitive scenarios and successfully simulated the gradual emergence of cooperation, findings that align closely with human behavioral data. This approach not only aids the computational social science community in bridging the gap between simulations and real-world dynamics but also offers the AI community a novel method to assess LLMs' capability of deliberate reasoning.",
            "link": "https://www.semanticscholar.org/paper/11d4478587c4d2ecb195fe911809946928767657",
            "authors": "Zengqing Wu, Run Peng, Shuyuan Zheng, Qianying Liu, Xu Han, Brian Inhyuk Kwon, Makoto Onizuka, Shaojie Tang, Chuan Xiao",
            "matchScore": 331.1396,
            "original title": "Shall We Team Up: Exploring Spontaneous Cooperation of Competing LLM Agents",
            "original authors": "Zengqing Wu, Run Peng, Shuyuan Zheng, Qianying Liu, Xu Han, Brian I. Kwon, Makoto Onizuka, Shaojie Tang, Chuan Xiao",
            "EMNLP Paper ID": "1020",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "885bc1bf9dbe546ae9aaca7000b595688eda8d96",
            "title": "MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate",
            "abstract": "Large Language Models (LLMs) have shown exceptional results on current benchmarks when working individually. The advancement in their capabilities, along with a reduction in parameter size and inference times, has facilitated the use of these models as agents, enabling interactions among multiple models to execute complex tasks. Such collaborations offer several advantages, including the use of specialized models (e.g. coding), improved confidence through multiple computations, and enhanced divergent thinking, leading to more diverse outputs. Thus, the collaborative use of language models is expected to grow significantly in the coming years. In this work, we evaluate the behavior of a network of models collaborating through debate under the influence of an adversary. We introduce pertinent metrics to assess the adversary's effectiveness, focusing on system accuracy and model agreement. Our findings highlight the importance of a model's persuasive ability in influencing others. Additionally, we explore inference-time methods to generate more compelling arguments and evaluate the potential of prompt-based mitigation as a defensive strategy.",
            "link": "https://www.semanticscholar.org/paper/885bc1bf9dbe546ae9aaca7000b595688eda8d96",
            "authors": "Alfonso Amayuelas, Xianjun Yang, Antonis Antoniades, Wenyue Hua, Liangming Pan, W. Wang",
            "matchScore": 308.1272,
            "original title": "MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate",
            "original authors": "Alfonso Amayuelas, Xianjun Yang, Antonis Antoniades, Wenyue Hua, Liangming Pan, William Yang Wang",
            "EMNLP Paper ID": "1418",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "c04e6bab808c0d5f0344159190ed21a20039c4e9",
            "title": "Improving Multi-Agent Debate with Sparse Communication Topology",
            "abstract": "Multi-agent debate has proven effective in improving large language models quality for reasoning and factuality tasks. While various role-playing strategies in multi-agent debates have been explored, in terms of the communication among agents, existing approaches adopt a brute force algorithm -- each agent can communicate with all other agents. In this paper, we systematically investigate the effect of communication connectivity in multi-agent systems. Our experiments on GPT and Mistral models reveal that multi-agent debates leveraging sparse communication topology can achieve comparable or superior performance while significantly reducing computational costs. Furthermore, we extend the multi-agent debate framework to multimodal reasoning and alignment labeling tasks, showcasing its broad applicability and effectiveness. Our findings underscore the importance of communication connectivity on enhancing the efficiency and effectiveness of the\"society of minds\"approach.",
            "link": "https://www.semanticscholar.org/paper/c04e6bab808c0d5f0344159190ed21a20039c4e9",
            "authors": "Yunxuan Li, Yibing Du, Jiageng Zhang, Le Hou, Peter Grabowski, Yeqing Li, Eugene Ie",
            "matchScore": 232.40863,
            "original title": "Improving Multi-Agent Debate with Sparse Communication Topology",
            "original authors": "Yunxuan Li, Yibing Du, Jiageng Zhang, Le Hou, Peter Grabowski, Yeqing Li, Eugene Ie",
            "EMNLP Paper ID": "1483",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "099621bec723c5b3435e9be6a4f70e4c871f3f2f",
            "title": "BASES: Large-scale Web Search User Simulation with Large Language Model based Agents",
            "abstract": "Due to the excellent capacities of large language models (LLMs), it becomes feasible to develop LLM-based agents for reliable user simulation. Considering the scarcity and limit (e.g., privacy issues) of real user data, in this paper, we conduct large-scale user simulation for web search, to improve the analysis and modeling of user search behavior. Specially, we propose BASES, a novel user simulation framework with LLM-based agents, designed to facilitate comprehensive simulations of web search user behaviors. Our simulation framework can generate unique user profiles at scale, which subsequently leads to diverse search behaviors. To demonstrate the effectiveness of BASES, we conduct evaluation experiments based on two human benchmarks in both Chinese and English, demonstrating that BASES can effectively simulate large-scale human-like search behaviors. To further accommodate the research on web search, we develop WARRIORS, a new large-scale dataset encompassing web search user behaviors, including both Chinese and English versions, which can greatly bolster research in the field of information retrieval. Our code and data will be publicly released soon.",
            "link": "https://www.semanticscholar.org/paper/099621bec723c5b3435e9be6a4f70e4c871f3f2f",
            "authors": "Ruiyang Ren, Peng Qiu, Yingqi Qu, Jing Liu, Wayne Xin Zhao, Huaqin Wu, Ji-Rong Wen, Haifeng Wang",
            "matchScore": 260.76547,
            "original title": "BASES: Large-scale Web Search User Simulation with Large Language Model based Agents",
            "original authors": "Ruiyang Ren, Peng Qiu, Yingqi Qu, Jing Liu, Xin Zhao, Hua Wu, Ji-Rong Wen, Haifeng Wang",
            "EMNLP Paper ID": "185",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "da2880c1b525ae49125b9b2c627126d6891aab5a",
            "title": "TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution",
            "abstract": "The emergence of LLM-based agents has garnered considerable attention, yet their trustwor-thiness remains an under-explored area. As agents can directly interact with the physical environment, their reliability and safety is critical. This paper presents an Agent-Constitution -based agent framework, TrustAgent , an initial investigation into improving the safety dimension of trust-worthiness in LLM-based agents. This framework consists of threefold strategies: pre-planning strategy which injects safety knowledge to the model prior to plan generation, in-planning strategy which bolsters safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Through experimental analysis, we demonstrate how these approaches can effectively elevate an LLM agent\u2019s safety by identifying and preventing potential dangers. Furthermore, we explore the intricate relationships between safety and helpfulness, and between the model\u2019s reasoning ability and its efficacy as a safe agent. This paper underscores the imperative of integrating safety awareness and trustworthiness into the design and deployment of LLM-based agents, not only to enhance their performance but also to ensure their responsible integration into human-centric environments. Data and code are available at https://github. com/agiresearch/TrustAgent .",
            "link": "https://www.semanticscholar.org/paper/da2880c1b525ae49125b9b2c627126d6891aab5a",
            "authors": "Wenyue Hua, Xianjun Yang, Zelong Li, Cheng Wei, Yongfeng Zhang",
            "matchScore": 341.2483,
            "original title": "TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution",
            "original authors": "Wenyue Hua, Xianjun Yang, Mingyu Jin, Zelong Li, Wei Cheng, Ruixiang Tang, Yongfeng Zhang",
            "EMNLP Paper ID": "2063",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "d0973f15e23e41caa665bf5cec6164de4ddf2e7a",
            "title": "Communicate to Play: Pragmatic Reasoning for Efficient Cross-Cultural Communication in Codenames",
            "abstract": "Cultural differences in common ground may result in pragmatic failure and misunderstandings during communication. We develop our method Rational Speech Acts for Cross-Cultural Communication (RSA+C3) to resolve cross-cultural differences in common ground. To measure the success of our method, we study RSA+C3 in the collaborative referential game of Codenames Duet and show that our method successfully improves collaboration between simulated players of different cultures. Our contributions are threefold: (1) creating Codenames players using contrastive learning of an embedding space and LLM prompting that are aligned with human patterns of play, (2) studying culturally induced differences in common ground reflected in our trained models, and (3) demonstrating that our method RSA+C3 can ease cross-cultural communication in gameplay by inferring sociocultural context from interaction. Our code is publicly available at github.com/icwhite/codenames.",
            "link": "https://www.semanticscholar.org/paper/d0973f15e23e41caa665bf5cec6164de4ddf2e7a",
            "authors": "Isadora White, Sashrika Pandey, Michelle Pan",
            "matchScore": 213.2231,
            "original title": "Communicate to Play: Pragmatic Reasoning for Efficient Cross-Cultural Communication",
            "original authors": "Isadora White, Sashrika Pandey, Michelle Pan",
            "EMNLP Paper ID": "2395",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "a70f0f9b9b9dc7d5caadcb23a551ea4213727548",
            "title": "Large Language Model-based Human-Agent Collaboration for Complex Task Solving",
            "abstract": "In recent developments within the research community, the integration of Large Language Models (LLMs) in creating fully autonomous agents has garnered significant interest. Despite this, LLM-based agents frequently demonstrate notable shortcomings in adjusting to dynamic environments and fully grasping human needs. In this work, we introduce the problem of LLM-based human-agent collaboration for complex task-solving, exploring their synergistic potential. In addition, we propose a Reinforcement Learning-based Human-Agent Collaboration method, ReHAC. This approach includes a policy model designed to determine the most opportune stages for human intervention within the task-solving process. We construct a human-agent collaboration dataset to train this policy model in an offline reinforcement learning environment. Our validation tests confirm the model's effectiveness. The results demonstrate that the synergistic efforts of humans and LLM-based agents significantly improve performance in complex tasks, primarily through well-planned, limited human intervention. Datasets and code are available at: https://github.com/XueyangFeng/ReHAC.",
            "link": "https://www.semanticscholar.org/paper/a70f0f9b9b9dc7d5caadcb23a551ea4213727548",
            "authors": "Xueyang Feng, Zhiyuan Chen, Yujia Qin, Yankai Lin, Xu Chen, Zhiyuan Liu, Ji-Rong Wen",
            "matchScore": 259.27084,
            "original title": "Large Language Model-based Human-Agent Collaboration for Complex Task Solving",
            "original authors": "Xueyang Feng, Zhi-Yuan Chen, Yujia Qin, Yankai Lin, Xu Chen, Zhiyuan Liu, Ji-Rong Wen",
            "EMNLP Paper ID": "267",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Findings"
        },
        {
            "paperId": "fcbd3eaa5ac589303b6634152f11d79b04faa4ec",
            "title": "Debate as Optimization: Adaptive Conformal Prediction and Diverse Retrieval for Event Extraction",
            "abstract": "We propose a multi-agent debate as optimization (DAO) system for event extraction, where the primary objective is to iteratively refine the large language models (LLMs) outputs through debating without parameter tuning. In DAO, we introduce two novel modules: the Diverse-RAG (DRAG) module and the Adaptive Conformal Prediction (AdaCP) module. DRAG systematically retrieves supporting information that best fits the debate discussion, while AdaCP enhances the accuracy and reliability of event extraction by effectively rejecting less promising answers. Experimental results demonstrate a significant reduction in the performance gap between supervised approaches and tuning-free LLM-based methods by 18.1% and 17.8% on ACE05 and 17.9% and 15.2% on CASIE for event detection and argument extraction respectively.",
            "link": "https://www.semanticscholar.org/paper/fcbd3eaa5ac589303b6634152f11d79b04faa4ec",
            "authors": "Sijia Wang, Lifu Huang",
            "matchScore": 248.47713,
            "original title": "Debate as Optimization: Adaptive Conformal Prediction and Diverse Retrieval for Event Extraction",
            "original authors": "Sijia Wang, Lifu Huang",
            "EMNLP Paper ID": "3147",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "675d137ad4a82df7bdfabda2f8987b1c823e4df0",
            "title": "SRAP-Agent: Simulating and Optimizing Scarce Resource Allocation Policy with LLM-based Agent",
            "abstract": "Public scarce resource allocation plays a crucial role in economics as it directly influences the efficiency and equity in society. Traditional studies including theoretical model-based, empirical study-based and simulation-based methods encounter limitations due to the idealized assumption of complete information and individual rationality, as well as constraints posed by limited available data. In this work, we propose an innovative framework, SRAP-Agent (Simulating and Optimizing Scarce Resource Allocation Policy with LLM-based Agent), which integrates Large Language Models (LLMs) into economic simulations, aiming to bridge the gap between theoretical models and real-world dynamics. Using public housing allocation scenarios as a case study, we conduct extensive policy simulation experiments to verify the feasibility and effectiveness of the SRAP-Agent and employ the Policy Optimization Algorithm with certain optimization objectives. The source code can be found in https://github.com/jijiarui-cather/SRAPAgent_Framework",
            "link": "https://www.semanticscholar.org/paper/675d137ad4a82df7bdfabda2f8987b1c823e4df0",
            "authors": "Jiarui Ji, Yang Li, Hongtao Liu, Zhicheng Du, Zhewei Wei, Weiran Shen, Qi Qi, Yankai Lin",
            "matchScore": 300.02252,
            "original title": "SRAP-Agent: Simulating and Optimizing Scarce Resource Allocation Policy with LLM-based Agent",
            "original authors": "Jiarui Ji, Yang Li, Hongtao Liu, Zhicheng Du, Zhewei Wei, Qi Qi, Weiran Shen, Yankai Lin",
            "EMNLP Paper ID": "50",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "7c33415ee78eaf12dd082ed305638d0c384a279c",
            "title": "mABC: multi-Agent Blockchain-Inspired Collaboration for root cause analysis in micro-services architecture",
            "abstract": "The escalating complexity of micro-services architecture in cloud-native technologies poses significant challenges for maintaining system stability and efficiency. To conduct root cause analysis (RCA) and resolution of alert events, we propose a pioneering framework, multi-Agent Blockchain-inspired Collaboration for root cause analysis in micro-services architecture (mABC), to revolutionize the AI for IT operations (AIOps) domain, where multiple agents based on the powerful large language models (LLMs) perform blockchain-inspired voting to reach a final agreement following a standardized process for processing tasks and queries provided by Agent Workflow. Specifically, seven specialized agents derived from Agent Workflow each provide valuable insights towards root cause analysis based on their expertise and the intrinsic software knowledge of LLMs collaborating within a decentralized chain. To avoid potential instability issues in LLMs and fully leverage the transparent and egalitarian advantages inherent in a decentralized structure, mABC adopts a decision-making process inspired by blockchain governance principles while considering the contribution index and expertise index of each agent. Experimental results on the public benchmark AIOps challenge dataset and our created train-ticket dataset demonstrate superior performance in accurately identifying root causes and formulating effective solutions, compared to previous strong baselines. The ablation study further highlights the significance of each component within mABC, with Agent Workflow, multi-agent, and blockchain-inspired voting being crucial for achieving optimal performance. mABC offers a comprehensive automated root cause analysis and resolution in micro-services architecture and achieves a significant improvement in the AIOps domain compared to existing baselines",
            "link": "https://www.semanticscholar.org/paper/7c33415ee78eaf12dd082ed305638d0c384a279c",
            "authors": "Wei Zhang, Hongcheng Guo, Jian Yang, Yi Zhang, Chaoran Yan, Zhoujin Tian, Hangyuan Ji, Zhoujun Li, Tongliang Li, Tieqiao Zheng, Chao Chen, Yi Liang, Xu Shi, Liangfan Zheng, Bowei Zhang",
            "matchScore": 306.48212,
            "original title": "MABC: Multi-Agent Blockchain-inspired Collaboration for Root Cause Analysis in Micro-Services Architecture",
            "original authors": "Wei Zhang, Hongcheng Guo, Jian Yang, Zhoujin Tian, Yi Zhang, Yan Chaoran, Zhoujun Li, Tongliang Li, xu Shi, liangfan zheng, Bo Zhang",
            "EMNLP Paper ID": "793",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "eebb45d3d4e122c3d776bff33fd82989c669406f",
            "title": "NegotiationToM: A Benchmark for Stress-testing Machine Theory of Mind on Negotiation Surrounding",
            "abstract": "Large Language Models (LLMs) have sparked substantial interest and debate concerning their potential emergence of Theory of Mind (ToM) ability. Theory of mind evaluations currently focuses on testing models using machine-generated data or game settings prone to shortcuts and spurious correlations, which lacks evaluation of machine ToM ability in real-world human interaction scenarios. This poses a pressing demand to develop new real-world scenario benchmarks. We introduce NegotiationToM, a new benchmark designed to stress-test machine ToM in real-world negotiation surrounding covered multi-dimensional mental states (i.e., desires, beliefs, and intentions). Our benchmark builds upon the Belief-Desire-Intention (BDI) agent modeling theory and conducts the necessary empirical experiments to evaluate large language models. Our findings demonstrate that NegotiationToM is challenging for state-of-the-art LLMs, as they consistently perform significantly worse than humans, even when employing the chain-of-thought (CoT) method.",
            "link": "https://www.semanticscholar.org/paper/eebb45d3d4e122c3d776bff33fd82989c669406f",
            "authors": "Chunkit Chan, Cheng Jiayang, Yauwai Yim, Zheye Deng, Wei Fan, Haoran Li, Xin Liu, Hongming Zhang, Weiqi Wang, Yangqiu Song",
            "matchScore": 272.34497,
            "original title": "NegotiationToM: A Benchmark for Stress-testing Machine Theory of Mind on Negotiation Surrounding",
            "original authors": "Chunkit Chan, Cheng Jiayang, Yauwai Yim, Zheye Deng, Wei Fan, Haoran Li, Xin Liu, Hongming Zhang, Weiqi Wang, Yangqiu Song",
            "EMNLP Paper ID": "845",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Findings"
        }
    ],
    "Performance, Consistency, and Reliability in Large Language Models": [
        {
            "paperId": "af40586366f32af5f912bfb7fc43d56a34bb1f0b",
            "title": "Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?",
            "abstract": "Large language models (LLMs) have demonstrated impressive capabilities, but still suffer from inconsistency issues (e.g. LLMs can react differently to disturbances like rephrasing or inconsequential order change). In addition to these inconsistencies, we also observe that LLMs, while capable of solving hard problems, can paradoxically fail at easier ones. To evaluate this hard-to-easy inconsistency, we develop the ConsisEval benchmark, where each entry comprises a pair of questions with a strict order of difficulty. Furthermore, we introduce the concept of consistency score to quantitatively measure this inconsistency and analyze the potential for improvement in consistency by relative consistency score. Based on comprehensive experiments across a variety of existing models, we find: (1) GPT-4 achieves the highest consistency score of 92.2\\% but is still inconsistent to specific questions due to distraction by redundant information, misinterpretation of questions, etc.; (2) models with stronger capabilities typically exhibit higher consistency, but exceptions also exist; (3) hard data enhances consistency for both fine-tuning and in-context learning. Our data and code will be publicly available on GitHub.",
            "link": "https://www.semanticscholar.org/paper/af40586366f32af5f912bfb7fc43d56a34bb1f0b",
            "authors": "Zhe Yang, Yichang Zhang, Tianyu Liu, Jian Yang, Junyang Lin, Chang Zhou, Zhifang Sui",
            "EMNLP Paper ID": "182",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "1544f956492b613b47efd9ac79c3987df373fb81",
            "title": "Aligning Language Models to Explicitly Handle Ambiguity",
            "abstract": "In interactions between users and language model agents, user utterances frequently exhibit ellipsis (omission of words or phrases) or imprecision (lack of exactness) to prioritize efficiency. This can lead to varying interpretations of the same input based on different assumptions or background knowledge. It is thus crucial for agents to adeptly handle the inherent ambiguity in queries to ensure reliability. However, even state-of-the-art large language models (LLMs) still face challenges in such scenarios, primarily due to the following hurdles: (1) LLMs are not explicitly trained to deal with ambiguous utterances; (2) the degree of ambiguity perceived by the LLMs may vary depending on the possessed knowledge. To address these issues, we propose Alignment with Perceived Ambiguity (APA), a novel pipeline that aligns LLMs to manage ambiguous queries by leveraging their own assessment of ambiguity (i.e., perceived ambiguity). Experimental results on question-answering datasets demonstrate that APA empowers LLMs to explicitly detect and manage ambiguous queries while retaining the ability to answer clear questions. Furthermore, our finding proves that APA excels beyond training with gold-standard labels, especially in out-of-distribution scenarios. The data and code are available at https://github.com/heyjoonkim/APA.",
            "link": "https://www.semanticscholar.org/paper/1544f956492b613b47efd9ac79c3987df373fb81",
            "authors": "Hyuhng Joon Kim, Youna Kim, Cheonbok Park, Junyeob Kim, Choonghyun Park, Kang Min Yoo, Sang-goo Lee, Taeuk Kim",
            "EMNLP Paper ID": "223",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "483997ca3e9b18b9dc1a1c8edf7defd984d60c51",
            "title": "QUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios",
            "abstract": "Reasoning is key to many decision making processes. It requires consolidating a set of rule-like premises that are often associated with degrees of uncertainty and observations to draw conclusions. In this work, we address both the case where premises are specified as numeric probabilistic rules and situations in which humans state their estimates using words expressing degrees of certainty. Existing probabilistic reasoning datasets simplify the task, e.g., by requiring the model to only rank textual alternatives, by including only binary random variables, or by making use of a limited set of templates that result in less varied text. In this work, we present QUITE, a question answering dataset of real-world Bayesian reasoning scenarios with categorical random variables and complex relationships. QUITE provides high-quality natural language verbalizations of premises together with evidence statements and expects the answer to a question in the form of an estimated probability. We conduct an extensive set of experiments, finding that logic-based models outperform out-of-the-box large language models on all reasoning types (causal, evidential, and explaining-away). Our results provide evidence that neuro-symbolic models are a promising direction for improving complex reasoning. We release QUITE and code for training and experiments on Github.",
            "link": "https://www.semanticscholar.org/paper/483997ca3e9b18b9dc1a1c8edf7defd984d60c51",
            "authors": "Timo Pierre Schrader, Lukas Lange, S. Razniewski, Annemarie Friedrich",
            "EMNLP Paper ID": "292",
            "Oral/Poster": "Oral",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "9450448e64e0f7f86cd14540d7b8c52ae0e4f328",
            "title": "Decompose and Compare Consistency: Measuring VLMs' Answer Reliability via Task-Decomposition Consistency Comparison",
            "abstract": "Despite tremendous advancements, current state-of-the-art Vision-Language Models (VLMs) are still far from perfect. They tend to hallucinate and may generate biased responses. In such circumstances, having a way to assess the reliability of a given response generated by a VLM is quite useful. Existing methods, such as estimating uncertainty using answer likelihoods or prompt-based confidence generation, often suffer from overconfidence. Other methods use self-consistency comparison but are affected by confirmation biases. To alleviate these, we propose Decompose and Compare Consistency (DeCC) for reliability measurement. By comparing the consistency between the direct answer generated using the VLM's internal reasoning process, and the indirect answers obtained by decomposing the question into sub-questions and reasoning over the sub-answers produced by the VLM, DeCC measures the reliability of VLM's direct answer. Experiments across six vision-language tasks with three VLMs show DeCC's reliability estimation achieves better correlation with task accuracy compared to the existing methods.",
            "link": "https://www.semanticscholar.org/paper/9450448e64e0f7f86cd14540d7b8c52ae0e4f328",
            "authors": "Qian Yang, Weixiang Yan, Aishwarya Agrawal",
            "EMNLP Paper ID": "408",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "01bf8ade86d33ee2fe08780a51bdf59f9ee171b6",
            "title": "Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism",
            "abstract": "Large language models (LLMs) have demonstrated impressive language understanding and generation capabilities, enabling them to answer a wide range of questions across various domains. However, these models are not flawless and often produce responses that contain errors or misinformation. These inaccuracies, commonly referred to as hallucinations, render LLMs unreliable and even unusable in many scenarios. In this paper, our focus is on mitigating the issue of hallucination in LLMs, particularly in the context of question-answering. Instead of attempting to answer all questions, we explore a refusal mechanism that instructs LLMs to refuse to answer challenging questions in order to avoid errors. We then propose a simple yet effective solution called Learn to Refuse (L2R), which incorporates the refusal mechanism to enable LLMs to recognize and refuse to answer questions that they find difficult to address. To achieve this, we utilize a structured knowledge base to represent all the LLM's understanding of the world, enabling it to provide traceable gold knowledge. This knowledge base is separate from the LLM and initially empty. It can be filled with validated knowledge and progressively expanded. When an LLM encounters questions outside its domain, the system recognizes its knowledge scope and determines whether it can answer the question independently. Additionally, we introduce a method for automatically and efficiently expanding the knowledge base of LLMs. Through qualitative and quantitative analysis, we demonstrate that our approach enhances the controllability and reliability of LLMs.",
            "link": "https://www.semanticscholar.org/paper/01bf8ade86d33ee2fe08780a51bdf59f9ee171b6",
            "authors": "Lang Cao",
            "EMNLP Paper ID": "411",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "f4ac895ad3b7398515a3913f815e40516b197a31",
            "title": "I Could've Asked That: Reformulating Unanswerable Questions",
            "abstract": "When seeking information from unfamiliar documents, users frequently pose questions that cannot be answered by the documents. While existing large language models (LLMs) identify these unanswerable questions, they do not assist users in reformulating their questions, thereby reducing their overall utility. We curate CouldAsk, an evaluation benchmark composed of existing and new datasets for document-grounded question answering, specifically designed to study reformulating unanswerable questions. We evaluate state-of-the-art open-source and proprietary LLMs on CouldAsk. The results demonstrate the limited capabilities of these models in reformulating questions. Specifically, GPT-4 and Llama2-7B successfully reformulate questions only 26% and 12% of the time, respectively. Error analysis shows that 62% of the unsuccessful reformulations stem from the models merely rephrasing the questions or even generating identical questions. We publicly release the benchmark and the code to reproduce the experiments.",
            "link": "https://www.semanticscholar.org/paper/f4ac895ad3b7398515a3913f815e40516b197a31",
            "authors": "Wenting Zhao, Ge Gao, Claire Cardie, Alexander M. Rush",
            "EMNLP Paper ID": "465",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "e399fb6a97130fb3ec6db57d583a62f5db45fe18",
            "title": "Can Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?",
            "abstract": "We posit that large language models (LLMs) should be capable of expressing their intrinsic uncertainty in natural language. For example, if the LLM is equally likely to output two contradicting answers to the same question, then its generated response should reflect this uncertainty by hedging its answer (e.g.,\"I'm not sure, but I think...\"). We formalize faithful response uncertainty based on the gap between the model's intrinsic confidence in the assertions it makes and the decisiveness by which they are conveyed. This example-level metric reliably indicates whether the model reflects its uncertainty, as it penalizes both excessive and insufficient hedging. We evaluate a variety of aligned LLMs at faithfully communicating uncertainty on several knowledge-intensive question answering tasks. Our results provide strong evidence that modern LLMs are poor at faithfully conveying their uncertainty, and that better alignment is necessary to improve their trustworthiness.",
            "link": "https://www.semanticscholar.org/paper/e399fb6a97130fb3ec6db57d583a62f5db45fe18",
            "authors": "G. Yona, Roee Aharoni, Mor Geva",
            "EMNLP Paper ID": "883",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "5882749d7392f14e9583973188dbe87703f95227",
            "title": "Perceptions of Linguistic Uncertainty by Language Models and Humans",
            "abstract": "Uncertainty expressions such as ``probably'' or ``highly unlikely'' are pervasive in human language. While prior work has established that there is population-level agreement in terms of how humans interpret these expressions, there has been little inquiry into the abilities of language models to interpret such expressions. In this paper, we investigate how language models map linguistic expressions of uncertainty to numerical responses. Our approach assesses whether language models can employ theory of mind in this setting: understanding the uncertainty of another agent about a particular statement, independently of the model's own certainty about that statement. We evaluate both humans and 10 popular language models on a task created to assess these abilities. Unexpectedly, we find that 8 out of 10 models are able to map uncertainty expressions to probabilistic responses in a human-like manner. However, we observe systematically different behavior depending on whether a statement is actually true or false. This sensitivity indicates that language models are substantially more susceptible to bias based on their prior knowledge (as compared to humans). These findings raise important questions and have broad implications for human-AI alignment and AI-AI communication.",
            "link": "https://www.semanticscholar.org/paper/5882749d7392f14e9583973188dbe87703f95227",
            "authors": "Catarina Bel\u00e9m, Markelle Kelly, M. Steyvers, Sameer Singh, P. Smyth",
            "EMNLP Paper ID": "979",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "fa49021ce42229385f39bf5e1a2dcff48c2e2157",
            "title": "Synchronous Faithfulness Monitoring for Trustworthy Retrieval-Augmented Generation",
            "abstract": "Retrieval-augmented language models (RALMs) have shown strong performance and wide applicability in knowledge-intensive tasks. However, there are significant trustworthiness concerns as RALMs are prone to generating unfaithful outputs, including baseless information or contradictions with the retrieved context. This paper proposes SynCheck, a lightweight monitor that leverages fine-grained decoding dynamics including sequence likelihood, uncertainty quantification, context influence, and semantic alignment to synchronously detect unfaithful sentences. By integrating efficiently measurable and complementary signals, SynCheck enables accurate and immediate feedback and intervention, achieving 0.85 AUROC in detecting faithfulness errors across six long-form retrieval-augmented generation tasks, improving prior best method by 4%. Leveraging SynCheck, we further introduce FOD, a faithfulness-oriented decoding algorithm guided by beam search for long-form retrieval-augmented generation. Empirical results demonstrate that FOD outperforms traditional strategies such as abstention, reranking, or contrastive decoding significantly in terms of faithfulness, achieving over 10% improvement across six datasets.",
            "link": "https://www.semanticscholar.org/paper/fa49021ce42229385f39bf5e1a2dcff48c2e2157",
            "authors": "Di Wu, Jia-Chen Gu, Fan Yin, Nanyun Peng, Kai-Wei Chang",
            "EMNLP Paper ID": "1056",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "d556249d65f0dd87b60b41b23dfe4b626040f2eb",
            "title": "Atomic Inference for NLI with Generated Facts as Atoms",
            "abstract": "With recent advances, neural models can achieve human-level performance on various natural language tasks. However, there are no guarantees that any explanations from these models are faithful, i.e. that they reflect the inner workings of the model. Atomic inference overcomes this issue, providing interpretable and faithful model decisions. This approach involves making predictions for different components (or atoms) of an instance, before using interpretable and deterministic rules to derive the overall prediction based on the individual atom-level predictions. We investigate the effectiveness of using LLM-generated facts as atoms, decomposing Natural Language Inference premises into lists of facts. While directly using generated facts in atomic inference systems can result in worse performance, with 1) a multi-stage fact generation process, and 2) a training regime that incorporates the facts, our fact-based method outperforms other approaches.",
            "link": "https://www.semanticscholar.org/paper/d556249d65f0dd87b60b41b23dfe4b626040f2eb",
            "authors": "Joe Stacey, Pasquale Minervini, Haim Dubossarsky, Oana-Maria Camburu, Marek Rei",
            "EMNLP Paper ID": "1141",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "0c004aae99072461bb42ee0584dadded6bec7def",
            "title": "AmbigNLG: Addressing Task Ambiguity in Instruction for NLG",
            "abstract": "We introduce AmbigNLG, a novel task designed to tackle the challenge of task ambiguity in instructions for Natural Language Generation (NLG). Ambiguous instructions often impede the performance of Large Language Models (LLMs), especially in complex NLG tasks. To tackle this issue, we propose an ambiguity taxonomy that categorizes different types of instruction ambiguities and refines initial instructions with clearer specifications. Accompanying this task, we present AmbigSNI-NLG, a dataset comprising 2,500 instances annotated to facilitate research in AmbigNLG. Through comprehensive experiments with state-of-the-art LLMs, we demonstrate that our method significantly enhances the alignment of generated text with user expectations, achieving up to a 15.02-point increase in ROUGE scores. Our findings highlight the critical importance of addressing task ambiguity to fully harness the capabilities of LLMs in NLG tasks. Furthermore, we confirm the effectiveness of our method in practical settings involving interactive ambiguity mitigation with users, underscoring the benefits of leveraging LLMs for interactive clarification.",
            "link": "https://www.semanticscholar.org/paper/0c004aae99072461bb42ee0584dadded6bec7def",
            "authors": "Ayana Niwa, Hayate Iso",
            "EMNLP Paper ID": "1216",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "42d4ca16b92d4d82370f5fdb36e847af864b9376",
            "title": "What Are the Odds? Language Models Are Capable of Probabilistic Reasoning",
            "abstract": "Language models (LM) are capable of remarkably complex linguistic tasks; however, numerical reasoning is an area in which they frequently struggle. An important but rarely evaluated form of reasoning is understanding probability distributions. In this paper, we focus on evaluating the probabilistic reasoning capabilities of LMs using idealized and real-world statistical distributions. We perform a systematic evaluation of state-of-the-art LMs on three tasks: estimating percentiles, drawing samples, and calculating probabilities. We evaluate three ways to provide context to LMs 1) anchoring examples from within a distribution or family of distributions, 2) real-world context, 3) summary statistics on which to base a Normal approximation. Models can make inferences about distributions, and can be further aided by the incorporation of real-world context, example shots and simplified assumptions, even if these assumptions are incorrect or misspecified. To conduct this work, we developed a comprehensive benchmark distribution dataset with associated question-answer pairs that we have released publicly.",
            "link": "https://www.semanticscholar.org/paper/42d4ca16b92d4d82370f5fdb36e847af864b9376",
            "authors": "Akshay Paruchuri, Jake Garrison, Shun Liao, John Hernandez, Jacob Sunshine, Tim Althoff, Xin Liu, Daniel McDuff",
            "EMNLP Paper ID": "1362",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 12): 17:45\u201318:45 (Evening)",
            "Type": "Main"
        },
        {
            "paperId": "9111fc83b652c871c8e223b29009de9698b8f9b2",
            "title": "Atomic Self-Consistency for Better Long Form Generations",
            "abstract": "Recent work has aimed to improve LLM generations by filtering out hallucinations, thereby improving the precision of the information in responses. Correctness of a long-form response, however, also depends on the recall of multiple pieces of information relevant to the question. In this paper, we introduce Atomic Self-Consistency (ASC), a technique for improving the recall of relevant information in an LLM response. ASC follows recent work, Universal Self-Consistency (USC) in using multiple stochastic samples from an LLM to improve the long-form response. Unlike USC which only focuses on selecting the best single generation, ASC picks authentic subparts from the samples and merges them into a superior composite answer. Through extensive experiments and ablations, we show that merging relevant subparts of multiple samples performs significantly better than picking a single sample. ASC demonstrates significant gains over USC on multiple factoids and open-ended QA datasets - ASQA, QAMPARI, QUEST, ELI5 with ChatGPT and Llama2. Our analysis also reveals untapped potential for enhancing long-form generations using approach of merging multiple samples.",
            "link": "https://www.semanticscholar.org/paper/9111fc83b652c871c8e223b29009de9698b8f9b2",
            "authors": "Raghuveer Thirukovalluru, Yukun Huang, Bhuwan Dhingra",
            "EMNLP Paper ID": "1472",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "92de96ddb32a75138a00532587fb86920f7731ce",
            "title": "CONTESTS: a Framework for Consistency Testing of Span Probabilities in Language Models",
            "abstract": "Although language model scores are often treated as probabilities, their reliability as probability estimators has mainly been studied through calibration, overlooking other aspects. In particular, it is unclear whether language models produce the same value for different ways of assigning joint probabilities to word spans. Our work introduces a novel framework, ConTestS (Consistency Testing over Spans), involving statistical tests to assess score consistency across interchangeable completion and conditioning orders. We conduct experiments on post-release real and synthetic data to eliminate training effects. Our findings reveal that both Masked Language Models (MLMs) and autoregressive models exhibit inconsistent predictions, with autoregressive models showing larger discrepancies. Larger MLMs tend to produce more consistent predictions, while autoregressive models show the opposite trend. Moreover, for both model types, prediction entropies offer insights into the true word span likelihood and therefore can aid in selecting optimal decoding strategies. The inconsistencies revealed by our analysis, as well their connection to prediction entropies and differences between model types, can serve as useful guides for future research on addressing these limitations.",
            "link": "https://www.semanticscholar.org/paper/92de96ddb32a75138a00532587fb86920f7731ce",
            "authors": "Eitan Wagner, Yuli Slavutsky, Omri Abend",
            "EMNLP Paper ID": "1809",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "fa9becea34e062a04a1c153e7417d7d75e195da9",
            "title": "\"I Never Said That\": A dataset, taxonomy and baselines on response clarity classification",
            "abstract": "Equivocation and ambiguity in public speech are well-studied discourse phenomena, especially in political science and analysis of political interviews. Inspired by the well-grounded theory on equivocation, we aim to resolve the closely related problem of response clarity in questions extracted from political interviews, leveraging the capabilities of Large Language Models (LLMs) and human expertise. To this end, we introduce a novel taxonomy that frames the task of detecting and classifying response clarity and a corresponding clarity classification dataset which consists of question-answer (QA) pairs drawn from political interviews and annotated accordingly. Our proposed two-level taxonomy addresses the clarity of a response in terms of the information provided for a given question (high-level) and also provides a fine-grained taxonomy of evasion techniques that relate to unclear, ambiguous responses (lower-level). We combine ChatGPT and human annotators to collect, validate and annotate discrete QA pairs from political interviews, to be used for our newly introduced response clarity task. We provide a detailed analysis and conduct several experiments with different model architectures, sizes and adaptation methods to gain insights and establish new baselines over the proposed dataset and task.",
            "link": "https://www.semanticscholar.org/paper/fa9becea34e062a04a1c153e7417d7d75e195da9",
            "authors": "Konstantinos Thomas, Giorgos Filandrianos, Maria Lymperaiou, Chrysoula Zerva, G. Stamou",
            "matchScore": 278.57837,
            "original title": "\u201dI Never Said That\u201d: A dataset, taxonomy and baselines on response clarity classification",
            "original authors": "Konstantinos Thomas, Giorgos Filandrianos, Maria Lymperaiou, Chrysoula Zerva, Giorgos Stamou",
            "EMNLP Paper ID": "1035",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "a06a9d4b0c70a38f625a7587b39467407573665b",
            "title": "Knowledge-based Consistency Testing of Large Language Models",
            "abstract": "In this work, we systematically expose and measure the inconsistency and knowledge gaps of Large Language Models (LLMs). Specifically, we propose an automated testing framework (called KonTest) which leverages a knowledge graph to construct test cases. KonTest probes and measures the inconsistencies in the LLM's knowledge of the world via a combination of semantically-equivalent queries and test oracles (metamorphic or ontological oracle). KonTest further mitigates knowledge gaps via a weighted LLM model ensemble. Using four state-of-the-art LLMs (Falcon, Gemini, GPT3.5, and Llama2), we show that KonTest generates 19.2% error inducing inputs (1917 errors from 9979 test inputs). It also reveals a 16.5% knowledge gap across all tested LLMs. A mitigation method informed by KonTest's test suite reduces LLM knowledge gap by 32.48%. Our ablation study further shows that GPT3.5 is not suitable for knowledge-based consistency testing because it is only 60%-68% effective in knowledge construction.",
            "link": "https://www.semanticscholar.org/paper/a06a9d4b0c70a38f625a7587b39467407573665b",
            "authors": "Sai Sathiesh Rajan, E. Soremekun, Sudipta Chattopadhyay",
            "matchScore": 192.44557,
            "original title": "Knowledge-based Consistency Testing of Large Language Models",
            "original authors": "Sai Sathiesh Rajan, Ezekiel Soremekun, Sudipta Chattopadhyay",
            "EMNLP Paper ID": "2085",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "aa1fbd6e8d1c8e99b6ca34c17bcdb36e987b68a6",
            "title": "Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection",
            "abstract": "Self-detection for Large Language Models (LLMs) seeks to evaluate the trustworthiness of the LLM's output by leveraging its own capabilities, thereby alleviating the issue of output hallucination. However, existing self-detection approaches only retrospectively evaluate answers generated by LLM, typically leading to the over-trust in incorrectly generated answers. To tackle this limitation, we propose a novel self-detection paradigm that considers the comprehensive answer space beyond LLM-generated answers. It thoroughly compares the trustworthiness of multiple candidate answers to mitigate the over-trust in LLM-generated incorrect answers. Building upon this paradigm, we introduce a two-step framework, which firstly instructs LLM to reflect and provide justifications for each candidate answer, and then aggregates the justifications for comprehensive target answer evaluation. This framework can be seamlessly integrated with existing approaches for superior self-detection. Extensive experiments on six datasets spanning three tasks demonstrate the effectiveness of the proposed framework.",
            "link": "https://www.semanticscholar.org/paper/aa1fbd6e8d1c8e99b6ca34c17bcdb36e987b68a6",
            "authors": "Moxin Li, Wenjie Wang, Fuli Feng, Fengbin Zhu, Qifan Wang, Tat-Seng Chua",
            "matchScore": 321.17288,
            "original title": "Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection",
            "original authors": "Moxin Li, Wenjie Wang, Fuli Feng, Fengbin ZHU, Qifan Wang, Tat-Seng Chua",
            "EMNLP Paper ID": "2333",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "5e5be5fb051c1b542fce450b50092b3f2c92b5a7",
            "title": "Are LLMs Aware that Some Questions are not Open-ended?",
            "abstract": "Large Language Models (LLMs) have shown the impressive capability of answering questions in a wide range of scenarios. However, when LLMs face different types of questions, it is worth exploring whether LLMs are aware that some questions have limited answers and need to respond more deterministically but some do not. We refer to this as question awareness of LLMs. The lack of question awareness in LLMs leads to two phenomena that LLMs are: (1) too casual to answer non-open-ended questions or (2) too boring to answer open-ended questions. In this paper, we first evaluate the question awareness in LLMs. The experimental results show that LLMs have the issues of lacking awareness of questions in certain domains, e.g. factual knowledge, resulting in hallucinations during the generation. To mitigate these, we propose a method called Question Awareness Temperature Sampling (QuATS). This method enhances the question awareness of LLMs by adaptively adjusting the output distributions based on question features. The automatic adjustment in QuATS eliminates the need for manual temperature tuning in text generation and consistently improves model performance in various benchmarks.",
            "link": "https://www.semanticscholar.org/paper/5e5be5fb051c1b542fce450b50092b3f2c92b5a7",
            "authors": "Dongjie Yang, Hai Zhao",
            "matchScore": 201.64822,
            "original title": "Are LLMs Aware that Some Questions are not Open-ended?",
            "original authors": "Dongjie Yang, hai zhao",
            "EMNLP Paper ID": "429",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "3162050ef9a598109594e4290ef5a403b1517cdf",
            "title": "Mirror-Consistency: Harnessing Inconsistency in Majority Voting",
            "abstract": "Self-Consistency, a widely-used decoding strategy, significantly boosts the reasoning capabilities of Large Language Models (LLMs). However, it depends on the plurality voting rule, which focuses on the most frequent answer while overlooking all other minority responses. These inconsistent minority views often illuminate areas of uncertainty within the model's generation process. To address this limitation, we present Mirror-Consistency, an enhancement of the standard Self-Consistency approach. Our method incorporates a 'reflective mirror' into the self-ensemble decoding process and enables LLMs to critically examine inconsistencies among multiple generations. Additionally, just as humans use the mirror to better understand themselves, we propose using Mirror-Consistency to enhance the sample-based confidence calibration methods, which helps to mitigate issues of overconfidence. Our experimental results demonstrate that Mirror-Consistency yields superior performance in both reasoning accuracy and confidence calibration compared to Self-Consistency.",
            "link": "https://www.semanticscholar.org/paper/3162050ef9a598109594e4290ef5a403b1517cdf",
            "authors": "Siyuan Huang, Zhiyuan Ma, Jintao Du, Changhua Meng, Weiqiang Wang, Zhouhan Lin",
            "matchScore": 240.78085,
            "original title": "Mirror-Consistency: Harnessing Inconsistency in Majority Voting",
            "original authors": "Siyuan Huang, Zhiyuan Ma, Jintao Du, Changhua Meng, Weiqiang Wang, Zhouhan Lin",
            "EMNLP Paper ID": "496",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "38264290a8a3f71fd5b3a86ea35a8b528d4e3d10",
            "title": "Characterizing LLM Abstention Behavior in Science QA with Context Perturbations",
            "abstract": "The correct model response in the face of uncertainty is to abstain from answering a question so as not to mislead the user. In this work, we study the ability of LLMs to abstain from answering context-dependent science questions when provided insufficient or incorrect context. We probe model sensitivity in several settings: removing gold context, replacing gold context with irrelevant context, and providing additional context beyond what is given. In experiments on four QA datasets with six LLMs, we show that performance varies greatly across models, across the type of context provided, and also by question type; in particular, many LLMs seem unable to abstain from answering boolean questions using standard QA prompts. Our analysis also highlights the unexpected impact of abstention performance on QA task accuracy. Counter-intuitively, in some settings, replacing gold context with irrelevant context or adding irrelevant context to gold context can improve abstention performance in a way that results in improvements in task performance. Our results imply that changes are needed in QA dataset design and evaluation to more effectively assess the correctness and downstream impacts of model abstention.",
            "link": "https://www.semanticscholar.org/paper/38264290a8a3f71fd5b3a86ea35a8b528d4e3d10",
            "authors": "Bingbing Wen, Bill Howe, Lucy Lu Wang",
            "matchScore": 281.55484,
            "original title": "Characterizing LLM Abstention Behavior in Science QA with Context Perturbations",
            "original authors": "Bingbing Wen, Bill Howe, Lucy Lu Wang",
            "EMNLP Paper ID": "702",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        }
    ],
    "Long-Context Processing Techniques for Large Language Models": [
        {
            "paperId": "df5e4ba166480329b066f8619891332820b09c86",
            "title": "Rethinking Token Reduction for State Space Models",
            "abstract": "Recent advancements in State Space Models (SSMs) have attracted significant interest, particularly in models optimized for parallel training and handling long-range dependencies. Architectures like Mamba have scaled to billions of parameters with selective SSM. To facilitate broader applications using Mamba, exploring its efficiency is crucial. While token reduction techniques offer a straightforward post-training strategy, we find that applying existing methods directly to SSMs leads to substantial performance drops. Through insightful analysis, we identify the reasons for this failure and the limitations of current techniques. In response, we propose a tailored, unified post-training token reduction method for SSMs. Our approach integrates token importance and similarity, thus taking advantage of both pruning and merging, to devise a fine-grained intra-layer token reduction strategy. Extensive experiments show that our method improves the average accuracy by 5.7% to 13.1% on six benchmarks with Mamba-2 compared to existing methods, while significantly reducing computational demands and memory requirements.",
            "link": "https://www.semanticscholar.org/paper/df5e4ba166480329b066f8619891332820b09c86",
            "authors": "Zheng Zhan, Yushu Wu, Zhenglun Kong, Changdi Yang, Yifan Gong, Xuan Shen, Xue Lin, Pu Zhao, Yanzhi Wang",
            "EMNLP Paper ID": "194",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "50c1ac57e25b82387898c9d7ce25e6a6e181d6c5",
            "title": "CItruS: Chunked Instruction-aware State Eviction for Long Sequence Modeling",
            "abstract": "Long sequence modeling has gained broad interest as large language models (LLMs) continue to advance. Recent research has identified that a large portion of hidden states within the key-value caches of Transformer models can be discarded (also termed evicted) without affecting the perplexity performance in generating long sequences. However, we show that these methods, despite preserving perplexity performance, often drop information that is important for solving downstream tasks, a problem which we call information neglect. To address this issue, we introduce Chunked Instruction-aware State Eviction (CItruS), a novel modeling technique that integrates the attention preferences useful for a downstream task into the eviction process of hidden states. In addition, we design a method for chunked sequence processing to further improve efficiency. Our training-free method exhibits superior performance on long sequence comprehension and retrieval tasks over several strong baselines under the same memory budget, while preserving language modeling perplexity. The code and data have been released at https://github.com/ybai-nlp/CItruS.",
            "link": "https://www.semanticscholar.org/paper/50c1ac57e25b82387898c9d7ce25e6a6e181d6c5",
            "authors": "Yu Bai, Xiyuan Zou, Heyan Huang, Sanxing Chen, Marc-Antoine Rondeau, Yang Gao, Jackie Chi Kit Cheung",
            "EMNLP Paper ID": "656",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "ba6d36889a4a59f4d1e8a53492a52bf9a791c6d8",
            "title": "Chain and Causal Attention for Efficient Entity Tracking",
            "abstract": "This paper investigates the limitations of transformers for entity-tracking tasks in large language models. We identify a theoretical constraint, showing that transformers require at least $\\log_2 (n+1)$ layers to handle entity tracking with $n$ state changes. To address this issue, we propose an efficient and frugal enhancement to the standard attention mechanism, enabling it to manage long-term dependencies more efficiently. By considering attention as an adjacency matrix, our model can track entity states with a single layer. Empirical results demonstrate significant improvements in entity tracking datasets while keeping competitive performance on standard natural language modeling. Our modified attention allows us to achieve the same performance with drastically fewer layers. Additionally, our enhanced mechanism reveals structured internal representations of attention. Extensive experiments on both toy and complex datasets validate our approach. Our contributions include theoretical insights, an improved attention mechanism, and empirical validation.",
            "link": "https://www.semanticscholar.org/paper/ba6d36889a4a59f4d1e8a53492a52bf9a791c6d8",
            "authors": "Erwan Fagnou, Paul Caillon, Blaise Delattre, Alexandre Allauzen",
            "EMNLP Paper ID": "1525",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "cbf384b33159469d2f952d9ea0228150c236df94",
            "title": "Shaking Up VLMs: Comparing Transformers and Structured State Space Models for Vision & Language Modeling",
            "abstract": "This study explores replacing Transformers in Visual Language Models (VLMs) with Mamba, a recent structured state space model (SSM) that demonstrates promising performance in sequence modeling. We test models up to 3B parameters under controlled conditions, showing that Mamba-based VLMs outperforms Transformers-based VLMs in captioning, question answering, and reading comprehension. However, we find that Transformers achieve greater performance in visual grounding and the performance gap widens with scale. We explore two hypotheses to explain this phenomenon: 1) the effect of task-agnostic visual encoding on the updates of the hidden states, and 2) the difficulty in performing visual grounding from the perspective of in-context multimodal retrieval. Our results indicate that a task-aware encoding yields minimal performance gains on grounding, however, Transformers significantly outperform Mamba at in-context multimodal retrieval. Overall, Mamba shows promising performance on tasks where the correct output relies on a summary of the image but struggles when retrieval of explicit information from the context is required.",
            "link": "https://www.semanticscholar.org/paper/cbf384b33159469d2f952d9ea0228150c236df94",
            "authors": "Georgios Pantazopoulos, Malvina Nikandrou, Alessandro Suglia, Oliver Lemon, Arash Eshghi",
            "EMNLP Paper ID": "1656",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "4ee831fd7892471f60fb6ea61c43a0eab6972a4a",
            "title": "InfiniPot: Infinite Context Processing on Memory-Constrained LLMs",
            "abstract": "Handling long input contexts remains a significant challenge for Large Language Models (LLMs), particularly in resource-constrained environments such as mobile devices. Our work aims to address this limitation by introducing InfiniPot, a novel KV cache control framework designed to enable pre-trained LLMs to manage extensive sequences within fixed memory constraints efficiently, without requiring additional training. InfiniPot leverages Continual Context Distillation (CCD), an iterative process that compresses and retains essential information through novel importance metrics, effectively maintaining critical data even without access to future context. Our comprehensive evaluations indicate that InfiniPot significantly outperforms models trained for long contexts in various NLP tasks, establishing its efficacy and versatility. This work represents a substantial advancement toward making LLMs applicable to a broader range of real-world scenarios.",
            "link": "https://www.semanticscholar.org/paper/4ee831fd7892471f60fb6ea61c43a0eab6972a4a",
            "authors": "Minsoo Kim, Kyuhong Shim, Jungwook Choi, Simyung Chang",
            "EMNLP Paper ID": "1888",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "61e113406312785f8471e53dc28da7377ab6ced4",
            "title": "LLoCO: Learning Long Contexts Offline",
            "abstract": "Processing long contexts remains a challenge for large language models (LLMs) due to the quadratic computational and memory overhead of the self-attention mechanism and the substantial KV cache sizes during generation. We propose LLoCO, a novel approach to address this problem by learning contexts offline through context compression and in-domain parameter-efficient finetuning with LoRA. Our method enables an LLM to create a concise representation of the original context and efficiently retrieve relevant information to answer questions accurately. Our approach extends the effective context window of a 4k token LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on several long-context question-answering datasets, demonstrating that LLoCO significantly outperforms in-context learning while using $30\\times$ fewer tokens during inference. LLoCO achieves up to $7.62\\times$ speed-up during inference and $11.52\\times$ higher throughput during finetuning, substantially reduces the cost of long document question answering. This makes it a promising solution for efficient long context processing. Our code is publicly available on https://github.com/jeffreysijuntan/lloco.",
            "link": "https://www.semanticscholar.org/paper/61e113406312785f8471e53dc28da7377ab6ced4",
            "authors": "Sijun Tan, Xiuyu Li, Shishir G. Patil, Ziyang Wu, Tianjun Zhang, Kurt Keutzer, Joseph E. Gonzalez, Raluca A. Popa",
            "EMNLP Paper ID": "2104",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "52c537f4cc67b60b758c9d7df8778f7d30538dff",
            "title": "A Simple and Effective L2 Norm-Based Strategy for KV Cache Compression",
            "abstract": "The deployment of large language models (LLMs) is often hindered by the extensive memory requirements of the Key-Value (KV) cache, especially as context lengths increase. Existing approaches to reduce the KV cache size involve either fine-tuning the model to learn a compression strategy or leveraging attention scores to reduce the sequence length. We analyse the attention distributions in decoder-only Transformers-based models and observe that attention allocation patterns stay consistent across most layers. Surprisingly, we find a clear correlation between the $L_2$ and the attention scores over cached KV pairs, where a low $L_2$ of a key embedding usually leads to a high attention score during decoding. This finding indicates that the influence of a KV pair is potentially determined by the key embedding itself before being queried. Based on this observation, we compress the KV cache based on the $L_2$ of key embeddings. Our experimental results show that this simple strategy can reduce the KV cache size by 50% on language modelling and needle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing accuracy. Moreover, without relying on the attention scores, this approach remains compatible with FlashAttention, enabling broader applicability.",
            "link": "https://www.semanticscholar.org/paper/52c537f4cc67b60b758c9d7df8778f7d30538dff",
            "authors": "Alessio Devoto, Yu Zhao, Simone Scardapane, Pasquale Minervini",
            "EMNLP Paper ID": "2297",
            "Oral/Poster": "Oral",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "3e8d4062ec4353ff2701c7769336dbdb97f8814c",
            "title": "Transformers are Multi-State RNNs",
            "abstract": "Transformers are considered conceptually different from the previous generation of state-of-the-art NLP models - recurrent neural networks (RNNs). In this work, we demonstrate that decoder-only transformers can in fact be conceptualized as unbounded multi-state RNNs - an RNN variant with unlimited hidden state size. We further show that transformers can be converted into $\\textit{bounded}$ multi-state RNNs by fixing the size of their hidden state, effectively compressing their key-value cache. We introduce a novel, training-free compression policy - $\\textbf{T}$oken $\\textbf{O}$mission $\\textbf{V}$ia $\\textbf{A}$ttention (TOVA). Our experiments with four long range tasks and several LLMs show that TOVA outperforms several baseline compression policies. Particularly, our results are nearly on par with the full model, using in some cases only $\\frac{1}{8}$ of the original cache size, which translates to 4.8X higher throughput. Our results shed light on the connection between transformers and RNNs, and help mitigate one of LLMs' most painful computational bottlenecks - the size of their key-value cache. We publicly release our code at https://github.com/schwartz-lab-NLP/TOVA",
            "link": "https://www.semanticscholar.org/paper/3e8d4062ec4353ff2701c7769336dbdb97f8814c",
            "authors": "Matanel Oren, Michael Hassid, Yossi Adi, Roy Schwartz",
            "EMNLP Paper ID": "2340",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "6cd88ba6ec5ca40a9bf901c89bcc4542407c2457",
            "title": "Attention Score is not All You Need for Token Importance Indicator in KV Cache Reduction: Value Also Matters",
            "abstract": "Scaling the context size of large language models (LLMs) enables them to perform various new tasks, e.g., book summarization. However, the memory cost of the Key and Value (KV) cache in attention significantly limits the practical applications of LLMs. Recent works have explored token pruning for KV cache reduction in LLMs, relying solely on attention scores as a token importance indicator. However, our investigation into value vector norms revealed a notably non-uniform pattern questioning their reliance only on attention scores. Inspired by this, we propose a new method: Value-Aware Token Pruning (VATP) which uses both attention scores and the $ \\ell_{1} $ norm of value vectors to evaluate token importance. Extensive experiments on LLaMA2-7B-chat and Vicuna-v1.5-7B across 16 LongBench tasks demonstrate that VATP outperforms attention-score-only baselines in over 12 tasks, confirming the effectiveness of incorporating value vector norms into token importance evaluation of LLMs.",
            "link": "https://www.semanticscholar.org/paper/6cd88ba6ec5ca40a9bf901c89bcc4542407c2457",
            "authors": "Zhiyu Guo, Hidetaka Kamigaito, Taro Watanabe",
            "EMNLP Paper ID": "2854",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "efc8e68d9a0d002141c49cf0101678880621132c",
            "title": "T-FREE: Tokenizer-Free Generative LLMs via Sparse Representations for Memory-Efficient Embeddings",
            "abstract": "Tokenizers are crucial for encoding information in Large Language Models, but their development has recently stagnated, and they contain inherent weaknesses. Major limitations include computational overhead, ineffective vocabulary use, and unnecessarily large embedding and head layers. Additionally, their performance is biased towards a reference corpus, leading to reduced effectiveness for underrepresented languages. To remedy these issues, we propose T-FREE, which directly embeds words through sparse activation patterns over character triplets, and does not require a reference corpus. T-FREE inherently exploits morphological similarities and allows for strong compression of embedding layers. In our exhaustive experimental evaluation, we achieve competitive downstream performance with a parameter reduction of more than 85% on these layers. Further, T-FREE shows significant improvements in cross-lingual transfer learning.",
            "link": "https://www.semanticscholar.org/paper/efc8e68d9a0d002141c49cf0101678880621132c",
            "authors": "Bjorn Deiseroth, Manuel Brack, P. Schramowski, K. Kersting, Samuel Weinbach",
            "EMNLP Paper ID": "3037",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "c98b5eeb8f553e56a725f3b88e13c8350fe818c3",
            "title": "DAdEE: Unsupervised Domain Adaptation in Early Exit PLMs",
            "abstract": "Pre-trained Language Models (PLMs) exhibit good accuracy and generalization ability across various tasks using self-supervision, but their large size results in high inference latency. Early Exit (EE) strategies handle the issue by allowing the samples to exit from classifiers attached to the intermediary layers, but they do not generalize well, as exit classifiers can be sensitive to domain changes. To address this, we propose Unsupervised Domain Adaptation in EE framework (DADEE) that employs multi-level adaptation using knowledge distillation. DADEE utilizes GAN-based adversarial adaptation at each layer to achieve domain-invariant representations, reducing the domain gap between the source and target domain across all layers. The attached exits not only speed up inference but also enhance domain adaptation by reducing catastrophic forgetting and mode collapse, making it more suitable for real-world scenarios. Experiments on tasks such as sentiment analysis, entailment classification, and natural language inference demonstrate that DADEE consistently outperforms not only early exit methods but also various domain adaptation methods under domain shift scenarios. The anonymized source code is available at https://github.com/Div290/DAdEE.",
            "link": "https://www.semanticscholar.org/paper/c98b5eeb8f553e56a725f3b88e13c8350fe818c3",
            "authors": "D. J. Bajpai, M. Hanawal",
            "matchScore": 247.71872,
            "original title": "DAdEE: Unsupervised Domain Adaptation in Early Exit PLMs",
            "original authors": "Divya Jyoti Bajpai, Manjesh Kumar Hanawal",
            "EMNLP Paper ID": "1305",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "720651c0bcfe510d121cb19984128d815c7c7bff",
            "title": "CAPEEN: Image Captioning with Early Exits and Knowledge Distillation",
            "abstract": "Deep neural networks (DNNs) have made significant progress in recognizing visual elements and generating descriptive text in image-captioning tasks. However, their improved performance comes from increased computational burden and inference latency. Early Exit (EE) strategies can be used to enhance their efficiency, but their adaptation presents challenges in image captioning as it requires varying levels of semantic information for accurate predictions. To overcome this, we introduce CAPEEN to improve the performance of EE strategies using knowledge distillation. Inference in CAPEEN is completed at intermediary layers if prediction confidence exceeds a predefined value learned from the training data. To account for real-world deployments, where target distributions could drift from that of training samples, we introduce a variant A-CAPEEN to adapt the thresholds on the fly using Multiarmed bandits framework. Experiments on the MS COCO and Flickr30k datasets show that CAPEEN gains speedup of 1.77x while maintaining competitive performance compared to the final layer, and A-CAPEEN additionally offers robustness against distortions. The source code is available at https://github.com/Div290/CapEEN",
            "link": "https://www.semanticscholar.org/paper/720651c0bcfe510d121cb19984128d815c7c7bff",
            "authors": "D. J. Bajpai, M. Hanawal",
            "matchScore": 239.12427,
            "original title": "CapEEN: Image Captioning with Early Exits and Knowledge Distillation",
            "original authors": "Divya Jyoti Bajpai, Manjesh Kumar Hanawal",
            "EMNLP Paper ID": "1316",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "f6440a16ccc5c13d2a86af91b76e078685abfd16",
            "title": "LongHeads: Multi-Head Attention is Secretly a Long Context Processor",
            "abstract": "Large language models (LLMs) have achieved impressive performance in numerous domains but often struggle to process lengthy inputs effectively and efficiently due to limited length generalization and attention's quadratic computational demands. Many sought to mitigate this by restricting the attention window within the pre-trained length. However, these methods introduce new issues such as ignoring the middle context and requiring additional training. To address these problems, we propose LongHeads, a training-free framework that enhances LLM's long context ability by unlocking multi-head attention's untapped potential. Instead of allowing each head to attend to the full sentence, which struggles with generalizing to longer sequences due to out-of-distribution (OOD) issues, we allow each head to process in-distribution length by selecting and attending to important context chunks. To this end, we propose a chunk selection strategy that relies on the inherent correlation between the query and the key representations, efficiently distributing context chunks to different heads. In this way, each head ensures it can effectively process attended tokens within the trained length, while different heads in different layers can collectively process longer contexts. LongHeads works efficiently in linear time, fits seamlessly with many LLMs that use relative positional encoding. LongHeads achieves 100% accuracy at the 128k length on passkey retrieval task, verifying LongHeads's efficacy in extending the usable context window for existing models. We release our code at https://github.com/LuLuLuyi/LongHeads .",
            "link": "https://www.semanticscholar.org/paper/f6440a16ccc5c13d2a86af91b76e078685abfd16",
            "authors": "Yi Lu, Xin Zhou, Wei He, Jun Zhao, Tao Ji, Tao Gui, Qi Zhang, Xuanjing Huang",
            "matchScore": 260.73914,
            "original title": "LongHeads: Multi-Head Attention is Secretly a Long Context Processor",
            "original authors": "Yi Lu, Xin Zhou, Wei He, Jun Zhao, Tao Ji, Tao Gui, Qi Zhang, Xuanjing Huang",
            "EMNLP Paper ID": "1449",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "b44a1b65bb8de87d6b4f6fc092ade342d344f400",
            "title": "Long Sequence Modeling with Attention Tensorization: From Sequence to Tensor Learning",
            "abstract": "As the demand for processing extended textual data grows, the ability to handle long-range dependencies and maintain computational efficiency is more critical than ever. One of the key issues for long-sequence modeling using attention-based model is the mismatch between the limited-range modeling power of full attention and the long-range token dependency in the input sequence. In this work, we propose to scale up the attention receptive field by tensorizing long input sequences into compact tensor representations followed by attention on each transformed dimension. The resulting Tensorized Attention can be adopted as efficient transformer backbones to extend input context length with improved memory and time efficiency. We show that the proposed attention tensorization encodes token dependencies as a multi-hop attention process, and is equivalent to Kronecker decomposition of full attention. Extensive experiments show that tensorized attention can be used to adapt pretrained LLMs with improved efficiency. Notably, Llama-8B with tensorization is trained under 32,768 context length and can steadily extrapolate to 128k length during inference with $11\\times$ speedup, compared to full attention with FlashAttention-2.",
            "link": "https://www.semanticscholar.org/paper/b44a1b65bb8de87d6b4f6fc092ade342d344f400",
            "authors": "Aosong Feng, Rex Ying, L. Tassiulas",
            "matchScore": 288.48822,
            "original title": "Long Sequence Modeling with Attention Tensorization: From Sequence to Tensor Learning",
            "original authors": "Aosong Feng, Rex Ying, Leandros Tassiulas",
            "EMNLP Paper ID": "2829",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "f34c0d2e10737531ac028cf7de8a01e0db9d4728",
            "title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
            "abstract": "In-context learning (ICL) approaches typically leverage prompting to condition decoder-only language model generation on reference information. Just-in-time processing of a context is inefficient due to the quadratic cost of self-attention operations, and caching is desirable. However, caching transformer states can easily require almost as much space as the model parameters. When the right context isn't known in advance, caching ICL can be challenging. This work addresses these limitations by introducing models that, inspired by the encoder-decoder architecture, use cross-attention to condition generation on reference text without the prompt. More precisely, we leverage pre-trained decoder-only models and only train a small number of added layers. We use Question-Answering (QA) as a testbed to evaluate the ability of our models to perform conditional generation and observe that they outperform ICL, are comparable to fine-tuned prompted LLMs, and drastically reduce the space footprint relative to standard KV caching by two orders of magnitude.",
            "link": "https://www.semanticscholar.org/paper/f34c0d2e10737531ac028cf7de8a01e0db9d4728",
            "authors": "Jo\u02dcao Monteiro, \u00c9tienne Marcotte, Pierre-Andr\u00e9 No\u00ebl, Valentina Zantedeschi, David V\u00e1zquez, Nicolas Chapados, Christopher Pal, Perouz Taslakian",
            "matchScore": 315.30835,
            "original title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
            "original authors": "Joao Monteiro, \u00c9tienne Marcotte, Pierre-Andre Noel, Valentina Zantedeschi, David Vazquez, Nicolas Chapados, Christopher Pal, Perouz Taslakian",
            "EMNLP Paper ID": "2929",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "6bae288e831c26e0fa8724578558daa2cd0f4780",
            "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
            "abstract": "Large language models (LLMs) represent a groundbreaking advancement in the domain of natural language processing due to their impressive reasoning abilities. Recently, there has been considerable interest in increasing the context lengths for these models to enhance their applicability to complex tasks. However, at long context lengths and large batch sizes, the key-value (KV) cache, which stores the attention keys and values, emerges as the new bottleneck in memory usage during inference. To address this, we propose Eigen Attention, which performs the attention operation in a low-rank space, thereby reducing the KV cache memory overhead. Our proposed approach is orthogonal to existing KV cache compression techniques and can be used synergistically with them. Through extensive experiments over OPT, MPT, and Llama model families, we demonstrate that Eigen Attention results in up to 40% reduction in KV cache sizes and up to 60% reduction in attention operation latency with minimal drop in performance.",
            "link": "https://www.semanticscholar.org/paper/6bae288e831c26e0fa8724578558daa2cd0f4780",
            "authors": "Utkarsh Saxena, Gobinda Saha, Sakshi Choudhary, Kaushik Roy",
            "matchScore": 306.11346,
            "original title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
            "original authors": "Utkarsh Saxena, Gobinda Saha, Sakshi Choudhary, Kaushik Roy",
            "EMNLP Paper ID": "2941",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "4f086c9c995ed72afa7f58e455f9f0e6c48fa937",
            "title": "EAVE: Efficient Product Attribute Value Extraction via Lightweight Sparse-layer Interaction",
            "abstract": "Product attribute value extraction involves identifying the specific values associated with various attributes from a product profile. While existing methods often prioritize the development of effective models to improve extraction performance, there has been limited emphasis on extraction efficiency. However, in real-world scenarios, products are typically associated with multiple attributes, necessitating multiple extractions to obtain all corresponding values. In this work, we propose an Efficient product Attribute Value Extraction (EAVE) approach via lightweight sparse-layer interaction. Specifically, we employ a heavy encoder to separately encode the product context and attribute. The resulting non-interacting heavy representations of the context can be cached and reused for all attributes. Additionally, we introduce a light encoder to jointly encode the context and the attribute, facilitating lightweight interactions between them. To enrich the interaction within the lightweight encoder, we design a sparse-layer interaction module to fuse the non-interacting heavy representation into the lightweight encoder. Comprehensive evaluation on two benchmarks demonstrate that our method achieves significant efficiency gains with neutral or marginal loss in performance when the context is long and number of attributes is large. Our code is available \\href{https://anonymous.4open.science/r/EAVE-EA18}{here}.",
            "link": "https://www.semanticscholar.org/paper/4f086c9c995ed72afa7f58e455f9f0e6c48fa937",
            "authors": "Li Yang, Qifan Wang, Jianfeng Chi, Jiahao Liu, Jingang Wang, Fuli Feng, Zenglin Xu, Yi Fang, Lifu Huang, Dongfang Liu",
            "matchScore": 336.18088,
            "original title": "EAVE: Efficient Product Attribute Value Extraction via Lightweight Sparse-layer Interaction",
            "original authors": "Li Yang, Qifan Wang, Jianfeng Chi, Jiahao Liu, Jingang Wang, Fuli Feng, Zenglin Xu, Yi Fang, Lifu Huang, Dongfang Liu",
            "EMNLP Paper ID": "298",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "67af926a717625476f50cf08466ad8e3c644f5c7",
            "title": "In-Context Former: Lightning-fast Compressing Context for Large Language Model",
            "abstract": "With the rising popularity of Transformer-based large language models (LLMs), reducing their high inference costs has become a significant research focus. One effective approach is to compress the long input contexts. Existing methods typically leverage the self-attention mechanism of the LLM itself for context compression. While these methods have achieved notable results, the compression process still involves quadratic time complexity, which limits their applicability. To mitigate this limitation, we propose the In-Context Former (IC-Former). Unlike previous methods, IC-Former does not depend on the target LLMs. Instead, it leverages the cross-attention mechanism and a small number of learnable digest tokens to directly condense information from the contextual word embeddings. This approach significantly reduces inference time, which achieves linear growth in time complexity within the compression range. Experimental results indicate that our method requires only 1/32 of the floating-point operations of the baseline during compression and improves processing speed by 68 to 112 times while achieving over 90% of the baseline performance on evaluation metrics. Overall, our model effectively reduces compression costs and makes real-time compression scenarios feasible.",
            "link": "https://www.semanticscholar.org/paper/67af926a717625476f50cf08466ad8e3c644f5c7",
            "authors": "Xiangfeng Wang, Zaiyi Chen, Zheyong Xie, Tong Xu, Yongyi He, Enhong Chen",
            "matchScore": 294.0963,
            "original title": "In-Context Former: Lightning-fast Compressing Context for Large Language Model",
            "original authors": "Xiangfeng Wang, Zaiyi Chen, Tong Xu, Zheyong Xie, Yongyi He, Enhong Chen",
            "EMNLP Paper ID": "507",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "6c5e09cef64fe7fbeab9a6f3f062363bffba917d",
            "title": "LOOK-M: Look-Once Optimization in KV Cache for Efficient Multimodal Long-Context Inference",
            "abstract": "Long-context Multimodal Large Language Models (MLLMs) demand substantial computational resources for inference as the growth of their multimodal Key-Value (KV) cache, in response to increasing input lengths, challenges memory and time efficiency. Unlike single-modality LLMs that manage only textual contexts, the KV cache of long-context MLLMs includes representations from multiple images with temporal and spatial relationships and related textual contexts. The predominance of image tokens means traditional optimizations for LLMs' KV caches are unsuitable for multimodal long-context settings, and no prior works have addressed this challenge. In this work, we introduce LOOK-M, a pioneering, fine-tuning-free approach that efficiently reduces the multimodal KV cache size while maintaining performance comparable to a full cache. We observe that during prompt prefill, the model prioritizes more textual attention over image features, and based on the multimodal interaction observation, a new proposed text-prior method is explored to compress the KV cache. Furthermore, to mitigate the degradation of image contextual information, we propose several compensatory strategies using KV pairs merging. LOOK-M demonstrates that with a significant reduction in KV Cache memory usage, such as reducing it by 80% in some cases, it not only achieves up to 1.5x faster decoding but also maintains or even enhances performance across a variety of long context multimodal tasks.",
            "link": "https://www.semanticscholar.org/paper/6c5e09cef64fe7fbeab9a6f3f062363bffba917d",
            "authors": "Zhongwei Wan, Ziang Wu, Che Liu, Jinfa Huang, Zhihong Zhu, Peng Jin, Longyue Wang, Li Yuan",
            "matchScore": 346.44128,
            "original title": "LOOK-M: Look-Once Optimization in KV Cache for Efficient Multimodal Long-Context Inference",
            "original authors": "Zhongwei Wan, ZiangWu, Che Liu, Jinfa Huang, Zhihong Zhu, Peng Jin, Longyue Wang, Li Yuan",
            "EMNLP Paper ID": "804",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "fbfe920579cc1c13358521d403cfce31f2afbead",
            "title": "KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches",
            "abstract": "Long context capability is a crucial competency for large language models (LLMs) as it mitigates the human struggle to digest long-form texts. This capability enables complex task-solving scenarios such as book summarization, code assistance, and many more tasks that are traditionally manpower-intensive. However, transformer-based LLMs face significant challenges with long context input due to the growing size of the KV cache and the intrinsic complexity of attending to extended inputs; where multiple schools of efficiency-driven approaches - such as KV cache quantization, token dropping, prompt compression, linear-time sequence models, and hybrid architectures - have been proposed to produce efficient yet long context-capable models. Despite these advancements, no existing work has comprehensively benchmarked these methods in a reasonably aligned environment. In this work, we fill this gap by providing a taxonomy of current methods and evaluating 10+ state-of-the-art approaches across seven categories of long context tasks. Our work reveals numerous previously unknown phenomena and offers insights - as well as a friendly workbench - for the future development of long context-capable LLMs. The source code is available at https://github.com/henryzhongsc/longctx_bench.",
            "link": "https://www.semanticscholar.org/paper/fbfe920579cc1c13358521d403cfce31f2afbead",
            "authors": "Jiayi Yuan, Hongyi Liu, Shaochen Zhong, Yu-Neng Chuang, Songchen Li, Guanchu Wang, Duy Le, Hongye Jin, V. Chaudhary, Zhaozhuo Xu, Zirui Liu, Xia Hu",
            "matchScore": 349.03012,
            "original title": "KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches",
            "original authors": "Jiayi Yuan, Hongyi Liu, Shaochen Zhong, Yu-Neng Chuang, Songchen Li, Guanchu Wang, Duy Le, Hongye Jin, Vipin Chaudhary, Zhaozhuo Xu, Zirui Liu, Xia Hu",
            "EMNLP Paper ID": "918",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        }
    ],
    "Knowledge Editing in Large Language Models": [
        {
            "paperId": "4d04fa53fa9cd499c7070dae5ff3bc1e3357715b",
            "title": "Why Does New Knowledge Create Messy Ripple Effects in LLMs?",
            "abstract": "Extensive previous research has focused on post-training knowledge editing (KE) for language models (LMs) to ensure that knowledge remains accurate and up-to-date. One desired property and open question in KE is to let edited LMs correctly handle ripple effects, where LM is expected to answer its logically related knowledge accurately. In this paper, we answer the question of why most KE methods still create messy ripple effects. We conduct extensive analysis and identify a salient indicator, GradSim, that effectively reveals when and why updated knowledge ripples in LMs. GradSim is computed by the cosine similarity between gradients of the original fact and its related knowledge. We observe a strong positive correlation between ripple effect performance and GradSim across different LMs, KE methods, and evaluation metrics. Further investigations into three counter-intuitive failure cases (Negation, Over-Ripple, Multi-Lingual) of ripple effects demonstrate that these failures are often associated with very low GradSim. This finding validates that GradSim is an effective indicator of when knowledge ripples in LMs.",
            "link": "https://www.semanticscholar.org/paper/4d04fa53fa9cd499c7070dae5ff3bc1e3357715b",
            "authors": "Jiaxin Qin, Zixuan Zhang, Chi Han, Manling Li, Pengfei Yu, Heng Ji",
            "EMNLP Paper ID": "1463",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "b6333e824db2c6364250362eaab40fad770d7603",
            "title": "Lifelong Knowledge Editing for LLMs with Retrieval-Augmented Continuous Prompt Learning",
            "abstract": "Model editing aims to correct outdated or erroneous knowledge in large language models (LLMs) without the need for costly retraining. Lifelong model editing is the most challenging task that caters to the continuous editing requirements of LLMs. Prior works primarily focus on single or batch editing; nevertheless, these methods fall short in lifelong editing scenarios due to catastrophic knowledge forgetting and the degradation of model performance. Although retrieval-based methods alleviate these issues, they are impeded by slow and cumbersome processes of integrating the retrieved knowledge into the model. In this work, we introduce RECIPE, a RetriEval-augmented ContInuous Prompt lEarning method, to boost editing efficacy and inference efficiency in lifelong learning. RECIPE first converts knowledge statements into short and informative continuous prompts, prefixed to the LLM's input query embedding, to efficiently refine the response grounded on the knowledge. It further integrates the Knowledge Sentinel (KS) that acts as an intermediary to calculate a dynamic threshold, determining whether the retrieval repository contains relevant knowledge. Our retriever and prompt encoder are jointly trained to achieve editing properties, i.e., reliability, generality, and locality. In our experiments, RECIPE is assessed extensively across multiple LLMs and editing datasets, where it achieves superior editing performance. RECIPE also demonstrates its capability to maintain the overall performance of LLMs alongside showcasing fast editing and inference speed.",
            "link": "https://www.semanticscholar.org/paper/b6333e824db2c6364250362eaab40fad770d7603",
            "authors": "Qizhou Chen, Taolin Zhang, Dongyang Li, Longtao Huang, Hui Xue, Chengyu Wang, Xiaofeng He",
            "EMNLP Paper ID": "1565",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 12): 17:45\u201318:45 (Evening)",
            "Type": "Main"
        },
        {
            "paperId": "fe2303d338109be0f25bcf5479f5a2cb88255299",
            "title": "Consecutive Batch Model Editing with HooK Layers",
            "abstract": "As the typical retraining paradigm is unacceptably time- and resource-consuming, researchers are turning to model editing to find an effective way that supports both consecutive and batch scenarios to edit the model behavior directly. Despite all these practical expectations, existing model editing methods fail to realize all of them. Furthermore, the memory demands for such sequential model editing approaches tend to be prohibitive, frequently necessitating an external memory that grows incrementally over time. To cope with these challenges, we propose CoachHooK, a model editing method that simultaneously supports sequential and batch editing. CoachHooK is memory-friendly as it only needs a small amount of it to store several hook layers whose size remains unchanged over time. Experimental results demonstrate the superiority of our method over other batch-supportive model editing methods under both single-round and consecutive batch editing scenarios. Extensive analyses of CoachHooK have been conducted to verify the stability of our method over a number of consecutive steps.",
            "link": "https://www.semanticscholar.org/paper/fe2303d338109be0f25bcf5479f5a2cb88255299",
            "authors": "Shuaiyi Li, Yang Deng, Deng Cai, Hongyuan Lu, Liang Chen, Wai Lam",
            "EMNLP Paper ID": "1592",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "4098bc3bfaa4c8f5e92f5e55670234e29d80a63c",
            "title": "Are Large Language Models Good Classifiers? A Study on Edit Intent Classification in Scientific Document Revisions",
            "abstract": "Classification is a core NLP task architecture with many potential applications. While large language models (LLMs) have brought substantial advancements in text generation, their potential for enhancing classification tasks remains underexplored. To address this gap, we propose a framework for thoroughly investigating fine-tuning LLMs for classification, including both generation- and encoding-based approaches. We instantiate this framework in edit intent classification (EIC), a challenging and underexplored classification task. Our extensive experiments and systematic comparisons with various training approaches and a representative selection of LLMs yield new insights into their application for EIC. We investigate the generalizability of these findings on five further classification tasks. To demonstrate the proposed methods and address the data shortage for empirical edit analysis, we use our best-performing EIC model to create Re3-Sci2.0, a new large-scale dataset of 1,780 scientific document revisions with over 94k labeled edits. The quality of the dataset is assessed through human evaluation. The new dataset enables an in-depth empirical study of human editing behavior in academic writing. We make our experimental framework, models and data publicly available.",
            "link": "https://www.semanticscholar.org/paper/4098bc3bfaa4c8f5e92f5e55670234e29d80a63c",
            "authors": "Qian Ruan, Ilia Kuznetsov, Iryna Gurevych",
            "EMNLP Paper ID": "1751",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "5040acef429108015991dc5480f1cedf6851f0b7",
            "title": "AKEW: Assessing Knowledge Editing in the Wild",
            "abstract": "Knowledge editing injects knowledge updates into language models to keep them correct and up-to-date. However, its current evaluations deviate significantly from practice: their knowledge updates solely consist of structured facts derived from meticulously crafted datasets, instead of practical sources -- unstructured texts like news articles, and they often overlook practical real-world knowledge updates. To address these issues, in this paper we propose AKEW (Assessing Knowledge Editing in the Wild), a new practical benchmark for knowledge editing. AKEW fully covers three editing settings of knowledge updates: structured facts, unstructured texts as facts, and extracted triplets. It further introduces new datasets featuring both counterfactual and real-world knowledge updates. Through extensive experiments, we demonstrate the considerable gap between state-of-the-art knowledge-editing methods and practical scenarios. Our analyses further highlight key insights to motivate future research for practical knowledge editing.",
            "link": "https://www.semanticscholar.org/paper/5040acef429108015991dc5480f1cedf6851f0b7",
            "authors": "Xiaobao Wu, Liangming Pan, W. Wang, A. Luu",
            "EMNLP Paper ID": "1762",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "a3e01da0b01909d4555d643c03a7f9d674feba02",
            "title": "On the Robustness of Editing Large Language Models",
            "abstract": "Large language models (LLMs) have played a pivotal role in building communicative AI, yet they encounter the challenge of efficient updates. Model editing enables the manipulation of specific knowledge memories and the behavior of language generation without retraining. However, the robustness of model editing remains an open question. This work seeks to understand the strengths and limitations of editing methods, facilitating practical applications of communicative AI. We focus on three key research questions. RQ1: Can edited LLMs behave consistently resembling communicative AI in realistic situations? RQ2: To what extent does the rephrasing of prompts lead LLMs to deviate from the edited knowledge memory? RQ3: Which knowledge features are correlated with the performance and robustness of editing? Our empirical studies uncover a substantial disparity between existing editing methods and the practical application of LLMs. On rephrased prompts that are flexible but common in realistic applications, the performance of editing experiences a significant decline. Further analysis shows that more popular knowledge is memorized better, easier to recall, and more challenging to edit effectively. Code is publicly available at https://github.com/xbmxb/edit_analysis .",
            "link": "https://www.semanticscholar.org/paper/a3e01da0b01909d4555d643c03a7f9d674feba02",
            "authors": "Xinbei Ma, Tianjie Ju, Jiyang Qiu, Zhuosheng Zhang, Hai Zhao, Lifeng Liu, Yulong Wang",
            "EMNLP Paper ID": "1905",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "d2b48bcee27974c10025422b76c8943ed524722c",
            "title": "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue",
            "abstract": "Model editing is a technique that edits the large language models (LLMs) with updated knowledge to alleviate hallucinations without resource-intensive retraining. While current model editing methods can effectively modify a model's behavior within a specific area of interest, they often overlook the potential unintended side effects on the general abilities of LLMs such as reasoning, natural language inference, and question answering. In this paper, we raise concerns that model editing's improvements on factuality may come at the cost of a significant degradation of the model's general abilities. We systematically analyze the side effects by evaluating four popular editing methods on three LLMs across eight representative tasks. Our extensive empirical experiments show that it is challenging for current editing methods to simultaneously improve factuality of LLMs and maintain their general abilities. Our analysis reveals that the side effects are caused by model editing altering the original model weights excessively, leading to overfitting to the edited facts. To mitigate this, a method named RECT is proposed to regularize the edit update weights by imposing constraints on their complexity based on the RElative Change in weighT. Evaluation results show that RECT can significantly mitigate the side effects of editing while still maintaining over 94% editing performance.",
            "link": "https://www.semanticscholar.org/paper/d2b48bcee27974c10025422b76c8943ed524722c",
            "authors": "Jia-Chen Gu, Haoyang Xu, Jun-Yu Ma, Pan Lu, Zhen-Hua Ling, Kai-Wei Chang, Nanyun Peng",
            "EMNLP Paper ID": "1981",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "a91e6515a73b06bb1e30dce418e00c961775a46a",
            "title": "Rebuilding ROME : Resolving Model Collapse during Sequential Model Editing",
            "abstract": "Recent work using Rank-One Model Editing (ROME), a popular model editing method, has shown that there are certain facts that the algorithm is unable to edit without breaking the model. Such edits have previously been called disabling edits. These disabling edits cause immediate model collapse and limits the use of ROME for sequential editing. In this paper, we show that disabling edits are an artifact of irregularities in the implementation of ROME. With this paper, we provide a more stable implementation ROME, which we call r-ROME and show that model collapse is no longer observed when making large scale sequential edits with r-ROME, while further improving generalization and locality of model editing compared to the original implementation of ROME. We also provide a detailed mathematical explanation of the reason behind disabling edits.",
            "link": "https://www.semanticscholar.org/paper/a91e6515a73b06bb1e30dce418e00c961775a46a",
            "authors": "Akshat Gupta, G. Anumanchipalli",
            "EMNLP Paper ID": "2996",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "9acb2df43927a2df64a4b9d39d68ab0a482d05f5",
            "title": "Knowledge Graph Enhanced Large Language Model Editing",
            "abstract": "Large language models (LLMs) are pivotal in advancing natural language processing (NLP) tasks, yet their efficacy is hampered by inaccuracies and outdated knowledge. Model editing emerges as a promising solution to address these challenges. However, existing editing methods struggle to track and incorporate changes in knowledge associated with edits, which limits the generalization ability of postedit LLMs in processing edited knowledge. To tackle these problems, we propose a novel model editing method that leverages knowledge graphs for enhancing LLM editing, namely GLAME. Specifically, we first utilize a knowledge graph augmentation module to uncover associated knowledge that has changed due to editing, obtaining its internal representations within LLMs. This approach allows knowledge alterations within LLMs to be reflected through an external graph structure. Subsequently, we design a graph-based knowledge edit module to integrate structured knowledge into the model editing. This ensures that the updated parameters reflect not only the modifications of the edited knowledge but also the changes in other associated knowledge resulting from the editing process. Comprehensive experiments conducted on GPT-J and GPT-2 XL demonstrate that GLAME significantly improves the generalization capabilities of post-edit LLMs in employing edited knowledge.",
            "link": "https://www.semanticscholar.org/paper/9acb2df43927a2df64a4b9d39d68ab0a482d05f5",
            "authors": "Mengqi Zhang, Xiaotian Ye, Q. Liu, Pengjie Ren, Shu Wu, Zhumin Chen",
            "EMNLP Paper ID": "3268",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "7626a56ffd47689bd1564e138bd61d0ed5014b75",
            "title": "RIPPLECOT: Amplifying Ripple Effect of Knowledge Editing in Language Models via Chain-of-Thought In-Context Learning",
            "abstract": "The ripple effect poses a significant challenge in knowledge editing for large language models. Namely, when a single fact is edited, the model struggles to accurately update the related facts in a sequence, which is evaluated by multi-hop questions linked to a chain of related facts. Recent strategies have moved away from traditional parameter updates to more flexible, less computation-intensive methods, proven to be more effective in addressing the ripple effect. In-context learning (ICL) editing uses a simple demonstration `Imagine that + new fact` to guide LLMs, but struggles with complex multi-hop questions as the new fact alone fails to specify the chain of facts involved in such scenarios. Besides, memory-based editing maintains additional storage for all edits and related facts, requiring continuous updates to stay effective. As a result of these design limitations, the challenge remains, with the highest accuracy being only 33.8% on the MQuAKE-cf benchmarks for Vicuna-7B. To address this, we propose RippleCOT, a novel ICL editing approach integrating Chain-of-Thought (COT) reasoning. RippleCOT structures demonstrations as `newfact, question, thought, answer`, incorporating a thought component to identify and decompose the multi-hop logic within questions. This approach effectively guides the model through complex multi-hop questions with chains of related facts. Comprehensive experiments demonstrate that RippleCOT significantly outperforms the state-of-the-art on the ripple effect, achieving accuracy gains ranging from 7.8% to 87.1%.",
            "link": "https://www.semanticscholar.org/paper/7626a56ffd47689bd1564e138bd61d0ed5014b75",
            "authors": "Zihao Zhao, Yuchen Yang, Yijiang Li, Yinzhi Cao",
            "matchScore": 329.74902,
            "original title": "RippleCOT: Amplifying Ripple Effect of Knowledge Editing in Language Models via Chain-of-Thought In-Context Learning",
            "original authors": "Zihao Zhao, Yuchen Yang, Yijiang Li, Yinzhi Cao",
            "EMNLP Paper ID": "1295",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "184753b614b35cde3f2e221cea3bc60fe016d29e",
            "title": "Editing Conceptual Knowledge for Large Language Models",
            "abstract": "Recently, there has been a growing interest in knowledge editing for Large Language Models (LLMs). Current approaches and evaluations merely explore the instance-level editing, while whether LLMs possess the capability to modify concepts remains unclear. This paper pioneers the investigation of editing conceptual knowledge for LLMs, by constructing a novel benchmark dataset ConceptEdit and establishing a suite of new metrics for evaluation. The experimental results reveal that, although existing editing methods can efficiently modify concept-level definition to some extent, they also have the potential to distort the related instantial knowledge in LLMs, leading to poor performance. We anticipate this can inspire further progress in better understanding LLMs. Our project homepage is available at https://zjunlp.github.io/project/ConceptEdit.",
            "link": "https://www.semanticscholar.org/paper/184753b614b35cde3f2e221cea3bc60fe016d29e",
            "authors": "Xiaohan Wang, Shengyu Mao, Ningyu Zhang, Shumin Deng, Yunzhi Yao, Yue Shen, Lei Liang, Jinjie Gu, Huajun Chen",
            "matchScore": 170.98451,
            "original title": "Editing Conceptual Knowledge for Large Language Models",
            "original authors": "Xiaohan Wang, Shengyu Mao, Shumin Deng, Yunzhi Yao, YUE SHEN, Lei Liang, Jinjie GU, Huajun Chen, Ningyu Zhang",
            "EMNLP Paper ID": "141",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "ce07487b2944d56a67353a8edbc556ee4b72f4cb",
            "title": "Better Call SAUL: Fluent and Consistent Language Model Editing with Generation Regularization",
            "abstract": "To ensure large language models contain up-to-date knowledge, they need to be updated regularly. However, model editing is challenging as it might also affect knowledge that is unrelated to the new data. State-of-the-art methods identify parameters associated with specific knowledge and then modify them via direct weight updates. However, these locate-and-edit methods suffer from heavy computational overhead and lack theoretical validation. In contrast, directly fine-tuning the model on requested edits affects the model's behavior on unrelated knowledge, and significantly damages the model's generation fluency and consistency. To address these challenges, we propose SAUL, a streamlined model editing method that uses sentence concatenation with augmented random facts for generation regularization. Evaluations on three model editing benchmarks show that SAUL is a practical and reliable solution for model editing outperforming state-of-the-art methods while maintaining generation quality and reducing computational overhead.",
            "link": "https://www.semanticscholar.org/paper/ce07487b2944d56a67353a8edbc556ee4b72f4cb",
            "authors": "Mingyang Wang, Lukas Lange, Heike Adel, Jannik Strotgen, Hinrich Schutze",
            "matchScore": 301.25793,
            "original title": "Better Call SAUL: Fluent and Consistent Language Model Editing with Generation Regularization",
            "original authors": "Mingyang Wang, Lukas Lange, Heike Adel, Jannik Str\u00f6tgen, Hinrich Schuetze",
            "EMNLP Paper ID": "1672",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "55b6f5b907d75d14e0e8f3b6018a43351333e5d9",
            "title": "DyKnow: Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs",
            "abstract": "LLMs acquire knowledge from massive data snapshots collected at different timestamps. Their knowledge is then commonly evaluated using static benchmarks. However, factual knowledge is generally subject to time-sensitive changes, and static benchmarks cannot address those cases. We present an approach to dynamically evaluate the knowledge in LLMs and their time-sensitiveness against Wikidata, a publicly available up-to-date knowledge graph. We evaluate the time-sensitive knowledge in twenty-four private and open-source LLMs, as well as the effectiveness of four editing methods in updating the outdated facts. Our results show that 1) outdatedness is a critical problem across state-of-the-art LLMs; 2) LLMs output inconsistent answers when prompted with slight variations of the question prompt; and 3) the performance of the state-of-the-art knowledge editing algorithms is very limited, as they can not reduce the cases of outdatedness and output inconsistency.",
            "link": "https://www.semanticscholar.org/paper/55b6f5b907d75d14e0e8f3b6018a43351333e5d9",
            "authors": "Seyed Mahed Mousavi, Simone Alghisi, Giuseppe Riccardi",
            "matchScore": 274.3592,
            "original title": "DyKnow: Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs",
            "original authors": "Seyed Mahed Mousavi, Simone Alghisi, giuseppe riccardi",
            "EMNLP Paper ID": "1691",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "3827865d55d09bfbc7a0b777cb87a72f6422cfac",
            "title": "Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of Knowledge Editing in Large Language Models",
            "abstract": "Knowledge editing is a rising technique for efficiently updating factual knowledge in large language models (LLMs) with minimal alteration of parameters. However, recent studies have identified side effects, such as knowledge distortion and the deterioration of general abilities, that have emerged after editing. Despite these findings, evaluating the pitfalls of knowledge editing often relies on inconsistent metrics and benchmarks, lacking a uniform standard. In response, this survey presents a comprehensive study of these side effects, providing a unified perspective on the challenges of knowledge editing in LLMs by conducting experiments with consistent metrics and benchmarks. Additionally, we review related works and outline potential research directions to address these limitations. Our survey highlights the limitations of current knowledge editing methods, emphasizing the need for a deeper understanding of the inner knowledge structures of LLMs and improved knowledge editing methods. To foster future research, we have released the complementary materials publicly in https://github.com/MiuLab/EditLLM-Survey.",
            "link": "https://www.semanticscholar.org/paper/3827865d55d09bfbc7a0b777cb87a72f6422cfac",
            "authors": "Cheng-Hsun Hsueh, Paul Kuo-Ming Huang, Tzu-Han Lin, Che-Wei Liao, Hung-Chieh Fang, Chao-Wei Huang, Yun-Nung Chen",
            "matchScore": 313.01413,
            "original title": "Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of Knowledge Editing in Large Language Models",
            "original authors": "Cheng-Hsun Hsueh, Paul Kuo-Ming Huang, Tzu-Han Lin, CHE WEI LIAO, Hung-Chieh Fang, Chao-Wei Huang, Yun-Nung Chen",
            "EMNLP Paper ID": "1967",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "b92166c63eba57a5d3d225c2db43d81accd4911b",
            "title": "Adaptive Token Biaser: Knowledge Editing via Biasing Key Entities",
            "abstract": "The parametric knowledge memorized by large language models (LLMs) becomes outdated quickly. In-context editing (ICE) is currently the most effective method for updating the knowledge of LLMs. Recent advancements involve enhancing ICE by modifying the decoding strategy, obviating the need for altering internal model structures or adjusting external prompts. However, this enhancement operates across the entire sequence generation, encompassing a plethora of non-critical tokens. In this work, we introduce $\\textbf{A}$daptive $\\textbf{T}$oken $\\textbf{Bias}$er ($\\textbf{ATBias}$), a new decoding technique designed to enhance ICE. It focuses on the tokens that are mostly related to knowledge during decoding, biasing their logits by matching key entities related to new and parametric knowledge. Experimental results show that ATBias significantly enhances ICE performance, achieving up to a 32.3% improvement over state-of-the-art ICE methods while incurring only half the latency. ATBias not only improves the knowledge editing capabilities of ICE but can also be widely applied to LLMs with negligible cost.",
            "link": "https://www.semanticscholar.org/paper/b92166c63eba57a5d3d225c2db43d81accd4911b",
            "authors": "Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei, Hongcheng Gao, Yilong Xu, Xueqi Cheng",
            "matchScore": 296.87198,
            "original title": "Adaptive Token Biaser: Knowledge Editing via Biasing Key Entities",
            "original authors": "Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei, Hongcheng Gao, Yilong Xu, Xueqi Cheng",
            "EMNLP Paper ID": "2205",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "3a626e2adf0e9c39d8e3deb90a9d6c0090c40d9f",
            "title": "Cross-Lingual Multi-Hop Knowledge Editing - Benchmarks, Analysis and a Simple Contrastive Learning based Approach",
            "abstract": "Large language models are often expected to constantly adapt to new sources of knowledge and knowledge editing techniques aim to efficiently patch the outdated model knowledge, with minimal modification. Most prior works focus on monolingual knowledge editing in English, even though new information can emerge in any language from any part of the world. We propose the Cross-Lingual Multi-Hop Knowledge Editing paradigm, for measuring and analyzing the performance of various SoTA knowledge editing techniques in a cross-lingual setup. Specifically, we create a parallel cross-lingual benchmark, CROLIN-MQUAKE for measuring the knowledge editing capabilities. Our extensive analysis over various knowledge editing techniques uncover significant gaps in performance between the cross-lingual and English-centric setting. Following this, we propose a significantly improved system for cross-lingual multi-hop knowledge editing, CLEVER-CKE. CLEVER-CKE is based on a retrieve, verify and generate knowledge editing framework, where a retriever is formulated to recall edited facts and support an LLM to adhere to knowledge edits. We develop language-aware and hard-negative based contrastive objectives for improving the cross-lingual and fine-grained fact retrieval and verification process used in this framework. Extensive experiments on three LLMs, eight languages, and two datasets show CLEVER-CKE's significant gains of up to 30% over prior methods.",
            "link": "https://www.semanticscholar.org/paper/3a626e2adf0e9c39d8e3deb90a9d6c0090c40d9f",
            "authors": "Aditi Khandelwal, Harman Singh, Hengrui Gu, Tianlong Chen, Kaixiong Zhou",
            "matchScore": 301.85995,
            "original title": "Cross-Lingual Multi-Hop Knowledge Editing \u2013 Benchmarks, Analysis and a Simple Contrastive Learning based Approach",
            "original authors": "Aditi Khandelwal, Harman Singh, Hengrui Gu, Tianlong Chen, Kaixiong Zhou",
            "EMNLP Paper ID": "2367",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "5256907fb4128dc7f6f7c918099fb52ecc65450d",
            "title": "Pioneering Reliable Assessment in Text-to-Image Knowledge Editing: Leveraging a Fine-Grained Dataset and an Innovative Criterion",
            "abstract": "During pre-training, the Text-to-Image (T2I) diffusion models encode factual knowledge into their parameters. These parameterized facts enable realistic image generation, but they may become obsolete over time, thereby misrepresenting the current state of the world. Knowledge editing techniques aim to update model knowledge in a targeted way. However, facing the dual challenges posed by inadequate editing datasets and unreliable evaluation criterion, the development of T2I knowledge editing encounter difficulties in effectively generalizing injected knowledge. In this work, we design a T2I knowledge editing framework by comprehensively spanning on three phases: First, we curate a dataset \\textbf{CAKE}, comprising paraphrase and multi-object test, to enable more fine-grained assessment on knowledge generalization. Second, we propose a novel criterion, \\textbf{adaptive CLIP threshold}, to effectively filter out false successful images under the current criterion and achieve reliable editing evaluation. Finally, we introduce \\textbf{MPE}, a simple but effective approach for T2I knowledge editing. Instead of tuning parameters, MPE precisely recognizes and edits the outdated part of the conditioning text-prompt to accommodate the up-to-date knowledge. A straightforward implementation of MPE (Based on in-context learning) exhibits better overall performance than previous model editors. We hope these efforts can further promote faithful evaluation of T2I knowledge editing methods.",
            "link": "https://www.semanticscholar.org/paper/5256907fb4128dc7f6f7c918099fb52ecc65450d",
            "authors": "Hengrui Gu, Kaixiong Zhou, Yili Wang, Ruobing Wang, Xin Wang",
            "matchScore": 362.9126,
            "original title": "Pioneering Reliable Assessment in Text-to-Image Knowledge Editing: Leveraging a Fine-Grained Dataset and an Innovative Criterion",
            "original authors": "Hengrui Gu, Kaixiong Zhou, Yili Wang, Ruobing Wang, Xin Wang",
            "EMNLP Paper ID": "2935",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "65d9e725278da08fa29144b4f9a80d7d31f1c27f",
            "title": "A Unified Framework for Model Editing",
            "abstract": "ROME and MEMIT are largely believed to be two different model editing algorithms, with the major difference between them being the ability to perform batched edits. In this paper, we unify these two algorithms under a single conceptual umbrella, optimizing for the same goal, which we call the preservation-memorization objective. ROME uses an equality constraint to optimize this objective to perform one edit at a time, whereas MEMIT employs a more flexible least-square constraint that allows for batched edits. We generalize ROME and enable batched editing with equality constraint in the form of EMMET - an Equality-constrained Mass Model Editing algorithm for Transformers, a new batched memory-editing algorithm. EMMET can perform batched-edits up to a batch-size of 10,000, with very similar performance to MEMIT across multiple dimensions. With the introduction of EMMET, we truly unify ROME and MEMIT and show that both algorithms are equivalent in terms of their optimization objective, their abilities (singular and batched editing), their model editing performance and their limitations.",
            "link": "https://www.semanticscholar.org/paper/65d9e725278da08fa29144b4f9a80d7d31f1c27f",
            "authors": "Akshat Gupta, Dev Sajnani, G. Anumanchipalli",
            "matchScore": 143.60136,
            "original title": "A Unified Framework for Model Editing",
            "original authors": "Akshat Gupta, Dev Sajnani, Gopala Anumanchipalli",
            "EMNLP Paper ID": "2964",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "508c85097273a26273f6c20ef96ceab629a0d691",
            "title": "Knowledge Editing in Language Models via Adapted Direct Preference Optimization",
            "abstract": "Large Language Models (LLMs) can become outdated over time as they may lack updated world knowledge, leading to factual knowledge errors and gaps. Knowledge Editing (KE) aims to overcome this challenge using weight updates that do not require expensive retraining. We propose treating KE as an LLM alignment problem. Toward this goal, we introduce Knowledge Direct Preference Optimization (KDPO), a variation of the Direct Preference Optimization (DPO) that is more effective for knowledge modifications. Our method is based on an online approach that continually updates the knowledge stored in the model. We use the current knowledge as a negative sample and the new knowledge we want to introduce as a positive sample in a process called DPO. We also use teacher-forcing for negative sample generation and optimize using the positive sample, which helps maintain localized changes. We tested our KE method on various datasets and models, comparing it to several cutting-edge methods, with 100 and 500 sequential edits. Additionally, we conducted an ablation study comparing our method to the standard DPO approach. Our experimental results show that our modified DPO method allows for more refined KE, achieving similar or better performance compared to previous methods.",
            "link": "https://www.semanticscholar.org/paper/508c85097273a26273f6c20ef96ceab629a0d691",
            "authors": "Amit Rozner, Barak Battash, Lior Wolf, Ofir Lindenbaum",
            "matchScore": 249.96811,
            "original title": "Knowledge Editing in Language Models via Adapted Direct Preference Optimization",
            "original authors": "Amit Rozner, Barak Battash, Lior Wolf, Ofir Lindenbaum",
            "EMNLP Paper ID": "936",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 14): 13:00\u201314:00 (Afternoon)",
            "Type": "Findings"
        }
    ],
    "Advancements in Table Understanding and Summarization": [
        {
            "paperId": "261c76cbd1cc6adaf27589091a8720b5ee1cf9d8",
            "title": "Table Question Answering for Low-resourced Indic Languages",
            "abstract": "TableQA is the task of answering questions over tables of structured information, returning individual cells or tables as output. TableQA research has focused primarily on high-resource languages, leaving medium- and low-resource languages with little progress due to scarcity of annotated data and neural models. We address this gap by introducing a fully automatic large-scale tableQA data generation process for low-resource languages with limited budget. We incorporate our data generation method on two Indic languages, Bengali and Hindi, which have no tableQA datasets or models. TableQA models trained on our large-scale datasets outperform state-of-the-art LLMs. We further study the trained models on different aspects, including mathematical reasoning capabilities and zero-shot cross-lingual transfer. Our work is the first on low-resource tableQA focusing on scalable data generation and evaluation procedures. Our proposed data generation method can be applied to any low-resource language with a web presence. We release datasets, models, and code (https://github.com/kolk/Low-Resource-TableQA-Indic-languages).",
            "link": "https://www.semanticscholar.org/paper/261c76cbd1cc6adaf27589091a8720b5ee1cf9d8",
            "authors": "Vaishali Pal, Evangelos Kanoulas, Andrew Yates, M. D. Rijke",
            "EMNLP Paper ID": "6",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "86cbb54b25892c004fab29ab73c4e82cc5d4eb28",
            "title": "Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction",
            "abstract": "The task of condensing large chunks of textual information into concise and structured tables has gained attention recently due to the emergence of Large Language Models (LLMs) and their potential benefit for downstream tasks, such as text summarization and text mining. Previous approaches often generate tables that directly replicate information from the text, limiting their applicability in broader contexts, as text-to-table generation in real-life scenarios necessitates information extraction, reasoning, and integration. However, there is a lack of both datasets and methodologies towards this task. In this paper, we introduce LiveSum, a new benchmark dataset created for generating summary tables of competitions based on real-time commentary texts. We evaluate the performances of state-of-the-art LLMs on this task in both fine-tuning and zero-shot settings, and additionally propose a novel pipeline called $T^3$(Text-Tuple-Table) to improve their performances. Extensive experimental results demonstrate that LLMs still struggle with this task even after fine-tuning, while our approach can offer substantial performance gains without explicit training. Further analyses demonstrate that our method exhibits strong generalization abilities, surpassing previous approaches on several other text-to-table datasets. Our code and data can be found at https://github.com/HKUST-KnowComp/LiveSum-TTT.",
            "link": "https://www.semanticscholar.org/paper/86cbb54b25892c004fab29ab73c4e82cc5d4eb28",
            "authors": "Zheye Deng, Chunkit Chan, Weiqi Wang, Yuxi Sun, Wei Fan, Tianshi Zheng, Yauwai Yim, Yangqiu Song",
            "EMNLP Paper ID": "1051",
            "Oral/Poster": "Oral",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "5aa2eea66c968a6fff6014ede118f2007a62ea1c",
            "title": "Modeling Layout Reading Order as Ordering Relations for Visually-rich Document Understanding",
            "abstract": "Modeling and leveraging layout reading order in visually-rich documents (VrDs) is critical in document intelligence as it captures the rich structure semantics within documents. Previous works typically formulated layout reading order as a permutation of layout elements, i.e. a sequence containing all the layout elements. However, we argue that this formulation does not adequately convey the complete reading order information in the layout, which may potentially lead to performance decline in downstream VrD tasks. To address this issue, we propose to model the layout reading order as ordering relations over the set of layout elements, which have sufficient expressive capability for the complete reading order information. To enable empirical evaluation on methods towards the improved form of reading order prediction (ROP), we establish a comprehensive benchmark dataset including the reading order annotation as relations over layout elements, together with a relation-extraction-based method that outperforms previous methods. Moreover, to highlight the practical benefits of introducing the improved form of layout reading order, we propose a reading-order-relation-enhancing pipeline to improve model performance on any arbitrary VrD task by introducing additional reading order relation inputs. Comprehensive results demonstrate that the pipeline generally benefits downstream VrD tasks: (1) with utilizing the reading order relation information, the enhanced downstream models achieve SOTA results on both two task settings of the targeted dataset; (2) with utilizing the pseudo reading order information generated by the proposed ROP model, the performance of the enhanced models has improved across all three models and eight cross-domain VrD-IE/QA task settings without targeted optimization.",
            "link": "https://www.semanticscholar.org/paper/5aa2eea66c968a6fff6014ede118f2007a62ea1c",
            "authors": "Chong Zhang, Yi Tu, Yixi Zhao, Chenshu Yuan, Huan Chen, Yue Zhang, Mingxu Chai, Ya Guo, Huijia Zhu, Qi Zhang, Tao Gui",
            "EMNLP Paper ID": "1078",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "9c133bce1fe4bd19b33c3052ccb5f345d6043f8c",
            "title": "MMoE: Enhancing Multimodal Models with Mixtures of Multimodal Interaction Experts",
            "abstract": "Advances in multimodal models have greatly improved how interactions relevant to various tasks are modeled. Today's multimodal models mainly focus on the correspondence between images and text, using this for tasks like image-text matching. However, this covers only a subset of real-world interactions. Novel interactions, such as sarcasm expressed through opposing spoken words and gestures or humor expressed through utterances and tone of voice, remain challenging. In this paper, we introduce an approach to enhance multimodal models, which we call Multimodal Mixtures of Experts (MMoE). The key idea in MMoE is to train separate expert models for each type of multimodal interaction, such as redundancy present in both modalities, uniqueness in one modality, or synergy that emerges when both modalities are fused. On a sarcasm detection task (MUStARD) and a humor detection task (URFUNNY), we obtain new state-of-the-art results. MMoE is also able to be applied to various types of models to gain improvement.",
            "link": "https://www.semanticscholar.org/paper/9c133bce1fe4bd19b33c3052ccb5f345d6043f8c",
            "authors": "Haofei Yu, Zhengyang Qi, Lawrence Jang, R. Salakhutdinov, Louis-Philippe Morency, P. Liang",
            "EMNLP Paper ID": "1119",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "4b311ca4cd2be91cc9a29be5c11192d857facc3a",
            "title": "SpreadsheetLLM: Encoding Spreadsheets for Large Language Models",
            "abstract": "Spreadsheets, with their extensive two-dimensional grids, various layouts, and diverse formatting options, present notable challenges for large language models (LLMs). In response, we introduce SpreadsheetLLM, pioneering an efficient encoding method designed to unleash and optimize LLMs' powerful understanding and reasoning capability on spreadsheets. Initially, we propose a vanilla serialization approach that incorporates cell addresses, values, and formats. However, this approach was limited by LLMs' token constraints, making it impractical for most applications. To tackle this challenge, we develop SheetCompressor, an innovative encoding framework that compresses spreadsheets effectively for LLMs. It comprises three modules: structural-anchor-based compression, inverse index translation, and data-format-aware aggregation. It significantly improves performance in spreadsheet table detection task, outperforming the vanilla approach by 25.6% in GPT4's in-context learning setting. Moreover, fine-tuned LLM with SheetCompressor has an average compression ratio of 25 times, but achieves a state-of-the-art 78.9% F1 score, surpassing the best existing models by 12.3%. Finally, we propose Chain of Spreadsheet for downstream tasks of spreadsheet understanding and validate in a new and demanding spreadsheet QA task. We methodically leverage the inherent layout and structure of spreadsheets, demonstrating that SpreadsheetLLM is highly effective across a variety of spreadsheet tasks.",
            "link": "https://www.semanticscholar.org/paper/4b311ca4cd2be91cc9a29be5c11192d857facc3a",
            "authors": "Yuzhang Tian, Jianbo Zhao, Haoyu Dong, Junyu Xiong, Shiyu Xia, Mengyu Zhou, Yun Lin, Jos'e Cambronero, Yeye He, Shi Han, Dongmei Zhang",
            "EMNLP Paper ID": "2752",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "85c3f843490abb87c0e97a388858bccbcefd3f3f",
            "title": "Is this a bad table? A Closer Look at the Evaluation of Table Generation from Text",
            "abstract": "Understanding whether a generated table is of good quality is important to be able to use it in creating or editing documents using automatic methods. In this work, we underline that existing measures for table quality evaluation fail to capture the overall semantics of the tables, and sometimes unfairly penalize good tables and reward bad ones. We propose TabEval, a novel table evaluation strategy that captures table semantics by first breaking down a table into a list of natural language atomic statements and then compares them with ground truth statements using entailment-based measures. To validate our approach, we curate a dataset comprising of text descriptions for 1,250 diverse Wikipedia tables, covering a range of topics and structures, in contrast to the limited scope of existing datasets. We compare TabEval with existing metrics using unsupervised and supervised text-to-table generation methods, demonstrating its stronger correlation with human judgments of table quality across four datasets.",
            "link": "https://www.semanticscholar.org/paper/85c3f843490abb87c0e97a388858bccbcefd3f3f",
            "authors": "Pritika Ramu, Aparna Garimella, Sambaran Bandyopadhyay",
            "EMNLP Paper ID": "3149",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "86189148e6890e77d8a5dd9c7be499db2bb88d74",
            "title": "UniMEEC: Towards Unified Multimodal Emotion Recognition and Emotion Cause",
            "abstract": "Multimodal emotion recognition in conversation (MERC) and multimodal emotion-cause pair extraction (MECPE) have recently garnered significant attention. Emotions are the expression of affect or feelings; responses to specific events, or situations -- known as emotion causes. Both collectively explain the causality between human emotion and intents. However, existing works treat emotion recognition and emotion cause extraction as two individual problems, ignoring their natural causality. In this paper, we propose a Unified Multimodal Emotion recognition and Emotion-Cause analysis framework (UniMEEC) to explore the causality between emotion and emotion cause. Concretely, UniMEEC reformulates the MERC and MECPE tasks as mask prediction problems and unifies them with a causal prompt template. To differentiate the modal effects, UniMEEC proposes a multimodal causal prompt to probe the pre-trained knowledge specified to modality and implements cross-task and cross-modality interactions under task-oriented settings. Experiment results on four public benchmark datasets verify the model performance on MERC and MECPE tasks and achieve consistent improvements compared with the previous state-of-the-art methods.",
            "link": "https://www.semanticscholar.org/paper/86189148e6890e77d8a5dd9c7be499db2bb88d74",
            "authors": "Guimin Hu, Zhihong Zhu, Daniel Hershcovich, Hasti Seifi, Jiayuan Xie",
            "matchScore": 253.55869,
            "original title": "UniMEEC: Towards Unified Multimodal Emotion Recognition and Emotion Cause",
            "original authors": "Guimin Hu, Zhihong Zhu, Daniel Hershcovich, Lijie Hu, Hasti Seifi, Jiayuan Xie",
            "EMNLP Paper ID": "1043",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "313c43a4c62fa8223fc8fa0b8b40f7a4e6f1dca8",
            "title": "LOCR: Location-Guided Transformer for Optical Character Recognition",
            "abstract": "Academic documents are packed with texts, equations, tables, and figures, requiring comprehensive understanding for accurate Optical Character Recognition (OCR). While end-to-end OCR methods offer improved accuracy over layout-based approaches, they often grapple with significant repetition issues, especially with complex layouts in Out-Of-Domain (OOD) documents.To tackle this issue, we propose LOCR, a model that integrates location guiding into the transformer architecture during autoregression. We train the model on a dataset comprising over 77M text-location pairs from 125K academic document pages, including bounding boxes for words, tables and mathematical symbols. LOCR adeptly handles various formatting elements and generates content in Markdown language. It outperforms all existing methods in our test set constructed from arXiv, as measured by edit distance, BLEU, METEOR and F-measure.LOCR also reduces repetition frequency from 4.4% of pages to 0.5% in the arXiv dataset, from 13.2% to 1.3% in OOD quantum physics documents and from 8.1% to 1.8% in OOD marketing documents. Additionally, LOCR features an interactive OCR mode, facilitating the generation of complex documents through a few location prompts from human.",
            "link": "https://www.semanticscholar.org/paper/313c43a4c62fa8223fc8fa0b8b40f7a4e6f1dca8",
            "authors": "Yu Sun, Dongzhan Zhou, Chen Lin, Conghui He, Wanli Ouyang, Han-sen Zhong",
            "matchScore": 251.94121,
            "original title": "LOCR: Location-Guided Transformer for Optical Character Recognition",
            "original authors": "Yu Sun, Dongzhan Zhou, Chen Lin, Conghui He, Wanli Ouyang, Han-Sen Zhong",
            "EMNLP Paper ID": "1094",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "fce3a56cf9359911fa77039560fc60f3611bfa46",
            "title": "UniTabNet: Bridging Vision and Language Models for Enhanced Table Structure Recognition",
            "abstract": "In the digital era, table structure recognition technology is a critical tool for processing and analyzing large volumes of tabular data. Previous methods primarily focus on visual aspects of table structure recovery but often fail to effectively comprehend the textual semantics within tables, particularly for descriptive textual cells. In this paper, we introduce UniTabNet, a novel framework for table structure parsing based on the image-to-text model. UniTabNet employs a ``divide-and-conquer'' strategy, utilizing an image-to-text model to decouple table cells and integrating both physical and logical decoders to reconstruct the complete table structure. We further enhance our framework with the Vision Guider, which directs the model's focus towards pertinent areas, thereby boosting prediction accuracy. Additionally, we introduce the Language Guider to refine the model's capability to understand textual semantics in table images. Evaluated on prominent table structure datasets such as PubTabNet, PubTables1M, WTW, and iFLYTAB, UniTabNet achieves a new state-of-the-art performance, demonstrating the efficacy of our approach. The code will also be made publicly available.",
            "link": "https://www.semanticscholar.org/paper/fce3a56cf9359911fa77039560fc60f3611bfa46",
            "authors": "Zhenrong Zhang, Shuhang Liu, Pengfei Hu, Jie Ma, Jun Du, Jianshu Zhang, Yu Hu",
            "matchScore": 269.8344,
            "original title": "UniTabNet: Bridging Vision and Language Models for Enhanced Table Structure Recognition",
            "original authors": "Zhenrong Zhang, Shuhang Liu, Pengfei Hu, Jiefeng Ma, Jun Du, Jianshu Zhang, Yu Hu",
            "EMNLP Paper ID": "1269",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "256fd76bbff33f687119a1255f8bd17cd2e46294",
            "title": "MMUTF: Multimodal Multimedia Event Argument Extraction with Unified Template Filling",
            "abstract": "With the advancement of multimedia technologies, news documents and user-generated content are often represented as multiple modalities, making Multimedia Event Extraction (MEE) an increasingly important challenge. However, recent MEE methods employ weak alignment strategies and data augmentation with simple classification models, which ignore the capabilities of natural language-formulated event templates for the challenging Event Argument Extraction (EAE) task. In this work, we focus on EAE and address this issue by introducing a unified template filling model that connects the textual and visual modalities via textual prompts. This approach enables the exploitation of cross-ontology transfer and the incorporation of event-specific semantics. Experiments on the M2E2 benchmark demonstrate the effectiveness of our approach. Our system surpasses the current SOTA on textual EAE by +7% F1, and performs generally better than the second-best systems for multimedia EAE.",
            "link": "https://www.semanticscholar.org/paper/256fd76bbff33f687119a1255f8bd17cd2e46294",
            "authors": "Philipp Seeberger, Dominik Wagner, K. Riedhammer",
            "matchScore": 307.4027,
            "original title": "MMUTF: Multimodal Multimedia Event Argument Extraction with Unified Template Filling",
            "original authors": "Philipp Seeberger, Dominik Wagner, Korbinian Riedhammer",
            "EMNLP Paper ID": "1330",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "00a67af3b7dc785b4813b61d232cc76b4fb2b189",
            "title": "TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning",
            "abstract": "Table reasoning tasks have shown remarkable progress with the development of large language models (LLMs), which involve interpreting and drawing conclusions from tabular data based on natural language (NL) questions. Existing solutions mainly tested on smaller tables face scalability issues and struggle with complex queries due to incomplete or dispersed data across different table sections. To alleviate these challenges, we propose TAP4LLM as a versatile pre-processor suite for leveraging LLMs in table-based tasks effectively. It covers several distinct components: (1) table sampling to decompose large tables into manageable sub-tables based on query semantics, (2) table augmentation to enhance tables with additional knowledge from external sources or models, and (3) table packing&serialization to convert tables into various formats suitable for LLMs' understanding. In each module, we design and compare several common methods under various usage scenarios, aiming to shed light on the best practices for leveraging LLMs for table-reasoning tasks. Our experiments show that our method improves LLMs' reasoning capabilities in various tabular tasks and enhances the interaction between LLMs and tabular data by employing effective pre-processing.",
            "link": "https://www.semanticscholar.org/paper/00a67af3b7dc785b4813b61d232cc76b4fb2b189",
            "authors": "Yuan Sui, Jiaru Zou, Mengyu Zhou, Xinyi He, Lun Du, Shi Han, Dongmei Zhang",
            "matchScore": 338.75568,
            "original title": "TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning",
            "original authors": "Yuan Sui, Jiaru Zou, Mengyu Zhou, Xinyi He, Lun Du, Shi Han, Dongmei Zhang",
            "EMNLP Paper ID": "2102",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "279094413f8be6c75ac90ed9fa2066819007f13b",
            "title": "Unveiling Implicit Table Knowledge with Question-Then-Pinpoint Reasoner for Insightful Table Summarization",
            "abstract": "Implicit knowledge hidden within the explicit table cells, such as data insights, is the key to generating a high-quality table summary. However, unveiling such implicit knowledge is a non-trivial task. Due to the complex nature of structured tables, it is challenging even for large language models (LLMs) to mine the implicit knowledge in an insightful and faithful manner. To address this challenge, we propose a novel table reasoning framework Question-then-Pinpoint. Our work focuses on building a plug-and-play table reasoner that can self-question the insightful knowledge and answer it by faithfully pinpointing evidence on the table to provide explainable guidance for the summarizer. To train a reliable reasoner, we collect table knowledge by guiding a teacher LLM to follow the coarse-to-fine reasoning paths and refine it through two quality enhancement strategies to selectively distill the high-quality knowledge to the reasoner. Extensive experiments on two table summarization datasets, including our newly proposed InsTaSumm, validate the general effectiveness of our framework.",
            "link": "https://www.semanticscholar.org/paper/279094413f8be6c75ac90ed9fa2066819007f13b",
            "authors": "Kwangwook Seo, Jinyoung Yeo, Dongha Lee",
            "matchScore": 330.08636,
            "original title": "Unveiling Implicit Table Knowledge with Question-Then-Pinpoint Reasoner for Insightful Table Summarization",
            "original authors": "Kwangwook Seo, Jinyoung Yeo, Dongha Lee",
            "EMNLP Paper ID": "2414",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "43f890b814b808ed7e5a8e326205b7cdd50975f7",
            "title": "Knowledge-Aware Reasoning over Multimodal Semi-structured Tables",
            "abstract": "Existing datasets for tabular question answering typically focus exclusively on text within cells. However, real-world data is inherently multimodal, often blending images such as symbols, faces, icons, patterns, and charts with textual content in tables. With the evolution of AI models capable of multimodal reasoning, it is pertinent to assess their efficacy in handling such structured data. This study investigates whether current AI models can perform knowledge-aware reasoning on multimodal structured data. We explore their ability to reason on tables that integrate both images and text, introducing MMTabQA, a new dataset designed for this purpose. Our experiments highlight substantial challenges for current AI models in effectively integrating and interpreting multiple text and image inputs, understanding visual context, and comparing visual content across images. These findings establish our dataset as a robust benchmark for advancing AI's comprehension and capabilities in analyzing multimodal structured data.",
            "link": "https://www.semanticscholar.org/paper/43f890b814b808ed7e5a8e326205b7cdd50975f7",
            "authors": "Suyash Vardhan Mathur, J. Bafna, Kunal Kartik, Harshita Khandelwal, Manish Shrivastava, Vivek Gupta, Mohit Bansal, Dan Roth",
            "matchScore": 265.40112,
            "original title": "Knowledge-Aware Reasoning over Multimodal Semi-structured Tables",
            "original authors": "Suyash Vardhan Mathur, Jainit Sushil Bafna, Kunal Kartik, Harshita Khandelwal, Manish Shrivastava, Vivek Gupta, Mohit Bansal, Dan Roth",
            "EMNLP Paper ID": "2724",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "7f6c0dbe409fafb6541d4955a0a5e1796a491cd3",
            "title": "Knowledge-Guided Dynamic Modality Attention Fusion Framework for Multimodal Sentiment Analysis",
            "abstract": "Multimodal Sentiment Analysis (MSA) utilizes multimodal data to infer the users' sentiment. Previous methods focus on equally treating the contribution of each modality or statically using text as the dominant modality to conduct interaction, which neglects the situation where each modality may become dominant. In this paper, we propose a Knowledge-Guided Dynamic Modality Attention Fusion Framework (KuDA) for multimodal sentiment analysis. KuDA uses sentiment knowledge to guide the model dynamically selecting the dominant modality and adjusting the contributions of each modality. In addition, with the obtained multimodal representation, the model can further highlight the contribution of dominant modality through the correlation evaluation loss. Extensive experiments on four MSA benchmark datasets indicate that KuDA achieves state-of-the-art performance and is able to adapt to different scenarios of dominant modality.",
            "link": "https://www.semanticscholar.org/paper/7f6c0dbe409fafb6541d4955a0a5e1796a491cd3",
            "authors": "Xinyu Feng, Yuming Lin, Lihua He, You Li, Liang Chang, Ya Zhou",
            "matchScore": 292.84326,
            "original title": "Knowledge-Guided Dynamic Modality Attention Fusion Framework for Multimodal Sentiment Analysis",
            "original authors": "Xinyu Feng, Yuming Lin, Lihua He, You Li, Liang Chang, Ya Zhou",
            "EMNLP Paper ID": "2838",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "6e2a7acd06f3ce26cc0ec77ea42bdc15c7559298",
            "title": "SynTQA: Synergistic Table-based Question Answering via Mixture of Text-to-SQL and E2E TQA",
            "abstract": "Text-to-SQL parsing and end-to-end question answering (E2E TQA) are two main approaches for Table-based Question Answering task. Despite success on multiple benchmarks, they have yet to be compared and their synergy remains unexplored. In this paper, we identify different strengths and weaknesses through evaluating state-of-the-art models on benchmark datasets: Text-to-SQL demonstrates superiority in handling questions involving arithmetic operations and long tables; E2E TQA excels in addressing ambiguous questions, non-standard table schema, and complex table contents. To combine both strengths, we propose a Synergistic Table-based Question Answering approach that integrate different models via answer selection, which is agnostic to any model types. Further experiments validate that ensembling models by either feature-based or LLM-based answer selector significantly improves the performance over individual models.",
            "link": "https://www.semanticscholar.org/paper/6e2a7acd06f3ce26cc0ec77ea42bdc15c7559298",
            "authors": "Siyue Zhang, A. Luu, Chen Zhao",
            "matchScore": 362.59198,
            "original title": "SynTQA: Synergistic Table-based Question Answering via Mixture of Text-to-SQL and E2E TQA",
            "original authors": "Siyue Zhang, Anh Tuan Luu, Chen Zhao",
            "EMNLP Paper ID": "479",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "15ef0417570a190241caac0615eabdff11abb4de",
            "title": "mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding",
            "abstract": "Structure information is critical for understanding the semantics of text-rich images, such as documents, tables, and charts. Existing Multimodal Large Language Models (MLLMs) for Visual Document Understanding are equipped with text recognition ability but lack general structure understanding abilities for text-rich document images. In this work, we emphasize the importance of structure information in Visual Document Understanding and propose the Unified Structure Learning to boost the performance of MLLMs. Our Unified Structure Learning comprises structure-aware parsing tasks and multi-grained text localization tasks across 5 domains: document, webpage, table, chart, and natural image. To better encode structure information, we design a simple and effective vision-to-text module H-Reducer, which can not only maintain the layout information but also reduce the length of visual features by merging horizontal adjacent patches through convolution, enabling the LLM to understand high-resolution images more efficiently. Furthermore, by constructing structure-aware text sequences and multi-grained pairs of texts and bounding boxes for publicly available text-rich images, we build a comprehensive training set DocStruct4M to support structure learning. Finally, we construct a small but high-quality reasoning tuning dataset DocReason25K to trigger the detailed explanation ability in the document domain. Our model DocOwl 1.5 achieves state-of-the-art performance on 10 visual document understanding benchmarks, improving the SOTA performance of MLLMs with a 7B LLM by more than 10 points in 5/10 benchmarks. Our codes, models, and datasets are publicly available at https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5.",
            "link": "https://www.semanticscholar.org/paper/15ef0417570a190241caac0615eabdff11abb4de",
            "authors": "Anwen Hu, Haiyang Xu, Jiabo Ye, Mingshi Yan, Liang Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei Huang, Jingren Zhou",
            "matchScore": 351.58228,
            "original title": "mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding",
            "original authors": "Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Ji Zhang, Qin Jin, Fei Huang, Jingren Zhou",
            "EMNLP Paper ID": "632",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "ee6dc202727c2f3e414c2b4c879b9f4265ac200b",
            "title": "NormTab: Improving Symbolic Reasoning in LLMs Through Tabular Data Normalization",
            "abstract": "In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities in parsing textual data and generating code. However, their performance in tasks involving tabular data, especially those requiring symbolic reasoning, faces challenges due to the structural variance and inconsistency in table cell values often found in web tables. In this paper, we introduce NormTab, a novel framework aimed at enhancing the symbolic reasoning performance of LLMs by normalizing web tables. We study table normalization as a stand-alone, one-time preprocessing step using LLMs to support symbolic reasoning on tabular data. Our experimental evaluation, conducted on challenging web table datasets such as WikiTableQuestion and TabFact, demonstrates that leveraging NormTab significantly improves symbolic reasoning performance, showcasing the importance and effectiveness of web table normalization for enhancing LLM-based symbolic reasoning tasks.",
            "link": "https://www.semanticscholar.org/paper/ee6dc202727c2f3e414c2b4c879b9f4265ac200b",
            "authors": "Md Mahadi Hasan Nahid, Davood Rafiei",
            "matchScore": 314.68237,
            "original title": "NormTab: Improving Symbolic Reasoning in LLMs Through Tabular Data Normalization",
            "original authors": "Md Mahadi Hasan Nahid, Davood Rafiei",
            "EMNLP Paper ID": "719",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "2268d8104f4dd921fd90f4d2d6df9ecf8e7ce3eb",
            "title": "ProTrix: Building Models for Planning and Reasoning over Tables with Sentence Context",
            "abstract": "Tables play a crucial role in conveying information in various domains. We propose a Plan-then-Reason framework to answer different types of user queries over tables with sentence context. The framework first plans the reasoning paths over the context, then assigns each step to program-based or textual reasoning to reach the final answer. This framework enhances the table reasoning abilities for both in-context learning and fine-tuning methods. GPT-3.5-Turbo following Plan-then-Reason framework surpasses other prompting baselines without self-consistency while using less API calls and in-context demonstrations. We also construct an instruction tuning set TrixInstruct to evaluate the effectiveness of fine-tuning with this framework. We present ProTrix model family by finetuning models on TrixInstruct. Our experiments show that ProTrix family generalizes to diverse unseen tabular tasks with only 6k training instances. We further demonstrate that ProTrix can generate accurate and faithful explanations to answer complex free-form questions. Our work underscores the importance of the planning and reasoning abilities towards a model over tabular tasks with generalizability and interpretability. We open-source our dataset and models at https://github.com/WilliamZR/ProTrix.",
            "link": "https://www.semanticscholar.org/paper/2268d8104f4dd921fd90f4d2d6df9ecf8e7ce3eb",
            "authors": "Zirui Wu, Yansong Feng",
            "matchScore": 251.77231,
            "original title": "ProTrix: Building Models for Planning and Reasoning over Tables with Sentence Context",
            "original authors": "Zirui Wu, Yansong Feng",
            "EMNLP Paper ID": "869",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Findings"
        }
    ],
    "Evaluation and Enhancement of Large Language Models (LLMs)": [
        {
            "paperId": "d6beb9cc394f1e2046371678737346f05270ca91",
            "title": "RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning",
            "abstract": "Tool learning has generated widespread interest as a vital means of interaction between Large Language Models (LLMs) and the physical world. Current research predominantly emphasizes LLMs' capacity to utilize tools in well-structured environments while overlooking their stability when confronted with the inevitable noise of the real world. To bridge this gap, we introduce RoTBench, a multi-level benchmark for evaluating the robustness of LLMs in tool learning. Specifically, we establish five external environments, each featuring varying levels of noise (i.e., Clean, Slight, Medium, Heavy, and Union), providing an in-depth analysis of the model's resilience across three critical phases: tool selection, parameter identification, and content filling. Experiments involving six widely-used models underscore the urgent necessity for enhancing the robustness of LLMs in tool learning. For instance, the performance of GPT-4 even drops significantly from 80.00 to 58.10 when there is no substantial change in manual accuracy. More surprisingly, the noise correction capability inherent in the GPT family paradoxically impedes its adaptability in the face of mild noise. In light of these findings, we propose RoTTuning, a strategy that enriches the diversity of training environments to bolster the robustness of LLMs in tool learning. The code and data are available at https://github.com/Junjie-Ye/RoTBench.",
            "link": "https://www.semanticscholar.org/paper/d6beb9cc394f1e2046371678737346f05270ca91",
            "authors": "Junjie Ye, Yilong Wu, Songyang Gao, Sixian Li, Guanyu Li, Xiaoran Fan, Qi Zhang, Tao Gui, Xuanjing Huang",
            "EMNLP Paper ID": "32",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "6b702e51640eb0d598f10bee2555d4a50937e1df",
            "title": "Detection and Measurement of Syntactic Templates in Generated Text",
            "abstract": "Recent work on evaluating the diversity of text generated by LLMs has focused on word-level features. Here we offer an analysis of syntactic features to characterize general repetition in models, beyond frequent n-grams. Specifically, we define syntactic templates and show that models tend to produce templated text in downstream tasks at a higher rate than what is found in human-reference texts. We find that most (76%) templates in model-generated text can be found in pre-training data (compared to only 35% of human-authored text), and are not overwritten during fine-tuning processes such as RLHF. This connection to the pre-training data allows us to analyze syntactic templates in models where we do not have the pre-training data. We also find that templates as features are able to differentiate between models, tasks, and domains, and are useful for qualitatively evaluating common model constructions. Finally, we demonstrate the use of templates as a useful tool for analyzing style memorization of training data in LLMs.",
            "link": "https://www.semanticscholar.org/paper/6b702e51640eb0d598f10bee2555d4a50937e1df",
            "authors": "Chantal Shaib, Yanai Elazar, Junyi Jessy Li, Byron C. Wallace",
            "EMNLP Paper ID": "721",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "64deb56a947b1409c1bfa67e7b702148a683a0fb",
            "title": "Detecting Subtle Differences between Human and Model Languages Using Spectrum of Relative Likelihood",
            "abstract": "Human and model-generated texts can be distinguished by examining the magnitude of likelihood in language. However, it is becoming increasingly difficult as language model's capabilities of generating human-like texts keep evolving. This study provides a new perspective by using the relative likelihood values instead of absolute ones, and extracting useful features from the spectrum-view of likelihood for the human-model text detection task. We propose a detection procedure with two classification methods, supervised and heuristic-based, respectively, which results in competitive performances with previous zero-shot detection methods and a new state-of-the-art on short-text detection. Our method can also reveal subtle differences between human and model languages, which find theoretical roots in psycholinguistics studies. Our code is available at https://github.com/CLCS-SUSTech/FourierGPT",
            "link": "https://www.semanticscholar.org/paper/64deb56a947b1409c1bfa67e7b702148a683a0fb",
            "authors": "Yang Xu, Yu Wang, Hao An, Zhichen Liu, Yongyuan Li",
            "EMNLP Paper ID": "1135",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "e3313d425b794ab732fb290f819ea90faa1a86eb",
            "title": "Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models",
            "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in various natural language processing tasks. However, LLMs may rely on dataset biases as shortcuts for prediction, which can significantly impair their robustness and generalization capabilities. This paper presents Shortcut Suite, a comprehensive test suite designed to evaluate the impact of shortcuts on LLMs' performance, incorporating six shortcut types, five evaluation metrics, and four prompting strategies. Our extensive experiments yield several key findings: 1) LLMs demonstrate varying reliance on shortcuts for downstream tasks, significantly impairing their performance. 2) Larger LLMs are more likely to utilize shortcuts under zero-shot and few-shot in-context learning prompts. 3) Chain-of-thought prompting notably reduces shortcut reliance and outperforms other prompting strategies, while few-shot prompts generally underperform compared to zero-shot prompts. 4) LLMs often exhibit overconfidence in their predictions, especially when dealing with datasets that contain shortcuts. 5) LLMs generally have a lower explanation quality in shortcut-laden datasets, with errors falling into three types: distraction, disguised comprehension, and logical fallacy. Our findings offer new insights for evaluating robustness and generalization in LLMs and suggest potential directions for mitigating the reliance on shortcuts. The code is available at \\url {https://github.com/yyhappier/ShortcutSuite.git}.",
            "link": "https://www.semanticscholar.org/paper/e3313d425b794ab732fb290f819ea90faa1a86eb",
            "authors": "Yu Yuan, Lili Zhao, Kai Zhang, Guangting Zheng, Qi Liu",
            "EMNLP Paper ID": "1423",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "c2643f5653a1fe02c286802fada83dc6c1fd5730",
            "title": "Enhancing Data Quality through Simple De-duplication: Navigating Responsible Computational Social Science Research",
            "abstract": "Research in natural language processing (NLP) for Computational Social Science (CSS) heavily relies on data from social media platforms. This data plays a crucial role in the development of models for analysing socio-linguistic phenomena within online communities. In this work, we conduct an in-depth examination of 20 datasets extensively used in NLP for CSS to comprehensively examine data quality. Our analysis reveals that social media datasets exhibit varying levels of data duplication. Consequently, this gives rise to challenges like label inconsistencies and data leakage, compromising the reliability of models. Our findings also suggest that data duplication has an impact on the current claims of state-of-the-art performance, potentially leading to an overestimation of model effectiveness in real-world scenarios. Finally, we propose new protocols and best practices for improving dataset development from social media data and its usage.",
            "link": "https://www.semanticscholar.org/paper/c2643f5653a1fe02c286802fada83dc6c1fd5730",
            "authors": "Yida Mu, Mali Jin, Xingyi Song, Nikolaos Aletras",
            "EMNLP Paper ID": "1456",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "939d0fe4830a5c2d8baa4e93ba1036def95db910",
            "title": "Evaluating n-Gram Novelty of Language Models Using Rusty-DAWG",
            "abstract": "How novel are texts generated by language models (LMs) relative to their training corpora? In this work, we investigate the extent to which modern LMs generate $n$-grams from their training data, evaluating both (i) the probability LMs assign to complete training $n$-grams and (ii) $n$-novelty, the proportion of $n$-grams generated by an LM that did not appear in the training data (for arbitrarily large $n$). To enable arbitrary-length $n$-gram search over a corpus in constant time w.r.t. corpus size, we develop Rusty-DAWG, a novel search tool inspired by indexing of genomic data. We compare the novelty of LM-generated text to human-written text and explore factors that affect generation novelty, focusing on the Pythia models. We find that, for $n>4$, LM-generated text is less novel than human-written text, though it is more novel for smaller $n$. Larger LMs and more constrained decoding strategies both decrease novelty. Finally, we show that LMs complete $n$-grams with lower loss if they are more frequent in the training data. Overall, our results reveal factors influencing the novelty of LM-generated text, and we release Rusty-DAWG to facilitate further pretraining data research.",
            "link": "https://www.semanticscholar.org/paper/939d0fe4830a5c2d8baa4e93ba1036def95db910",
            "authors": "William Merrill, Noah A. Smith, Yanai Elazar",
            "EMNLP Paper ID": "1667",
            "Oral/Poster": "Oral",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "a2f1d6db3725b23eeafd07da95442fa42415728e",
            "title": "CopyBench: Measuring Literal and Non-Literal Reproduction of Copyright-Protected Text in Language Model Generation",
            "abstract": "Evaluating the degree of reproduction of copyright-protected content by language models (LMs) is of significant interest to the AI and legal communities. Although both literal and non-literal similarities are considered by courts when assessing the degree of reproduction, prior research has focused only on literal similarities. To bridge this gap, we introduce CopyBench, a benchmark designed to measure both literal and non-literal copying in LM generations. Using copyrighted fiction books as text sources, we provide automatic evaluation protocols to assess literal and non-literal copying, balanced against the model utility in terms of the ability to recall facts from the copyrighted works and generate fluent completions. We find that, although literal copying is relatively rare, two types of non-literal copying -- event copying and character copying -- occur even in models as small as 7B parameters. Larger models demonstrate significantly more copying, with literal copying rates increasing from 0.2\\% to 10.5\\% and non-literal copying from 2.3\\% to 5.9\\% when comparing Llama3-8B and 70B models, respectively. We further evaluate the effectiveness of current strategies for mitigating copying and show that (1) training-time alignment can reduce literal copying but may increase non-literal copying, and (2) current inference-time mitigation methods primarily reduce literal but not non-literal copying.",
            "link": "https://www.semanticscholar.org/paper/a2f1d6db3725b23eeafd07da95442fa42415728e",
            "authors": "Tong Chen, Akari Asai, Niloofar Mireshghallah, Sewon Min, James Grimmelmann, Yejin Choi, Hanna Hajishirzi, Luke S. Zettlemoyer, Pang Wei Koh",
            "EMNLP Paper ID": "1765",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "708c361a5665083fc71427a831f10f4df3a61989",
            "title": "Zero-Shot Detection of LLM-Generated Text using Token Cohesiveness",
            "abstract": "The increasing capability and widespread usage of large language models (LLMs) highlight the desirability of automatic detection of LLM-generated text. Zero-shot detectors, due to their training-free nature, have received considerable attention and notable success. In this paper, we identify a new feature, token cohesiveness, that is useful for zero-shot detection, and we demonstrate that LLM-generated text tends to exhibit higher token cohesiveness than human-written text. Based on this observation, we devise TOCSIN, a generic dual-channel detection paradigm that uses token cohesiveness as a plug-and-play module to improve existing zero-shot detectors. To calculate token cohesiveness, TOCSIN only requires a few rounds of random token deletion and semantic difference measurement, making it particularly suitable for a practical black-box setting where the source model used for generation is not accessible. Extensive experiments with four state-of-the-art base detectors on various datasets, source models, and evaluation settings demonstrate the effectiveness and generality of the proposed approach. Code available at: \\url{https://github.com/Shixuan-Ma/TOCSIN}.",
            "link": "https://www.semanticscholar.org/paper/708c361a5665083fc71427a831f10f4df3a61989",
            "authors": "Shixuan Ma, Quan Wang",
            "EMNLP Paper ID": "2093",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "473ddc41d294d51f7bf5ba4f65dd5922b7d21b56",
            "title": "NoiseBench: Benchmarking the Impact of Real Label Noise on Named Entity Recognition",
            "abstract": "Available training data for named entity recognition (NER) often contains a significant percentage of incorrect labels for entity types and entity boundaries. Such label noise poses challenges for supervised learning and may significantly deteriorate model quality. To address this, prior work proposed various noise-robust learning approaches capable of learning from data with partially incorrect labels. These approaches are typically evaluated using simulated noise where the labels in a clean dataset are automatically corrupted. However, as we show in this paper, this leads to unrealistic noise that is far easier to handle than real noise caused by human error or semi-automatic annotation. To enable the study of the impact of various types of real noise, we introduce NoiseBench, an NER benchmark consisting of clean training data corrupted with 6 types of real noise, including expert errors, crowdsourcing errors, automatic annotation errors and LLM errors. We present an analysis that shows that real noise is significantly more challenging than simulated noise, and show that current state-of-the-art models for noise-robust learning fall far short of their theoretically achievable upper bound. We release NoiseBench to the research community.",
            "link": "https://www.semanticscholar.org/paper/473ddc41d294d51f7bf5ba4f65dd5922b7d21b56",
            "authors": "Elena Merdjanovska, Ansar Aynetdinov, Alan Akbik",
            "EMNLP Paper ID": "2236",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "414f89c5564cea16c256e7653862acc9f7ce2617",
            "title": "'Quis custodiet ipsos custodes?' Who will watch the watchmen? On Detecting AI-generated peer-reviews",
            "abstract": "The integrity of the peer-review process is vital for maintaining scientific rigor and trust within the academic community. With the steady increase in the usage of large language models (LLMs) like ChatGPT in academic writing, there is a growing concern that AI-generated texts could compromise scientific publishing, including peer-reviews. Previous works have focused on generic AI-generated text detection or have presented an approach for estimating the fraction of peer-reviews that can be AI-generated. Our focus here is to solve a real-world problem by assisting the editor or chair in determining whether a review is written by ChatGPT or not. To address this, we introduce the Term Frequency (TF) model, which posits that AI often repeats tokens, and the Review Regeneration (RR) model, which is based on the idea that ChatGPT generates similar outputs upon re-prompting. We stress test these detectors against token attack and paraphrasing. Finally, we propose an effective defensive strategy to reduce the effect of paraphrasing on our models. Our findings suggest both our proposed methods perform better than the other AI text detectors. Our RR model is more robust, although our TF model performs better than the RR model without any attacks. We make our code, dataset, and model public.",
            "link": "https://www.semanticscholar.org/paper/414f89c5564cea16c256e7653862acc9f7ce2617",
            "authors": "Sandeep Kumar, Mohit Sahu, Vardhan Gacche, Tirthankar Ghosal, Asif Ekbal",
            "EMNLP Paper ID": "3269",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "41ce74933b4bf5e7acf618352dedf0058a64dbba",
            "title": "Generative Deduplication For Socia Media Data Selection",
            "abstract": "Social media data exhibits severe redundancy caused by its noisy nature. It leads to increased training time and model bias in its processing. To address this issue, we propose a novel Generative Deduplication framework for social media data selection by removing semantically duplicate data. While related work involves data selection in task-specific training, our model acts as an efficient pre-processing method to universally enhance social media NLP pipelines. Specifically, we train a generative model via self-supervised learning to predict a keyword to capture the semantics of noisy social media text for deduplication. Meanwhile, time-dimensional Gaussian noise is added to improve training complexity and avoid learning trivial features. Extensive experiments suggest that our model can better reduce training samples while improving performance than baselines. The results show our model's potential to broadly advance social media language understanding in effectiveness and efficiency.",
            "link": "https://www.semanticscholar.org/paper/41ce74933b4bf5e7acf618352dedf0058a64dbba",
            "authors": "Xianming Li, Jing Li",
            "matchScore": 237.8056,
            "original title": "Generative Deduplication For Socia Media Data Selection",
            "original authors": "Xianming LI, Jing Li",
            "EMNLP Paper ID": "1166",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "62ac45c894b38a9ec6ca089d1bea292281089d04",
            "title": "Authorship Obfuscation in Multilingual Machine-Generated Text Detection",
            "abstract": "High-quality text generation capability of recent Large Language Models (LLMs) causes concerns about their misuse (e.g., in massive generation/spread of disinformation). Machine-generated text (MGT) detection is important to cope with such threats. However, it is susceptible to authorship obfuscation (AO) methods, such as paraphrasing, which can cause MGTs to evade detection. So far, this was evaluated only in monolingual settings. Thus, the susceptibility of recently proposed multilingual detectors is still unknown. We fill this gap by comprehensively benchmarking the performance of 10 well-known AO methods, attacking 37 MGT detection methods against MGTs in 11 languages (i.e., 10 $\\times$ 37 $\\times$ 11 = 4,070 combinations). We also evaluate the effect of data augmentation on adversarial robustness using obfuscated texts. The results indicate that all tested AO methods can cause evasion of automated detection in all tested languages, where homoglyph attacks are especially successful. However, some of the AO methods severely damaged the text, making it no longer readable or easily recognizable by humans (e.g., changed language, weird characters).",
            "link": "https://www.semanticscholar.org/paper/62ac45c894b38a9ec6ca089d1bea292281089d04",
            "authors": "Dominik Macko, R\u00f3bert M\u00f3ro, Adaku Uchendu, Ivan Srba, Jason Samuel Lucas, Michiharu Yamashita, Nafis Irtiza Tripto, Dongwon Lee, Jakub Simko, M. Bielikov\u00e1",
            "matchScore": 257.78363,
            "original title": "Authorship Obfuscation in Multilingual Machine-Generated Text Detection",
            "original authors": "Dominik Macko, Robert Moro, Adaku Uchendu, Ivan Srba, Jason S Lucas, Michiharu Yamashita, Nafis Irtiza Tripto, Dongwon Lee, Jakub Simko, Maria Bielikova",
            "EMNLP Paper ID": "1297",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "21ac39bddb50cdcf7e3469e848f33c173b2c3f3f",
            "title": "AutoDetect: Towards a Unified Framework for Automated Weakness Detection in Large Language Models",
            "abstract": "Although Large Language Models (LLMs) are becoming increasingly powerful, they still exhibit significant but subtle weaknesses, such as mistakes in instruction-following or coding tasks. As these unexpected errors could lead to severe consequences in practical deployments, it is crucial to investigate the limitations within LLMs systematically. Traditional benchmarking approaches cannot thoroughly pinpoint specific model deficiencies, while manual inspections are costly and not scalable. In this paper, we introduce a unified framework, AutoDetect, to automatically expose weaknesses in LLMs across various tasks. Inspired by the educational assessment process that measures students' learning outcomes, AutoDetect consists of three LLM-powered agents: Examiner, Questioner, and Assessor. The collaboration among these three agents is designed to realize comprehensive and in-depth weakness identification. Our framework demonstrates significant success in uncovering flaws, with an identification success rate exceeding 30% in prominent models such as ChatGPT and Claude. More importantly, these identified weaknesses can guide specific model improvements, proving more effective than untargeted data augmentation methods like Self-Instruct. Our approach has led to substantial enhancements in popular LLMs, including the Llama series and Mistral-7b, boosting their performance by over 10% across several benchmarks. Code and data are publicly available at https://github.com/thu-coai/AutoDetect.",
            "link": "https://www.semanticscholar.org/paper/21ac39bddb50cdcf7e3469e848f33c173b2c3f3f",
            "authors": "Jiale Cheng, Yida Lu, Xiaotao Gu, Pei Ke, Xiao Liu, Yuxiao Dong, Hongning Wang, Jie Tang, Minlie Huang",
            "matchScore": 261.58768,
            "original title": "AutoDetect: Towards a Unified Framework for Automated Weakness Detection in Large Language Models",
            "original authors": "Jiale Cheng, Yida Lu, Xiaotao Gu, Pei Ke, Xiao Liu, Yuxiao Dong, Hongning Wang, Jie Tang, Minlie Huang",
            "EMNLP Paper ID": "1387",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "feb457c12242d4980b5df2eefc4ee6f5855428e2",
            "title": "On the Generalization of Training-based ChatGPT Detection Methods",
            "abstract": "ChatGPT is one of the most popular language models which achieve amazing performance on various natural language tasks. Consequently, there is also an urgent need to detect the texts generated ChatGPT from human written. One of the extensively studied methods trains classification models to distinguish both. However, existing studies also demonstrate that the trained models may suffer from distribution shifts (during test), i.e., they are ineffective to predict the generated texts from unseen language tasks or topics. In this work, we aim to have a comprehensive investigation on these methods' generalization behaviors under distribution shift caused by a wide range of factors, including prompts, text lengths, topics, and language tasks. To achieve this goal, we first collect a new dataset with human and ChatGPT texts, and then we conduct extensive studies on the collected dataset. Our studies unveil insightful findings which provide guidance for developing future methodologies or data collection strategies for ChatGPT detection.",
            "link": "https://www.semanticscholar.org/paper/feb457c12242d4980b5df2eefc4ee6f5855428e2",
            "authors": "Han Xu, Jie Ren, Pengfei He, Shenglai Zeng, Yingqian Cui, Amy Liu, Hui Liu, Jiliang Tang",
            "matchScore": 211.93335,
            "original title": "On the Generalization of Training-based ChatGPT Detection Methods",
            "original authors": "Han Xu, Jie Ren, Pengfei He, Shenglai Zeng, Yingqian Cui, Amy Liu, Hui Liu, Jiliang Tang",
            "EMNLP Paper ID": "1475",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "60bd26bdb23ba353e5d79f161542dd074bc8391c",
            "title": "A Survey on Detection of LLMs-Generated Content",
            "abstract": "The burgeoning capabilities of advanced large language models (LLMs) such as ChatGPT have led to an increase in synthetic content generation with implications across a variety of sectors, including media, cybersecurity, public discourse, and education. As such, the ability to detect LLMs-generated content has become of paramount importance. We aim to provide a detailed overview of existing detection strategies and benchmarks, scrutinizing their differences and identifying key challenges and prospects in the field, advocating for more adaptable and robust models to enhance detection accuracy. We also posit the necessity for a multi-faceted approach to defend against various attacks to counter the rapidly advancing capabilities of LLMs. To the best of our knowledge, this work is the first comprehensive survey on the detection in the era of LLMs. We hope it will provide a broad understanding of the current landscape of LLMs-generated content detection, offering a guiding reference for researchers and practitioners striving to uphold the integrity of digital information in an era increasingly dominated by synthetic content. The relevant papers are summarized and will be consistently updated at https://github.com/Xianjun-Yang/Awesome_papers_on_LLMs_detection.git.",
            "link": "https://www.semanticscholar.org/paper/60bd26bdb23ba353e5d79f161542dd074bc8391c",
            "authors": "Xianjun Yang, Liangming Pan, Xuandong Zhao, Haifeng Chen, L. Petzold, William Yang Wang, Wei Cheng",
            "matchScore": 183.24356,
            "original title": "A Survey on Detection of LLMs-Generated Content",
            "original authors": "Xianjun Yang, Liangming Pan, Xuandong Zhao, Haifeng Chen, Linda Ruth Petzold, William Yang Wang, Wei Cheng",
            "EMNLP Paper ID": "2014",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "47ad4e57955df51227f9968a01f31e76686760f0",
            "title": "Detecting Machine-Generated Long-Form Content with Latent-Space Variables",
            "abstract": "The increasing capability of large language models (LLMs) to generate fluent long-form texts is presenting new challenges in distinguishing machine-generated outputs from human-written ones, which is crucial for ensuring authenticity and trustworthiness of expressions. Existing zero-shot detectors primarily focus on token-level distributions, which are vulnerable to real-world domain shifts, including different prompting and decoding strategies, and adversarial attacks. We propose a more robust method that incorporates abstract elements, such as event transitions, as key deciding factors to detect machine versus human texts by training a latent-space model on sequences of events or topics derived from human-written texts. In three different domains, machine-generated texts, which are originally inseparable from human texts on the token level, can be better distinguished with our latent-space model, leading to a 31% improvement over strong baselines such as DetectGPT. Our analysis further reveals that, unlike humans, modern LLMs like GPT-4 generate event triggers and their transitions differently, an inherent disparity that helps our method to robustly detect machine-generated texts.",
            "link": "https://www.semanticscholar.org/paper/47ad4e57955df51227f9968a01f31e76686760f0",
            "authors": "Yufei Tian, Zeyu Pan, Nanyun Peng",
            "matchScore": 270.60968,
            "original title": "Detecting Machine-Generated Long-Form Content with Latent-Space Variables",
            "original authors": "Yufei Tian, Zeyu Pan, Nanyun Peng",
            "EMNLP Paper ID": "2120",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "4e528581e8b45b7d955455f825079825eeb66701",
            "title": "Unveiling the Flaws: Exploring Imperfections in Synthetic Data and Mitigation Strategies for Large Language Models",
            "abstract": "Synthetic data has been proposed as a solution to address the issue of high-quality data scarcity in the training of large language models (LLMs). Studies have shown that synthetic data can effectively improve the performance of LLMs on downstream benchmarks. However, despite its potential benefits, our analysis suggests that there may be inherent flaws in synthetic data. The uniform format of synthetic data can lead to pattern overfitting and cause significant shifts in the output distribution, thereby reducing the model's instruction-following capabilities. Our work delves into these specific flaws associated with question-answer (Q-A) pairs, a prevalent type of synthetic data, and presents a method based on unlearning techniques to mitigate these flaws. The empirical results demonstrate the effectiveness of our approach, which can reverse the instruction-following issues caused by pattern overfitting without compromising performance on benchmarks at relatively low cost. Our work has yielded key insights into the effective use of synthetic data, aiming to promote more robust and efficient LLM training.",
            "link": "https://www.semanticscholar.org/paper/4e528581e8b45b7d955455f825079825eeb66701",
            "authors": "Jie Chen, Yupeng Zhang, Bingning Wang, Wayne Xin Zhao, Ji-Rong Wen, Weipeng Chen",
            "matchScore": 280.70776,
            "original title": "Unveiling the Flaws: Exploring Imperfections in Synthetic Data and Mitigation Strategies for Large Language Models",
            "original authors": "Jie Chen, Yupeng Zhang, Bingning Wang, Xin Zhao, Ji-Rong Wen",
            "EMNLP Paper ID": "2855",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "3df085054b72d988e0d3abfceaf5e04d7d0cba46",
            "title": "Navigating the Shortcut Maze: A Comprehensive Analysis of Shortcut Learning in Text Classification by Language Models",
            "abstract": "Language models (LMs), despite their advances, often depend on spurious correlations, undermining their accuracy and generalizability. This study addresses the overlooked impact of subtler, more complex shortcuts that compromise model reliability beyond oversimplified shortcuts. We introduce a comprehensive benchmark that categorizes shortcuts into occurrence, style, and concept, aiming to explore the nuanced ways in which these shortcuts influence the performance of LMs. Through extensive experiments across traditional LMs, large language models, and state-of-the-art robust models, our research systematically investigates models' resilience and susceptibilities to sophisticated shortcuts. Our benchmark and code can be found at: https://github.com/yuqing-zhou/shortcut-learning-in-text-classification.",
            "link": "https://www.semanticscholar.org/paper/3df085054b72d988e0d3abfceaf5e04d7d0cba46",
            "authors": "Yuqing Zhou, Ruixiang Tang, Ziyu Yao, Ziwei Zhu",
            "matchScore": 327.15363,
            "original title": "Navigating the Shortcut Maze: A Comprehensive Analysis of Shortcut Learning in Text Classification by Language Models",
            "original authors": "Yuqing Zhou, Ruixiang Tang, Ziyu Yao, Ziwei Zhu",
            "EMNLP Paper ID": "537",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        }
    ],
    "Explainability and Evaluation of Large Language Models": [
        {
            "paperId": "3e46e2a679d5220169ac4f3ae1111e8a7052aee2",
            "title": "Evaluating Readability and Faithfulness of Concept-based Explanations",
            "abstract": "With the growing popularity of general-purpose Large Language Models (LLMs), comes a need for more global explanations of model behaviors. Concept-based explanations arise as a promising avenue for explaining high-level patterns learned by LLMs. Yet their evaluation poses unique challenges, especially due to their non-local nature and high dimensional representation in a model's hidden space. Current methods approach concepts from different perspectives, lacking a unified formalization. This makes evaluating the core measures of concepts, namely faithfulness or readability, challenging. To bridge the gap, we introduce a formal definition of concepts generalizing to diverse concept-based explanations' settings. Based on this, we quantify the faithfulness of a concept explanation via perturbation. We ensure adequate perturbation in the high-dimensional space for different concepts via an optimization problem. Readability is approximated via an automatic and deterministic measure, quantifying the coherence of patterns that maximally activate a concept while aligning with human understanding. Finally, based on measurement theory, we apply a meta-evaluation method for evaluating these measures, generalizable to other types of explanations or tasks as well. Extensive experimental analysis has been conducted to inform the selection of explanation evaluation measures.",
            "link": "https://www.semanticscholar.org/paper/3e46e2a679d5220169ac4f3ae1111e8a7052aee2",
            "authors": "Meng Li, Haoran Jin, Ruixuan Huang, Zhihao Xu, Defu Lian, Zijia Lin, Di Zhang, Xiting Wang",
            "EMNLP Paper ID": "81",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "c7084e6778222430b1b52bd522d0dad93121acc7",
            "title": "XplainLLM: A Knowledge-Augmented Dataset for Reliable Grounded Explanations in LLMs",
            "abstract": "Large Language Models (LLMs) have achieved remarkable success in natural language tasks, yet understanding their reasoning processes remains a significant challenge. We address this by introducing XplainLLM, a dataset accompanying an explanation framework designed to enhance LLM transparency and reliability. Our dataset comprises 24,204 instances where each instance interprets the LLM's reasoning behavior using knowledge graphs (KGs) and graph attention networks (GAT), and includes explanations of LLMs such as the decoder-only Llama-3 and the encoder-only RoBERTa. XplainLLM also features a framework for generating grounded explanations and the debugger-scores for multidimensional quality analysis. Our explanations include why-choose and why-not-choose components, reason-elements, and debugger-scores that collectively illuminate the LLM's reasoning behavior. Our evaluations demonstrate XplainLLM's potential to reduce hallucinations and improve grounded explanation generation in LLMs. XplainLLM is a resource for researchers and practitioners to build trust and verify the reliability of LLM outputs.",
            "link": "https://www.semanticscholar.org/paper/c7084e6778222430b1b52bd522d0dad93121acc7",
            "authors": "Zichen Chen, Jianda Chen, Ambuj Singh, Misha Sra",
            "EMNLP Paper ID": "859",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "eb789ac5b8cd4af9ce095917f767a9f73de8b5b1",
            "title": "ECon: On the Detection and Resolution of Evidence Conflicts",
            "abstract": "The rise of large language models (LLMs) has significantly influenced the quality of information in decision-making systems, leading to the prevalence of AI-generated content and challenges in detecting misinformation and managing conflicting information, or\"inter-evidence conflicts.\"This study introduces a method for generating diverse, validated evidence conflicts to simulate real-world misinformation scenarios. We evaluate conflict detection methods, including Natural Language Inference (NLI) models, factual consistency (FC) models, and LLMs, on these conflicts (RQ1) and analyze LLMs' conflict resolution behaviors (RQ2). Our key findings include: (1) NLI and LLM models exhibit high precision in detecting answer conflicts, though weaker models suffer from low recall; (2) FC models struggle with lexically similar answer conflicts, while NLI and LLM models handle these better; and (3) stronger models like GPT-4 show robust performance, especially with nuanced conflicts. For conflict resolution, LLMs often favor one piece of conflicting evidence without justification and rely on internal knowledge if they have prior beliefs.",
            "link": "https://www.semanticscholar.org/paper/eb789ac5b8cd4af9ce095917f767a9f73de8b5b1",
            "authors": "Jiayang Cheng, Chunkit Chan, Qianqian Zhuang, Lin Qiu, Tianhang Zhang, Tengxiao Liu, Yangqiu Song, Yue Zhang, Pengfei Liu, Zheng Zhang",
            "EMNLP Paper ID": "895",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "315773f41083f1e9c790168a55d40c79b7117c43",
            "title": "Latent Concept-based Explanation of NLP Models",
            "abstract": "Interpreting and understanding the predictions made by deep learning models poses a formidable challenge due to their inherently opaque nature. Many previous efforts aimed at explaining these predictions rely on input features, specifically, the words within NLP models. However, such explanations are often less informative due to the discrete nature of these words and their lack of contextual verbosity. To address this limitation, we introduce the Latent Concept Attribution method (LACOAT), which generates explanations for predictions based on latent concepts. Our foundational intuition is that a word can exhibit multiple facets, contingent upon the context in which it is used. Therefore, given a word in context, the latent space derived from our training process reflects a specific facet of that word. LACOAT functions by mapping the representations of salient input words into the training latent space, allowing it to provide latent context-based explanations of the prediction.",
            "link": "https://www.semanticscholar.org/paper/315773f41083f1e9c790168a55d40c79b7117c43",
            "authors": "Xuemin Yu, Fahim Dalvi, Nadir Durrani, Hassan Sajjad",
            "EMNLP Paper ID": "1454",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "9402cd05b5e77518d76907fbee979e12cdf31ac1",
            "title": "Detecting Errors through Ensembling Prompts (DEEP): An End-to-End LLM Framework for Detecting Factual Errors",
            "abstract": "Accurate text summarization is one of the most common and important tasks performed by Large Language Models, where the costs of human review for an entire document may be high, but the costs of errors in summarization may be even greater. We propose Detecting Errors through Ensembling Prompts (DEEP) - an end-to-end large language model framework for detecting factual errors in text summarization. Our framework uses a diverse set of LLM prompts to identify factual inconsistencies, treating their outputs as binary features, which are then fed into ensembling models. We then calibrate the ensembled models to produce empirically accurate probabilities that a text is factually consistent or free of hallucination. We demonstrate that prior models for detecting factual errors in summaries perform significantly worse without optimizing the thresholds on subsets of the evaluated dataset. Our framework achieves state-of-the-art (SOTA) balanced accuracy on the AggreFact-XSUM FTSOTA, TofuEval Summary-Level, and HaluEval Summarization benchmarks in detecting factual errors within transformer-generated text summaries. It does so without any fine-tuning of the language model or reliance on thresholding techniques not available in practical settings.",
            "link": "https://www.semanticscholar.org/paper/9402cd05b5e77518d76907fbee979e12cdf31ac1",
            "authors": "Alex Chandler, Devesh Surve, Hui Su",
            "EMNLP Paper ID": "1516",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "fd6e6891c71100e8158ca165784ff7860c281038",
            "title": "Towards Faithful Knowledge Graph Explanation Through Deep Alignment in Commonsense Question Answering",
            "abstract": "The fusion of language models (LMs) and knowledge graphs (KGs) is widely used in commonsense question answering, but generating faithful explanations remains challenging. Current methods often overlook path decoding faithfulness, leading to divergence between graph encoder outputs and model predictions. We identify confounding effects and LM-KG misalignment as key factors causing spurious explanations. To address this, we introduce the LM-KG Fidelity metric to assess KG representation reliability and propose the LM-KG Distribution-aware Alignment (\\textit{LKDA}) algorithm to improve explanation faithfulness. Without ground truth, we evaluate KG explanations using the proposed Fidelity-Sparsity Trade-off Curve. Experiments on CommonsenseQA and OpenBookQA show that LKDA significantly enhances explanation fidelity and model performance, highlighting the need to address distributional misalignment for reliable commonsense reasoning.",
            "link": "https://www.semanticscholar.org/paper/fd6e6891c71100e8158ca165784ff7860c281038",
            "authors": "Weihe Zhai, A. Zubiaga",
            "EMNLP Paper ID": "2376",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "ff74eae326493f8c4d3bfda14f330194031f3437",
            "title": "The Illusion of Competence: Evaluating the Effect of Explanations on Users' Mental Models of Visual Question Answering Systems",
            "abstract": "We examine how users perceive the limitations of an AI system when it encounters a task that it cannot perform perfectly and whether providing explanations alongside its answers aids users in constructing an appropriate mental model of the system's capabilities and limitations. We employ a visual question answer and explanation task where we control the AI system's limitations by manipulating the visual inputs: during inference, the system either processes full-color or grayscale images. Our goal is to determine whether participants can perceive the limitations of the system. We hypothesize that explanations will make limited AI capabilities more transparent to users. However, our results show that explanations do not have this effect. Instead of allowing users to more accurately assess the limitations of the AI system, explanations generally increase users' perceptions of the system's competence - regardless of its actual performance.",
            "link": "https://www.semanticscholar.org/paper/ff74eae326493f8c4d3bfda14f330194031f3437",
            "authors": "Judith Sieker, Simeon Junker, R. Utescher, Nazia Attari, H. Wersing, Hendrik Buschmeier, Sina Zarrie\u00df",
            "EMNLP Paper ID": "2489",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "e0ff4dd9befd05f06ca9011386768b53db80c2b6",
            "title": "Discovering Biases in Information Retrieval Models Using Relevance Thesaurus as Global Explanation",
            "abstract": "Most efforts in interpreting neural relevance models have focused on local explanations, which explain the relevance of a document to a query but are not useful in predicting the model's behavior on unseen query-document pairs. We propose a novel method to globally explain neural relevance models by constructing a\"relevance thesaurus\"containing semantically relevant query and document term pairs. This thesaurus is used to augment lexical matching models such as BM25 to approximate the neural model's predictions. Our method involves training a neural relevance model to score the relevance of partial query and document segments, which is then used to identify relevant terms across the vocabulary space. We evaluate the obtained thesaurus explanation based on ranking effectiveness and fidelity to the target neural ranking model. Notably, our thesaurus reveals the existence of brand name bias in ranking models, demonstrating one advantage of our explanation method.",
            "link": "https://www.semanticscholar.org/paper/e0ff4dd9befd05f06ca9011386768b53db80c2b6",
            "authors": "Youngwoo Kim, Razieh Rahimi, James Allan",
            "EMNLP Paper ID": "2517",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "46f3e2997875a3632481cf9b4b58be9879b6ffc1",
            "title": "On Evaluating Explanation Utility for Human-AI Decision Making in NLP",
            "abstract": "Is explainability a false promise? This debate has emerged from the insufficient evidence that explanations aid people in situations they are introduced for. More human-centered, application-grounded evaluations of explanations are needed to settle this. Yet, with no established guidelines for such studies in NLP, researchers accustomed to standardized proxy evaluations must discover appropriate measurements, tasks, datasets, and sensible models for human-AI teams in their studies. To help with this, we first review fitting existing metrics. We then establish requirements for datasets to be suitable for application-grounded evaluations. Among over 50 datasets available for explainability research in NLP, we find that 4 meet our criteria. By finetuning Flan-T5-3B, we demonstrate the importance of reassessing the state of the art to form and study human-AI teams. Finally, we present the exemplar studies of human-AI decision-making for one of the identified suitable tasks -- verifying the correctness of a legal claim given a contract.",
            "link": "https://www.semanticscholar.org/paper/46f3e2997875a3632481cf9b4b58be9879b6ffc1",
            "authors": "Fateme Hashemi Chaleshtori, Atreya Ghosal, Ana Marasovi\u0107",
            "matchScore": 254.03394,
            "original title": "On Evaluating Explanation Utility for Human-AI Decision Making in NLP",
            "original authors": "Fateme Hashemi Chaleshtori, Atreya Ghosal, Alexander Gill, Purbid bambroo, Ana Marasovic",
            "EMNLP Paper ID": "1554",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "40da525d570cec153979c7ff709a7783ec5561ca",
            "title": "\"Seeing the Big through the Small\": Can LLMs Approximate Human Judgment Distributions on NLI from a Few Explanations?",
            "abstract": "Human label variation (HLV) is a valuable source of information that arises when multiple human annotators provide different labels for valid reasons. In Natural Language Inference (NLI) earlier approaches to capturing HLV involve either collecting annotations from many crowd workers to represent human judgment distribution (HJD) or use expert linguists to provide detailed explanations for their chosen labels. While the former method provides denser HJD information, obtaining it is resource-intensive. In contrast, the latter offers richer textual information but it is challenging to scale up to many human judges. Besides, large language models (LLMs) are increasingly used as evaluators (\"LLM judges\") but with mixed results, and few works aim to study HJDs. This study proposes to exploit LLMs to approximate HJDs using a small number of expert labels and explanations. Our experiments show that a few explanations significantly improve LLMs' ability to approximate HJDs with and without explicit labels, thereby providing a solution to scale up annotations for HJD. However, fine-tuning smaller soft-label aware models with the LLM-generated model judgment distributions (MJDs) presents partially inconsistent results: while similar in distance, their resulting fine-tuned models and visualized distributions differ substantially. We show the importance of complementing instance-level distance measures with a global-level shape metric and visualization to more effectively evaluate MJDs against human judgment distributions.",
            "link": "https://www.semanticscholar.org/paper/40da525d570cec153979c7ff709a7783ec5561ca",
            "authors": "Beiduo Chen, Xinpeng Wang, Siyao Peng, Robert Litschko, Anna Korhonen, Barbara Plank",
            "matchScore": 362.3527,
            "original title": "\u201cSeeing the Big through the Small\u201d: Can LLMs Approximate Human Judgment Distributions on NLI from a Few Explanations?",
            "original authors": "Beiduo Chen, Xinpeng Wang, Siyao Peng, Robert Litschko, Anna Korhonen, Barbara Plank",
            "EMNLP Paper ID": "2785",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "7f525a54f2b990f803b7e3309d3c9e416619a12b",
            "title": "CoXQL: A Dataset for Parsing Explanation Requests in Conversational XAI Systems",
            "abstract": "Conversational explainable artificial intelligence (ConvXAI) systems based on large language models (LLMs) have garnered significant interest from the research community in natural language processing (NLP) and human-computer interaction (HCI). Such systems can provide answers to user questions about explanations in dialogues, have the potential to enhance users' comprehension and offer more information about the decision-making and generation processes of LLMs. Currently available ConvXAI systems are based on intent recognition rather than free chat, as this has been found to be more precise and reliable in identifying users' intentions. However, the recognition of intents still presents a challenge in the case of ConvXAI, since little training data exist and the domain is highly specific, as there is a broad range of XAI methods to map requests onto. In order to bridge this gap, we present CoXQL, the first dataset in the NLP domain for user intent recognition in ConvXAI, covering 31 intents, seven of which require filling multiple slots. Subsequently, we enhance an existing parsing approach by incorporating template validations, and conduct an evaluation of several LLMs on CoXQL using different parsing strategies. We conclude that the improved parsing approach (MP+) surpasses the performance of previous approaches. We also discover that intents with multiple slots remain highly challenging for LLMs.",
            "link": "https://www.semanticscholar.org/paper/7f525a54f2b990f803b7e3309d3c9e416619a12b",
            "authors": "Qianli Wang, Tatiana Anikina, Nils Feldhus, Simon Ostermann, Sebastian Moller",
            "matchScore": 325.49948,
            "original title": "CoXQL: A Dataset for Parsing Explanation Requests in Conversational XAI Systems",
            "original authors": "Qianli Wang, Tatiana Anikina, Nils Feldhus, Simon Ostermann, Sebastian M\u00f6ller",
            "EMNLP Paper ID": "283",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "480fb17e3e0617e19cde71b14d9fff1b68020c81",
            "title": "LLMs for Generating and Evaluating Counterfactuals: A Comprehensive Study",
            "abstract": "As NLP models become more complex, understanding their decisions becomes more crucial. Counterfactuals (CFs), where minimal changes to inputs flip a model's prediction, offer a way to explain these models. While Large Language Models (LLMs) have shown remarkable performance in NLP tasks, their efficacy in generating high-quality CFs remains uncertain. This work fills this gap by investigating how well LLMs generate CFs for two NLU tasks. We conduct a comprehensive comparison of several common LLMs, and evaluate their CFs, assessing both intrinsic metrics, and the impact of these CFs on data augmentation. Moreover, we analyze differences between human and LLM-generated CFs, providing insights for future research directions. Our results show that LLMs generate fluent CFs, but struggle to keep the induced changes minimal. Generating CFs for Sentiment Analysis (SA) is less challenging than NLI where LLMs show weaknesses in generating CFs that flip the original label. This also reflects on the data augmentation performance, where we observe a large gap between augmenting with human and LLMs CFs. Furthermore, we evaluate LLMs' ability to assess CFs in a mislabelled data setting, and show that they have a strong bias towards agreeing with the provided labels. GPT4 is more robust against this bias and its scores correlate well with automatic metrics. Our findings reveal several limitations and point to potential future work directions.",
            "link": "https://www.semanticscholar.org/paper/480fb17e3e0617e19cde71b14d9fff1b68020c81",
            "authors": "Van Bach Nguyen, Paul Youssef, Jorg Schlotterer, Christin Seifert",
            "matchScore": 204.02548,
            "original title": "LLMs for Generating and Evaluating Counterfactuals: A Comprehensive Study",
            "original authors": "Van Bach Nguyen, Paul Youssef, J\u00f6rg Schl\u00f6tterer, Christin Seifert",
            "EMNLP Paper ID": "2850",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "21dc1fa01663a50504431ee81c5e5bbff31a89cb",
            "title": "AXCEL: Automated eXplainable Consistency Evaluation using LLMs",
            "abstract": "Large Language Models (LLMs) are widely used in both industry and academia for various tasks, yet evaluating the consistency of generated text responses continues to be a challenge. Traditional metrics like ROUGE and BLEU show a weak correlation with human judgment. More sophisticated metrics using Natural Language Inference (NLI) have shown improved correlations but are complex to implement, require domain-specific training due to poor cross-domain generalization, and lack explainability. More recently, prompt-based metrics using LLMs as evaluators have emerged; while they are easier to implement, they still lack explainability and depend on task-specific prompts, which limits their generalizability. This work introduces Automated eXplainable Consistency Evaluation using LLMs (AXCEL), a prompt-based consistency metric which offers explanations for the consistency scores by providing detailed reasoning and pinpointing inconsistent text spans. AXCEL is also a generalizable metric which can be adopted to multiple tasks without changing the prompt. AXCEL outperforms both non-prompt and prompt-based state-of-the-art (SOTA) metrics in detecting inconsistencies across summarization by 8.7%, free text generation by 6.2%, and data-to-text conversion tasks by 29.4%. We also evaluate the influence of underlying LLMs on prompt based metric performance and recalibrate the SOTA prompt-based metrics with the latest LLMs for fair comparison. Further, we show that AXCEL demonstrates strong performance using open source LLMs.",
            "link": "https://www.semanticscholar.org/paper/21dc1fa01663a50504431ee81c5e5bbff31a89cb",
            "authors": "Aditya Sreekar, Sahil Verma, Suransh Chopra, Sarik Ghazarian, Abhishek Persad, Narayanan Sadagopan",
            "matchScore": 248.02046,
            "original title": "AXCEL: Automated eXplainable Consistency Evaluation using LLMs",
            "original authors": "P Aditya Sreekar, Sahil Verma, Suransh Chopra, Abhishek Persad, Sarik Ghazarian, Narayanan Sadagopan",
            "EMNLP Paper ID": "2873",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "ef07e9f3da89887a26f0895e00877c5bd620b0d5",
            "title": "Faithful and Plausible Natural Language Explanations for Image Classification: A Pipeline Approach",
            "abstract": "Existing explanation methods for image classification struggle to provide faithful and plausible explanations. This paper addresses this issue by proposing a post-hoc natural language explanation method that can be applied to any CNN-based classifier without altering its training process or affecting predictive performance. By analysing influential neurons and the corresponding activation maps, the method generates a faithful description of the classifier's decision process in the form of a structured meaning representation, which is then converted into text by a language model. Through this pipeline approach, the generated explanations are grounded in the neural network architecture, providing accurate insight into the classification process while remaining accessible to non-experts. Experimental results show that the NLEs constructed by our method are significantly more plausible and faithful. In particular, user interventions in the neural network structure (masking of neurons) are three times more effective than the baselines.",
            "link": "https://www.semanticscholar.org/paper/ef07e9f3da89887a26f0895e00877c5bd620b0d5",
            "authors": "Adam Wojciechowski, Mateusz Lango, Ondrej Dusek",
            "matchScore": 255.06561,
            "original title": "Faithful and Plausible Natural Language Explanations for Image Classification: A Pipeline Approach",
            "original authors": "Adam Wojciechowski, Mateusz Lango, Ondrej Dusek",
            "EMNLP Paper ID": "468",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "20f28c6222fd8fe6efd4807fac0e5fbdd41c2420",
            "title": "XRec: Large Language Models for Explainable Recommendation",
            "abstract": "Recommender systems help users navigate information overload by providing personalized recommendations aligned with their preferences. Collaborative Filtering (CF) is a widely adopted approach, but while advanced techniques like graph neural networks (GNNs) and self-supervised learning (SSL) have enhanced CF models for better user representations, they often lack the ability to provide explanations for the recommended items. Explainable recommendations aim to address this gap by offering transparency and insights into the recommendation decision-making process, enhancing users' understanding. This work leverages the language capabilities of Large Language Models (LLMs) to push the boundaries of explainable recommender systems. We introduce a model-agnostic framework called XRec, which enables LLMs to provide comprehensive explanations for user behaviors in recommender systems. By integrating collaborative signals and designing a lightweight collaborative adaptor, the framework empowers LLMs to understand complex patterns in user-item interactions and gain a deeper understanding of user preferences. Our extensive experiments demonstrate the effectiveness of XRec, showcasing its ability to generate comprehensive and meaningful explanations that outperform baseline approaches in explainable recommender systems. We open-source our model implementation at https://github.com/HKUDS/XRec.",
            "link": "https://www.semanticscholar.org/paper/20f28c6222fd8fe6efd4807fac0e5fbdd41c2420",
            "authors": "Qiyao Ma, Xubin Ren, Chao Huang",
            "matchScore": 188.97885,
            "original title": "XRec: Large Language Models for Explainable Recommendation",
            "original authors": "Qiyao Ma, Xubin Ren, Chao Huang",
            "EMNLP Paper ID": "65",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Findings"
        },
        {
            "paperId": "15ceaf9932a3283ce06c06caaec0a35726df3c0a",
            "title": "Measuring and Improving Attentiveness to Partial Inputs with Counterfactuals",
            "abstract": "The inevitable appearance of spurious correlations in training datasets hurts the generalization of NLP models on unseen data. Previous work has found that datasets with paired inputs are prone to correlations between a specific part of the input (e.g., the hypothesis in NLI) and the label; consequently, models trained only on those outperform chance. Are these correlations picked up by models trained on the full input data? To address this question, we propose a new evaluation method, Counterfactual Attentiveness Test (CAT). CAT uses counterfactuals by replacing part of the input with its counterpart from a different example (subject to some restrictions), expecting an attentive model to change its prediction. Using CAT, we systematically investigate established supervised and in-context learning models on ten datasets spanning four tasks: natural language inference, reading comprehension, paraphrase detection, and visual&language reasoning. CAT reveals that reliance on such correlations is mainly data-dependent. Surprisingly, we find that GPT3 becomes less attentive with an increased number of demonstrations, while its accuracy on the test data improves. Our results demonstrate that augmenting training or demonstration data with counterfactuals is effective in improving models' attentiveness. We show that models' attentiveness measured by CAT reveals different conclusions from solely measuring correlations in data.",
            "link": "https://www.semanticscholar.org/paper/15ceaf9932a3283ce06c06caaec0a35726df3c0a",
            "authors": "Yanai Elazar, Bhargavi Paranjape, Hao Peng, Sarah Wiegreffe, Khyathi Raghavi, Vivek Srikumar, Sameer Singh, Noah A. Smith",
            "matchScore": 215.93277,
            "original title": "Measuring and Improving Attentiveness to Partial Inputs with Counterfactuals",
            "original authors": "Yanai Elazar, Bhargavi Paranjape, Hao Peng, Sarah Wiegreffe, Khyathi Chandu, Vivek Srikumar, Sameer Singh, Noah A. Smith",
            "EMNLP Paper ID": "726",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "8cb888872c6ea0d30b990dd5de489fd908133793",
            "title": "Beyond Persuasion: Towards Conversational Recommender System with Credible Explanations",
            "abstract": "With the aid of large language models, current conversational recommender system (CRS) has gaining strong abilities to persuade users to accept recommended items. While these CRSs are highly persuasive, they can mislead users by incorporating incredible information in their explanations, ultimately damaging the long-term trust between users and the CRS. To address this, we propose a simple yet effective method, called PC-CRS, to enhance the credibility of CRS's explanations during persuasion. It guides the explanation generation through our proposed credibility-aware persuasive strategies and then gradually refines explanations via post-hoc self-reflection. Experimental results demonstrate the efficacy of PC-CRS in promoting persuasive and credible explanations. Further analysis reveals the reason behind current methods producing incredible explanations and the potential of credible explanations to improve recommendation accuracy.",
            "link": "https://www.semanticscholar.org/paper/8cb888872c6ea0d30b990dd5de489fd908133793",
            "authors": "Peixin Qin, Chen Huang, Yang Deng, Wenqiang Lei, Tat-Seng Chua",
            "matchScore": 263.48984,
            "original title": "Beyond Persuasion: Towards Conversational Recommender System with Credible Explanations",
            "original authors": "Peixin Qin, Chen Huang, Yang Deng, Wenqiang Lei, Tat-Seng Chua",
            "EMNLP Paper ID": "858",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "8d0bcbbc3c3505e7e7d9399fac404c14b6a17cb1",
            "title": "A Survey on Natural Language Counterfactual Generation",
            "abstract": "Natural language counterfactual generation aims to minimally modify a given text such that the modified text will be classified into a different class. The generated counterfactuals provide insight into the reasoning behind a model's predictions by highlighting which words significantly influence the outcomes. Additionally, they can be used to detect model fairness issues and augment the training data to enhance the model's robustness. A substantial amount of research has been conducted to generate counterfactuals for various NLP tasks, employing different models and methodologies. With the rapid growth of studies in this field, a systematic review is crucial to guide future researchers and developers. To bridge this gap, this survey provides a comprehensive overview of textual counterfactual generation methods, particularly those based on Large Language Models. We propose a new taxonomy that systematically categorizes the generation methods into four groups and summarizes the metrics for evaluating the generation quality. Finally, we discuss ongoing research challenges and outline promising directions for future work.",
            "link": "https://www.semanticscholar.org/paper/8d0bcbbc3c3505e7e7d9399fac404c14b6a17cb1",
            "authors": "Yongjie Wang, Xiaoqi Qiu, Yu Yue, Xu Guo, Zhiwei Zeng, Yuhong Feng, Zhiqi Shen",
            "matchScore": 172.0988,
            "original title": "A Survey on Natural Language Counterfactual Generation",
            "original authors": "Yongjie Wang, Xiaoqi Qiu, Yu Yue, Xu Guo, Zhiwei Zeng, Yuhong Feng, Zhiqi Shen",
            "EMNLP Paper ID": "943",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Findings"
        }
    ],
    "Methodological Advances in Vision-Language Models: Transfer Learning and Bias Mitigation": [
        {
            "paperId": "e287f3c941f4143eff2eb73f279a0c796239e187",
            "title": "Towards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models",
            "abstract": "Vision-language models (VLMs) like CLIP have demonstrated remarkable applicability across a variety of downstream tasks, including zero-shot image classification. Recently, the use of prompts or adapters for efficient transfer learning (ETL) has gained significant attention for effectively adapting to downstream tasks. However, previous studies have overlooked the challenge of varying transfer difficulty of downstream tasks. In this paper, we empirically analyze how each ETL method behaves with respect to transfer difficulty. Our observations indicate that utilizing vision prompts and text adapters is crucial for adaptability and generalizability in domains with high difficulty. Also, by applying an adaptive ensemble approach that integrates task-adapted VLMs with pre-trained VLMs and strategically leverages more general knowledge in low-difficulty and less in high-difficulty domains, we consistently enhance performance across both types of domains. Based on these observations, we propose an adaptive ensemble method that combines visual prompts and text adapters with pre-trained VLMs, tailored by transfer difficulty, to achieve optimal performance for any target domain. Upon experimenting with extensive benchmarks, our method consistently outperforms all baselines, particularly on unseen tasks, demonstrating its effectiveness.",
            "link": "https://www.semanticscholar.org/paper/e287f3c941f4143eff2eb73f279a0c796239e187",
            "authors": "Yongjin Yang, Jongwoo Ko, SeYoung Yun",
            "EMNLP Paper ID": "235",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "1e8b76a27cc57793c09fff0be151532070f800e7",
            "title": "RWKV-CLIP: A Robust Vision-Language Representation Learner",
            "abstract": "Contrastive Language-Image Pre-training (CLIP) has significantly improved performance in various vision-language tasks by expanding the dataset with image-text pairs obtained from websites. This paper further explores CLIP from the perspectives of data and model architecture. To address the prevalence of noisy data and enhance the quality of large-scale image-text data crawled from the internet, we introduce a diverse description generation framework that can leverage Large Language Models (LLMs) to synthesize and refine content from web-based texts, synthetic captions, and detection tags. Furthermore, we propose RWKV-CLIP, the first RWKV-driven vision-language representation learning model that combines the effective parallel training of transformers with the efficient inference of RNNs. Comprehensive experiments across various model scales and pre-training datasets demonstrate that RWKV-CLIP is a robust and efficient vision-language representation learner, it achieves state-of-the-art performance in several downstream tasks, including linear probe, zero-shot classification, and zero-shot image-text retrieval. To facilitate future research, the code and pre-trained models are released at https://github.com/deepglint/RWKV-CLIP",
            "link": "https://www.semanticscholar.org/paper/1e8b76a27cc57793c09fff0be151532070f800e7",
            "authors": "Tiancheng Gu, Kaicheng Yang, Xiang An, Ziyong Feng, Dongnan Liu, Weidong Cai, Jiankang Deng",
            "EMNLP Paper ID": "525",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "6ad2776e368dade4da2ce71e4250087e0ca85868",
            "title": "Distilling Knowledge from Text-to-Image Generative Models Improves Visio-Linguistic Reasoning in CLIP",
            "abstract": "Image-text contrastive models like CLIP have wide applications in zero-shot classification, image-text retrieval, and transfer learning. However, they often struggle on compositional visio-linguistic tasks (e.g., attribute-binding or object-relationships) where their performance is no better than random chance. To address this, we introduce SDS-CLIP, a lightweight and sample-efficient distillation method to enhance CLIP's compositional visio-linguistic reasoning. Our approach fine-tunes CLIP using a distillation objective borrowed from large text-to-image generative models like Stable-Diffusion, which are known for their strong visio-linguistic reasoning abilities. On the challenging Winoground benchmark, SDS-CLIP improves the visio-linguistic performance of various CLIP models by up to 7%, while on the ARO dataset, it boosts performance by up to 3%. This work underscores the potential of well-designed distillation objectives from generative models to enhance contrastive image-text models with improved visio-linguistic reasoning capabilities.",
            "link": "https://www.semanticscholar.org/paper/6ad2776e368dade4da2ce71e4250087e0ca85868",
            "authors": "S. Basu, Maziar Sanjabi, Daniela Massiceti, S. Hu, S. Feizi",
            "EMNLP Paper ID": "683",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "75a7a3ab20a620f612db3337fcf6df03b304242d",
            "title": "SignCLIP: Connecting Text and Sign Language by Contrastive Learning",
            "abstract": "We present SignCLIP, which re-purposes CLIP (Contrastive Language-Image Pretraining) to project spoken language text and sign language videos, two classes of natural languages of distinct modalities, into the same space. SignCLIP is an efficient method of learning useful visual representations for sign language processing from large-scale, multilingual video-text pairs, without directly optimizing for a specific task or sign language which is often of limited size. We pretrain SignCLIP on Spreadthesign, a prominent sign language dictionary consisting of ~500 thousand video clips in up to 44 sign languages, and evaluate it with various downstream datasets. SignCLIP discerns in-domain signing with notable text-to-video/video-to-text retrieval accuracy. It also performs competitively for out-of-domain downstream tasks such as isolated sign language recognition upon essential few-shot prompting or fine-tuning. We analyze the latent space formed by the spoken language text and sign language poses, which provides additional linguistic insights. Our code and models are openly available.",
            "link": "https://www.semanticscholar.org/paper/75a7a3ab20a620f612db3337fcf6df03b304242d",
            "authors": "Zifan Jiang, Gerard Sant, Amit Moryossef, Mathias Muller, Rico Sennrich, Sarah Ebling",
            "EMNLP Paper ID": "1044",
            "Oral/Poster": "Oral",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "7571307f33a29f3b2431ec7e4a7a78ec1ce11dc1",
            "title": "If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions",
            "abstract": "Recent works often assume that Vision-Language Model (VLM) representations are based on visual attributes like shape. However, it is unclear to what extent VLMs prioritize this information to represent concepts. We propose Extract and Explore (EX2), a novel approach to characterize important textual features for VLMs. EX2 uses reinforcement learning to align a large language model with VLM preferences and generates descriptions that incorporate the important features for the VLM. Then, we inspect the descriptions to identify the features that contribute to VLM representations. We find that spurious descriptions have a major role in VLM representations despite providing no helpful information, e.g., Click to enlarge photo of CONCEPT. More importantly, among informative descriptions, VLMs rely significantly on non-visual attributes like habitat to represent visual concepts. Also, our analysis reveals that different VLMs prioritize different attributes in their representations. Overall, we show that VLMs do not simply match images to scene descriptions and that non-visual or even spurious descriptions significantly influence their representations.",
            "link": "https://www.semanticscholar.org/paper/7571307f33a29f3b2431ec7e4a7a78ec1ce11dc1",
            "authors": "Reza Esfandiarpoor, Cristina Menghini, Stephen H. Bach",
            "EMNLP Paper ID": "1091",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "4b923b9217be6ff5b8355ed12c6a7bf8a2cbaafb",
            "title": "Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification",
            "abstract": "Recent advances in fine-tuning Vision-Language Models (VLMs) have witnessed the success of prompt tuning and adapter tuning, while the classic model fine-tuning on inherent parameters seems to be overlooked. It is believed that fine-tuning the parameters of VLMs with few-shot samples corrupts the pre-trained knowledge since fine-tuning the CLIP model even degrades performance. In this paper, we revisit this viewpoint, and propose a new perspective: fine-tuning the specific parameters instead of all will uncover the power of classic model fine-tuning on VLMs. Through our meticulous study, we propose ClipFit, a simple yet effective method to fine-tune CLIP without introducing any overhead of extra parameters. We demonstrate that by only fine-tuning the specific bias terms and normalization layers, ClipFit can improve the performance of zero-shot CLIP by 7.27\\% average harmonic mean accuracy. Lastly, to understand how fine-tuning in CLIPFit affects the pre-trained models, we conducted extensive experimental analyses w.r.t. changes in internal parameters and representations. We found that low-level text bias layers and the first layer normalization layer change much more than other layers. The code is available at \\url{https://github.com/minglllli/CLIPFit}.",
            "link": "https://www.semanticscholar.org/paper/4b923b9217be6ff5b8355ed12c6a7bf8a2cbaafb",
            "authors": "Ming Li, Jike Zhong, Chenxing Li, Liuzhuozheng Li, Nie Lin, Masashi Sugiyama",
            "EMNLP Paper ID": "1664",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "b43d4a3077555752de4ca0ffcf4daab60227968d",
            "title": "VideoCLIP-XL: Advancing Long Description Understanding for Video CLIP Models",
            "abstract": "Contrastive Language-Image Pre-training (CLIP) has been widely studied and applied in numerous applications. However, the emphasis on brief summary texts during pre-training prevents CLIP from understanding long descriptions. This issue is particularly acute regarding videos given that videos often contain abundant detailed contents. In this paper, we propose the VideoCLIP-XL (eXtra Length) model, which aims to unleash the long-description understanding capability of video CLIP models. Firstly, we establish an automatic data collection system and gather a large-scale VILD pre-training dataset with VIdeo and Long-Description pairs. Then, we propose Text-similarity-guided Primary Component Matching (TPCM) to better learn the distribution of feature space while expanding the long description capability. We also introduce two new tasks namely Detail-aware Description Ranking (DDR) and Hallucination-aware Description Ranking (HDR) for further understanding improvement. Finally, we construct a Long Video Description Ranking (LVDR) benchmark for evaluating the long-description capability more comprehensively. Extensive experimental results on widely-used text-video retrieval benchmarks with both short and long descriptions and our LVDR benchmark can fully demonstrate the effectiveness of our method.",
            "link": "https://www.semanticscholar.org/paper/b43d4a3077555752de4ca0ffcf4daab60227968d",
            "authors": "Jiapeng Wang, Chengyu Wang, Kunzhe Huang, Jun Huang, Lianwen Jin",
            "EMNLP Paper ID": "1890",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "9772d4924aa01208046b9664e64d5bb54507b9df",
            "title": "Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving Vision-Linguistic Compositionality",
            "abstract": "In this paper, we propose a new method to enhance compositional understanding in pre-trained vision and language models (VLMs) without sacrificing performance in zero-shot multi-modal tasks. Traditional fine-tuning approaches often improve compositional reasoning at the cost of degrading multi-modal capabilities, primarily due to the use of global hard negative (HN) loss, which contrasts global representations of images and texts. This global HN loss pushes HN texts that are highly similar to the original ones, damaging the model's multi-modal representations. To overcome this limitation, we propose Fine-grained Selective Calibrated CLIP (FSC-CLIP), which integrates local hard negative loss and selective calibrated regularization. These innovations provide fine-grained negative supervision while preserving the model's representational integrity. Our extensive evaluations across diverse benchmarks for both compositionality and multi-modal tasks show that FSC-CLIP not only achieves compositionality on par with state-of-the-art models but also retains strong multi-modal capabilities. Code is available at: https://github.com/ytaek-oh/fsc-clip.",
            "link": "https://www.semanticscholar.org/paper/9772d4924aa01208046b9664e64d5bb54507b9df",
            "authors": "Youngtaek Oh, Jae Won Cho, Dong-Jin Kim, I. Kweon, Junmo Kim",
            "EMNLP Paper ID": "2403",
            "Oral/Poster": "Oral",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "4254ed35e15a99f6c7fbb634e42301e15212b696",
            "title": "Altogether: Image Captioning via Re-aligning Alt-text",
            "abstract": "This paper focuses on creating synthetic data to improve the quality of image captions. Existing works typically have two shortcomings. First, they caption images from scratch, ignoring existing alt-text metadata, and second, lack transparency if the captioners' training data (e.g. GPT) is unknown. In this paper, we study a principled approach Altogether based on the key idea to edit and re-align existing alt-texts associated with the images. To generate training data, we perform human annotation where annotators start with the existing alt-text and re-align it to the image content in multiple rounds, consequently constructing captions with rich visual concepts. This differs from prior work that carries out human annotation as a one-time description task solely based on images and annotator knowledge. We train a captioner on this data that generalizes the process of re-aligning alt-texts at scale. Our results show our Altogether approach leads to richer image captions that also improve text-to-image generation and zero-shot image classification tasks.",
            "link": "https://www.semanticscholar.org/paper/4254ed35e15a99f6c7fbb634e42301e15212b696",
            "authors": "Hu Xu, Po-Yao Huang, Xiaoqing Ellen Tan, Ching-Feng Yeh, Jacob Kahn, Christine Jou, Gargi Ghosh, Omer Levy, Luke Zettlemoyer, Wen-tau Yih, Shang-Wen Li, Saining Xie, Christoph Feichtenhofer",
            "EMNLP Paper ID": "2450",
            "Oral/Poster": "Oral",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "eff8782850cca1291fec91bbb5607b054cbbe892",
            "title": "IntCoOp: Interpretability-Aware Vision-Language Prompt Tuning",
            "abstract": "Image-text contrastive models such as CLIP learn transferable and robust representations for zero-shot transfer to a variety of downstream tasks. However, to obtain strong downstream performances, prompts need to be carefully curated, which can be a tedious engineering task. To address the issue of manual prompt engineering, prompt-tuning is used where a set of contextual vectors are learned by leveraging information from the training data. Despite their effectiveness, existing prompt-tuning frameworks often lack interpretability, thus limiting their ability to understand the compositional nature of images. In this work, we first identify that incorporating compositional attributes (e.g., a\"green\"tree frog) in the design of manual prompts can significantly enhance image-text alignment scores. Building upon this observation, we propose a novel and interpretable prompt-tuning method named IntCoOp, which learns to jointly align attribute-level inductive biases and class embeddings during prompt-tuning. To assess the effectiveness of our approach, we evaluate IntCoOp across two representative tasks in a few-shot learning setup: generalization to novel classes, and unseen domain shifts. Through extensive experiments across 10 downstream datasets on CLIP, we find that introducing attribute-level inductive biases leads to superior performance against state-of-the-art prompt tuning frameworks. Notably, in a 16-shot setup, IntCoOp improves CoOp by 7.35% in average performance across 10 diverse datasets.",
            "link": "https://www.semanticscholar.org/paper/eff8782850cca1291fec91bbb5607b054cbbe892",
            "authors": "Soumya Suvra Ghosal, Samyadeep Basu, S. Feizi, Dinesh Manocha",
            "EMNLP Paper ID": "2531",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "1e04c68f4cf4b3717c611bb31bcb5d289d66b095",
            "title": "Updating CLIP to Prefer Descriptions Over Captions",
            "abstract": "Although CLIPScore is a powerful generic metric that captures the similarity between a text and an image, it fails to distinguish between a caption that is meant to complement the information in an image and a description that is meant to replace an image entirely, e.g., for accessibility. We address this shortcoming by updating the CLIP model with the Concadia dataset to assign higher scores to descriptions than captions using parameter efficient fine-tuning and a loss objective derived from work on causal interpretability. This model correlates with the judgements of blind and low-vision people while preserving transfer capabilities and has interpretable structure that sheds light on the caption--description distinction.",
            "link": "https://www.semanticscholar.org/paper/1e04c68f4cf4b3717c611bb31bcb5d289d66b095",
            "authors": "Amir Zur, Elisa Kreiss, Karel D'Oosterlinck, Christopher Potts, Atticus Geiger",
            "EMNLP Paper ID": "2630",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "76255242639fb6d6b95d3486b300828fea72d9dd",
            "title": "IFCap: Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning",
            "abstract": "Recent advancements in image captioning have explored text-only training methods to overcome the limitations of paired image-text data. However, existing text-only training methods often overlook the modality gap between using text data during training and employing images during inference. To address this issue, we propose a novel approach called Image-like Retrieval, which aligns text features with visually relevant features to mitigate the modality gap. Our method further enhances the accuracy of generated captions by designing a Fusion Module that integrates retrieved captions with input features. Additionally, we introduce a Frequency-based Entity Filtering technique that significantly improves caption quality. We integrate these methods into a unified framework, which we refer to as IFCap ($\\textbf{I}$mage-like Retrieval and $\\textbf{F}$requency-based Entity Filtering for Zero-shot $\\textbf{Cap}$tioning). Through extensive experimentation, our straightforward yet powerful approach has demonstrated its efficacy, outperforming the state-of-the-art methods by a significant margin in both image captioning and video captioning compared to zero-shot captioning based on text-only training.",
            "link": "https://www.semanticscholar.org/paper/76255242639fb6d6b95d3486b300828fea72d9dd",
            "authors": "Soeun Lee, Si-Woo Kim, Taewhan Kim, Dong-Jin Kim",
            "EMNLP Paper ID": "2749",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "2547fe9a2c1b9b9b6c2d0273e0cbcfe5066db1d4",
            "title": "Can CLIP Count Stars? An Empirical Study on Quantity Bias in CLIP",
            "abstract": "CLIP has demonstrated great versatility in adapting to various downstream tasks, such as image editing and generation, visual question answering, and video understanding. However, CLIP-based applications often suffer from misunderstandings regarding user intent, leading to discrepancies between the required number of objects and the actual outputs in image generation tasks. In this work, we empirically investigate the quantity bias in CLIP. By carefully designing different experimental settings and datasets, we comprehensively evaluate CLIP's understanding of quantity from text, image, and cross-modal perspectives. Our experimental results reveal a quantity bias in CLIP embeddings, impacting the reliability of downstream tasks.",
            "link": "https://www.semanticscholar.org/paper/2547fe9a2c1b9b9b6c2d0273e0cbcfe5066db1d4",
            "authors": "Zeliang Zhang, Zhuo Liu, Mingqian Feng, Chenliang Xu",
            "matchScore": 264.30804,
            "original title": "Can CLIP Count Stars? An Empirical Study on Quantity Bias in CLIP",
            "original authors": "Zeliang Zhang, Zhuo Liu, Mingqian Feng, Chenliang Xu",
            "EMNLP Paper ID": "231",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "13f951d3d1a89a5b623a3b5c632a2efb676909f9",
            "title": "Semantic Token Reweighting for Interpretable and Controllable Text Embeddings in CLIP",
            "abstract": "A text encoder within Vision-Language Models (VLMs) like CLIP plays a crucial role in translating textual input into an embedding space shared with images, thereby facilitating the interpretative analysis of vision tasks through natural language. Despite the varying significance of different textual elements within a sentence depending on the context, efforts to account for variation of importance in constructing text embeddings have been lacking. We propose a framework of Semantic Token Reweighting to build Interpretable text embeddings (SToRI), which incorporates controllability as well. SToRI refines the text encoding process in CLIP by differentially weighting semantic elements based on contextual importance, enabling finer control over emphasis responsive to data-driven insights and user preferences. The efficacy of SToRI is demonstrated through comprehensive experiments on few-shot image classification and image retrieval tailored to user preferences.",
            "link": "https://www.semanticscholar.org/paper/13f951d3d1a89a5b623a3b5c632a2efb676909f9",
            "authors": "Eunji Kim, Kyuhong Shim, Simyung Chang, Sungroh Yoon",
            "matchScore": 291.61462,
            "original title": "Semantic Token Reweighting for Interpretable and Controllable Text Embeddings in CLIP",
            "original authors": "Eunji Kim, Kyuhong Shim, Simyung Chang, Sungroh Yoon",
            "EMNLP Paper ID": "2769",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "04784c3afd1fc6d6a60da49761e3d34f6b4109b6",
            "title": "Towards Efficient Visual-Language Alignment of the Q-Former for Visual Reasoning Tasks",
            "abstract": "Recent advancements in large language models have demonstrated enhanced capabilities in visual reasoning tasks by employing additional encoders for aligning different modalities. While the Q-Former has been widely used as a general encoder for aligning several modalities including image, video, audio, and 3D with large language models, previous works on its efficient training and the analysis of its individual components have been limited. In this work, we investigate the effectiveness of parameter efficient fine-tuning (PEFT) the Q-Former using InstructBLIP with visual reasoning benchmarks ScienceQA and IconQA. We observe that applying PEFT to the Q-Former achieves comparable performance to full fine-tuning using under 2% of the trainable parameters. Additionally, we employ AdaLoRA for dynamic parameter budget reallocation to examine the relative importance of the Q-Former's sublayers with 4 different benchmarks. Our findings reveal that the self-attention layers are noticeably more important in perceptual visual-language reasoning tasks, and relative importance of FFN layers depends on the complexity of visual-language patterns involved in tasks. The code is available at https://github.com/AttentionX/InstructBLIP_PEFT.",
            "link": "https://www.semanticscholar.org/paper/04784c3afd1fc6d6a60da49761e3d34f6b4109b6",
            "authors": "Sungkyung Kim, Adam Lee, Junyoung Park, Andrew Chung, Jusang Oh, Jayyoon Lee",
            "matchScore": 283.32126,
            "original title": "Towards Efficient Visual-Language Alignment of the Q-Former for Visual Reasoning Tasks",
            "original authors": "Sungkyung Kim, Adam Lee, Junyoung Park, Andrew Chung, Jusang Oh, Jay-Yoon Lee",
            "EMNLP Paper ID": "2899",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "b396bc377404e188251f66fea092b2cead9396b0",
            "title": "Enhancing Fine-Grained Image Classifications via Cascaded Vision Language Models",
            "abstract": "Fine-grained image classification, particularly in zero/few-shot scenarios, presents a significant challenge for vision-language models (VLMs), such as CLIP. These models often struggle with the nuanced task of distinguishing between semantically similar classes due to limitations in their pre-trained recipe, which lacks supervision signals for fine-grained categorization. This paper introduces CascadeVLM, an innovative framework that overcomes the constraints of previous CLIP-based methods by effectively leveraging the granular knowledge encapsulated within large vision-language models (LVLMs). Experiments across various fine-grained image datasets demonstrate that CascadeVLM significantly outperforms existing models, specifically on the Stanford Cars dataset, achieving an impressive 85.6% zero-shot accuracy. Performance gain analysis validates that LVLMs produce more accurate predictions for challenging images that CLIPs are uncertain about, bringing the overall accuracy boost. Our framework sheds light on a holistic integration of VLMs and LVLMs for effective and efficient fine-grained image classification.",
            "link": "https://www.semanticscholar.org/paper/b396bc377404e188251f66fea092b2cead9396b0",
            "authors": "Canshi Wei",
            "matchScore": 269.5055,
            "original title": "Enhancing Fine-Grained Image Classifications via Cascaded Vision Language Models",
            "original authors": "Canshi Wei",
            "EMNLP Paper ID": "374",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "c203ee6d2d823ef51538e2aa57c2fe35c3a45fe8",
            "title": "TROPE: TRaining-Free Object-Part Enhancement for Seamlessly Improving Fine-Grained Zero-Shot Image Captioning",
            "abstract": "Zero-shot inference, where pre-trained models perform tasks without specific training data, is an exciting emergent ability of large models like CLIP. Although there has been considerable exploration into enhancing zero-shot abilities in image captioning (IC) for popular datasets such as MSCOCO and Flickr8k, these approaches fall short with fine-grained datasets like CUB, FLO, UCM-Captions, and Sydney-Captions. These datasets require captions to discern between visually and semantically similar classes, focusing on detailed object parts and their attributes. To overcome this challenge, we introduce TRaining-Free Object-Part Enhancement (TROPE). TROPE enriches a base caption with additional object-part details using object detector proposals and Natural Language Processing techniques. It complements rather than alters the base caption, allowing seamless integration with other captioning methods and offering users enhanced flexibility. Our evaluations show that TROPE consistently boosts performance across all tested zero-shot IC approaches and achieves state-of-the-art results on fine-grained IC datasets.",
            "link": "https://www.semanticscholar.org/paper/c203ee6d2d823ef51538e2aa57c2fe35c3a45fe8",
            "authors": "Joshua Forster Feinglass, Yezhou Yang",
            "matchScore": 369.7884,
            "original title": "TROPE: TRaining-Free Object-Part Enhancement for Seamlessly Improving Fine-Grained Zero-Shot Image Captioning",
            "original authors": "Joshua Feinglass, Yezhou Yang",
            "EMNLP Paper ID": "736",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        }
    ],
    "Knowledge Management and Lifelong Learning in Large Language Models": [
        {
            "paperId": "6202c20b152b6b9f379aa1714bbd01594dcc990a",
            "title": "LEMoE: Advanced Mixture of Experts Adaptor for Lifelong Model Editing of Large Language Models",
            "abstract": "Large language models (LLMs) require continual knowledge updates to stay abreast of the ever-changing world facts, prompting the formulation of lifelong model editing task. While recent years have witnessed the development of various techniques for single and batch editing, these methods either fail to apply or perform sub-optimally when faced with lifelong editing. In this paper, we introduce LEMoE, an advanced Mixture of Experts (MoE) adaptor for lifelong model editing. We first analyze the factors influencing the effectiveness of conventional MoE adaptor in lifelong editing, including catastrophic forgetting, inconsistent routing and order sensitivity. Based on these insights, we propose a tailored module insertion method to achieve lifelong editing, incorporating a novel KV anchor routing to enhance routing consistency between training and inference stage, along with a concise yet effective clustering-based editing order planning. Experimental results demonstrate the effectiveness of our method in lifelong editing, surpassing previous model editing techniques while maintaining outstanding performance in batch editing task. Our code will be available.",
            "link": "https://www.semanticscholar.org/paper/6202c20b152b6b9f379aa1714bbd01594dcc990a",
            "authors": "Renzhi Wang, Piji Li",
            "EMNLP Paper ID": "286",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "bf3cd8a979617ef0021e88e8c1844bfb513beee9",
            "title": "Dissecting Fine-Tuning Unlearning in Large Language Models",
            "abstract": "Fine-tuning-based unlearning methods prevail for preventing targeted harmful, sensitive, or copyrighted information within large language models while preserving overall capabilities. However, the true effectiveness of these methods is unclear. In this work, we delve into the limitations of fine-tuning-based unlearning through activation patching and parameter restoration experiments. Our findings reveal that these methods alter the model's knowledge retrieval process, providing further evidence that they do not genuinely erase the problematic knowledge embedded in the model parameters. Instead, the coefficients generated by the MLP components in the model's final layer are the primary contributors to these seemingly positive unlearning effects, playing a crucial role in controlling the model's behaviors. Furthermore, behavioral tests demonstrate that this unlearning mechanism inevitably impacts the global behavior of the models, affecting unrelated knowledge or capabilities. The code is released at https://github.com/yihuaihong/Dissecting-FT-Unlearning.",
            "link": "https://www.semanticscholar.org/paper/bf3cd8a979617ef0021e88e8c1844bfb513beee9",
            "authors": "Yihuai Hong, Yuelin Zou, Lijie Hu, Ziqian Zeng, Di Wang, Haiqin Yang",
            "EMNLP Paper ID": "443",
            "Oral/Poster": "Oral",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "4095e273f5efc419d3b8bb36a2e3c564c346fc33",
            "title": "SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning",
            "abstract": "Large Language Models (LLMs) have highlighted the necessity of effective unlearning mechanisms to comply with data regulations and ethical AI practices. LLM unlearning aims at removing undesired data influences and associated model capabilities without compromising utility beyond the scope of unlearning. While interest in studying LLM unlearning is growing, the impact of the optimizer choice for LLM unlearning remains unexplored. In this work, we shed light on the significance of optimizer selection in LLM unlearning for the first time, establishing a clear connection between second-order optimization and influence unlearning (a classical approach using influence functions to update the model for data influence removal). This insight propels us to develop a second-order optimization-based LLM unlearning framework, termed Second-Order UnLearning (SOUL), which extends the static, one-shot model update using influence unlearning to a dynamic, iterative unlearning process. Our extensive experiments show that SOUL consistently outperforms conventional first-order methods across various unlearning tasks, models, and metrics, indicating that second-order optimization offers an effective and broadly applicable solution for LLM unlearning. Codes are available at https://github.com/OPTML-Group/SOUL.",
            "link": "https://www.semanticscholar.org/paper/4095e273f5efc419d3b8bb36a2e3c564c346fc33",
            "authors": "Jinghan Jia, Yihua Zhang, Yimeng Zhang, Jiancheng Liu, Bharat Runwal, James Diffenderfer, B. Kailkhura, Sijia Liu",
            "EMNLP Paper ID": "471",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "e8982f1478986a617ffbeebdc6e0482ce2bc6811",
            "title": "Forgetting Curve: A Reliable Method for Evaluating Memorization Capability for Long-context Models",
            "abstract": "Numerous recent works target to extend effective context length for language models and various methods, tasks and benchmarks exist to measure model's effective memorization length. However, through thorough investigations, we find limitations for currently existing evaluations on model's memorization capability. We provide an extensive survey for limitations in this work and propose a new method called forgetting curve to measure the memorization capability of long-context models. We show that forgetting curve has the advantage of being robust to the tested corpus and the experimental settings, of not relying on prompts and can be applied to any model size. We apply our forgetting curve to a large variety of models involving both transformer and RNN/SSM based architectures. Our measurement provides empirical evidence for the effectiveness of transformer extension techniques while raises questions for the effective length of RNN/SSM based models. We also examine the difference between our measurement and existing benchmarks as well as popular metrics for various models. Our code and results can be found at https://github.com/1azybug/ForgettingCurve.",
            "link": "https://www.semanticscholar.org/paper/e8982f1478986a617ffbeebdc6e0482ce2bc6811",
            "authors": "Xinyu Liu, Runsong Zhao, Pengcheng Huang, Chunyang Xiao, Bei Li, Jingang Wang, Tong Xiao, Jingbo Zhu",
            "EMNLP Paper ID": "514",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "2d5fbc53c2a7d142eeb9a53863413b7688a8b00f",
            "title": "More Than Catastrophic Forgetting: Integrating General Capabilities For Domain-Specific LLMs",
            "abstract": "The performance on general tasks decreases after Large Language Models (LLMs) are fine-tuned on domain-specific tasks, the phenomenon is known as Catastrophic Forgetting (CF). However, this paper presents a further challenge for real application of domain-specific LLMs beyond CF, called General Capabilities Integration (GCI), which necessitates the integration of both the general capabilities and domain knowledge within a single instance. The objective of GCI is not merely to retain previously acquired general capabilities alongside new domain knowledge, but to harmonize and utilize both sets of skills in a cohesive manner to enhance performance on domain-specific tasks. Taking legal domain as an example, we carefully design three groups of training and testing tasks without lacking practicability, and construct the corresponding datasets. To better incorporate general capabilities across domain-specific scenarios, we introduce ALoRA, which utilizes a multi-head attention module upon LoRA, facilitating direct information transfer from preceding tokens to the current one. This enhancement permits the representation to dynamically switch between domain-specific knowledge and general competencies according to the attention. Extensive experiments are conducted on the proposed tasks. The results exhibit the significance of our setting, and the effectiveness of our method.",
            "link": "https://www.semanticscholar.org/paper/2d5fbc53c2a7d142eeb9a53863413b7688a8b00f",
            "authors": "Chengyuan Liu, Shihang Wang, Yangyang Kang, Lizhi Qing, Fubang Zhao, Changlong Sun, Kun Kuang, Fei Wu",
            "EMNLP Paper ID": "854",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "be12751d3dc6f7d8aa79ae6de3696fb444f8b3a6",
            "title": "Revisiting Who's Harry Potter: Towards Targeted Unlearning from a Causal Intervention Perspective",
            "abstract": "This paper investigates Who's Harry Potter (WHP), a pioneering yet insufficiently understood method for LLM unlearning. We explore it in two steps. First, we introduce a new task of LLM targeted unlearning, where given an unlearning target (e.g., a person) and some unlearning documents, we aim to unlearn only the information about the target, rather than everything in the unlearning documents. We further argue that a successful unlearning should satisfy criteria such as not outputting gibberish, not fabricating facts about the unlearning target, and not releasing factual information under jailbreak attacks. Second, we construct a causal intervention framework for targeted unlearning, where the knowledge of the unlearning target is modeled as a confounder between LLM input and output, and the unlearning process as a deconfounding process. This framework justifies and extends WHP, deriving a simple unlearning algorithm that includes WHP as a special case. Experiments on existing and new datasets show that our approach, without explicitly optimizing for the aforementioned criteria, achieves competitive performance in all of them. Our code is available at https://github.com/UCSB-NLP-Chang/causal_unlearn.git.",
            "link": "https://www.semanticscholar.org/paper/be12751d3dc6f7d8aa79ae6de3696fb444f8b3a6",
            "authors": "Yujian Liu, Yang Zhang, T. Jaakkola, Shiyu Chang",
            "EMNLP Paper ID": "1004",
            "Oral/Poster": "Oral",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "d62c17ea0537f6f02541ee35c1cdce78d4e1cce6",
            "title": "Demystifying Verbatim Memorization in Large Language Models",
            "abstract": "Large Language Models (LLMs) frequently memorize long sequences verbatim, often with serious legal and privacy implications. Much prior work has studied such verbatim memorization using observational data. To complement such work, we develop a framework to study verbatim memorization in a controlled setting by continuing pre-training from Pythia checkpoints with injected sequences. We find that (1) non-trivial amounts of repetition are necessary for verbatim memorization to happen; (2) later (and presumably better) checkpoints are more likely to verbatim memorize sequences, even for out-of-distribution sequences; (3) the generation of memorized sequences is triggered by distributed model states that encode high-level features and makes important use of general language modeling capabilities. Guided by these insights, we develop stress tests to evaluate unlearning methods and find they often fail to remove the verbatim memorized information, while also degrading the LM. Overall, these findings challenge the hypothesis that verbatim memorization stems from specific model weights or mechanisms. Rather, verbatim memorization is intertwined with the LM's general capabilities and thus will be very difficult to isolate and suppress without degrading model quality.",
            "link": "https://www.semanticscholar.org/paper/d62c17ea0537f6f02541ee35c1cdce78d4e1cce6",
            "authors": "Jing Huang, Diyi Yang, Christopher Potts",
            "EMNLP Paper ID": "1213",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "8119fb5cfa07e0cd3ee748a2d3c1f5bd48b0f5d8",
            "title": "A Multi-Perspective Analysis of Memorization in Large Language Models",
            "abstract": "Large Language Models (LLMs), trained on massive corpora with billions of parameters, show unprecedented performance in various fields. Though surprised by their excellent performances, researchers also noticed some special behaviors of those LLMs. One of those behaviors is memorization, in which LLMs can generate the same content used to train them. Though previous research has discussed memorization, the memorization of LLMs still lacks explanation, especially the cause of memorization and the dynamics of generating them. In this research, we comprehensively discussed memorization from various perspectives and extended the discussion scope to not only just the memorized content but also less and unmemorized content. Through various studies, we found that: (1) Through experiments, we revealed the relation of memorization between model size, continuation size, and context size. Further, we showed how unmemorized sentences transition to memorized sentences. (2) Through embedding analysis, we showed the distribution and decoding dynamics across model size in embedding space for sentences with different memorization scores. The n-gram statistics analysis presents d (3) An analysis over n-gram and entropy decoding dynamics discovered a boundary effect when the model starts to generate memorized sentences or unmemorized sentences. (4)We trained a Transformer model to predict the memorization of different models, showing that it is possible to predict memorizations by context.",
            "link": "https://www.semanticscholar.org/paper/8119fb5cfa07e0cd3ee748a2d3c1f5bd48b0f5d8",
            "authors": "Bowen Chen, Namgi Han, Yusuke Miyao",
            "EMNLP Paper ID": "1299",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "487f18b2419571757e0a17eacf345bf39219bb56",
            "title": "Lifelong Event Detection via Optimal Transport",
            "abstract": "Continual Event Detection (CED) poses a formidable challenge due to the catastrophic forgetting phenomenon, where learning new tasks (with new coming event types) hampers performance on previous ones. In this paper, we introduce a novel approach, Lifelong Event Detection via Optimal Transport (LEDOT), that leverages optimal transport principles to align the optimization of our classification module with the intrinsic nature of each class, as defined by their pre-trained language modeling. Our method integrates replay sets, prototype latent representations, and an innovative Optimal Transport component. Extensive experiments on MAVEN and ACE datasets demonstrate LEDOT's superior performance, consistently outperforming state-of-the-art baselines. The results underscore LEDOT as a pioneering solution in continual event detection, offering a more effective and nuanced approach to addressing catastrophic forgetting in evolving environments.",
            "link": "https://www.semanticscholar.org/paper/487f18b2419571757e0a17eacf345bf39219bb56",
            "authors": "Viet Dao, Van-Cuong Pham, Quyen Tran, Thanh-Thien Le, L. Van, Thien Huu Nguyen",
            "EMNLP Paper ID": "1464",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "ad5184b1542b967935b4cedab62ddfa4757894c4",
            "title": "Language Concept Erasure for Language-invariant Dense Retrieval",
            "abstract": "Multilingual models aim for language-invariant representations but still prominently encode language identity. This, along with the scarcity of high-quality parallel retrieval data, limits their performance in retrieval. We introduce LANCER, a multi-task learning framework that improves language-invariant dense retrieval by reducing language-specific signals in the embedding space. Leveraging the notion of linear concept erasure, we design a loss function that penalizes cross-correlation between representations and their language labels. LANCER leverages only English retrieval data and general multilingual corpora, training models to focus on language-invariant retrieval by semantic similarity without necessitating a vast parallel corpus. Experimental results on various datasets show our method consistently improves over baselines, with extensive analyses demonstrating greater language agnosticism.",
            "link": "https://www.semanticscholar.org/paper/ad5184b1542b967935b4cedab62ddfa4757894c4",
            "authors": "Zhiqi Huang, Shauli Ravfogel, James Allan",
            "EMNLP Paper ID": "1539",
            "Oral/Poster": "Oral",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "9d85ea26518bace986a2f7cdffa24edad2b20c87",
            "title": "Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion",
            "abstract": "Model fusion research aims to aggregate the knowledge of multiple individual models to enhance performance by combining their weights. In this work, we study the inverse problem: investigating whether model fusion can be used to reduce unwanted knowledge. We investigate the effects of model fusion in three scenarios: the learning of shortcuts, social biases, and memorization of training data in fine-tuned language models. Through experiments covering classification and generation tasks, our analysis highlights that shared knowledge among models is enhanced during model fusion, while unshared knowledge is usually forgotten. Based on this observation, we demonstrate the potential of model fusion as a debiasing tool and showcase its efficacy in addressing privacy concerns associated with language models.",
            "link": "https://www.semanticscholar.org/paper/9d85ea26518bace986a2f7cdffa24edad2b20c87",
            "authors": "Kerem Zaman, Leshem Choshen, Shashank Srivastava",
            "EMNLP Paper ID": "2345",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "f00344d9b790411232de8ff2ca06af909e92f1b7",
            "title": "Unlocking Continual Learning Abilities in Language Models",
            "abstract": "Language models (LMs) exhibit impressive performance and generalization capabilities. However, LMs struggle with the persistent challenge of catastrophic forgetting, which undermines their long-term sustainability in continual learning (CL). Existing approaches usually address the issue by incorporating old task data or task-wise inductive bias into LMs. However, old data and accurate task information are often unavailable or costly to collect, hindering the availability of current CL approaches for LMs. To address this limitation, we introduce $\\textbf{MIGU}$ ($\\textbf{M}$agn$\\textbf{I}$tude-based $\\textbf{G}$radient $\\textbf{U}$pdating for continual learning), a rehearsal-free and task-label-free method that only updates the model parameters with large magnitudes of output in LMs' linear layers. MIGU is based on our observation that the L1-normalized magnitude distribution of the output in LMs' linear layers is different when the LM models deal with different task data. By imposing this simple constraint on the gradient update process, we can leverage the inherent behaviors of LMs, thereby unlocking their innate CL abilities. Our experiments demonstrate that MIGU is universally applicable to all three LM architectures (T5, RoBERTa, and Llama2), delivering state-of-the-art or on-par performance across continual finetuning and continual pre-training settings on four CL benchmarks. For example, MIGU brings a 15.2% average accuracy improvement over conventional parameter-efficient finetuning baselines in a 15-task CL benchmark. MIGU can also seamlessly integrate with all three existing CL types to further enhance performance. Code is available at https://github.com/wenyudu/MIGU.",
            "link": "https://www.semanticscholar.org/paper/f00344d9b790411232de8ff2ca06af909e92f1b7",
            "authors": "Wenyu Du, Shuang Cheng, Tongxu Luo, Zihan Qiu, Zeyu Huang, Ka Chun Cheung, Reynold Cheng, Jie Fu",
            "matchScore": 195.74756,
            "original title": "Unlocking Continual Learning Abilities in Language Models",
            "original authors": "Wenyu Du, Shuang Cheng, Tongxu Luo, Zihan Qiu, Zeyu Huang, Ka Chun Cheung, Reynold Cheng, Jie Fu",
            "EMNLP Paper ID": "1325",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "04da681a5cd508826c134f0a65c5c12acd0edd46",
            "title": "Cross-Lingual Unlearning of Selective Knowledge in Multilingual Language Models",
            "abstract": "Pretrained language models memorize vast amounts of information, including private and copyrighted data, raising significant safety concerns. Retraining these models after excluding sensitive data is prohibitively expensive, making machine unlearning a viable, cost-effective alternative. Previous research has focused on machine unlearning for monolingual models, but we find that unlearning in one language does not necessarily transfer to others. This vulnerability makes models susceptible to low-resource language attacks, where sensitive information remains accessible in less dominant languages. This paper presents a pioneering approach to machine unlearning for multilingual language models, selectively erasing information across different languages while maintaining overall performance. Specifically, our method employs an adaptive unlearning scheme that assigns language-dependent weights to address different language performances of multilingual language models. Empirical results demonstrate the effectiveness of our framework compared to existing unlearning baselines, setting a new standard for secure and adaptable multilingual language models.",
            "link": "https://www.semanticscholar.org/paper/04da681a5cd508826c134f0a65c5c12acd0edd46",
            "authors": "Minseok Choi, Kyunghyun Min, Jaegul Choo",
            "matchScore": 251.58084,
            "original title": "Cross-Lingual Unlearning of Selective Knowledge in Multilingual Language Models",
            "original authors": "Minseok Choi, Kyunghyun Min, Jaegul Choo",
            "EMNLP Paper ID": "2164",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "541d06a0b586187a3d1b85e222c86777442f27ec",
            "title": "Scaling Laws for Fact Memorization of Large Language Models",
            "abstract": "Fact knowledge memorization is crucial for Large Language Models (LLM) to generate factual and reliable responses. However, the behaviors of LLM fact memorization remain under-explored. In this paper, we analyze the scaling laws for LLM's fact knowledge and LLMs' behaviors of memorizing different types of facts. We find that LLMs' fact knowledge capacity has a linear and negative exponential law relationship with model size and training epochs, respectively. Estimated by the built scaling law, memorizing the whole Wikidata's facts requires training an LLM with 1000B non-embed parameters for 100 epochs, suggesting that using LLMs to memorize all public facts is almost implausible for a general pre-training setting. Meanwhile, we find that LLMs can generalize on unseen fact knowledge and its scaling law is similar to general pre-training. Additionally, we analyze the compatibility and preference of LLMs' fact memorization. For compatibility, we find LLMs struggle with memorizing redundant facts in a unified way. Only when correlated facts have the same direction and structure, the LLM can compatibly memorize them. This shows the inefficiency of LLM memorization for redundant facts. For preference, the LLM pays more attention to memorizing more frequent and difficult facts, and the subsequent facts can overwrite prior facts' memorization, which significantly hinders low-frequency facts memorization. Our findings reveal the capacity and characteristics of LLMs' fact knowledge learning, which provide directions for LLMs' fact knowledge augmentation.",
            "link": "https://www.semanticscholar.org/paper/541d06a0b586187a3d1b85e222c86777442f27ec",
            "authors": "Xingyu Lu, Xiaonan Li, Qinyuan Cheng, Kai Ding, Xuanjing Huang, Xipeng Qiu",
            "matchScore": 207.26764,
            "original title": "Scaling Laws for Fact Memorization of Large Language Models",
            "original authors": "Xingyu Lu, Xiaonan Li, Qinyuan Cheng, Kai Ding, Xipeng Qiu",
            "EMNLP Paper ID": "2235",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "9669862f69b2e4cf05ecf02b7e6f38c2899acf28",
            "title": "To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models",
            "abstract": "Large Language Models (LLMs) trained on extensive corpora inevitably retain sensitive data, such as personal privacy information and copyrighted material. Recent advancements in knowledge unlearning involve updating LLM parameters to erase specific knowledge. However, current unlearning paradigms are mired in vague forgetting boundaries, often erasing knowledge indiscriminately. In this work, we introduce KnowUnDo, a benchmark containing copyrighted content and user privacy domains to evaluate if the unlearning process inadvertently erases essential knowledge. Our findings indicate that existing unlearning methods often suffer from excessive unlearning. To address this, we propose a simple yet effective method, MemFlex, which utilizes gradient information to precisely target and unlearn sensitive parameters. Experimental results show that MemFlex is superior to existing methods in both precise knowledge unlearning and general knowledge retaining of LLMs. Code and dataset are released at https://github.com/zjunlp/KnowUnDo.",
            "link": "https://www.semanticscholar.org/paper/9669862f69b2e4cf05ecf02b7e6f38c2899acf28",
            "authors": "Bo Tian, Xiaozhuan Liang, Siyuan Cheng, Qingbin Liu, Meng Wang, Dianbo Sui, Xi Chen, Huajun Chen, Ningyu Zhang",
            "matchScore": 273.47305,
            "original title": "To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models",
            "original authors": "Bozhong Tian, Xiaozhuan Liang, Siyuan Cheng, Qingbin Liu, Mengru Wang, Dianbo Sui, Xi Chen, Huajun Chen, Ningyu Zhang",
            "EMNLP Paper ID": "305",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "d3767fa551f15ca6967db1f55562cee902c747ab",
            "title": "Revisiting Catastrophic Forgetting in Large Language Model Tuning",
            "abstract": "Catastrophic Forgetting (CF) means models forgetting previously acquired knowledge when learning new data. It compromises the effectiveness of large language models (LLMs) during fine-tuning, yet the underlying causes have not been thoroughly investigated. This paper takes the first step to reveal the direct link between the flatness of the model loss landscape and the extent of CF in the field of LLMs. Based on this, we introduce the sharpness-aware minimization to mitigate CF by flattening the loss landscape. Experiments on three widely-used fine-tuning datasets, spanning different model scales, demonstrate the effectiveness of our method in alleviating CF. Analyses show that we nicely complement the existing anti-forgetting strategies, further enhancing the resistance of LLMs to CF.",
            "link": "https://www.semanticscholar.org/paper/d3767fa551f15ca6967db1f55562cee902c747ab",
            "authors": "Hongyu Li, Liang Ding, Meng Fang, D. Tao",
            "matchScore": 229.51898,
            "original title": "Revisiting Catastrophic Forgetting in Large Language Model Tuning",
            "original authors": "Hongyu Li, Liang Ding, Meng Fang, Dacheng Tao",
            "EMNLP Paper ID": "864",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "7a8a753359a42985690e01b1f061b5048e396474",
            "title": "Boosting Large Language Models with Continual Learning for Aspect-based Sentiment Analysis",
            "abstract": "Aspect-based sentiment analysis (ABSA) is an important subtask of sentiment analysis, which aims to extract the aspects and predict their sentiments. Most existing studies focus on improving the performance of the target domain by fine-tuning domain-specific models (trained on source domains) based on the target domain dataset. Few works propose continual learning tasks for ABSA, which aim to learn the target domain's ability while maintaining the history domains' abilities. In this paper, we propose a Large Language Model-based Continual Learning (\\texttt{LLM-CL}) model for ABSA. First, we design a domain knowledge decoupling module to learn a domain-invariant adapter and separate domain-variant adapters dependently with an orthogonal constraint. Then, we introduce a domain knowledge warmup strategy to align the representation between domain-invariant and domain-variant knowledge. In the test phase, we index the corresponding domain-variant knowledge via domain positioning to not require each sample's domain ID. Extensive experiments over 19 datasets indicate that our \\texttt{LLM-CL} model obtains new state-of-the-art performance.",
            "link": "https://www.semanticscholar.org/paper/7a8a753359a42985690e01b1f061b5048e396474",
            "authors": "Xuanwen Ding, Jie Zhou, Liang Dou, Qin Chen, Yuanbin Wu, Chengcai Chen, Liang He",
            "matchScore": 234.71132,
            "original title": "Boosting Large Language Models with Continual Learning for Aspect-based Sentiment Analysis",
            "original authors": "Xuanwen Ding, Jie Zhou, Liang Dou, Qin Chen, Yuanbin Wu, Arlene Chen, Liang He",
            "EMNLP Paper ID": "868",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        }
    ],
    "Improving and Adapting Large Language Models": [
        {
            "paperId": "66521063549cc84da1eab51f9f0b42108dd3efc7",
            "title": "ScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws",
            "abstract": "High-quality data is crucial for the pre-training performance of large language models. Unfortunately, existing quality filtering methods rely on a known high-quality dataset as reference, which can introduce potential bias and compromise diversity. In this paper, we propose ScalingFilter, a novel approach that evaluates text quality based on the perplexity difference between two language models trained on the same data, thereby eliminating the influence of the reference dataset in the filtering process. An theoretical analysis shows that ScalingFilter is equivalent to an inverse utilization of scaling laws. Through training models with 1.3B parameters on the same data source processed by various quality filters, we find ScalingFilter can improve zero-shot performance of pre-trained models in downstream tasks. To assess the bias introduced by quality filtering, we introduce semantic diversity, a metric of utilizing text embedding models for semantic representations. Extensive experiments reveal that semantic diversity is a reliable indicator of dataset diversity, and ScalingFilter achieves an optimal balance between downstream performance and semantic diversity.",
            "link": "https://www.semanticscholar.org/paper/66521063549cc84da1eab51f9f0b42108dd3efc7",
            "authors": "Ruihang Li, Yixuan Wei, Miaosen Zhang, Nenghai Yu, Han Hu, Houwen Peng",
            "EMNLP Paper ID": "356",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "c6cdfa6b103980fecfdf185ef7ab2bc6e61625de",
            "title": "KidLM: Advancing Language Models for Children -- Early Insights and Future Directions",
            "abstract": "Recent studies highlight the potential of large language models in creating educational tools for children, yet significant challenges remain in maintaining key child-specific properties such as linguistic nuances, cognitive needs, and safety standards. In this paper, we explore foundational steps toward the development of child-specific language models, emphasizing the necessity of high-quality pre-training data. We introduce a novel user-centric data collection pipeline that involves gathering and validating a corpus specifically written for and sometimes by children. Additionally, we propose a new training objective, Stratified Masking, which dynamically adjusts masking probabilities based on our domain-specific child language data, enabling models to prioritize vocabulary and concepts more suitable for children. Experimental evaluations demonstrate that our model excels in understanding lower grade-level text, maintains safety by avoiding stereotypes, and captures children's unique preferences. Furthermore, we provide actionable insights for future research and development in child-specific language modeling.",
            "link": "https://www.semanticscholar.org/paper/c6cdfa6b103980fecfdf185ef7ab2bc6e61625de",
            "authors": "Mir Tafseer Nayeem, Davood Rafiei",
            "EMNLP Paper ID": "527",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "90a3ebfd0aa17589de87f0a17d04d27eab4dd384",
            "title": "What is 'Typological Diversity' in NLP?",
            "abstract": "The NLP research community has devoted increased attention to languages beyond English, resulting in considerable improvements for multilingual NLP. However, these improvements only apply to a small subset of the world's languages. Aiming to extend this, an increasing number of papers aspires to enhance generalizable multilingual performance across languages. To this end, linguistic typology is commonly used to motivate language selection, on the basis that a broad typological sample ought to imply generalization across a broad range of languages. These selections are often described as being 'typologically diverse'. In this work, we systematically investigate NLP research that includes claims regarding 'typological diversity'. We find there are no set definitions or criteria for such claims. We introduce metrics to approximate the diversity of language selection along several axes and find that the results vary considerably across papers. Crucially, we show that skewed language selection can lead to overestimated multilingual performance. We recommend future work to include an operationalization of 'typological diversity' that empirically justifies the diversity of language samples.",
            "link": "https://www.semanticscholar.org/paper/90a3ebfd0aa17589de87f0a17d04d27eab4dd384",
            "authors": "Esther Ploeger, Wessel Poelman, Miryam de Lhoneux, Johannes Bjerva",
            "EMNLP Paper ID": "633",
            "Oral/Poster": "Oral",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "22dfd3f9c056f61e5432b56ab6fcf138bf915b66",
            "title": "Methods for Automatic Matrix Language Determination of Code-Switched Speech",
            "abstract": "Code-switching (CS) is the process of speakers interchanging between two or more languages which in the modern world becomes increasingly common. In order to better describe CS speech the Matrix Language Frame (MLF) theory introduces the concept of a Matrix Language, which is the language that provides the grammatical structure for a CS utterance. In this work the MLF theory was used to develop systems for Matrix Language Identity (MLID) determination. The MLID of English/Mandarin and English/Spanish CS text and speech was compared to acoustic language identity (LID), which is a typical way to identify a language in monolingual utterances. MLID predictors from audio show higher correlation with the textual principles than LID in all cases while also outperforming LID in an MLID recognition task based on F1 macro (60\\%) and correlation score (0.38). This novel approach has identified that non-English languages (Mandarin and Spanish) are preferred over the English language as the ML contrary to the monolingual choice of LID.",
            "link": "https://www.semanticscholar.org/paper/22dfd3f9c056f61e5432b56ab6fcf138bf915b66",
            "authors": "Olga Iakovenko, Thomas Hain",
            "EMNLP Paper ID": "645",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "0126fea92578e34ec9e3e1a0047c4e85dc4c93b4",
            "title": "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?",
            "abstract": "Multilingual large language models are designed, claimed, and expected to cater to speakers of varied languages. We hypothesise that the current practices of fine-tuning and evaluating these models may not perfectly align with this objective owing to a heavy reliance on translation, which cannot cover language-specific knowledge but can introduce translation defects. It remains unknown whether the nature of the instruction data has an impact on the model output; conversely, it is questionable whether translated test sets can capture such nuances. Due to the often coupled practices of using translated data in both stages, such imperfections could have been overlooked. This work investigates these issues using controlled native or translated data during the instruction tuning and evaluation stages. We show that native or generation benchmarks reveal a notable difference between native and translated instruction data especially when model performance is high, whereas other types of test sets cannot. The comparison between round-trip and single-pass translations reflects the importance of knowledge from language-native resources. Finally, we demonstrate that regularization is beneficial to bridging this gap on structured but not generative tasks.",
            "link": "https://www.semanticscholar.org/paper/0126fea92578e34ec9e3e1a0047c4e85dc4c93b4",
            "authors": "Pinzhen Chen, Simon Yu, Zhicheng Guo, B. Haddow",
            "EMNLP Paper ID": "1080",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "9697bc15d52b2a8c38baed62f38726a04c4a2f77",
            "title": "Data, Data Everywhere: A Guide for Pretraining Dataset Construction",
            "abstract": "The impressive capabilities of recent language models can be largely attributed to the multi-trillion token pretraining datasets that they are trained on. However, model developers fail to disclose their construction methodology which has lead to a lack of open information on how to develop effective pretraining sets. To address this issue, we perform the first systematic study across the entire pipeline of pretraining set construction. First, we run ablations on existing techniques for pretraining set development to identify which methods translate to the largest gains in model accuracy on downstream evaluations. Then, we categorize the most widely used data source, web crawl snapshots, across the attributes of toxicity, quality, type of speech, and domain. Finally, we show how such attribute information can be used to further refine and improve the quality of a pretraining set. These findings constitute an actionable set of steps that practitioners can use to develop high quality pretraining sets.",
            "link": "https://www.semanticscholar.org/paper/9697bc15d52b2a8c38baed62f38726a04c4a2f77",
            "authors": "Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Bo Liu, Aastha Jhunjhunwala, Zhilin Wang, M. Patwary, Mohammad Shoeybi, Bryan Catanzaro",
            "EMNLP Paper ID": "1205",
            "Oral/Poster": "Oral",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "75171b38850d1a818539851d322c8667e93ac582",
            "title": "What is lost in Normalization? Exploring Pitfalls in Multilingual ASR Model Evaluations",
            "abstract": "This paper explores the pitfalls in evaluating multilingual automatic speech recognition (ASR) models, with a particular focus on Indic language scripts. We investigate the text normalization routine employed by leading ASR models, including OpenAI Whisper, Meta's MMS, Seamless, and Assembly AI's Conformer, and their unintended consequences on performance metrics. Our research reveals that current text normalization practices, while aiming to standardize ASR outputs for fair comparison, by removing inconsistencies such as variations in spelling, punctuation, and special characters, are fundamentally flawed when applied to Indic scripts. Through empirical analysis using text similarity scores and in-depth linguistic examination, we demonstrate that these flaws lead to artificially improved performance metrics for Indic languages. We conclude by proposing a shift towards developing text normalization routines that leverage native linguistic expertise, ensuring more robust and accurate evaluations of multilingual ASR models.",
            "link": "https://www.semanticscholar.org/paper/75171b38850d1a818539851d322c8667e93ac582",
            "authors": "Kavya Manohar, Leena G. Pillai",
            "EMNLP Paper ID": "1239",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "cb3a10bec447a3ea3ea74bf5d297a34a6621bb1e",
            "title": "Target-Aware Language Modeling via Granular Data Sampling",
            "abstract": "Language model pretraining generally targets a broad range of use cases and incorporates data from diverse sources. However, there are instances where we desire a model that excels in specific areas without markedly compromising performance in other areas. A cost-effective and straightforward approach is sampling with low-dimensional data features, which allows to select large-scale pretraining data for domain-specific use cases. In this work, we revisit importance sampling with n-gram features consisting of multi-granular tokens, which strikes a good balance between sentence compression and representation capabilities. We observed the sampled data to have a high correlation with the target downstream task performance while preserving its effectiveness on other tasks. This leads to the proposed data sampling paradigm where language models can be pretrained more efficiently on selected documents. On eight benchmarks we demonstrate with $\\sim$1% of the data, pretrained models perform on par with the full RefinedWeb data and outperform randomly selected samples for model sizes ranging from 125M to 1.5B.",
            "link": "https://www.semanticscholar.org/paper/cb3a10bec447a3ea3ea74bf5d297a34a6621bb1e",
            "authors": "Ernie Chang, Pin-Jie Lin, Yang Li, Changsheng Zhao, Daeil Kim, Rastislav Rabatin, Zechun Liu, Yangyang Shi, Vikas Chandra",
            "EMNLP Paper ID": "1500",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "cf2f09a0b81f7b2b1b92c5bddeaa6544617d53a9",
            "title": "Evaluating Large Language Models along Dimensions of Language Variation: A Systematik Invesdigatiom uv Cross-lingual Generalization",
            "abstract": "While large language models exhibit certain cross-lingual generalization capabilities, they suffer from performance degradation (PD) on unseen closely-related languages (CRLs) and dialects relative to their high-resource language neighbour (HRLN). However, we currently lack a fundamental understanding of what kinds of linguistic distances contribute to PD, and to what extent. Furthermore, studies of cross-lingual generalization are confounded by unknown quantities of CRL language traces in the training data, and by the frequent lack of availability of evaluation data in lower-resource related languages and dialects. To address these issues, we model phonological, morphological, and lexical distance as Bayesian noise processes to synthesize artificial languages that are controllably distant from the HRLN. We analyse PD as a function of underlying noise parameters, offering insights on model robustness to isolated and composed linguistic phenomena, and the impact of task and HRL characteristics on PD. We calculate parameter posteriors on real CRL-HRLN pair data and show that they follow computed trends of artificial languages, demonstrating the viability of our noisers. Our framework offers a cheap solution to estimating task performance on an unseen CRL given HRLN performance using its posteriors, as well as for diagnosing observed PD on a CRL in terms of its linguistic distances from its HRLN, and opens doors to principled methods of mitigating performance degradation.",
            "link": "https://www.semanticscholar.org/paper/cf2f09a0b81f7b2b1b92c5bddeaa6544617d53a9",
            "authors": "Niyati Bafna, Kenton Murray, David Yarowsky",
            "EMNLP Paper ID": "2343",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "32b8734b4834acf22fac8399026119fa3e60de79",
            "title": "Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?",
            "abstract": "The adaption of multilingual pre-trained LLMs into eloquent and helpful assistants is essential to facilitate their use across different language regions. In that spirit, we are the first to conduct an extensive study of the performance of multilingual models instruction-tuned on different language compositions on parallel instruction-tuning benchmarks across a selection of the most spoken Indo-European languages. We systematically examine the effects of language and instruction dataset size on a mid-sized and a large, multilingual LLMs by instruction-tuning them on parallel instruction-tuning datasets. Our results demonstrate that instruction-tuning on parallel instead of monolingual corpora benefits cross-lingual instruction following capabilities by up to 9.9%. Furthermore, we show that the Superficial Alignment Hypothesis does not hold in general, as the investigated multilingual 7B parameter model presents a counter-example requiring large-scale instruction-tuning datasets. Finally, we conduct a human annotation study to understand the alignment between human-based and GPT-4-based evaluation within multilingual chat scenarios.",
            "link": "https://www.semanticscholar.org/paper/32b8734b4834acf22fac8399026119fa3e60de79",
            "authors": "Alexander Arno Weber, Klaudia Thellmann, Jan Ebert, Nicolas Flores-Herr, Jens Lehmann, Michael Fromm, Mehdi Ali",
            "EMNLP Paper ID": "2764",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "a06237d612ae0d53c0c591620c238bd8c2a46158",
            "title": "Task Oriented In-Domain Data Augmentation",
            "abstract": "Large Language Models (LLMs) have shown superior performance in various applications and fields. To achieve better performance on specialized domains such as law and advertisement, LLMs are often continue pre-trained on in-domain data. However, existing approaches suffer from two major issues. First, in-domain data are scarce compared with general domain-agnostic data. Second, data used for continual pre-training are not task-aware, such that they may not be helpful to downstream applications. We propose TRAIT, a task-oriented in-domain data augmentation framework. Our framework is divided into two parts: in-domain data selection and task-oriented synthetic passage generation. The data selection strategy identifies and selects a large amount of in-domain data from general corpora, and thus significantly enriches domain knowledge in the continual pre-training data. The synthetic passages contain guidance on how to use domain knowledge to answer questions about downstream tasks. By training on such passages, the model aligns with the need of downstream applications. We adapt LLMs to two domains: advertisement and math. On average, TRAIT improves LLM performance by 8% in the advertisement domain and 7.5% in the math domain.",
            "link": "https://www.semanticscholar.org/paper/a06237d612ae0d53c0c591620c238bd8c2a46158",
            "authors": "Xiao Liang, Xinyu Hu, Simiao Zuo, Yeyun Gong, Qiang Lou, Yi Liu, Shao-Lun Huang, Jian Jiao",
            "EMNLP Paper ID": "2775",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "1914a29816bb5f09f6c46c862d8db3ecd4b6923f",
            "title": "Is Child-Directed Speech Effective Training Data for Language Models?",
            "abstract": "While high-performing language models are typically trained on hundreds of billions of words, human children become fluent language users with a much smaller amount of data. What are the features of the data they receive, and how do these features support language modeling objectives? To investigate this question, we train GPT-2 and RoBERTa models on 29M words of English child-directed speech and a new matched, synthetic dataset (TinyDialogues), comparing to OpenSubtitles, Wikipedia, and a heterogeneous blend of datasets from the BabyLM challenge. We evaluate the syntactic and semantic knowledge of these models using developmentally-inspired evaluations. Through pretraining experiments, we test whether the global developmental ordering or the local discourse ordering of children's training data supports high performance relative to other datasets. The local properties of the data affect model results, but surprisingly, global properties do not. Further, child language input is not uniquely valuable for training language models. These findings support the hypothesis that, rather than proceeding from better data, the child's learning algorithm is substantially more data-efficient than current language modeling techniques.",
            "link": "https://www.semanticscholar.org/paper/1914a29816bb5f09f6c46c862d8db3ecd4b6923f",
            "authors": "Steven Y. Feng, Noah D. Goodman, Michael C. Frank",
            "EMNLP Paper ID": "3088",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "ff6c450f2ddb60778ea36975560ad956f63caf24",
            "title": "Leveraging Web-Crawled Data for High-Quality Fine-Tuning",
            "abstract": "Most large language models are fine-tuned using either expensive human-annotated data or GPT-4 generated data which cannot guarantee performance in certain domains. We argue that although the web-crawled data often has formatting errors causing semantic inaccuracies, it can still serve as a valuable source for high-quality supervised fine-tuning in specific domains without relying on advanced models like GPT-4. To this end, we create a paired training dataset automatically by aligning web-crawled data with a smaller set of high-quality data. By training a language model on this dataset, we can convert web data with irregular formats into high-quality ones. Our experiments show that training with the model-transformed data yields better results, surpassing training with only high-quality data by an average score of 9.4% in Chinese math problems. Additionally, our 7B model outperforms several open-source models larger than 32B and surpasses well-known closed-source models such as GPT-3.5, highlighting the efficacy of our approach.",
            "link": "https://www.semanticscholar.org/paper/ff6c450f2ddb60778ea36975560ad956f63caf24",
            "authors": "Jing Zhou, Chenglin Jiang, Wei Shen, Xiao Zhou, Xiaonan He",
            "matchScore": 229.43207,
            "original title": "Leveraging Web-Crawled Data for High-Quality Fine-Tuning",
            "original authors": "Jing Zhou, Chenglin Jiang, Wei Shen, Xiao Zhou, Xiaonan He",
            "EMNLP Paper ID": "2238",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "21cf7504bd8d9086af220939c2526c1e65b65135",
            "title": "Beyond Accuracy Optimization: Computer Vision Losses for Large Language Model Fine-Tuning",
            "abstract": "Large Language Models (LLMs) have demonstrated impressive performance across various tasks. However, current training approaches combine standard cross-entropy loss with extensive data, human feedback, or ad hoc methods to enhance performance. These solutions are often not scalable or feasible due to their associated costs, complexity, or resource requirements. This study investigates the use of established semantic segmentation loss functions in natural language generation to create a versatile, practical, and scalable solution for fine-tuning different architectures. We evaluate their effectiveness in solving Math Word Problems and question answering across different models of varying sizes. For the analyzed tasks, we found that the traditional Cross-Entropy loss represents a sub-optimal choice, while models trained to minimize alternative (task-dependent) losses, such as Focal or Lov\\'asz, achieve a mean improvement of +42% on exact match without requiring additional data or human feedback. These findings suggest a promising pathway for more efficient and accessible training processes.",
            "link": "https://www.semanticscholar.org/paper/21cf7504bd8d9086af220939c2526c1e65b65135",
            "authors": "Daniele Rege Cambrin, Giuseppe Gallipoli, Irene Benedetto, Luca Cagliero, Paolo Garza",
            "matchScore": 250.30453,
            "original title": "Beyond Accuracy Optimization: Computer Vision Losses for Large Language Model Fine-Tuning",
            "original authors": "Daniele Rege Cambrin, Giuseppe Gallipoli, Irene Benedetto, Luca Cagliero, Paolo Garza",
            "EMNLP Paper ID": "2371",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "d31518ee742214f897aa58b806dbef5b03d5d7c4",
            "title": "Fine-Tuning Language Models on Multiple Datasets for Citation Intention Classification",
            "abstract": "Citation intention Classification (CIC) tools classify citations by their intention (e.g., background, motivation) and assist readers in evaluating the contribution of scientific literature. Prior research has shown that pretrained language models (PLMs) such as SciBERT can achieve state-of-the-art performance on CIC benchmarks. PLMs are trained via self-supervision tasks on a large corpus of general text and can quickly adapt to CIC tasks via moderate fine-tuning on the corresponding dataset. Despite their advantages, PLMs can easily overfit small datasets during fine-tuning. In this paper, we propose a multi-task learning (MTL) framework that jointly fine-tunes PLMs on a dataset of primary interest together with multiple auxiliary CIC datasets to take advantage of additional supervision signals. We develop a data-driven task relation learning (TRL) method that controls the contribution of auxiliary datasets to avoid negative transfer and expensive hyper-parameter tuning. We conduct experiments on three CIC datasets and show that fine-tuning with additional datasets can improve the PLMs' generalization performance on the primary dataset. PLMs fine-tuned with our proposed framework outperform the current state-of-the-art models by 7% to 11% on small datasets while aligning with the best-performing model on a large dataset.",
            "link": "https://www.semanticscholar.org/paper/d31518ee742214f897aa58b806dbef5b03d5d7c4",
            "authors": "Zeren Shui, Petros Karypis, Daniel S. Karls, Mingjian Wen, Saurav Manchanda, E. Tadmor, George Karypis",
            "matchScore": 268.04083,
            "original title": "Fine-Tuning Language Models on Multiple Datasets for Citation Intention Classification",
            "original authors": "Zeren Shui, Petros Karypis, Daniel S. Karls, Mingjian Wen, Saurav Manchanda, Ellad B. Tadmor, George Karypis",
            "EMNLP Paper ID": "3207",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "5db663563f7299531409a30f539663d4c2ec0aec",
            "title": "Improving Multilingual Instruction Finetuning via Linguistically Natural and Diverse Datasets",
            "abstract": "Advancements in Large Language Models (LLMs) have significantly enhanced instruction-following capabilities. However, most Instruction Fine-Tuning (IFT) datasets are predominantly in English, limiting model performance in other languages. Traditional methods for creating multilingual IFT datasets such as translating existing English IFT datasets or converting existing NLP datasets into IFT datasets by templating, struggle to capture linguistic nuances and ensure prompt (instruction) diversity. To address this issue, we propose a novel method for collecting multilingual IFT datasets that preserves linguistic naturalness and ensures prompt diversity. This approach leverages English-focused LLMs, monolingual corpora, and a scoring function to create high-quality, diversified IFT datasets in multiple languages. Experiments demonstrate that LLMs finetuned using these IFT datasets show notable improvements in both generative and discriminative tasks, indicating enhanced language comprehension by LLMs in non-English contexts. Specifically, on the multilingual summarization task, LLMs using our IFT dataset achieved 17.57% and 15.23% improvements over LLMs fine-tuned with translation-based and template-based datasets, respectively.",
            "link": "https://www.semanticscholar.org/paper/5db663563f7299531409a30f539663d4c2ec0aec",
            "authors": "Sathish Indurthi, Wenxuan Zhou, Shamil Chollampatt, Ravi Agrawal, Kaiqiang Song, Lingxiao Zhao, Chenguang Zhu",
            "matchScore": 316.5335,
            "original title": "Improving Multilingual Instruction Finetuning via Linguistically Natural and Diverse Datasets",
            "original authors": "Sathish Reddy Indurthi, Wenxuan Zhou, Shamil Chollampatt, Ravi Agrawal, Kaiqiang Song, Lingxiao Zhao, Chenguang Zhu",
            "EMNLP Paper ID": "463",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "5d8bc6e9db998078806e616e91fc2080da2f18dd",
            "title": "Walia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and Generative Datasets",
            "abstract": "Large language models (LLMs) have received a lot of attention in natural language processing (NLP) research because of their exceptional performance in understanding and generating human languages. However, low-resource languages are left behind due to the unavailability of resources. In this work, we focus on enhancing the LLaMA-2-Amharic model by integrating task-specific and generative datasets to improve language model performance for Amharic. We compile an Amharic instruction fine-tuning dataset and fine-tuned LLaMA-2-Amharic model. The fine-tuned model shows promising results in different NLP tasks. We open-source our dataset creation pipeline, instruction datasets, trained models, and evaluation outputs to promote language-specific studies on these models.",
            "link": "https://www.semanticscholar.org/paper/5d8bc6e9db998078806e616e91fc2080da2f18dd",
            "authors": "Israel Abebe Azime, Mitiku Yohannes Fuge, A. Tonja, Tadesse Destaw Belay, A. Wassie, Eyasu Shiferaw Jada, Yonas Chanie, W. Sewunetie, Seid Muhie Yimam",
            "matchScore": 313.7644,
            "original title": "Walia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and Generative Datasets",
            "original authors": "Israel Abebe Azime, Atnafu Lambebo Tonja, Tadesse Destaw Belay, Mitiku Yohannes Fuge, Aman Kassahun Wassie, Eyasu Shiferaw Jada, Yonas Chanie, Walelign Tewabe Sewunetie, Seid Muhie Yimam",
            "EMNLP Paper ID": "76",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 12): 17:45\u201318:45 (Evening)",
            "Type": "Findings"
        }
    ],
    "Interpreting Mechanisms and Cognitive Alignment of Large Language Models": [
        {
            "paperId": "b1887ec025acda6f040c10cfb881c2e86068e4c6",
            "title": "Decoding the Echoes of Vision from fMRI: Memory Disentangling for Past Semantic Information",
            "abstract": "The human visual system is capable of processing continuous streams of visual information, but how the brain encodes and retrieves recent visual memories during continuous visual processing remains unexplored. This study investigates the capacity of working memory to retain past information under continuous visual stimuli. And then we propose a new task Memory Disentangling, which aims to extract and decode past information from fMRI signals. To address the issue of interference from past memory information, we design a disentangled contrastive learning method inspired by the phenomenon of proactive interference. This method separates the information between adjacent fMRI signals into current and past components and decodes them into image descriptions. Experimental results demonstrate that this method effectively disentangles the information within fMRI signals. This research could advance brain-computer interfaces and mitigate the problem of low temporal resolution in fMRI.",
            "link": "https://www.semanticscholar.org/paper/b1887ec025acda6f040c10cfb881c2e86068e4c6",
            "authors": "Runze Xia, Congchi Yin, Piji Li",
            "EMNLP Paper ID": "228",
            "Oral/Poster": "Oral",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "a893c8d9eac0b562161278ef5b135f4b15fc6738",
            "title": "Backward Lens: Projecting Language Model Gradients into the Vocabulary Space",
            "abstract": "Understanding how Transformer-based Language Models (LMs) learn and recall information is a key goal of the deep learning community. Recent interpretability methods project weights and hidden states obtained from the forward pass to the models' vocabularies, helping to uncover how information flows within LMs. In this work, we extend this methodology to LMs' backward pass and gradients. We first prove that a gradient matrix can be cast as a low-rank linear combination of its forward and backward passes' inputs. We then develop methods to project these gradients into vocabulary items and explore the mechanics of how new information is stored in the LMs' neurons.",
            "link": "https://www.semanticscholar.org/paper/a893c8d9eac0b562161278ef5b135f4b15fc6738",
            "authors": "Shahar Katz, Yonatan Belinkov, Mor Geva, Lior Wolf",
            "EMNLP Paper ID": "270",
            "Oral/Poster": "Oral",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "e30f0277e3e67b092e99a97d90f67144fde700c3",
            "title": "On the Role of Context in Reading Time Prediction",
            "abstract": "We present a new perspective on how readers integrate context during real-time language comprehension. Our proposals build on surprisal theory, which posits that the processing effort of a linguistic unit (e.g., a word) is an affine function of its in-context information content. We first observe that surprisal is only one out of many potential ways that a contextual predictor can be derived from a language model. Another one is the pointwise mutual information (PMI) between a unit and its context, which turns out to yield the same predictive power as surprisal when controlling for unigram frequency. Moreover, both PMI and surprisal are correlated with frequency. This means that neither PMI nor surprisal contains information about context alone. In response to this, we propose a technique where we project surprisal onto the orthogonal complement of frequency, yielding a new contextual predictor that is uncorrelated with frequency. Our experiments show that the proportion of variance in reading times explained by context is a lot smaller when context is represented by the orthogonalized predictor. From an interpretability standpoint, this indicates that previous studies may have overstated the role that context has in predicting reading times.",
            "link": "https://www.semanticscholar.org/paper/e30f0277e3e67b092e99a97d90f67144fde700c3",
            "authors": "Andreas Opedal, Eleanor Chodroff, Ryan Cotterell, E. Wilcox",
            "EMNLP Paper ID": "342",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "9af3c1be7a4cfd38f10b9373e4623f4b64d467cd",
            "title": "Neuron-Level Knowledge Attribution in Large Language Models",
            "abstract": "Identifying important neurons for final predictions is essential for understanding the mechanisms of large language models. Due to computational constraints, current attribution techniques struggle to operate at neuron level. In this paper, we propose a static method for pinpointing significant neurons. Compared to seven other methods, our approach demonstrates superior performance across three metrics. Additionally, since most static methods typically only identify\"value neurons\"directly contributing to the final prediction, we propose a method for identifying\"query neurons\"which activate these\"value neurons\". Finally, we apply our methods to analyze six types of knowledge across both attention and feed-forward network (FFN) layers. Our method and analysis are helpful for understanding the mechanisms of knowledge storage and set the stage for future research in knowledge editing. The code is available on https://github.com/zepingyu0512/neuron-attribution.",
            "link": "https://www.semanticscholar.org/paper/9af3c1be7a4cfd38f10b9373e4623f4b64d467cd",
            "authors": "Zeping Yu, Sophia Ananiadou",
            "EMNLP Paper ID": "367",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "40406a85c76f07dd2863298ef3050975b0459527",
            "title": "Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis",
            "abstract": "We find arithmetic ability resides within a limited number of attention heads, with each head specializing in distinct operations. To delve into the reason, we introduce the Comparative Neuron Analysis (CNA) method, which identifies an internal logic chain consisting of four distinct stages from input to prediction: feature enhancing with shallow FFN neurons, feature transferring by shallow attention layers, feature predicting by arithmetic heads, and prediction enhancing among deep FFN neurons. Moreover, we identify the human-interpretable FFN neurons within both feature-enhancing and feature-predicting stages. These findings lead us to investigate the mechanism of LoRA, revealing that it enhances prediction probabilities by amplifying the coefficient scores of FFN neurons related to predictions. Finally, we apply our method in model pruning for arithmetic tasks and model editing for reducing gender bias. Code is on https://github.com/zepingyu0512/arithmetic-mechanism.",
            "link": "https://www.semanticscholar.org/paper/40406a85c76f07dd2863298ef3050975b0459527",
            "authors": "Zeping Yu, Sophia Ananiadou",
            "EMNLP Paper ID": "372",
            "Oral/Poster": "Oral",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "31cd506c34fe52bec6ea83cd1cba9318415dff2e",
            "title": "Fine-Grained Prediction of Reading Comprehension from Eye Movements",
            "abstract": "Can human reading comprehension be assessed from eye movements in reading? In this work, we address this longstanding question using large-scale eyetracking data over textual materials that are geared towards behavioral analyses of reading comprehension. We focus on a fine-grained and largely unaddressed task of predicting reading comprehension from eye movements at the level of a single question over a passage. We tackle this task using three new multimodal language models, as well as a battery of prior models from the literature. We evaluate the models' ability to generalize to new textual items, new participants, and the combination of both, in two different reading regimes, ordinary reading and information seeking. The evaluations suggest that although the task is highly challenging, eye movements contain useful signals for fine-grained prediction of reading comprehension. Code and data will be made publicly available.",
            "link": "https://www.semanticscholar.org/paper/31cd506c34fe52bec6ea83cd1cba9318415dff2e",
            "authors": "Omer Shubi, Yoav Meiri, Cfir Avraham Hadar, Yevgeni Berzak",
            "EMNLP Paper ID": "384",
            "Oral/Poster": "Oral",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "729807e6984a7b506ea8019967fb486065200ef6",
            "title": "MMNeuron: Discovering Neuron-Level Domain-Specific Interpretation in Multimodal Large Language Model",
            "abstract": "Projecting visual features into word embedding space has become a significant fusion strategy adopted by Multimodal Large Language Models (MLLMs). However, its internal mechanisms have yet to be explored. Inspired by multilingual research, we identify domain-specific neurons in multimodal large language models. Specifically, we investigate the distribution of domain-specific neurons and the mechanism of how MLLMs process features from diverse domains. Furthermore, we propose a three-stage mechanism for language model modules in MLLMs when handling projected image features, and verify this hypothesis using logit lens. Extensive experiments indicate that while current MLLMs exhibit Visual Question Answering (VQA) capability, they may not fully utilize domain-specific information. Manipulating domain-specific neurons properly will result in a 10% change of accuracy at most, shedding light on the development of cross-domain, all-encompassing MLLMs in the future. The source code is available at https://github.com/Z1zs/MMNeuron.",
            "link": "https://www.semanticscholar.org/paper/729807e6984a7b506ea8019967fb486065200ef6",
            "authors": "Jiahao Huo, Yibo Yan, Boren Hu, Yutao Yue, Xuming Hu",
            "EMNLP Paper ID": "759",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 12): 17:45\u201318:45 (Evening)",
            "Type": "Main"
        },
        {
            "paperId": "57f703664560fef1122d6f894e5fbe90930d8db6",
            "title": "Reverse-Engineering the Reader",
            "abstract": "Numerous previous studies have sought to determine to what extent language models, pretrained on natural language text, can serve as useful models of human cognition. In this paper, we are interested in the opposite question: whether we can directly optimize a language model to be a useful cognitive model by aligning it to human psychometric data. To achieve this, we introduce a novel alignment technique in which we fine-tune a language model to implicitly optimize the parameters of a linear regressor that directly predicts humans' reading times of in-context linguistic units, e.g., phonemes, morphemes, or words, using surprisal estimates derived from the language model. Using words as a test case, we evaluate our technique across multiple model sizes and datasets and find that it improves language models' psychometric predictive power. However, we find an inverse relationship between psychometric power and a model's performance on downstream NLP tasks as well as its perplexity on held-out test data. While this latter trend has been observed before (Oh et al., 2022; Shain et al., 2024), we are the first to induce it by manipulating a model's alignment to psychometric data.",
            "link": "https://www.semanticscholar.org/paper/57f703664560fef1122d6f894e5fbe90930d8db6",
            "authors": "Samuel Kiegeland, E. Wilcox, Afra Amini, David Robert Reich, Ryan Cotterell",
            "EMNLP Paper ID": "1055",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "55d9e5615404cae4dba2ad31d4f72e8abd6e9de6",
            "title": "Towards Interpretable Sequence Continuation: Analyzing Shared Circuits in Large Language Models",
            "abstract": "While transformer models exhibit strong capabilities on linguistic tasks, their complex architectures make them difficult to interpret. Recent work has aimed to reverse engineer transformer models into human-readable representations called circuits that implement algorithmic functions. We extend this research by analyzing and comparing circuits for similar sequence continuation tasks, which include increasing sequences of Arabic numerals, number words, and months. By applying circuit interpretability analysis, we identify a key sub-circuit in both GPT-2 Small and Llama-2-7B responsible for detecting sequence members and for predicting the next member in a sequence. Our analysis reveals that semantically related sequences rely on shared circuit subgraphs with analogous roles. Additionally, we show that this sub-circuit has effects on various math-related prompts, such as on intervaled circuits, Spanish number word and months continuation, and natural language word problems. Overall, documenting shared computational structures enables better model behavior predictions, identification of errors, and safer editing procedures. This mechanistic understanding of transformers is a critical step towards building more robust, aligned, and interpretable language models.",
            "link": "https://www.semanticscholar.org/paper/55d9e5615404cae4dba2ad31d4f72e8abd6e9de6",
            "authors": "Michael Lan, Phillip H. S. Torr, Fazl Barez",
            "EMNLP Paper ID": "1462",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "93b058243165c2b2e06c6802ab12538713e167c3",
            "title": "LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law",
            "abstract": "Pretrained large language models (LLMs) are surprisingly effective at performing zero-shot tasks, including time-series forecasting. However, understanding the mechanisms behind such capabilities remains highly challenging due to the complexity of the models. We study LLMs' ability to extrapolate the behavior of dynamical systems whose evolution is governed by principles of physical interest. Our results show that LLaMA 2, a language model trained primarily on texts, achieves accurate predictions of dynamical system time series without fine-tuning or prompt engineering. Moreover, the accuracy of the learned physical rules increases with the length of the input context window, revealing an in-context version of neural scaling law. Along the way, we present a flexible and efficient algorithm for extracting probability density functions of multi-digit numbers directly from LLMs.",
            "link": "https://www.semanticscholar.org/paper/93b058243165c2b2e06c6802ab12538713e167c3",
            "authors": "T. J. Liu, Nicolas Boull'e, Raphael Sarfati, C. Earls",
            "EMNLP Paper ID": "1758",
            "Oral/Poster": "Oral",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "74add6bdd29ca6e84168b993c875b6f697159cf6",
            "title": "Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions",
            "abstract": "Understanding the inner workings of large language models (LLMs) is crucial for advancing their theoretical foundations and real-world applications. While the attention mechanism and multi-layer perceptrons (MLPs) have been studied independently, their interactions remain largely unexplored. This study investigates how attention heads and next-token neurons interact in LLMs to predict new words. We propose a methodology to identify next-token neurons, find prompts that highly activate them, and determine the upstream attention heads responsible. We then generate and evaluate explanations for the activity of these attention heads in an automated manner. Our findings reveal that some attention heads recognize specific contexts relevant to predicting a token and activate a downstream token-predicting neuron accordingly. This mechanism provides a deeper understanding of how attention heads work with MLP neurons to perform next-token prediction. Our approach offers a foundation for further research into the intricate workings of LLMs and their impact on text generation and understanding.",
            "link": "https://www.semanticscholar.org/paper/74add6bdd29ca6e84168b993c875b6f697159cf6",
            "authors": "Clement Neo, Shay B. Cohen, Fazl Barez",
            "EMNLP Paper ID": "1969",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "1784cd42207a12bdc4061bb0f78bc8d5c82f61dd",
            "title": "Information Flow Routes: Automatically Interpreting Language Models at Scale",
            "abstract": "Information flows by routes inside the network via mechanisms implemented in the model. These routes can be represented as graphs where nodes correspond to token representations and edges to operations inside the network. We automatically build these graphs in a top-down manner, for each prediction leaving only the most important nodes and edges. In contrast to the existing workflows relying on activation patching, we do this through attribution: this allows us to efficiently uncover existing circuits with just a single forward pass. Additionally, the applicability of our method is far beyond patching: we do not need a human to carefully design prediction templates, and we can extract information flow routes for any prediction (not just the ones among the allowed templates). As a result, we can talk about model behavior in general, for specific types of predictions, or different domains. We experiment with Llama 2 and show that the role of some attention heads is overall important, e.g. previous token heads and subword merging heads. Next, we find similarities in Llama 2 behavior when handling tokens of the same part of speech. Finally, we show that some model components can be specialized on domains such as coding or multilingual texts.",
            "link": "https://www.semanticscholar.org/paper/1784cd42207a12bdc4061bb0f78bc8d5c82f61dd",
            "authors": "Javier Ferrando, Elena Voita",
            "EMNLP Paper ID": "2077",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "17f637e069916e604ed79ff0d542c7c0de80f02d",
            "title": "Language models and brains align due to more than next-word prediction and word-level information",
            "abstract": "Pretrained language models have been shown to significantly predict brain recordings of people comprehending language. Recent work suggests that the prediction of the next word is a key mechanism that contributes to this alignment. What is not yet understood is whether prediction of the next word is necessary for this observed alignment or simply sufficient, and whether there are other shared mechanisms or information that are similarly important. In this work, we take a step towards understanding the reasons for brain alignment via two simple perturbations in popular pretrained language models. These perturbations help us design contrasts that can control for different types of information. By contrasting the brain alignment of these differently perturbed models, we show that improvements in alignment with brain recordings are due to more than improvements in next-word prediction and word-level information.",
            "link": "https://www.semanticscholar.org/paper/17f637e069916e604ed79ff0d542c7c0de80f02d",
            "authors": "Gabriele Merlin, Mariya Toneva",
            "EMNLP Paper ID": "2294",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "eb40b0f709f74e95f9ee6bc4ad2ca7403aec2049",
            "title": "Unveiling Multi-level and Multi-modal Semantic Representations in the Human Brain using Large Language Models",
            "abstract": "In recent studies, researchers have used large language models (LLMs) to explore semantic representations in the brain; however, they have typically assessed different levels of semantic content, such as speech, objects, and stories, separately. In this study, we recorded brain activity using functional magnetic resonance imaging (fMRI) while participants viewed 8.3 hours of dramas and movies. We annotated these stimuli at multiple semantic levels, which enabled us to extract latent representations of LLMs for this content. Our findings demonstrate that LLMs predict human brain activity more accurately than traditional language models, particularly for complex background stories. Furthermore, we identify distinct brain regions associated with different semantic representations, including multi-modal vision-semantic representations, which highlights the importance of modeling multi-level and multimodal semantic representations simultaneously. We will make our fMRI dataset publicly available to facilitate further research on aligning LLMs with human brain function. 1",
            "link": "https://www.semanticscholar.org/paper/eb40b0f709f74e95f9ee6bc4ad2ca7403aec2049",
            "authors": "Yuko Nakagi, Takuya Matsuyama, Naoko Koide-Majima, Hiroto Q. Yamaguchi, Rieko Kubo, Shinji Nishimoto, Yu Takagi",
            "EMNLP Paper ID": "2655",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "2138841e6e5ecf59d881a0108e0d9551484cfe32",
            "title": "Insights into LLM Long-Context Failures: When Transformers Know but Don't Tell",
            "abstract": "Large Language Models (LLMs) exhibit positional bias, struggling to utilize information from the middle or end of long contexts. Our study explores LLMs' long-context reasoning by probing their hidden representations. We find that while LLMs encode the position of target information, they often fail to leverage this in generating accurate responses. This reveals a disconnect between information retrieval and utilization, a\"know but don't tell\"phenomenon. We further analyze the relationship between extraction time and final accuracy, offering insights into the underlying mechanics of transformer models.",
            "link": "https://www.semanticscholar.org/paper/2138841e6e5ecf59d881a0108e0d9551484cfe32",
            "authors": "Taiming Lu, Muhan Gao, Kuai Yu, Adam Byerly, Daniel Khashabi",
            "matchScore": 296.2152,
            "original title": "Insights into LLM Long-Context Failures: When Transformers Know but Don\u2019t Tell",
            "original authors": "Muhan Gao, TaiMing Lu, Kuai Yu, Adam Byerly, Daniel Khashabi",
            "EMNLP Paper ID": "1586",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "fe3937902a58c6f8e7f8bc35480b387072527fc3",
            "title": "On the Similarity of Circuits across Languages: a Case Study on the Subject-verb Agreement Task",
            "abstract": "Several algorithms implemented by language models have recently been successfully reversed-engineered. However, these findings have been concentrated on specific tasks and models, leaving it unclear how universal circuits are across different settings. In this paper, we study the circuits implemented by Gemma 2B for solving the subject-verb agreement task across two different languages, English and Spanish. We discover that both circuits are highly consistent, being mainly driven by a particular attention head writing a `subject number' signal to the last residual stream, which is read by a small set of neurons in the final MLPs. Notably, this subject number signal is represented as a direction in the residual stream space, and is language-independent. We demonstrate that this direction has a causal effect on the model predictions, effectively flipping the Spanish predicted verb number by intervening with the direction found in English. Finally, we present evidence of similar behavior in other models within the Gemma 1 and Gemma 2 families.",
            "link": "https://www.semanticscholar.org/paper/fe3937902a58c6f8e7f8bc35480b387072527fc3",
            "authors": "Javier Ferrando, Marta R.Costa-jussa",
            "matchScore": 268.9234,
            "original title": "On the Similarity of Circuits across Languages: a Case Study on the Subject-verb Agreement Task",
            "original authors": "Javier Ferrando, Marta R. Costa-juss\u00e0",
            "EMNLP Paper ID": "2075",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        }
    ],
    "Tool-Augmented Large Language Models": [
        {
            "paperId": "6c076122ea53e18180255bb96c9ad547bb88d283",
            "title": "AutoScraper: A Progressive Understanding Web Agent for Web Scraper Generation",
            "abstract": "Web scraping is a powerful technique that extracts data from websites, enabling automated data collection, enhancing data analysis capabilities, and minimizing manual data entry efforts. Existing methods, wrappers-based methods suffer from limited adaptability and scalability when faced with a new website, while language agents, empowered by large language models (LLMs), exhibit poor reusability in diverse web environments. In this work, we introduce the paradigm of generating web scrapers with LLMs and propose AutoScraper, a two-stage framework that can handle diverse and changing web environments more efficiently. AutoScraper leverages the hierarchical structure of HTML and similarity across different web pages for generating web scrapers. Besides, we propose a new executability metric for better measuring the performance of web scraper generation tasks. We conduct comprehensive experiments with multiple LLMs and demonstrate the effectiveness of our framework. Resources of this paper can be found at \\url{https://github.com/EZ-hwh/AutoScraper}",
            "link": "https://www.semanticscholar.org/paper/6c076122ea53e18180255bb96c9ad547bb88d283",
            "authors": "Wenhao Huang, Zhouhong Gu, Chenghao Peng, Zhixu Li, Jiaqing Liang, Yanghua Xiao, Liqian Wen, Zulong Chen",
            "EMNLP Paper ID": "269",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "ac2fc8c5d4a1f44464f1415ea3dd3ed45398b9d9",
            "title": "Middleware for LLMs: Tools Are Instrumental for Language Agents in Complex Environments",
            "abstract": "The applications of large language models (LLMs) have expanded well beyond the confines of text processing, signaling a new era where LLMs are envisioned as generalist agents capable of operating within complex environments. These environments are often highly expansive, making it impossible for the LLM to process them within its short-term memory. Motivated by recent research on extending the capabilities of LLMs with tools, we seek to investigate the intriguing potential of tools to augment LLMs in handling such complexity by introducing a novel class of tools, termed middleware, to aid in the proactive exploration within these massive environments. Such specialized tools can serve as a middleware layer shielding the LLM from environmental complexity. In two representative complex environments -- knowledge bases (KBs) and databases -- we demonstrate the significant potential of augmenting language agents with tools in complex environments. Notably, equipped with the middleware, GPT-4 achieves 2.8X the performance of the best baseline in tasks requiring access to database content and 2.2X in KB tasks. Our findings illuminate the path for advancing language agents in real-world applications.",
            "link": "https://www.semanticscholar.org/paper/ac2fc8c5d4a1f44464f1415ea3dd3ed45398b9d9",
            "authors": "Yu Gu, Yiheng Shu, Hao Yu, Xiao Liu, Yuxiao Dong, Jie Tang, Jayanth Srinivasa, Hugo Latapie, Yu Su",
            "EMNLP Paper ID": "870",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "2026c795a2c0ea07d860bc01843c01f02ed16faf",
            "title": "AssistantBench: Can Web Agents Solve Realistic and Time-Consuming Tasks?",
            "abstract": "Language agents, built on top of language models (LMs), are systems that can interact with complex environments, such as the open web. In this work, we examine whether such agents can perform realistic and time-consuming tasks on the web, e.g., monitoring real-estate markets or locating relevant nearby businesses. We introduce AssistantBench, a challenging new benchmark consisting of 214 realistic tasks that can be automatically evaluated, covering different scenarios and domains. We find that AssistantBench exposes the limitations of current systems, including language models and retrieval-augmented language models, as no model reaches an accuracy of more than 26 points. While closed-book LMs perform well in terms of accuracy, they exhibit low precision and tend to hallucinate facts. State-of-the-art web agents reach a score of near zero. Additionally, we introduce SeePlanAct (SPA), a new web agent that significantly outperforms previous agents, and an ensemble of SPA and closed-book models reaches the best overall performance. Moreover, we analyze failures of current systems and highlight that open web navigation remains a major challenge.",
            "link": "https://www.semanticscholar.org/paper/2026c795a2c0ea07d860bc01843c01f02ed16faf",
            "authors": "Ori Yoran, S. Amouyal, Chaitanya Malaviya, Ben Bogin, Ofir Press, Jonathan Berant",
            "EMNLP Paper ID": "1022",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "053ef8299988680d47df36224bfccffc817472f1",
            "title": "SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories",
            "abstract": "Given that Large Language Models (LLMs) have made significant progress in writing code, can they now be used to autonomously reproduce results from research repositories? Such a capability would be a boon to the research community, helping researchers validate, understand, and extend prior work. To advance towards this goal, we introduce SUPER, the first benchmark designed to evaluate the capability of LLMs in setting up and executing tasks from research repositories. SUPERaims to capture the realistic challenges faced by researchers working with Machine Learning (ML) and Natural Language Processing (NLP) research repositories. Our benchmark comprises three distinct problem sets: 45 end-to-end problems with annotated expert solutions, 152 sub problems derived from the expert set that focus on specific challenges (e.g., configuring a trainer), and 602 automatically generated problems for larger-scale development. We introduce various evaluation measures to assess both task success and progress, utilizing gold solutions when available or approximations otherwise. We show that state-of-the-art approaches struggle to solve these problems with the best model (GPT-4o) solving only 16.3% of the end-to-end set, and 46.1% of the scenarios. This illustrates the challenge of this task, and suggests that SUPER can serve as a valuable resource for the community to make and measure progress.",
            "link": "https://www.semanticscholar.org/paper/053ef8299988680d47df36224bfccffc817472f1",
            "authors": "Ben Bogin, Kejuan Yang, Shashank Gupta, Kyle Richardson, Erin Bransom, Peter Clark, Ashish Sabharwal, Tushar Khot",
            "EMNLP Paper ID": "1465",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "0e28243cc36df0dfa857770e5a0a560275b225bf",
            "title": "DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models",
            "abstract": "We introduce DA-Code, a code generation benchmark specifically designed to assess LLMs on agent-based data science tasks. This benchmark features three core elements: First, the tasks within DA-Code are inherently challenging, setting them apart from traditional code generation tasks and demanding advanced coding skills in grounding and planning. Second, examples in DA-Code are all based on real and diverse data, covering a wide range of complex data wrangling and analytics tasks. Third, to solve the tasks, the models must utilize complex data science programming languages, to perform intricate data processing and derive the answers. We set up the benchmark in a controllable and executable environment that aligns with real-world data analysis scenarios and is scalable. The annotators meticulously design the evaluation suite to ensure the accuracy and robustness of the evaluation. We develop the DA-Agent baseline. Experiments show that although the baseline performs better than other existing frameworks, using the current best LLMs achieves only 30.5% accuracy, leaving ample room for improvement. We release our benchmark at https://da-code-bench.github.io.",
            "link": "https://www.semanticscholar.org/paper/0e28243cc36df0dfa857770e5a0a560275b225bf",
            "authors": "Yiming Huang, Jianwen Luo, Yan Yu, Yitong Zhang, Fangyu Lei, Yifan Wei, Shizhu He, Lifu Huang, Xiao Liu, Jun Zhao, Kang Liu",
            "EMNLP Paper ID": "1559",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "491e129c10bca2f9e50c9c4859b3cb825d217d37",
            "title": "Tools Fail: Detecting Silent Errors in Faulty Tools",
            "abstract": "Tools have become a mainstay of LLMs, allowing them to retrieve knowledge not in their weights, to perform tasks on the web, and even to control robots. However, most ontologies and surveys of tool-use have assumed the core challenge for LLMs is choosing the tool. Instead, we introduce a framework for tools more broadly which guides us to explore a model's ability to detect\"silent\"tool errors, and reflect on how to plan. This more directly aligns with the increasingly popular use of models as tools. We provide an initial approach to failure recovery with promising results both on a controlled calculator setting and embodied agent planning.",
            "link": "https://www.semanticscholar.org/paper/491e129c10bca2f9e50c9c4859b3cb825d217d37",
            "authors": "Jimin Sun, So Yeon Min, Yingshan Chang, Yonatan Bisk",
            "EMNLP Paper ID": "1646",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "60126292c0b31dfc8628d99001e057b9f8355000",
            "title": "SciAgent: Tool-augmented Language Models for Scientific Reasoning",
            "abstract": "Scientific reasoning poses an excessive challenge for even the most advanced Large Language Models (LLMs). To make this task more practical and solvable for LLMs, we introduce a new task setting named tool-augmented scientific reasoning. This setting supplements LLMs with scalable toolsets, and shifts the focus from pursuing an omniscient problem solver to a proficient tool-user. To facilitate the research of such setting, we construct a tool-augmented training corpus named MathFunc which encompasses over 30,000 samples and roughly 6,000 tools. Building on MathFunc, we develop SciAgent to retrieve, understand and, if necessary, use tools for scientific problem solving. Additionally, we craft a benchmark, SciToolBench, spanning five scientific domains to evaluate LLMs' abilities with tool assistance. Extensive experiments on SciToolBench confirm the effectiveness of SciAgent. Notably, SciAgent-Mistral-7B surpasses other LLMs with the same size by more than 13% in absolute accuracy. Furthermore, SciAgent-DeepMath-7B shows much superior performance than ChatGPT.",
            "link": "https://www.semanticscholar.org/paper/60126292c0b31dfc8628d99001e057b9f8355000",
            "authors": "Yubo Ma, Zhibin Gou, Junheng Hao, Ruochen Xu, Shuohang Wang, Liangming Pan, Yujiu Yang, Yixin Cao, Aixin Sun",
            "EMNLP Paper ID": "1846",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "ff61aef2fef3a235bfaa123158a990c4f5f27d1a",
            "title": "Small LLMs Are Weak Tool Learners: A Multi-LLM Agent",
            "abstract": "Large Language Model (LLM) agents significantly extend the capabilities of standalone LLMs, empowering them to interact with external tools (e.g., APIs, functions) and complete various tasks in a self-directed fashion. The challenge of tool use demands that LLMs not only understand user queries and generate answers accurately but also excel in task planning, tool invocation, and result summarization. While traditional works focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. To overcome these challenges, we propose a novel approach that decomposes the aforementioned capabilities into a planner, caller, and summarizer. Each component is implemented by a single LLM that focuses on a specific capability and collaborates with others to accomplish the task. This modular framework facilitates individual updates and the potential use of smaller LLMs for building each capability. To effectively train this framework, we introduce a two-stage training paradigm. First, we fine-tune a backbone LLM on the entire dataset without discriminating sub-tasks, providing the model with a comprehensive understanding of the task. Second, the fine-tuned LLM is used to instantiate the planner, caller, and summarizer respectively, which are continually fine-tuned on respective sub-tasks. Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.",
            "link": "https://www.semanticscholar.org/paper/ff61aef2fef3a235bfaa123158a990c4f5f27d1a",
            "authors": "Weizhou Shen, Chenliang Li, Hongzhan Chen, Ming Yan, Xiaojun Quan, Hehong Chen, Ji Zhang, Fei Huang",
            "EMNLP Paper ID": "1965",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "ace1a82d97d024c26e588cef084dcb322f157811",
            "title": "ToolPlanner: A Tool Augmented LLM for Multi Granularity Instructions with Path Planning and Feedback",
            "abstract": "Recently, tool-augmented LLMs have gained increasing attention. Given an instruction, tool-augmented LLMs can interact with various external tools in multiple rounds and provide a final answer. However, previous LLMs were trained on overly detailed instructions, which included API names or parameters, while real users would not explicitly mention these API details. This leads to a gap between trained LLMs and real-world scenarios. In addition, most works ignore whether the interaction process follows the instruction. To address these issues, we constructed a training dataset called MGToolBench, which contains statement and category-level instructions to better reflect real-world scenarios. In addition, we propose ToolPlanner, a two-stage reinforcement learning framework that utilizes path planning and two feedback mechanisms to enhance the LLM's task completion and instruction-following capabilities. Experimental results show that ToolPlanner significantly improves the Match Rate, Pass Rate and Win Rate by 26.8%, 20.2%, and 5.6% compared to the SOTA model. Human evaluation verifies that the multi-granularity instructions can better align with users' usage habits. Our data and code will be released upon acceptance.",
            "link": "https://www.semanticscholar.org/paper/ace1a82d97d024c26e588cef084dcb322f157811",
            "authors": "Qinzhuo Wu, Wei Liu, Jian Luan, Bin Wang",
            "EMNLP Paper ID": "2270",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "2ce7b3c1ceda4ac5139987951759a849c7a8186d",
            "title": "Toolken+: Improving LLM Tool Usage with Reranking and a Reject Option",
            "abstract": "The recently proposed ToolkenGPT tool learning paradigm demonstrates promising performance but suffers from two major issues: first, it cannot benefit from tool documentation, and second, it often makes mistakes in whether to use a tool at all. We introduce Toolken+ that mitigates the first problem by reranking top $k$ tools selected by ToolkenGPT and the second problem with a special\"Reject\"option such that the model will generate a vocabulary token if\"Reject\"is ranked first. We demonstrate the effectiveness of Toolken+ on multistep numerical reasoning and tool selection tasks.",
            "link": "https://www.semanticscholar.org/paper/2ce7b3c1ceda4ac5139987951759a849c7a8186d",
            "authors": "Konstantin Yakovlev, Sergey Nikolenko, A. Bout",
            "matchScore": 323.76532,
            "original title": "Toolken+: Improving LLM Tool Usage with Reranking and a Reject Option",
            "original authors": "Konstantin Yakovlev, Sergey Nikolenko, Andrey Bout",
            "EMNLP Paper ID": "1212",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "0f5ea5923d295a075a4f04f97dafd58e79ad89f6",
            "title": "Enhancing Tool Retrieval with Iterative Feedback from Large Language Models",
            "abstract": "Tool learning aims to enhance and expand large language models' (LLMs) capabilities with external tools, which has gained significant attention recently. Current methods have shown that LLMs can effectively handle a certain amount of tools through in-context learning or fine-tuning. However, in real-world scenarios, the number of tools is typically extensive and irregularly updated, emphasizing the necessity for a dedicated tool retrieval component. Tool retrieval is nontrivial due to the following challenges: 1) complex user instructions and tool descriptions; 2) misalignment between tool retrieval and tool usage models. To address the above issues, we propose to enhance tool retrieval with iterative feedback from the large language model. Specifically, we prompt the tool usage model, i.e., the LLM, to provide feedback for the tool retriever model in multi-round, which could progressively improve the tool retriever's understanding of instructions and tools and reduce the gap between the two standalone components. We build a unified and comprehensive benchmark to evaluate tool retrieval models. The extensive experiments indicate that our proposed approach achieves advanced performance in both in-domain evaluation and out-of-domain evaluation.",
            "link": "https://www.semanticscholar.org/paper/0f5ea5923d295a075a4f04f97dafd58e79ad89f6",
            "authors": "Qiancheng Xu, Yongqing Li, Heming Xia, Wenjie Li",
            "matchScore": 252.07391,
            "original title": "Enhancing Tool Retrieval with Iterative Feedback from Large Language Models",
            "original authors": "Xu Qiancheng, Yongqi Li, Heming Xia, Wenjie Li",
            "EMNLP Paper ID": "1987",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "966ba2acfe0700c2410efe15ed1b6c25340b7a95",
            "title": "Learning to Use Tools via Cooperative and Interactive Agents",
            "abstract": "Tool learning empowers large language models (LLMs) as agents to use external tools and extend their utility. Existing methods employ one single LLM-based agent to iteratively select and execute tools, thereafter incorporating execution results into the next action prediction. Despite their progress, these methods suffer from performance degradation when addressing practical tasks due to: (1) the pre-defined pipeline with restricted flexibility to calibrate incorrect actions, and (2) the struggle to adapt a general LLM-based agent to perform a variety of specialized actions. To mitigate these problems, we propose ConAgents, a Cooperative and interactive Agents framework, which coordinates three specialized agents for tool selection, tool execution, and action calibration separately. ConAgents introduces two communication protocols to enable the flexible cooperation of agents. To effectively generalize the ConAgents into open-source models, we also propose specialized action distillation, enhancing their ability to perform specialized actions in our framework. Our extensive experiments on three datasets show that the LLMs, when equipped with the ConAgents, outperform baselines with substantial improvement (i.e., up to 14% higher success rate).",
            "link": "https://www.semanticscholar.org/paper/966ba2acfe0700c2410efe15ed1b6c25340b7a95",
            "authors": "Zhengliang Shi, Shen Gao, Xiuyi Chen, Lingyong Yan, Haibo Shi, Dawei Yin, Zhumin Chen, Pengjie Ren, Suzan Verberne, Zhaochun Ren",
            "matchScore": 206.80309,
            "original title": "Learning to Use Tools via Cooperative and Interactive Agents",
            "original authors": "Zhengliang Shi, Shen Gao, Xiuyi Chen, Yue Feng, Lingyong Yan, Haibo Shi, Dawei Yin, Pengjie Ren, Suzan Verberne, Zhaochun Ren",
            "EMNLP Paper ID": "2153",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "937c907389bf53a08619a043328a650fc1406e12",
            "title": "BLADE: Benchmarking Language Model Agents for Data-Driven Science",
            "abstract": "Data-driven scientific discovery requires the iterative integration of scientific domain knowledge, statistical expertise, and an understanding of data semantics to make nuanced analytical decisions, e.g., about which variables, transformations, and statistical models to consider. LM-based agents equipped with planning, memory, and code execution capabilities have the potential to support data-driven science. However, evaluating agents on such open-ended tasks is challenging due to multiple valid approaches, partially correct steps, and different ways to express the same decisions. To address these challenges, we present BLADE, a benchmark to automatically evaluate agents' multifaceted approaches to open-ended research questions. BLADE consists of 12 datasets and research questions drawn from existing scientific literature, with ground truth collected from independent analyses by expert data scientists and researchers. To automatically evaluate agent responses, we developed corresponding computational methods to match different representations of analyses to this ground truth. Though language models possess considerable world knowledge, our evaluation shows that they are often limited to basic analyses. However, agents capable of interacting with the underlying data demonstrate improved, but still non-optimal, diversity in their analytical decision making. Our work enables the evaluation of agents for data-driven science and provides researchers deeper insights into agents' analysis approaches.",
            "link": "https://www.semanticscholar.org/paper/937c907389bf53a08619a043328a650fc1406e12",
            "authors": "Ken Gu, Ruoxi Shang, Ruien Jiang, Keying Kuang, Richard-John Lin, Donghe Lyu, Yue Mao, Youran Pan, Teng Wu, Jiaqian Yu, Yikun Zhang, Tianmai M. Zhang, Lanyi Zhu, Mike A. Merrill, Jeffrey Heer, Tim Althoff",
            "matchScore": 222.12617,
            "original title": "BLADE: Benchmarking Language Model Agents for Data-Driven Science",
            "original authors": "Ken Gu, Ruoxi Shang, Ruien Jiang, Keying Kuang, Richard-John Lin, Donghe Lyu, Yue Mao, Youran Pan, Teng Wu, Jiaqian Yu, Yikun Zhang, Tianmai M. Zhang, Lanyi Zhu, Mike A Merrill, Jeffrey Heer, Tim Althoff",
            "EMNLP Paper ID": "2700",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "a970be54c4df5f04c3fe65b7414e0c2879c55909",
            "title": "HoneyComb: A Flexible LLM-Based Agent System for Materials Science",
            "abstract": "The emergence of specialized large language models (LLMs) has shown promise in addressing complex tasks for materials science. Many LLMs, however, often struggle with distinct complexities of material science tasks, such as materials science computational tasks, and often rely heavily on outdated implicit knowledge, leading to inaccuracies and hallucinations. To address these challenges, we introduce HoneyComb, the first LLM-based agent system specifically designed for materials science. HoneyComb leverages a novel, high-quality materials science knowledge base (MatSciKB) and a sophisticated tool hub (ToolHub) to enhance its reasoning and computational capabilities tailored to materials science. MatSciKB is a curated, structured knowledge collection based on reliable literature, while ToolHub employs an Inductive Tool Construction method to generate, decompose, and refine API tools for materials science. Additionally, HoneyComb leverages a retriever module that adaptively selects the appropriate knowledge source or tools for specific tasks, thereby ensuring accuracy and relevance. Our results demonstrate that HoneyComb significantly outperforms baseline models across various tasks in materials science, effectively bridging the gap between current LLM capabilities and the specialized needs of this domain. Furthermore, our adaptable framework can be easily extended to other scientific domains, highlighting its potential for broad applicability in advancing scientific research and applications.",
            "link": "https://www.semanticscholar.org/paper/a970be54c4df5f04c3fe65b7414e0c2879c55909",
            "authors": "Huan Zhang, Yu Song, Ziyu Hou, Santiago Miret, Bang Liu",
            "matchScore": 233.59885,
            "original title": "HoneyComb: A Flexible LLM-Based Agent System for Materials Science",
            "original authors": "Huan Zhang, Yu Song, Ziyu Hou, Santiago Miret, Bang Liu",
            "EMNLP Paper ID": "691",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "078bedf41f7ff2850bd52f39e8c0b3b239f709d5",
            "title": "Re-Invoke: Tool Invocation Rewriting for Zero-Shot Tool Retrieval",
            "abstract": "Recent advances in large language models (LLMs) have enabled autonomous agents with complex reasoning and task-fulfillment capabilities using a wide range of tools. However, effectively identifying the most relevant tools for a given task becomes a key bottleneck as the toolset size grows, hindering reliable tool utilization. To address this, we introduce Re-Invoke, an unsupervised tool retrieval method designed to scale effectively to large toolsets without training. Specifically, we first generate a diverse set of synthetic queries that comprehensively cover different aspects of the query space associated with each tool document during the tool indexing phase. Second, we leverage LLM's query understanding capabilities to extract key tool-related context and underlying intents from user queries during the inference phase. Finally, we employ a novel multi-view similarity ranking strategy based on intents to pinpoint the most relevant tools for each query. Our evaluation demonstrates that Re-Invoke significantly outperforms state-of-the-art alternatives in both single-tool and multi-tool scenarios, all within a fully unsupervised setting. Notably, on the ToolE datasets, we achieve a 20% relative improvement in nDCG@5 for single-tool retrieval and a 39% improvement for multi-tool retrieval.",
            "link": "https://www.semanticscholar.org/paper/078bedf41f7ff2850bd52f39e8c0b3b239f709d5",
            "authors": "Yanfei Chen, Jinsung Yoon, Devendra Singh Sachan, Qingze Wang, Vincent Cohen-Addad, M. Bateni, Chen-Yu Lee, Tomas Pfister",
            "matchScore": 318.26947,
            "original title": "Re-Invoke: Tool Invocation Rewriting for Zero-Shot Tool Retrieval",
            "original authors": "Yanfei Chen, Jinsung Yoon, Devendra Singh Sachan, Qingze Wang, Vincent Cohen-Addad, Mohammadhossein Bateni, Chen-Yu Lee, Tomas Pfister",
            "EMNLP Paper ID": "927",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "57cbb0578326ed792d03981ca701214844462d22",
            "title": "TOOLVERIFIER: Generalization to New Tools via Self-Verification",
            "abstract": "Teaching language models to use tools is an important milestone towards building general assistants, but remains an open problem. While there has been significant progress on learning to use specific tools via fine-tuning, language models still struggle with learning how to robustly use new tools from only a few demonstrations. In this work we introduce a self-verification method which distinguishes between close candidates by self-asking contrastive questions during (1) tool selection; and (2) parameter generation. We construct synthetic, high-quality, self-generated data for this goal using Llama-2 70B, which we intend to release publicly. Extensive experiments on 4 tasks from the ToolBench benchmark, consisting of 17 unseen tools, demonstrate an average improvement of 22% over few-shot baselines, even in scenarios where the distinctions between candidate tools are finely nuanced.",
            "link": "https://www.semanticscholar.org/paper/57cbb0578326ed792d03981ca701214844462d22",
            "authors": "Dheeraj Mekala, Jason Weston, Jack Lanchantin, Roberta Raileanu, M. Lomeli, Jingbo Shang, Jane Dwivedi-Yu",
            "matchScore": 228.02348,
            "original title": "TOOLVERIFIER: Generalization to New Tools via Self-Verification",
            "original authors": "Dheeraj Mekala, Jason E Weston, Jack Lanchantin, Roberta Raileanu, Maria Lomeli, Jingbo Shang, Jane Dwivedi-Yu",
            "EMNLP Paper ID": "987",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        }
    ],
    "Benchmarks and Methodologies for Specialized LLM Contexts": [
        {
            "paperId": "3b2ad6382bee18c8d952539df8882997d68769a7",
            "title": "A Usage-centric Take on Intent Understanding in E-Commerce",
            "abstract": "Identifying and understanding user intents is a pivotal task for E-Commerce. Despite its essential role in product recommendation and business user profiling analysis, intent understanding has not been consistently defined or accurately benchmarked. In this paper, we focus on predicative user intents as\"how a customer uses a product\", and pose intent understanding as a natural language reasoning task, independent of product ontologies. We identify two weaknesses of FolkScope, the SOTA E-Commerce Intent Knowledge Graph: category-rigidity and property-ambiguity. They limit its ability to strongly align user intents with products having the most desirable property, and to recommend useful products across diverse categories. Following these observations, we introduce a Product Recovery Benchmark featuring a novel evaluation framework and an example dataset. We further validate the above FolkScope weaknesses on this benchmark. Our code and dataset are available at https://github.com/stayones/Usgae-Centric-Intent-Understanding.",
            "link": "https://www.semanticscholar.org/paper/3b2ad6382bee18c8d952539df8882997d68769a7",
            "authors": "Wendi Zhou, Tianyi Li, P. Vougiouklis, Mark Steedman, Jeff Z. Pan",
            "EMNLP Paper ID": "23",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "b4ed361980a8557081e3a2139a016a86a5e04a68",
            "title": "Surveying the Dead Minds: Historical-Psychological Text Analysis with Contextualized Construct Representation (CCR) for Classical Chinese",
            "abstract": "In this work, we develop a pipeline for historical-psychological text analysis in classical Chinese. Humans have produced texts in various languages for thousands of years; however, most of the computational literature is focused on contemporary languages and corpora. The emerging field of historical psychology relies on computational techniques to extract aspects of psychology from historical corpora using new methods developed in natural language processing (NLP). The present pipeline, called Contextualized Construct Representations (CCR), combines expert knowledge in psychometrics (i.e., psychological surveys) with text representations generated via transformer-based language models to measure psychological constructs such as traditionalism, norm strength, and collectivism in classical Chinese corpora. Considering the scarcity of available data, we propose an indirect supervised contrastive learning approach and build the first Chinese historical psychology corpus (C-HI-PSY) to fine-tune pre-trained models. We evaluate the pipeline to demonstrate its superior performance compared with other approaches. The CCR method outperforms word-embedding-based approaches across all of our tasks and exceeds prompting with GPT-4 in most tasks. Finally, we benchmark the pipeline against objective, external data to further verify its validity.",
            "link": "https://www.semanticscholar.org/paper/b4ed361980a8557081e3a2139a016a86a5e04a68",
            "authors": "Yuqi Chen, Sixuan Li, Ying Li, Mohammad Atari",
            "EMNLP Paper ID": "290",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "07a97a8280c8d98f1568d70581ff346ddecdc1d7",
            "title": "A User-Centric Multi-Intent Benchmark for Evaluating Large Language Models",
            "abstract": "Large language models (LLMs) are essential tools that users employ across various scenarios, so evaluating their performance and guiding users in selecting the suitable service is important. Although many benchmarks exist, they mainly focus on specific predefined model abilities, such as world knowledge, reasoning, etc. Based on these ability scores, it is hard for users to determine which LLM best suits their particular needs. To address these issues, we propose to evaluate LLMs from a user-centric perspective and design this benchmark to measure their efficacy in satisfying user needs under distinct intents. Firstly, we collect 1,846 real-world use cases from a user study with 712 participants from 23 countries. This first-hand data helps us understand actual user intents and needs in LLM interactions, forming the User Reported Scenarios (URS) dataset, which is categorized with six types of user intents. Secondly, based on this authentic dataset, we benchmark 10 LLM services with GPT-4-as-Judge. Thirdly, we show that benchmark scores align well with human preference in both real-world experience and pair-wise annotations, achieving Pearson correlations of 0.95 and 0.94, respectively. This alignment confirms that the URS dataset and our evaluation method establish an effective user-centric benchmark. The dataset, code, and process data are available at https://github.com/Alice1998/URS.",
            "link": "https://www.semanticscholar.org/paper/07a97a8280c8d98f1568d70581ff346ddecdc1d7",
            "authors": "Jiayin Wang, Fengran Mo, Weizhi Ma, Peijie Sun, Min Zhang, Jian-Yun Nie",
            "EMNLP Paper ID": "404",
            "Oral/Poster": "Oral",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "9061cc45c64846498f572c9ad2cb14f76324d665",
            "title": "Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA",
            "abstract": "Long-context modeling capabilities have garnered widespread attention, leading to the emergence of Large Language Models (LLMs) with ultra-context windows. Meanwhile, benchmarks for evaluating long-context LLMs are gradually catching up. However, existing benchmarks employ irrelevant noise texts to artificially extend the length of test cases, diverging from the real-world scenarios of long-context applications. To bridge this gap, we propose a novel long-context benchmark, Loong, aligning with realistic scenarios through extended multi-document question answering (QA). Unlike typical document QA, in Loong's test cases, each document is relevant to the final answer, ignoring any document will lead to the failure of the answer. Furthermore, Loong introduces four types of tasks with a range of context lengths: Spotlight Locating, Comparison, Clustering, and Chain of Reasoning, to facilitate a more realistic and comprehensive evaluation of long-context understanding. Extensive experiments indicate that existing long-context language models still exhibit considerable potential for enhancement. Retrieval augmented generation (RAG) achieves poor performance, demonstrating that Loong can reliably assess the model's long-context modeling capabilities.",
            "link": "https://www.semanticscholar.org/paper/9061cc45c64846498f572c9ad2cb14f76324d665",
            "authors": "Minzheng Wang, Longze Chen, Cheng Fu, Shengyi Liao, Xinghua Zhang, Bingli Wu, Haiyang Yu, Nan Xu, Lei Zhang, Run Luo, Yunshui Li, Min Yang, Fei Huang, Yongbin Li",
            "EMNLP Paper ID": "626",
            "Oral/Poster": "Oral",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "72099c9eae74859e5389b3806732ed1b7f2ce69b",
            "title": "MIND: Multimodal Shopping Intention Distillation from Large Vision-language Models for E-commerce Purchase Understanding",
            "abstract": "Improving user experience and providing personalized search results in E-commerce platforms heavily rely on understanding purchase intention. However, existing methods for acquiring large-scale intentions bank on distilling large language models with human annotation for verification. Such an approach tends to generate product-centric intentions, overlook valuable visual information from product images, and incurs high costs for scalability. To address these issues, we introduce MIND, a multimodal framework that allows Large Vision-Language Models (LVLMs) to infer purchase intentions from multimodal product metadata and prioritize human-centric ones. Using Amazon Review data, we apply MIND and create a multimodal intention knowledge base, which contains 1,264,441 million intentions derived from 126,142 co-buy shopping records across 107,215 products. Extensive human evaluations demonstrate the high plausibility and typicality of our obtained intentions and validate the effectiveness of our distillation framework and filtering mechanism. Additional experiments reveal that our obtained intentions significantly enhance large language models in two intention comprehension tasks.",
            "link": "https://www.semanticscholar.org/paper/72099c9eae74859e5389b3806732ed1b7f2ce69b",
            "authors": "Baixuan Xu, Weiqi Wang, Haochen Shi, Wenxuan Ding, Huihao Jing, Tianqing Fang, Jiaxin Bai, Long Chen, Yangqiu Song",
            "EMNLP Paper ID": "892",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "48372c125a66a5b313b9d93b34fe329464b5e952",
            "title": "DataTales: A Benchmark for Real-World Intelligent Data Narration",
            "abstract": "We introduce DataTales, a novel benchmark designed to assess the proficiency of language models in data narration, a task crucial for transforming complex tabular data into accessible narratives. Existing benchmarks often fall short in capturing the requisite analytical complexity for practical applications. DataTales addresses this gap by offering 4.9k financial reports paired with corresponding market data, showcasing the demand for models to create clear narratives and analyze large datasets while understanding specialized terminology in the field. Our findings highlights the significant challenge that language models face in achieving the necessary precision and analytical depth for proficient data narration, suggesting promising avenues for future model development and evaluation methodologies.",
            "link": "https://www.semanticscholar.org/paper/48372c125a66a5b313b9d93b34fe329464b5e952",
            "authors": "Yajing Yang, Qian Liu, Min-Yen Kan",
            "EMNLP Paper ID": "1226",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "344fa52471306a25133c9536992be15eecdd1c60",
            "title": "One Thousand and One Pairs: A \"novel\" challenge for long-context language models",
            "abstract": "Synthetic long-context LLM benchmarks (e.g.,\"needle-in-the-haystack\") test only surface-level retrieval capabilities, but how well can long-context LLMs retrieve, synthesize, and reason over information across book-length inputs? We address this question by creating NoCha, a dataset of 1,001 minimally different pairs of true and false claims about 67 recently-published English fictional books, written by human readers of those books. In contrast to existing long-context benchmarks, our annotators confirm that the largest share of pairs in NoCha require global reasoning over the entire book to verify. Our experiments show that while human readers easily perform this task, it is enormously challenging for all ten long-context LLMs that we evaluate: no open-weight model performs above random chance (despite their strong performance on synthetic benchmarks), while GPT-4o achieves the highest accuracy at 55.8%. Further analysis reveals that (1) on average, models perform much better on pairs that require only sentence-level retrieval vs. global reasoning; (2) model-generated explanations for their decisions are often inaccurate even for correctly-labeled claims; and (3) models perform substantially worse on speculative fiction books that contain extensive world-building. The methodology proposed in NoCha allows for the evolution of the benchmark dataset and the easy analysis of future models.",
            "link": "https://www.semanticscholar.org/paper/344fa52471306a25133c9536992be15eecdd1c60",
            "authors": "Marzena Karpinska, Katherine Thai, Kyle Lo, Tanya Goyal, Mohit Iyyer",
            "EMNLP Paper ID": "2012",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "82f041ed71f8ef1cf462fa03a7e732e440259bd7",
            "title": "LongWanjuan: Towards Systematic Measurement for Long Text Quality",
            "abstract": "The quality of training data are crucial for enhancing the long-text capabilities of foundation models. Despite existing efforts to refine data quality through heuristic rules and evaluations based on data diversity and difficulty, there's a lack of systematic approaches specifically tailored for assessing long texts. Addressing this gap, our work systematically measures the quality of long texts by evaluating three fundamental linguistic dimensions: coherence, cohesion, and complexity. Drawing inspiration from the aforementioned three dimensions, we introduce a suite of metrics designed to evaluate the quality of long texts, encompassing both statistical and pre-trained language model-based ones. Leveraging these metrics, we present LongWanjuan, a bilingual dataset specifically tailored to enhance the training of language models for long-text tasks with over 160B tokens. In LongWanjuan, we categorize long texts into holistic, aggregated, and chaotic types, enabling a detailed analysis of long-text quality. Furthermore, we devise a data mixture recipe that strategically balances different types of long texts within LongWanjuan, leading to significant improvements in model performance on long-text tasks. The code and dataset are available at https://github.com/OpenLMLab/LongWanjuan.",
            "link": "https://www.semanticscholar.org/paper/82f041ed71f8ef1cf462fa03a7e732e440259bd7",
            "authors": "Kai Lv, Xiaoran Liu, Qipeng Guo, Hang Yan, Conghui He, Xipeng Qiu, Dahua Lin",
            "matchScore": 239.15425,
            "original title": "LongWanjuan: Towards Systematic Measurement for Long Text Quality",
            "original authors": "Xiaoran Liu, Kai Lv, Qipeng Guo, Hang Yan, Conghui He, Xipeng Qiu, Dahua Lin",
            "EMNLP Paper ID": "1154",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "9fc5a55e96b63c4e981363c7a03c54cc6ae8976d",
            "title": "CERD: A Comprehensive Chinese Rhetoric Dataset for Rhetorical Understanding and Generation in Essays",
            "abstract": "Existing rhetorical understanding and generation datasets or corpora primarily focus on single coarse-grained categories or fine-grained categories, neglecting the common interrelations between different rhetorical devices by treating them as independent sub-tasks. In this paper, we propose the Chinese Essay Rhetoric Dataset (CERD), consisting of 4 commonly used coarse-grained categories including metaphor, personification, hyperbole and parallelism and 23 fine-grained categories across both form and content levels. CERD is a manually annotated and comprehensive Chinese rhetoric dataset with five interrelated sub-tasks. Unlike previous work, our dataset aids in understanding various rhetorical devices, recognizing corresponding rhetorical components, and generating rhetorical sentences under given conditions, thereby improving the author's writing proficiency and language usage skills. Extensive experiments are conducted to demonstrate the interrelations between multiple tasks in CERD, as well as to establish a benchmark for future research on rhetoric. The experimental results indicate that Large Language Models achieve the best performance across most tasks, and jointly fine-tuning with multiple tasks further enhances performance.",
            "link": "https://www.semanticscholar.org/paper/9fc5a55e96b63c4e981363c7a03c54cc6ae8976d",
            "authors": "Nuowei Liu, Xinhao Chen, Hongyi Wu, Changzhi Sun, Man Lan, Yuanbin Wu, Xiaopeng Bai, Shaoguang Mao, Yan Xia",
            "matchScore": 290.47757,
            "original title": "CERD: A Comprehensive Chinese Rhetoric Dataset for Rhetorical Understanding and Generation in Essays",
            "original authors": "Nuowei Liu, Xinhao Chen, Hongyi Wu, Changzhi Sun, Man Lan, Yuanbin Wu, Xiaopeng Bai, Shaoguang Mao, Yan Xia",
            "EMNLP Paper ID": "1377",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "6920cd2d28ed5194b51a7733b738d5e1ae79c6f5",
            "title": "LongGenBench: Long-context Generation Benchmark",
            "abstract": "Current long-context benchmarks primarily focus on retrieval-based tests, requiring Large Language Models (LLMs) to locate specific information within extensive input contexts, such as the needle-in-a-haystack (NIAH) benchmark. Long-context generation refers to the ability of a language model to generate coherent and contextually accurate text that spans across lengthy passages or documents. While recent studies show strong performance on NIAH and other retrieval-based long-context benchmarks, there is a significant lack of benchmarks for evaluating long-context generation capabilities. To bridge this gap and offer a comprehensive assessment, we introduce a synthetic benchmark, LongGenBench, which allows for flexible configurations of customized generation context lengths. LongGenBench advances beyond traditional benchmarks by redesigning the format of questions and necessitating that LLMs respond with a single, cohesive long-context answer. Upon extensive evaluation using LongGenBench, we observe that: (1) both API accessed and open source models exhibit performance degradation in long-context generation scenarios, ranging from 1.2% to 47.1%; (2) different series of LLMs exhibit varying trends of performance degradation, with the Gemini-1.5-Flash model showing the least degradation among API accessed models, and the Qwen2 series exhibiting the least degradation in LongGenBench among open source models.",
            "link": "https://www.semanticscholar.org/paper/6920cd2d28ed5194b51a7733b738d5e1ae79c6f5",
            "authors": "Xiang Liu, Peijie Dong, Xuming Hu, Xiaowen Chu",
            "matchScore": 198.73672,
            "original title": "LongGenBench: Long-context Generation Benchmark",
            "original authors": "Xiang Liu, Peijie Dong, Xuming Hu, Xiaowen Chu",
            "EMNLP Paper ID": "178",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Findings"
        },
        {
            "paperId": "9a392a2d97946c6fea1976ec44ee2160d9b92471",
            "title": "AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models",
            "abstract": "Given the importance of ancient Chinese in capturing the essence of rich historical and cultural heritage, the rapid advancements in Large Language Models (LLMs) necessitate benchmarks that can effectively evaluate their understanding of ancient contexts. To meet this need, we present AC-EVAL, an innovative benchmark designed to assess the advanced knowledge and reasoning capabilities of LLMs within the context of ancient Chinese. AC-EVAL is structured across three levels of difficulty reflecting different facets of language comprehension: general historical knowledge, short text understanding, and long text comprehension. The benchmark comprises 13 tasks, spanning historical facts, geography, social customs, art, philosophy, classical poetry and prose, providing a comprehensive assessment framework. Our extensive evaluation of top-performing LLMs, tailored for both English and Chinese, reveals a substantial potential for enhancing ancient text comprehension. By highlighting the strengths and weaknesses of LLMs, AC-EVAL aims to promote their development and application forward in the realms of ancient Chinese language education and scholarly research. The AC-EVAL data and evaluation code are available at https://github.com/yuting-wei/AC-EVAL.",
            "link": "https://www.semanticscholar.org/paper/9a392a2d97946c6fea1976ec44ee2160d9b92471",
            "authors": "Yuting Wei, Yuanxing Xu, Xinru Wei, Simin Yang, Yangfu Zhu, Yuqing Li, Di Liu, Bin Wu",
            "matchScore": 286.8382,
            "original title": "AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models",
            "original authors": "Yuting Wei, Yuanxing Xu, Xinru Wei, yangsimin, Yangfu Zhu, Yuqing Li, Di Liu, Bin Wu",
            "EMNLP Paper ID": "333",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "ee52a820e35cf5095e681ccf8ca8eea978d336ad",
            "title": "DetectBench: Can Large Language Model Detect and Piece Together Implicit Evidence?",
            "abstract": "Detecting evidence within the context is a key step in the process of reasoning task. Evaluating and enhancing the capabilities of LLMs in evidence detection will strengthen context-based reasoning performance. This paper proposes a benchmark called DetectBench for verifying the ability to detect and piece together implicit evidence within a long context. DetectBench contains 3,928 multiple-choice questions, with an average of 994 tokens per question. Each question contains an average of 4.55 pieces of implicit evidence, and solving the problem typically requires 7.62 logical jumps to find the correct answer. To enhance the performance of LLMs in evidence detection, this paper proposes Detective Reasoning Prompt and Finetune. Experiments demonstrate that the existing LLMs' abilities to detect evidence in long contexts are far inferior to humans. However, the Detective Reasoning Prompt effectively enhances the capability of powerful LLMs in evidence detection, while the Finetuning method shows significant effects in enhancing the performance of weaker LLMs. Moreover, when the abilities of LLMs in evidence detection are improved, their final reasoning performance is also enhanced accordingly.",
            "link": "https://www.semanticscholar.org/paper/ee52a820e35cf5095e681ccf8ca8eea978d336ad",
            "authors": "Zhouhong Gu, Lin Zhang, Xiaoxuan Zhu, Jiangjie Chen, Wenhao Huang, Yikai Zhang, Shusen Wang, Zheyu Ye, Yan Gao, Hongwei Feng, Yanghua Xiao",
            "matchScore": 325.6783,
            "original title": "DetectBench: Can Large Language Model Detect and Piece Together Implicit Evidence?",
            "original authors": "Zhouhong Gu, Lin Zhang, Xiaoxuan Zhu, Jiangjie Chen, Wenhao Huang, Yikai Zhang, Shusen Wang, Zheyu Ye, Yan Gao, Hongwei Feng, Yanghua Xiao",
            "EMNLP Paper ID": "34",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "42bf360fccb6ab719de7f6f0ac4ac8d9912f98e7",
            "title": "IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce",
            "abstract": "Enhancing Language Models' (LMs) ability to understand purchase intentions in E-commerce scenarios is crucial for their effective assistance in various downstream tasks. However, previous approaches that distill intentions from LMs often fail to generate meaningful and human-centric intentions applicable in real-world E-commerce contexts. This raises concerns about the true comprehension and utilization of purchase intentions by LMs. In this paper, we present IntentionQA, a double-task multiple-choice question answering benchmark to evaluate LMs' comprehension of purchase intentions in E-commerce. Specifically, LMs are tasked to infer intentions based on purchased products and utilize them to predict additional purchases. IntentionQA consists of 4,360 carefully curated problems across three difficulty levels, constructed using an automated pipeline to ensure scalability on large E-commerce platforms. Human evaluations demonstrate the high quality and low false-negative rate of our benchmark. Extensive experiments across 19 language models show that they still struggle with certain scenarios, such as understanding products and intentions accurately, jointly reasoning with products and intentions, and more, in which they fall far behind human performances. Our code and data are publicly available at https://github.com/HKUST-KnowComp/IntentionQA.",
            "link": "https://www.semanticscholar.org/paper/42bf360fccb6ab719de7f6f0ac4ac8d9912f98e7",
            "authors": "Wenxuan Ding, Weiqi Wang, Sze Heng Douglas Kwok, Minghao Liu, Tianqing Fang, Jiaxin Bai, Junxian He, Yangqiu Song",
            "matchScore": 290.6378,
            "original title": "IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce",
            "original authors": "Wenxuan Ding, Weiqi Wang, Sze Heng Douglas Kwok, Minghao LIU, Tianqing Fang, Jiaxin Bai, Xin Liu, Changlong Yu, Zheng Li, Chen Luo, Qingyu Yin, Bing Yin, Junxian He, Yangqiu Song",
            "EMNLP Paper ID": "447",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "0ed0d844544c4a2981acccb9332dead922294664",
            "title": "CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models",
            "abstract": "Developing Large Language Models (LLMs) with robust long-context capabilities has been the recent research focus, resulting in the emergence of long-context LLMs proficient in Chinese. However, the evaluation of these models remains underdeveloped due to a lack of benchmarks. To address this gap, we present CLongEval, a comprehensive Chinese benchmark for evaluating long-context LLMs. CLongEval is characterized by three key features: (1) Sufficient data volume, comprising 7 distinct tasks and 7,267 examples; (2) Broad applicability, accommodating to models with context windows size from 1K to 100K; (3) High quality, with over 2,000 manually annotated question-answer pairs in addition to the automatically constructed labels. With CLongEval, we undertake a comprehensive assessment of 6 open-source long-context LLMs and 2 leading commercial counterparts that feature both long-context abilities and proficiency in Chinese. We also provide in-depth analysis based on the empirical results, trying to shed light on the critical capabilities that present challenges in long-context settings. The dataset, evaluation scripts, and model outputs are released.",
            "link": "https://www.semanticscholar.org/paper/0ed0d844544c4a2981acccb9332dead922294664",
            "authors": "Zexuan Qiu, Jingjing Li, Shijue Huang, Wanjun Zhong, Irwin King",
            "matchScore": 272.83612,
            "original title": "CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models",
            "original authors": "Zexuan Qiu, Jingjing Li, Shijue Huang, Xiaoqi Jiao, Wanjun Zhong, Irwin King",
            "EMNLP Paper ID": "790",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "c9f9c547fe6d7c66af399d516c879e651518e586",
            "title": "TongGu: Mastering Classical Chinese Understanding with Knowledge-Grounded Large Language Models",
            "abstract": "Classical Chinese is a gateway to the rich heritage and wisdom of ancient China, yet its complexities pose formidable comprehension barriers for most modern people without specialized knowledge. While Large Language Models (LLMs) have shown remarkable capabilities in Natural Language Processing (NLP), they struggle with Classical Chinese Understanding (CCU), especially in data-demanding and knowledge-intensive tasks. In response to this dilemma, we propose \\textbf{TongGu} (mean understanding ancient and modern), the first CCU-specific LLM, underpinned by three core contributions. First, we construct a two-stage instruction-tuning dataset ACCN-INS derived from rich classical Chinese corpora, aiming to unlock the full CCU potential of LLMs. Second, we propose Redundancy-Aware Tuning (RAT) to prevent catastrophic forgetting, enabling TongGu to acquire new capabilities while preserving its foundational knowledge. Third, we present a CCU Retrieval-Augmented Generation (CCU-RAG) technique to reduce hallucinations based on knowledge-grounding. Extensive experiments across 24 diverse CCU tasks validate TongGu's superior ability, underscoring the effectiveness of RAT and CCU-RAG. The model and dataset are available at \\url{https://github.com/SCUT-DLVCLab/TongGu-LLM}.",
            "link": "https://www.semanticscholar.org/paper/c9f9c547fe6d7c66af399d516c879e651518e586",
            "authors": "Jiahuan Cao, Dezhi Peng, Peirong Zhang, Yongxin Shi, Yang Liu, Kai Ding, Lianwen Jin",
            "matchScore": 306.34045,
            "original title": "TongGu: Mastering Classical Chinese Understanding with Knowledge-Grounded Large Language Models",
            "original authors": "Jiahuan Cao, Dezhi Peng, Peirong Zhang, Yongxin Shi, Yang Liu, Kai Ding, Lianwen Jin",
            "EMNLP Paper ID": "841",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        }
    ],
    "LLM-based Ranking and Recommendation Techniques": [
        {
            "paperId": "477d98492acd671c08b964b6d1e25b1161748d72",
            "title": "Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing",
            "abstract": "The powerful generative abilities of large language models (LLMs) show potential in generating relevance labels for search applications. Previous work has found that directly asking about relevancy, such as ``How relevant is document A to query Q?\", results in sub-optimal ranking. Instead, the pairwise ranking prompting (PRP) approach produces promising ranking performance through asking about pairwise comparisons, e.g., ``Is document A more relevant than document B to query Q?\". Thus, while LLMs are effective at their ranking ability, this is not reflected in their relevance label generation. In this work, we propose a post-processing method to consolidate the relevance labels generated by an LLM with its powerful ranking abilities. Our method takes both LLM generated relevance labels and pairwise preferences. The labels are then altered to satisfy the pairwise preferences of the LLM, while staying as close to the original values as possible. Our experimental results indicate that our approach effectively balances label accuracy and ranking performance. Thereby, our work shows it is possible to combine both the ranking and labeling abilities of LLMs through post-processing.",
            "link": "https://www.semanticscholar.org/paper/477d98492acd671c08b964b6d1e25b1161748d72",
            "authors": "Le Yan, Zhen Qin, Honglei Zhuang, R. Jagerman, Xuanhui Wang, Michael Bendersky, Harrie Oosterhuis",
            "EMNLP Paper ID": "48",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 12): 17:45\u201318:45 (Evening)",
            "Type": "Main"
        },
        {
            "paperId": "0b8adc7a995a045393aa560abfcd3acf4bb2b386",
            "title": "FIRST: Faster Improved Listwise Reranking with Single Token Decoding",
            "abstract": "Large Language Models (LLMs) have significantly advanced the field of information retrieval, particularly for reranking. Listwise LLM rerankers have showcased superior performance and generalizability compared to existing supervised approaches. However, conventional listwise LLM reranking methods lack efficiency as they provide ranking output in the form of a generated ordered sequence of candidate passage identifiers. Further, they are trained with the typical language modeling objective, which treats all ranking errors uniformly--potentially at the cost of misranking highly relevant passages. Addressing these limitations, we introduce FIRST, a novel listwise LLM reranking approach leveraging the output logits of the first generated identifier to directly obtain a ranked ordering of the candidates. Further, we incorporate a learning-to-rank loss during training, prioritizing ranking accuracy for the more relevant passages. Empirical results demonstrate that FIRST accelerates inference by 50% while maintaining a robust ranking performance with gains across the BEIR benchmark. Finally, to illustrate the practical effectiveness of listwise LLM rerankers, we investigate their application in providing relevance feedback for retrievers during inference. Our results show that LLM rerankers can provide a stronger distillation signal compared to cross-encoders, yielding substantial improvements in retriever recall after relevance feedback.",
            "link": "https://www.semanticscholar.org/paper/0b8adc7a995a045393aa560abfcd3acf4bb2b386",
            "authors": "R. Reddy, Jae Doo, Yifei Xu, Md Arafat Sultan, Deevya Swain, Avirup Sil, Heng Ji",
            "EMNLP Paper ID": "998",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "fedaa7a32110c70946e1de719d6293e45ea8b9ab",
            "title": "Enhancing High-order Interaction Awareness in LLM-based Recommender Model",
            "abstract": "Large language models (LLMs) have demonstrated prominent reasoning capabilities in recommendation tasks by transforming them into text-generation tasks. However, existing approaches either disregard or ineffectively model the user-item high-order interactions. To this end, this paper presents an enhanced LLM-based recommender (ELMRec). We enhance whole-word embeddings to substantially enhance LLMs' interpretation of graph-constructed interactions for recommendations, without requiring graph pre-training. This finding may inspire endeavors to incorporate rich knowledge graphs into LLM-based recommenders via whole-word embedding. We also found that LLMs often recommend items based on users' earlier interactions rather than recent ones, and present a reranking solution. Our ELMRec outperforms state-of-the-art (SOTA) methods in both direct and sequential recommendations.",
            "link": "https://www.semanticscholar.org/paper/fedaa7a32110c70946e1de719d6293e45ea8b9ab",
            "authors": "Xinfeng Wang, Jin Cui, Fumiyo Fukumoto, Yoshimi Suzuki",
            "EMNLP Paper ID": "1361",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "407cae26fe0d45f911443a36fa4c56d45b3f6803",
            "title": "Leveraging Estimated Transferability Over Human Intuition for Model Selection in Text Ranking",
            "abstract": "Text ranking has witnessed significant advancements, attributed to the utilization of dual-encoder enhanced by Pre-trained Language Models (PLMs). Given the proliferation of available PLMs, selecting the most effective one for a given dataset has become a non-trivial challenge. As a promising alternative to human intuition and brute-force fine-tuning, Transferability Estimation (TE) has emerged as an effective approach to model selection. However, current TE methods are primarily designed for classification tasks, and their estimated transferability may not align well with the objectives of text ranking. To address this challenge, we propose to compute the expected rank as transferability, explicitly reflecting the model's ranking capability. Furthermore, to mitigate anisotropy and incorporate training dynamics, we adaptively scale isotropic sentence embeddings to yield an accurate expected rank score. Our resulting method, Adaptive Ranking Transferability (AiRTran), can effectively capture subtle differences between models. On challenging model selection scenarios across various text ranking datasets, it demonstrates significant improvements over previous classification-oriented TE methods, human intuition, and ChatGPT with minor time consumption.",
            "link": "https://www.semanticscholar.org/paper/407cae26fe0d45f911443a36fa4c56d45b3f6803",
            "authors": "Jun Bai, Zhuofan Chen, Zhenzi Li, Hanhua Hong, Jianfei Zhang, Chen Li, Chenghua Lin, Wenge Rong",
            "EMNLP Paper ID": "1443",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "d5da6f458b21c11bc87524120b8e82972b41b930",
            "title": "Recurrent Alignment with Hard Attention for Hierarchical Text Rating",
            "abstract": "While large language models (LLMs) excel at understanding and generating plain text, they are not tailored to handle hierarchical text structures or directly predict task-specific properties such as text rating. In fact, selectively and repeatedly grasping the hierarchical structure of large-scale text is pivotal for deciphering its essence. To this end, we propose a novel framework for hierarchical text rating utilizing LLMs, which incorporates Recurrent Alignment with Hard Attention (RAHA). Particularly, hard attention mechanism prompts a frozen LLM to selectively focus on pertinent leaf texts associated with the root text and generate symbolic representations of their relationships. Inspired by the gradual stabilization of the Markov Chain, recurrent alignment strategy involves feeding predicted ratings iteratively back into the prompts of another trainable LLM, aligning it to progressively approximate the desired target. Experimental results demonstrate that RAHA outperforms existing state-of-the-art methods on three hierarchical text rating datasets. Theoretical and empirical analysis confirms RAHA's ability to gradually converge towards the underlying target through multiple inferences. Additional experiments on plain text rating datasets verify the effectiveness of this Markov-like alignment. Our data and code can be available in https://github.com/ECNU-Text-Computing/Markov-LLM.",
            "link": "https://www.semanticscholar.org/paper/d5da6f458b21c11bc87524120b8e82972b41b930",
            "authors": "Chenxi Lin, Jiayu Ren, Guoxiu He, Zhuoren Jiang, Haiyan Yu, Xiaomin Zhu",
            "EMNLP Paper ID": "2324",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "f20dec2ff4cec8b5aa6c7371317debe14b24a87d",
            "title": "Comparing Neighbors Together Makes it Easy: Jointly Comparing Multiple Candidates for Efficient and Effective Retrieval",
            "abstract": "A common retrieve-and-rerank paradigm involves retrieving relevant candidates from a broad set using a fast bi-encoder (BE), followed by applying expensive but accurate cross-encoders (CE) to a limited candidate set. However, relying on this small subset is often susceptible to error propagation from the bi-encoders, which limits the overall performance. To address these issues, we propose the Comparing Multiple Candidates (CMC) framework. CMC compares a query and multiple embeddings of similar candidates (i.e., neighbors) through shallow self-attention layers, delivering rich representations contextualized to each other. Furthermore, CMC is scalable enough to handle multiple comparisons simultaneously. For example, comparing ~10K candidates with CMC takes a similar amount of time as comparing 16 candidates with CE. Experimental results on the ZeSHEL dataset demonstrate that CMC, when plugged in between bi-encoders and cross-encoders as a seamless intermediate reranker (BE-CMC-CE), can effectively improve recall@k (+4.8%-p, +3.5%-p for R@16, R@64) compared to using only bi-encoders (BE-CE), with negligible slowdown (<7%). Additionally, to verify CMC's effectiveness as the final-stage reranker in improving top-1 accuracy, we conduct experiments on downstream tasks such as entity, passage, and dialogue ranking. The results indicate that CMC is not only faster (11x) but also often more effective than CE, with improved prediction accuracy in Wikipedia entity linking (+0.7%-p) and DSTC7 dialogue ranking (+3.3%-p).",
            "link": "https://www.semanticscholar.org/paper/f20dec2ff4cec8b5aa6c7371317debe14b24a87d",
            "authors": "Jonghyun Song, Cheyon Jin, Wenlong Zhao, Jay-Yoon Lee",
            "EMNLP Paper ID": "3162",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "b8f6c8efbbeaba89f7730c9dfe56353f54f98399",
            "title": "LINKAGE: Listwise Ranking among Varied-Quality References for Non-Factoid QA Evaluation via LLMs",
            "abstract": "Non-Factoid (NF) Question Answering (QA) is challenging to evaluate due to diverse potential answers and no objective criterion. The commonly used automatic evaluation metrics like ROUGE or BERTScore cannot accurately measure semantic similarities or answers from different perspectives. Recently, Large Language Models (LLMs) have been resorted to for NFQA evaluation due to their compelling performance on various NLP tasks. Common approaches include pointwise scoring of each candidate answer and pairwise comparisons between answers. Inspired by the evolution from pointwise to pairwise to listwise in learning-to-rank methods, we propose a novel listwise NFQA evaluation approach, that utilizes LLMs to rank candidate answers in a list of reference answers sorted by descending quality. Moreover, for NF questions that do not have multi-grade or any golden answers, we leverage LLMs to generate the reference answer list of various quality to facilitate the listwise evaluation. Extensive experimental results on three NFQA datasets, i.e., ANTIQUE, the TREC-DL-NF, and WebGLM show that our method has significantly higher correlations with human annotations compared to automatic scores and common pointwise and pairwise approaches.",
            "link": "https://www.semanticscholar.org/paper/b8f6c8efbbeaba89f7730c9dfe56353f54f98399",
            "authors": "Sihui Yang, Keping Bi, Wanqing Cui, Jiafeng Guo, Xueqi Cheng",
            "matchScore": 376.41382,
            "original title": "LINKAGE: Listwise Ranking among Varied-Quality References for Non-Factoid QA Evaluation via LLMs",
            "original authors": "Sihui Yang, Keping Bi, Wanqing Cui, Jiafeng Guo, Xueqi Cheng",
            "EMNLP Paper ID": "1430",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "1eb3d80c68d45e6ff09965a3069787e41f10a090",
            "title": "Make Large Language Model a Better Ranker",
            "abstract": "Large Language Models (LLMs) demonstrate robust capabilities across various fields, leading to a paradigm shift in LLM-enhanced Recommender System (RS). Research to date focuses on point-wise and pair-wise recommendation paradigms, which are inefficient for LLM-based recommenders due to high computational costs. However, existing list-wise approaches also fall short in ranking tasks due to misalignment between ranking objectives and next-token prediction. Moreover, these LLM-based methods struggle to effectively address the order relation among candidates, particularly given the scale of ratings. To address these challenges, this paper introduces the large language model framework with Aligned Listwise Ranking Objectives (ALRO). ALRO is designed to bridge the gap between the capabilities of LLMs and the nuanced requirements of ranking tasks. Specifically, ALRO employs explicit feedback in a listwise manner by introducing soft lambda loss, a customized adaptation of lambda loss designed for optimizing order relations. This mechanism provides more accurate optimization goals, enhancing the ranking process. Additionally, ALRO incorporates a permutation-sensitive learning mechanism that addresses position bias, a prevalent issue in generative models, without imposing additional computational burdens during inference. Our evaluative studies reveal that ALRO outperforms both existing embedding-based recommendation methods and LLM-based recommendation baselines.",
            "link": "https://www.semanticscholar.org/paper/1eb3d80c68d45e6ff09965a3069787e41f10a090",
            "authors": "WenShuo Chao, Zhi Zheng, Hengshu Zhu, Hao Liu",
            "matchScore": 183.30556,
            "original title": "Make Large Language Model a Better Ranker",
            "original authors": "Wenshuo Chao, Zhi Zheng, Hengshu Zhu, Hao Liu",
            "EMNLP Paper ID": "188",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "b7c8feb968b5bef5096c14a7a7e8f4341b53acc9",
            "title": "Train Once, Use Flexibly: A Modular Framework for Multi-Aspect Neural News Recommendation",
            "abstract": "Recent neural news recommenders (NNRs) extend content-based recommendation (1) by aligning additional aspects (e.g., topic, sentiment) between candidate news and user history or (2) by diversifying recommendations w.r.t. these aspects. This customization is achieved by ``hardcoding`` additional constraints into the NNR's architecture and/or training objectives: any change in the desired recommendation behavior thus requires retraining the model with a modified objective. This impedes widespread adoption of multi-aspect news recommenders. In this work, we introduce MANNeR, a modular framework for multi-aspect neural news recommendation that supports on-the-fly customization over individual aspects at inference time. With metric-based learning as its backbone, MANNeR learns aspect-specialized news encoders and then flexibly and linearly combines the resulting aspect-specific similarity scores into different ranking functions, alleviating the need for ranking function-specific retraining of the model. Extensive experimental results show that MANNeR consistently outperforms state-of-the-art NNRs on both standard content-based recommendation and single- and multi-aspect customization. Lastly, we validate that MANNeR's aspect-customization module is robust to language and domain transfer.",
            "link": "https://www.semanticscholar.org/paper/b7c8feb968b5bef5096c14a7a7e8f4341b53acc9",
            "authors": "Andreea Iana, Goran Glavas, Heiko Paulheim",
            "matchScore": 283.5522,
            "original title": "Train Once, Use Flexibly: A Modular Framework for Multi-Aspect Neural News Recommendation",
            "original authors": "Andreea Iana, Goran Glava\u0161, Heiko Paulheim",
            "EMNLP Paper ID": "1979",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "a27ec251891ec0b55910c3a318fa6f2d71266d52",
            "title": "Numbers Matter! Bringing Quantity-awareness to Retrieval Systems",
            "abstract": "Quantitative information plays a crucial role in understanding and interpreting the content of documents. Many user queries contain quantities and cannot be resolved without understanding their semantics, e.g., ``car that costs less than $10k''. Yet, modern search engines apply the same ranking mechanisms for both words and quantities, overlooking magnitude and unit information. In this paper, we introduce two quantity-aware ranking techniques designed to rank both the quantity and textual content either jointly or independently. These techniques incorporate quantity information in available retrieval systems and can address queries with numerical conditions equal, greater than, and less than. To evaluate the effectiveness of our proposed models, we introduce two novel quantity-aware benchmark datasets in the domains of finance and medicine and compare our method against various lexical and neural models. The code and data are available under https://github.com/satya77/QuantityAwareRankers.",
            "link": "https://www.semanticscholar.org/paper/a27ec251891ec0b55910c3a318fa6f2d71266d52",
            "authors": "Satya Almasian, Milena Bruseva, Michael Gertz",
            "matchScore": 255.67744,
            "original title": "Numbers Matter! Bringing Quantity-awareness to Retrieval Systems",
            "original authors": "Satya Almasian, Milena Bruseva, Michael Gertz",
            "EMNLP Paper ID": "2379",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "1b2888a60fd95eed021bbdfb754e75f32338db83",
            "title": "Few-shot Prompting for Pairwise Ranking: An Effective Non-Parametric Retrieval Model",
            "abstract": "A supervised ranking model, despite its advantage of being effective, usually involves complex processing - typically multiple stages of task-specific pre-training and fine-tuning. This has motivated researchers to explore simpler pipelines leveraging large language models (LLMs) that are capable of working in a zero-shot manner. However, since zero-shot inference does not make use of a training set of pairs of queries and their relevant documents, its performance is mostly worse than that of supervised models, which are trained on such example pairs. Motivated by the existing findings that training examples generally improve zero-shot performance, in our work, we explore if this also applies to ranking models. More specifically, given a query and a pair of documents, the preference prediction task is improved by augmenting examples of preferences for similar queries from a training set. Our proposed pairwise few-shot ranker demonstrates consistent improvements over the zero-shot baseline on both in-domain (TREC DL) and out-domain (BEIR subset) retrieval benchmarks. Our method also achieves a close performance to that of a supervised model without requiring any complex training pipeline.",
            "link": "https://www.semanticscholar.org/paper/1b2888a60fd95eed021bbdfb754e75f32338db83",
            "authors": "Nilanjan Sinhababu, Andrew Parry, Debasis Ganguly, Debasis Samanta, Pabitra Mitra",
            "matchScore": 227.02655,
            "original title": "Few-shot Pairwise Ranking Prompting: An Effective Non-Parametric Retrieval Model",
            "original authors": "Nilanjan Sinhababu, Andrew Parry, Debasis Ganguly, Debasis Samanta, Pabitra Mitra",
            "EMNLP Paper ID": "2416",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "086dfd5a98f5501b84a9bcf258e2a25b30d39449",
            "title": "Efficient Pointwise-Pairwise Learning-to-Rank for News Recommendation",
            "abstract": "News recommendation is a challenging task that involves personalization based on the interaction history and preferences of each user. Recent works have leveraged the power of pretrained language models (PLMs) to directly rank news items by using inference approaches that predominately fall into three categories: pointwise, pairwise, and listwise learning-to-rank. While pointwise methods offer linear inference complexity, they fail to capture crucial comparative information between items that is more effective for ranking tasks. Conversely, pairwise and listwise approaches excel at incorporating these comparisons but suffer from practical limitations: pairwise approaches are either computationally expensive or lack theoretical guarantees, and listwise methods often perform poorly in practice. In this paper, we propose a novel framework for PLM-based news recommendation that integrates both pointwise relevance prediction and pairwise comparisons in a scalable manner. We present a rigorous theoretical analysis of our framework, establishing conditions under which our approach guarantees improved performance. Extensive experiments show that our approach outperforms the state-of-the-art methods on the MIND and Adressa news recommendation datasets.",
            "link": "https://www.semanticscholar.org/paper/086dfd5a98f5501b84a9bcf258e2a25b30d39449",
            "authors": "Nithish Kannen, Yao Ma, Gerrit J.J. van den Burg, J. Faddoul",
            "matchScore": 244.09402,
            "original title": "Efficient Pointwise-Pairwise Learning-to-Rank for News Recommendation",
            "original authors": "Nithish Kannen, Yao Ma, Gerrit J.J. Van den Burg, Jean Baptiste Faddoul",
            "EMNLP Paper ID": "2427",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "547c9c04fa67e79490976849a910c875e4f372e0",
            "title": "HyQE: Ranking Contexts with Hypothetical Query Embeddings",
            "abstract": "In retrieval-augmented systems, context ranking techniques are commonly employed to reorder the retrieved contexts based on their relevance to a user query. A standard approach is to measure this relevance through the similarity between contexts and queries in the embedding space. However, such similarity often fails to capture the relevance. Alternatively, large language models (LLMs) have been used for ranking contexts. However, they can encounter scalability issues when the number of candidate contexts grows and the context window sizes of the LLMs remain constrained. Additionally, these approaches require fine-tuning LLMs with domain-specific data. In this work, we introduce a scalable ranking framework that combines embedding similarity and LLM capabilities without requiring LLM fine-tuning. Our framework uses a pre-trained LLM to hypothesize the user query based on the retrieved contexts and ranks the context based on the similarity between the hypothesized queries and the user query. Our framework is efficient at inference time and is compatible with many other retrieval and ranking techniques. Experimental results show that our method improves the ranking performance across multiple benchmarks. The complete code and data are available at https://github.com/zwc662/hyqe",
            "link": "https://www.semanticscholar.org/paper/547c9c04fa67e79490976849a910c875e4f372e0",
            "authors": "Weichao Zhou, Jiaxin Zhang, Hilaf Hasson, Anu Singh, Wenchao Li",
            "matchScore": 232.6231,
            "original title": "HyQE: Ranking Contexts with Hypothetical Query Embeddings",
            "original authors": "Weichao Zhou, Jiaxin Zhang, Hilaf Hasson, Anu Singh, Wenchao Li",
            "EMNLP Paper ID": "2550",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "020b09bd0757bf41a8b3c99300feb223404035ed",
            "title": "Train Once, Deploy Anywhere: Matryoshka Representation Learning for Multimodal Recommendation",
            "abstract": "Despite recent advancements in language and vision modeling, integrating rich multimodal knowledge into recommender systems continues to pose significant challenges. This is primarily due to the need for efficient recommendation, which requires adaptive and interactive responses. In this study, we focus on sequential recommendation and introduce a lightweight framework called full-scale Matryoshka representation learning for multimodal recommendation (fMRLRec). Our fMRLRec captures item features at different granularities, learning informative representations for efficient recommendation across multiple dimensions. To integrate item features from diverse modalities, fMRLRec employs a simple mapping to project multimodal item features into an aligned feature space. Additionally, we design an efficient linear transformation that embeds smaller features into larger ones, substantially reducing memory requirements for large-scale training on recommendation data. Combined with improved state space modeling techniques, fMRLRec scales to different dimensions and only requires one-time training to produce multiple models tailored to various granularities. We demonstrate the effectiveness and efficiency of fMRLRec on multiple benchmark datasets, which consistently achieves superior performance over state-of-the-art baseline methods. We make our code and data publicly available at https://github.com/yueqirex/fMRLRec.",
            "link": "https://www.semanticscholar.org/paper/020b09bd0757bf41a8b3c99300feb223404035ed",
            "authors": "Yueqi Wang, Zhenrui Yue, Huimin Zeng, Dong Wang, Julian McAuley",
            "matchScore": 310.36563,
            "original title": "Train Once, Deploy Anywhere: Matryoshka Representation Learning for Multimodal Recommendation",
            "original authors": "Yueqi Wang, Zhenrui Yue, Huimin Zeng, Dong Wang, Julian McAuley",
            "EMNLP Paper ID": "2613",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "83c5a1221213183bd9a896b59a03c68984f4fdbe",
            "title": "Modeling News Interactions and Influence for Financial Market Prediction",
            "abstract": "The diffusion of financial news into market prices is a complex process, making it challenging to evaluate the connections between news events and market movements. This paper introduces FININ (Financial Interconnected News Influence Network), a novel market prediction model that captures not only the links between news and prices but also the interactions among news items themselves. FININ effectively integrates multi-modal information from both market data and news articles. We conduct extensive experiments on two datasets, encompassing the S&P 500 and NASDAQ 100 indices over a 15-year period and over 2.7 million news articles. The results demonstrate FININ's effectiveness, outperforming advanced market prediction models with an improvement of 0.429 and 0.341 in the daily Sharpe ratio for the two markets respectively. Moreover, our results reveal insights into the financial news, including the delayed market pricing of news, the long memory effect of news, and the limitations of financial sentiment analysis in fully extracting predictive power from news data.",
            "link": "https://www.semanticscholar.org/paper/83c5a1221213183bd9a896b59a03c68984f4fdbe",
            "authors": "Mengyu Wang, Shay B. Cohen, Tiejun Ma",
            "matchScore": 196.10281,
            "original title": "Modeling News Interactions and Influence for Financial Market Prediction",
            "original authors": "Mengyu Wang, Shay B Cohen, Tiejun Ma",
            "EMNLP Paper ID": "675",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Findings"
        }
    ],
    "Advanced Techniques in Task-Oriented and Interactive Dialogue Systems": [
        {
            "paperId": "5c659db3a4faecc7e0e1dcdcc7150d76cb64c0e5",
            "title": "What's Mine becomes Yours: Defining, Annotating and Detecting Context-Dependent Paraphrases in News Interview Dialogs",
            "abstract": "Best practices for high conflict conversations like counseling or customer support almost always include recommendations to paraphrase the previous speaker. Although paraphrase classification has received widespread attention in NLP, paraphrases are usually considered independent from context, and common models and datasets are not applicable to dialog settings. In this work, we investigate paraphrases in dialog (e.g., Speaker 1:\"That book is mine.\"becomes Speaker 2:\"That book is yours.\"). We provide an operationalization of context-dependent paraphrases, and develop a training for crowd-workers to classify paraphrases in dialog. We introduce a dataset with utterance pairs from NPR and CNN news interviews annotated for context-dependent paraphrases. To enable analyses on label variation, the dataset contains 5,581 annotations on 600 utterance pairs. We present promising results with in-context learning and with token classification models for automatic paraphrase detection in dialog.",
            "link": "https://www.semanticscholar.org/paper/5c659db3a4faecc7e0e1dcdcc7150d76cb64c0e5",
            "authors": "Anna Wegmann, T. Broek, Dong Nguyen",
            "EMNLP Paper ID": "113",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "b1c4383ee62ca14e539450bd30f2ee4a900aadc7",
            "title": "Dialog2Flow: Pre-training Soft-Contrastive Action-Driven Sentence Embeddings for Automatic Dialog Flow Extraction",
            "abstract": "Efficiently deriving structured workflows from unannotated dialogs remains an underexplored and formidable challenge in computational linguistics. Automating this process could significantly accelerate the manual design of workflows in new domains and enable the grounding of large language models in domain-specific flowcharts, enhancing transparency and controllability. In this paper, we introduce Dialog2Flow (D2F) embeddings, which differ from conventional sentence embeddings by mapping utterances to a latent space where they are grouped according to their communicative and informative functions (i.e., the actions they represent). D2F allows for modeling dialogs as continuous trajectories in a latent space with distinct action-related regions. By clustering D2F embeddings, the latent space is quantized, and dialogs can be converted into sequences of region/action IDs, facilitating the extraction of the underlying workflow. To pre-train D2F, we build a comprehensive dataset by unifying twenty task-oriented dialog datasets with normalized per-turn action annotations. We also introduce a novel soft contrastive loss that leverages the semantic information of these actions to guide the representation learning process, showing superior performance compared to standard supervised contrastive loss. Evaluation against various sentence embeddings, including dialog-specific ones, demonstrates that D2F yields superior qualitative and quantitative results across diverse domains.",
            "link": "https://www.semanticscholar.org/paper/b1c4383ee62ca14e539450bd30f2ee4a900aadc7",
            "authors": "Sergio Burdisso, S. Madikeri, P. Motl\u00edcek",
            "EMNLP Paper ID": "606",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "d4dcd6eaab30e47ed3ba526663bdaa99e74a16e4",
            "title": "Synergizing In-context Learning with Hints for End-to-end Task-oriented Dialog Systems",
            "abstract": "End-to-end Task-Oriented Dialog (TOD) systems typically require extensive training datasets to perform well. In contrast, large language model (LLM) based TOD systems can excel even with limited data due to their ability to learn tasks through in-context exemplars. However, these models lack alignment with the style of responses in training data and often generate comprehensive responses, making it difficult for users to grasp the information quickly. In response, we propose SyncTOD that synergizes LLMs with task-specific hints to improve alignment in low-data settings. SyncTOD employs small auxiliary models to provide hints and select exemplars for in-context prompts. With ChatGPT, SyncTOD achieves superior performance compared to LLM-based baselines and SoTA models in low-data settings, while retaining competitive performance in full-data settings.",
            "link": "https://www.semanticscholar.org/paper/d4dcd6eaab30e47ed3ba526663bdaa99e74a16e4",
            "authors": "Vishal Vivek Saley, Rocktim Jyoti Das, Dinesh Raghu, Mausam",
            "EMNLP Paper ID": "624",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "1d7955bf03dbd5cb808beafc67a987cb0acc4441",
            "title": "Unsupervised End-to-End Task-Oriented Dialogue with LLMs: The Power of the Noisy Channel",
            "abstract": "Training task-oriented dialogue systems typically requires turn-level annotations for interacting with their APIs: e.g. a dialogue state and the system actions taken at each step. These annotations can be costly to produce, error-prone, and require both domain and annotation expertise. With advances in LLMs, we hypothesize that unlabeled data and a schema definition are sufficient for building a working task-oriented dialogue system, completely unsupervised. We consider a novel unsupervised setting of only (1) a well-defined API schema (2) a set of unlabeled dialogues between a user and agent. We propose an innovative approach using expectation-maximization (EM) that infers turn-level annotations as latent variables using a noisy channel model to build an end-to-end dialogue agent. Evaluating our approach on the MultiWOZ benchmark, our method more than doubles the dialogue success rate of a strong GPT-3.5 baseline.",
            "link": "https://www.semanticscholar.org/paper/1d7955bf03dbd5cb808beafc67a987cb0acc4441",
            "authors": "Brendan King, Jeffrey Flanigan",
            "EMNLP Paper ID": "961",
            "Oral/Poster": "Oral",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "68f1b94bbc900d2b5c60192a7e9eea4b046dd18a",
            "title": "Ontologically Faithful Generation of Non-Player Character Dialogues",
            "abstract": "We introduce a language generation task grounded in a popular video game environment. KNUDGE (KNowledge Constrained User-NPC Dialogue GEneration) requires models to produce trees of dialogue between video game characters that accurately reflect quest and entity specifications stated in natural language. KNUDGE is constructed from side quest dialogues drawn directly from game data of Obsidian Entertainment's The Outer Worlds, leading to real-world complexities in generation: (1) dialogues are branching trees as opposed to linear chains of utterances; (2) utterances must remain faithful to the game lore- character personas, backstories, and entity relationships; and (3) a dialogue must accurately reveal new quest details to the human player. We report results for a set of neural generation models using supervised and in-context learning techniques; we find competent performance but room for future work addressing the challenges of creating realistic, game-quality dialogues.",
            "link": "https://www.semanticscholar.org/paper/68f1b94bbc900d2b5c60192a7e9eea4b046dd18a",
            "authors": "Nathaniel Weir, Ryan Thomas, Randolph D'Amore, Kellie Hill, Benjamin Van Durme, Harsh Jhamtani",
            "EMNLP Paper ID": "1047",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "a1bd59c18c5be05af721400393ae34f52e12ff99",
            "title": "Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations",
            "abstract": "While language model (LM)-powered chatbots and generative search engines excel at answering concrete queries, discovering information in the terrain of unknown unknowns remains challenging for users. To emulate the common educational scenario where children/students learn by listening to and participating in conversations of their parents/teachers, we create Collaborative STORM (Co-STORM). Unlike QA systems that require users to ask all the questions, Co-STORM lets users observe and occasionally steer the discourse among several LM agents. The agents ask questions on the user's behalf, allowing the user to discover unknown unknowns serendipitously. To facilitate user interaction, Co-STORM assists users in tracking the discourse by organizing the uncovered information into a dynamic mind map, ultimately generating a comprehensive report as takeaways. For automatic evaluation, we construct the WildSeek dataset by collecting real information-seeking records with user goals. Co-STORM outperforms baseline methods on both discourse trace and report quality. In a further human evaluation, 70% of participants prefer Co-STORM over a search engine, and 78% favor it over a RAG chatbot.",
            "link": "https://www.semanticscholar.org/paper/a1bd59c18c5be05af721400393ae34f52e12ff99",
            "authors": "Yucheng Jiang, Yijia Shao, Dekun Ma, Sina J. Semnani, Monica S. Lam",
            "EMNLP Paper ID": "1104",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "125ff68732c441e143cf33587bebd2ab01372e49",
            "title": "Beyond the Turn-Based Game: Enabling Real-Time Conversations with Duplex Models",
            "abstract": "As large language models (LLMs) increasingly permeate daily lives, there is a growing demand for real-time interactions that mirror human conversations. Traditional turn-based chat systems driven by LLMs prevent users from verbally interacting with the system while it is generating responses. To overcome these limitations, we adapt existing LLMs to \\textit{duplex models} so that these LLMs can listen for users while generating output and dynamically adjust themselves to provide users with instant feedback. % such as in response to interruptions. Specifically, we divide the queries and responses of conversations into several time slices and then adopt a time-division-multiplexing (TDM) encoding-decoding strategy to pseudo-simultaneously process these slices. Furthermore, to make LLMs proficient enough to handle real-time conversations, we build a fine-tuning dataset consisting of alternating time slices of queries and responses as well as covering typical feedback types in instantaneous interactions. Our experiments show that although the queries and responses of conversations are segmented into incomplete slices for processing, LLMs can preserve their original performance on standard benchmarks with a few fine-tuning steps on our dataset. Automatic and human evaluation indicate that duplex models make user-AI interactions more natural and human-like, and greatly improve user satisfaction compared to vanilla LLMs. Our duplex model and dataset will be released.",
            "link": "https://www.semanticscholar.org/paper/125ff68732c441e143cf33587bebd2ab01372e49",
            "authors": "Xinrong Zhang, Yingfa Chen, Shengding Hu, Xu Han, Zihang Xu, Yuanwei Xu, Weilin Zhao, Maosong Sun, Zhiyuan Liu",
            "EMNLP Paper ID": "1345",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "4757e3d37d4753af0df749e71de0faec7d19d054",
            "title": "TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities",
            "abstract": "Task-oriented dialogue (TOD) systems aim to efficiently handle task-oriented conversations, including information collection. How to utilize TOD accurately, efficiently and effectively for information collection has always been a critical and challenging task. Recent studies have demonstrated that Large Language Models (LLMs) excel in dialogue, instruction generation, and reasoning, and can significantly enhance the performance of TOD through fine-tuning. However, current datasets primarily cater to user-led systems and are limited to predefined specific scenarios and slots, thereby necessitating improvements in the proactiveness, diversity, and capabilities of TOD. In this study, we present a detailed multi-domain task-oriented data construction process for conversations, and a Chinese dialogue dataset generated based on this process, TransferTOD, which authentically simulates human-computer dialogues in 30 popular life service scenarios. Leveraging this dataset, we trained a model called TransferTOD-7B using full-parameter fine-tuning, showcasing notable abilities in slot filling and questioning. Our work has demonstrated its strong generalization capabilities in various downstream scenarios, significantly enhancing both data utilization efficiency and system performance. The data is released in https://github.com/KongLongGeFDU/TransferTOD.",
            "link": "https://www.semanticscholar.org/paper/4757e3d37d4753af0df749e71de0faec7d19d054",
            "authors": "Ming Zhang, Caishuang Huang, Yilong Wu, Shichun Liu, Huiyuan Zheng, Yurui Dong, Yujiong Shen, Shihan Dou, Jun Zhao, Junjie Ye, Qi Zhang, Tao Gui, Xuanjing Huang",
            "EMNLP Paper ID": "1484",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "6da55d9743eaf091750bcbbecaec6cd62ba8824b",
            "title": "MP2D: An Automated Topic Shift Dialogue Generation Framework Leveraging Knowledge Graphs",
            "abstract": "Despite advancements in on-topic dialogue systems, effectively managing topic shifts within dialogues remains a persistent challenge, largely attributed to the limited availability of training datasets. To address this issue, we propose Multi-Passage to Dialogue (MP2D), a data generation framework that automatically creates conversational question-answering datasets with natural topic transitions. By leveraging the relationships between entities in a knowledge graph, MP2D maps the flow of topics within a dialogue, effectively mirroring the dynamics of human conversation. It retrieves relevant passages corresponding to the topics and transforms them into dialogues through the passage-to-dialogue method. Through quantitative and qualitative experiments, we demonstrate MP2D's efficacy in generating dialogue with natural topic shifts. Furthermore, this study introduces a novel benchmark for topic shift dialogues, TS-WikiDialog. Utilizing the dataset, we demonstrate that even Large Language Models (LLMs) struggle to handle topic shifts in dialogue effectively, and we showcase the performance improvements of models trained on datasets generated by MP2D across diverse topic shift dialogue tasks.",
            "link": "https://www.semanticscholar.org/paper/6da55d9743eaf091750bcbbecaec6cd62ba8824b",
            "authors": "Yerin Hwang, Yongi-Mi Kim, Yunah Jang, Jeesoo Bang, Hyunkyung Bae, Kyomin Jung",
            "EMNLP Paper ID": "2119",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "2e83cdc9894417a65698c0fd8d0b751123365f34",
            "title": "Unsupervised Extraction of Dialogue Policies from Conversations",
            "abstract": "Dialogue policies play a crucial role in developing task-oriented dialogue systems, yet their development and maintenance are challenging and typically require substantial effort from experts in dialogue modeling. While in many situations, large amounts of conversational data are available for the task at hand, people lack an effective solution able to extract dialogue policies from this data. In this paper, we address this gap by first illustrating how Large Language Models (LLMs) can be instrumental in extracting dialogue policies from datasets, through the conversion of conversations into a unified intermediate representation consisting of canonical forms. We then propose a novel method for generating dialogue policies utilizing a controllable and interpretable graph-based methodology. By combining canonical forms across conversations into a flow network, we find that running graph traversal algorithms helps in extracting dialogue flows. These flows are a better representation of the underlying interactions than flows extracted by prompting LLMs. Our technique focuses on giving conversation designers greater control, offering a productivity tool to improve the process of developing dialogue policies.",
            "link": "https://www.semanticscholar.org/paper/2e83cdc9894417a65698c0fd8d0b751123365f34",
            "authors": "Makesh Narsimhan Sreedhar, Traian Rebedea, Christopher Parisien",
            "EMNLP Paper ID": "2396",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "77c12b7565774bc18e420f91336d02ba5bd6309f",
            "title": "Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents",
            "abstract": "Despite broad interest in modeling spoken dialogue agents, most approaches are inherently\"half-duplex\"-- restricted to turn-based interaction with responses requiring explicit prompting by the user or implicit tracking of interruption or silence events. Human dialogue, by contrast, is\"full-duplex\"allowing for rich synchronicity in the form of quick and dynamic turn-taking, overlapping speech, and backchanneling. Technically, the challenge of achieving full-duplex dialogue with LLMs lies in modeling synchrony as pre-trained LLMs do not have a sense of\"time\". To bridge this gap, we propose Synchronous LLMs for full-duplex spoken dialogue modeling. We design a novel mechanism to integrate time information into Llama3-8b so that they run synchronously with the real-world clock. We also introduce a training recipe that uses 212k hours of synthetic spoken dialogue data generated from text dialogue data to create a model that generates meaningful and natural spoken dialogue, with just 2k hours of real-world spoken dialogue data. Synchronous LLMs outperform state-of-the-art in dialogue meaningfulness while maintaining naturalness. Finally, we demonstrate the model's ability to participate in full-duplex dialogue by simulating interaction between two agents trained on different datasets, while considering Internet-scale latencies of up to 240 ms. Webpage: https://syncllm.cs.washington.edu/.",
            "link": "https://www.semanticscholar.org/paper/77c12b7565774bc18e420f91336d02ba5bd6309f",
            "authors": "Bandhav Veluri, Benjamin Peloquin, Bokai Yu, Hongyu Gong, Shyamnath Gollakota",
            "EMNLP Paper ID": "2940",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "2996161f77ed0c77462ed54955e261d9fec503e6",
            "title": "CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues",
            "abstract": "Recent advancements in instruction-tuning datasets have predominantly focused on specific tasks like mathematical or logical reasoning. There has been a notable gap in data designed for aligning language models to maintain topic relevance in conversations - a critical aspect for deploying chatbots to production. We introduce the CantTalkAboutThis dataset to help language models remain focused on the subject at hand during task-oriented interactions. It consists of synthetic dialogues on a wide range of conversation topics from different domains. These dialogues are interspersed with distractor turns that intentionally divert the chatbot from the predefined topic. Fine-tuning language models on this dataset helps make them resilient to deviating from the role assigned and improves their ability to maintain topical coherence compared to general-purpose instruction-tuned LLMs like GPT-4-turbo and Mixtral-Instruct. Additionally, preliminary observations suggest that training models on this dataset also enhance their performance on fine-grained instruction following tasks, including safety alignment.",
            "link": "https://www.semanticscholar.org/paper/2996161f77ed0c77462ed54955e261d9fec503e6",
            "authors": "Makesh Narsimhan Sreedhar, Traian Rebedea, Shaona Ghosh, Christopher Parisien",
            "matchScore": 245.2293,
            "original title": "CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues",
            "original authors": "Makesh Narsimhan Sreedhar, Traian Rebedea, Shaona Ghosh, Jiaqi Zeng, Christopher Parisien",
            "EMNLP Paper ID": "2398",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "6a091c300b0991d7f853c8c2c179577a07b5354e",
            "title": "Large Language Models Know What To Say But Not When To Speak",
            "abstract": "Turn-taking is a fundamental mechanism in human communication that ensures smooth and coherent verbal interactions. Recent advances in Large Language Models (LLMs) have motivated their use in improving the turn-taking capabilities of Spoken Dialogue Systems (SDS), such as their ability to respond at appropriate times. However, existing models often struggle to predict opportunities for speaking -- called Transition Relevance Places (TRPs) -- in natural, unscripted conversations, focusing only on turn-final TRPs and not within-turn TRPs. To address these limitations, we introduce a novel dataset of participant-labeled within-turn TRPs and use it to evaluate the performance of state-of-the-art LLMs in predicting opportunities for speaking. Our experiments reveal the current limitations of LLMs in modeling unscripted spoken interactions, highlighting areas for improvement and paving the way for more naturalistic dialogue systems.",
            "link": "https://www.semanticscholar.org/paper/6a091c300b0991d7f853c8c2c179577a07b5354e",
            "authors": "Muhammad Umair, Vasanth Sarathy, J. Ruiter",
            "matchScore": 249.81277,
            "original title": "Large Language Models Know What To Say But Not When To Speak",
            "original authors": "Muhammad Umair, Vasanth Sarathy, Jan Ruiter",
            "EMNLP Paper ID": "2990",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "8d32fc6a3db371977a2eebf215d18c5bede883fb",
            "title": "Multi-trait User Simulation with Adaptive Decoding for Conversational Task Assistants",
            "abstract": "Conversational systems must be robust to user interactions that naturally exhibit diverse conversational traits. Capturing and simulating these diverse traits coherently and efficiently presents a complex challenge. This paper introduces Multi-Trait Adaptive Decoding (mTAD), a method that generates diverse user profiles at decoding-time by sampling from various trait-specific Language Models (LMs). mTAD provides an adaptive and scalable approach to user simulation, enabling the creation of multiple user profiles without the need for additional fine-tuning. By analyzing real-world dialogues from the Conversational Task Assistant (CTA) domain, we identify key conversational traits and developed a framework to generate profile-aware dialogues that enhance conversational diversity. Experimental results validate the effectiveness of our approach in modeling single-traits using specialized LMs, which can capture less common patterns, even in out-of-domain tasks. Furthermore, the results demonstrate that mTAD is a robust and flexible framework for combining diverse user simulators.",
            "link": "https://www.semanticscholar.org/paper/8d32fc6a3db371977a2eebf215d18c5bede883fb",
            "authors": "Rafael Ferreira, David Semedo, Jo\u00e3o Magalh\u00e3es",
            "matchScore": 301.42206,
            "original title": "Multi-trait User Simulation with Adaptive Decoding for Conversational Task Assistants",
            "original authors": "Rafael Ferreira, David Semedo, Joao Magalhaes",
            "EMNLP Paper ID": "3100",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "455260c66c8a86481aa5776c4fa467e349faa2ff",
            "title": "Efficient Data Generation for Source-grounded Information-seeking Dialogs: A Use Case for Meeting Transcripts",
            "abstract": "Automating data generation with Large Language Models (LLMs) has become increasingly popular. In this work, we investigate the feasibility and effectiveness of LLM-based data generation in the challenging setting of source-grounded information-seeking dialogs, with response attribution, over long documents. Our source texts consist of long and noisy meeting transcripts, adding to the task complexity. Since automating attribution remains difficult, we propose a semi-automatic approach: dialog queries and responses are generated with LLMs, followed by human verification and identification of attribution spans. Using this approach, we created MISeD -- Meeting Information Seeking Dialogs dataset -- a dataset of information-seeking dialogs focused on meeting transcripts. Models finetuned with MISeD demonstrate superior performance compared to off-the-shelf models, even those of larger size. Finetuning on MISeD gives comparable response generation quality to finetuning on fully manual data, while improving attribution quality and reducing time and effort.",
            "link": "https://www.semanticscholar.org/paper/455260c66c8a86481aa5776c4fa467e349faa2ff",
            "authors": "Lotem Golany, Filippo Galgani, Maya Mamo, Nimrod Parasol, Omer Vandsburger, Nadav Bar, Ido Dagan",
            "matchScore": 300.3581,
            "original title": "Efficient Data Generation for Source-grounded Information-seeking Dialogs: A Use Case for Meeting Transcripts",
            "original authors": "Lotem Golany, Filippo Galgani, Maya Mamo, Nimrod Parasol, Omer Vandsburger, Nadav Bar, Ido Dagan",
            "EMNLP Paper ID": "385",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        }
    ],
    "Safety and Detoxification of Large Language Models": [
        {
            "paperId": "f44b23a1ad5396840845b64b4e23e660110b6083",
            "title": "CMD: a framework for Context-aware Model self-Detoxification",
            "abstract": "Text detoxification aims to minimize the risk of language models producing toxic content. Existing detoxification methods of directly constraining the model output or further training the model on the non-toxic corpus fail to achieve a decent balance between detoxification effectiveness and generation quality. This issue stems from the neglect of constrain imposed by the context since language models are designed to generate output that closely matches the context while detoxification methods endeavor to ensure the safety of the output even if it semantically deviates from the context. In view of this, we introduce a Context-aware Model self-Detoxification~(CMD) framework that pays attention to both the context and the detoxification process, i.e., first detoxifying the context and then making the language model generate along the safe context. Specifically, CMD framework involves two phases: utilizing language models to synthesize data and applying these data for training. We also introduce a toxic contrastive loss that encourages the model generation away from the negative toxic samples. Experiments on various LLMs have verified the effectiveness of our MSD framework, which can yield the best performance compared to baselines.",
            "link": "https://www.semanticscholar.org/paper/f44b23a1ad5396840845b64b4e23e660110b6083",
            "authors": "Zecheng Tang, Keyan Zhou, Pinzheng Wang, Yuyang Ding, Juntao Li, Minzhang",
            "EMNLP Paper ID": "217",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "84c393d1cd5c0d87f673fadcdfb0a2e99dba30ce",
            "title": "ToxiCloakCN: Evaluating Robustness of Offensive Language Detection in Chinese with Cloaking Perturbations",
            "abstract": "Detecting hate speech and offensive language is essential for maintaining a safe and respectful digital environment. This study examines the limitations of state-of-the-art large language models (LLMs) in identifying offensive content within systematically perturbed data, with a focus on Chinese, a language particularly susceptible to such perturbations. We introduce \\textsf{ToxiCloakCN}, an enhanced dataset derived from ToxiCN, augmented with homophonic substitutions and emoji transformations, to test the robustness of LLMs against these cloaking perturbations. Our findings reveal that existing models significantly underperform in detecting offensive content when these perturbations are applied. We provide an in-depth analysis of how different types of offensive content are affected by these perturbations and explore the alignment between human and model explanations of offensiveness. Our work highlights the urgent need for more advanced techniques in offensive language detection to combat the evolving tactics used to evade detection mechanisms.",
            "link": "https://www.semanticscholar.org/paper/84c393d1cd5c0d87f673fadcdfb0a2e99dba30ce",
            "authors": "Yunze Xiao, Yujia Hu, K. T. W. Choo, Roy Ka-wei Lee",
            "EMNLP Paper ID": "671",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "2e55732bf8fa0a11cf24c9b58b25347c31557964",
            "title": "Annotation alignment: Comparing LLM and human annotations of conversational safety",
            "abstract": "Do LLMs align with human perceptions of safety? We study this question via annotation alignment, the extent to which LLMs and humans agree when annotating the safety of user-chatbot conversations. We leverage the recent DICES dataset (Aroyo et al., 2023), in which 350 conversations are each rated for safety by 112 annotators spanning 10 race-gender groups. GPT-4 achieves a Pearson correlation of $r = 0.59$ with the average annotator rating, \\textit{higher} than the median annotator's correlation with the average ($r=0.51$). We show that larger datasets are needed to resolve whether LLMs exhibit disparities in how well they correlate with different demographic groups. Also, there is substantial idiosyncratic variation in correlation within groups, suggesting that race&gender do not fully capture differences in alignment. Finally, we find that GPT-4 cannot predict when one demographic group finds a conversation more unsafe than another.",
            "link": "https://www.semanticscholar.org/paper/2e55732bf8fa0a11cf24c9b58b25347c31557964",
            "authors": "Rajiv Movva, Pang Wei Koh, Emma Pierson",
            "EMNLP Paper ID": "1031",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "b0f8b91c2ca91803324069bc237b15314f70e6ca",
            "title": "CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference",
            "abstract": "As large language models (LLMs) constantly evolve, ensuring their safety remains a critical research problem. Previous red-teaming approaches for LLM safety have primarily focused on single prompt attacks or goal hijacking. To the best of our knowledge, we are the first to study LLM safety in multi-turn dialogue coreference. We created a dataset of 1,400 questions across 14 categories, each featuring multi-turn coreference safety attacks. We then conducted detailed evaluations on five widely used open-source LLMs. The results indicated that under multi-turn coreference safety attacks, the highest attack success rate was 56% with the LLaMA2-Chat-7b model, while the lowest was 13.9% with the Mistral-7B-Instruct model. These findings highlight the safety vulnerabilities in LLMs during dialogue coreference interactions.",
            "link": "https://www.semanticscholar.org/paper/b0f8b91c2ca91803324069bc237b15314f70e6ca",
            "authors": "Erxin Yu, Jing Li, Ming Liao, Siqi Wang, Zuchen Gao, Fei Mi, Lanqing Hong",
            "EMNLP Paper ID": "2086",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "dadb2272f0a3def68f6b3eef5fd26b74d864a3cb",
            "title": "DetoxLLM: A Framework for Detoxification with Explanations",
            "abstract": "Prior works on detoxification are scattered in the sense that they do not cover all aspects of detoxification needed in a real-world scenario. Notably, prior works restrict the task of developing detoxification models to only a seen subset of platforms, leaving the question of how the models would perform on unseen platforms unexplored. Additionally, these works do not address non-detoxifiability, a phenomenon whereby the toxic text cannot be detoxified without altering the meaning. We propose DetoxLLM, the first comprehensive end-to-end detoxification framework, which attempts to alleviate the aforementioned limitations. We first introduce a cross-platform pseudo-parallel corpus applying multi-step data processing and generation strategies leveraging ChatGPT. We then train a suite of detoxification models with our cross-platform corpus. We show that our detoxification models outperform the SoTA model trained with human-annotated parallel corpus. We further introduce explanation to promote transparency and trustworthiness. DetoxLLM additionally offers a unique paraphrase detector especially dedicated for the detoxification task to tackle the non-detoxifiable cases. Through experimental analysis, we demonstrate the effectiveness of our cross-platform corpus and the robustness of DetoxLLM against adversarial toxicity.",
            "link": "https://www.semanticscholar.org/paper/dadb2272f0a3def68f6b3eef5fd26b74d864a3cb",
            "authors": "Md. Tawkat Islam Khondaker, Muhammad Abdul-Mageed, L. Lakshmanan",
            "EMNLP Paper ID": "2417",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "73959867187b182046ff1f36e3d90ce496db5b3c",
            "title": "Accurate and Data-Efficient Toxicity Prediction when Annotators Disagree",
            "abstract": "When annotators disagree, predicting the labels given by individual annotators can capture nuances overlooked by traditional label aggregation. We introduce three approaches to predicting individual annotator ratings on the toxicity of text by incorporating individual annotator-specific information: a neural collaborative filtering (NCF) approach, an in-context learning (ICL) approach, and an intermediate embedding-based architecture. We also study the utility of demographic information for rating prediction. NCF showed limited utility; however, integrating annotator history, demographics, and survey information permits both the embedding-based architecture and ICL to substantially improve prediction accuracy, with the embedding-based architecture outperforming the other methods. We also find that, if demographics are predicted from survey information, using these imputed demographics as features performs comparably to using true demographic data. This suggests that demographics may not provide substantial information for modeling ratings beyond what is captured in survey responses. Our findings raise considerations about the relative utility of different types of annotator information and provide new approaches for modeling annotators in subjective NLP tasks.",
            "link": "https://www.semanticscholar.org/paper/73959867187b182046ff1f36e3d90ce496db5b3c",
            "authors": "Harbani Jaggi, Kashyap Murali, Eve Fleisig, Erdem Biyik",
            "EMNLP Paper ID": "3046",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "1376973d90d2d46c7676889a714026c270b027da",
            "title": "Can LLMs Recognize Toxicity? A Structured Investigation Framework and Toxicity Metric",
            "abstract": "In the pursuit of developing Large Language Models (LLMs) that adhere to societal standards, it is imperative to detect the toxicity in the generated text. The majority of existing toxicity metrics rely on encoder models trained on specific toxicity datasets, which are susceptible to out-of-distribution (OOD) problems and depend on the dataset's definition of toxicity. In this paper, we introduce a robust metric grounded on LLMs to flexibly measure toxicity according to the given definition. We first analyze the toxicity factors, followed by an examination of the intrinsic toxic attributes of LLMs to ascertain their suitability as evaluators. Finally, we evaluate the performance of our metric with detailed analysis. Our empirical results demonstrate outstanding performance in measuring toxicity within verified factors, improving on conventional metrics by 12 points in the F1 score. Our findings also indicate that upstream toxicity significantly influences downstream metrics, suggesting that LLMs are unsuitable for toxicity evaluations within unverified factors.",
            "link": "https://www.semanticscholar.org/paper/1376973d90d2d46c7676889a714026c270b027da",
            "authors": "Hyukhun Koh, Dohyung Kim, Minwoo Lee, Kyomin Jung",
            "matchScore": 292.8681,
            "original title": "Can LLMs Recognize Toxicity? A Structured Investigation Framework and Toxicity Metric",
            "original authors": "Hyukhun Koh, Dohyung Kim, Minwoo Lee, Kyomin Jung",
            "EMNLP Paper ID": "1259",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "a4fe9129afaf2ce1876057d0f33314a82a6731a4",
            "title": "ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors",
            "abstract": "The safety of Large Language Models (LLMs) has gained increasing attention in recent years, but there still lacks a comprehensive approach for detecting safety issues within LLMs' responses in an aligned, customizable and explainable manner. In this paper, we propose ShieldLM, an LLM-based safety detector, which aligns with general human safety standards, supports customizable detection rules, and provides explanations for its decisions. To train ShieldLM, we compile a large bilingual dataset comprising 14,387 query-response pairs, annotating the safety of responses based on various safety standards. Through extensive experiments, we demonstrate that ShieldLM surpasses strong baselines across four test sets, showcasing remarkable customizability and explainability. Besides performing well on standard detection datasets, ShieldLM has also been shown to be effective in real-world situations as a safety evaluator for advanced LLMs. We release ShieldLM at \\url{https://github.com/thu-coai/ShieldLM} to support accurate and explainable safety detection under various safety standards, contributing to the ongoing efforts to enhance the safety of LLMs.",
            "link": "https://www.semanticscholar.org/paper/a4fe9129afaf2ce1876057d0f33314a82a6731a4",
            "authors": "Zhexin Zhang, Yida Lu, Jingyuan Ma, Di Zhang, Rui Li, Pei Ke, Hao Sun, Lei Sha, Zhifang Sui, Hongning Wang, Minlie Huang",
            "matchScore": 302.64203,
            "original title": "ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors",
            "original authors": "Zhexin Zhang, Yida Lu, Jingyuan Ma, Di Zhang, Rui Li, Pei Ke, Hao Sun, Lei Sha, Zhifang Sui, Hongning Wang, Minlie Huang",
            "EMNLP Paper ID": "2129",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "a7f034be22a3335192c2d86d5d26aa7cd761e39c",
            "title": "SAFETY-J: Evaluating Safety with Critique",
            "abstract": "The deployment of Large Language Models (LLMs) in content generation raises significant safety concerns, particularly regarding the transparency and interpretability of content evaluations. Current methods, primarily focused on binary safety classifications, lack mechanisms for detailed critique, limiting their utility for model improvement and user trust. To address these limitations, we introduce SAFETY-J, a bilingual generative safety evaluator for English and Chinese with critique-based judgment. SAFETY-J utilizes a robust training dataset that includes diverse dialogues and augmented query-response pairs to assess safety across various scenarios comprehensively. We establish an automated meta-evaluation benchmark that objectively assesses the quality of critiques with minimal human intervention, facilitating scalable and continuous improvement. Additionally, SAFETY-J employs an iterative preference learning technique to dynamically refine safety assessments based on meta-evaluations and critiques. Our evaluations demonstrate that SAFETY-J provides more nuanced and accurate safety evaluations, thereby enhancing both critique quality and predictive reliability in complex content scenarios. To facilitate further research and application, we open-source SAFETY-J's training protocols, datasets, and code at https://github.com/GAIR-NLP/Safety-J.",
            "link": "https://www.semanticscholar.org/paper/a7f034be22a3335192c2d86d5d26aa7cd761e39c",
            "authors": "Yixiu Liu, Yuxiang Zheng, Shijie Xia, Yuan Guo, Jiajun Li, Yi Tu, Chaoling Song, Pengfei Liu",
            "matchScore": 188.6749,
            "original title": "SAFETY-J: Evaluating Safety with Critique",
            "original authors": "Yixiu Liu, Yuxiang Zheng, Shijie Xia, Jiajun Li, Yi Tu, Chaoling Song, Pengfei Liu",
            "EMNLP Paper ID": "244",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "3a742d6d261cb9fabd6c0dd8bd016f4d6b12fb51",
            "title": "Attribute Controlled Fine-tuning for Large Language Models: A Case Study on Detoxification",
            "abstract": "We propose a constraint learning schema for fine-tuning Large Language Models (LLMs) with attribute control. Given a training corpus and control criteria formulated as a sequence-level constraint on model outputs, our method fine-tunes the LLM on the training corpus while enhancing constraint satisfaction with minimal impact on its utility and generation quality. Specifically, our approach regularizes the LLM training by penalizing the KL divergence between the desired output distribution, which satisfies the constraints, and the LLM's posterior. This regularization term can be approximated by an auxiliary model trained to decompose the sequence-level constraints into token-level guidance, allowing the term to be measured by a closed-form formulation. To further improve efficiency, we design a parallel scheme for concurrently updating both the LLM and the auxiliary model. We evaluate the empirical performance of our approach by controlling the toxicity when training an LLM. We show that our approach leads to an LLM that produces fewer inappropriate responses while achieving competitive performance on benchmarks and a toxicity detection task.",
            "link": "https://www.semanticscholar.org/paper/3a742d6d261cb9fabd6c0dd8bd016f4d6b12fb51",
            "authors": "Tao Meng, Ninareh Mehrabi, Palash Goyal, Anil Ramakrishna, A. Galstyan, Richard Zemel, Kai-Wei Chang, Rahul Gupta, Charith Peris",
            "matchScore": 222.7218,
            "original title": "Attribute Controlled Fine-tuning for Large Language Models: A Case Study on Detoxification",
            "original authors": "Tao Meng, Ninareh Mehrabi, Palash Goyal, Anil Ramakrishna, Aram Galstyan, Richard Zemel, Kai-Wei Chang, Rahul Gupta, Charith Peris",
            "EMNLP Paper ID": "2597",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "6789a2f409af02915e48223e4be3e451ae7aa008",
            "title": "Preference Tuning For Toxicity Mitigation Generalizes Across Languages",
            "abstract": "Detoxifying multilingual Large Language Models (LLMs) has become crucial due to their increasing global use. In this work, we explore zero-shot cross-lingual generalization of preference tuning in detoxifying LLMs. Unlike previous studies that show limited cross-lingual generalization for other safety tasks, we demonstrate that Direct Preference Optimization (DPO) training with only English data can significantly reduce toxicity in multilingual open-ended generations. For example, the probability of mGPT-1.3B generating toxic continuations drops from 46.8% to 3.9% across 17 different languages after training. Our results also extend to other multilingual LLMs, such as BLOOM, Llama3, and Aya-23. Using mechanistic interpretability tools like causal intervention and activation analysis, we identified the dual multilinguality property of MLP layers in LLMs, which explains the cross-lingual generalization of DPO. Finally, we show that bilingual sentence retrieval can predict the cross-lingual transferability of DPO preference tuning.",
            "link": "https://www.semanticscholar.org/paper/6789a2f409af02915e48223e4be3e451ae7aa008",
            "authors": "Xiaochen Li, Zheng-Xin Yong, Stephen H. Bach",
            "matchScore": 243.15068,
            "original title": "Preference Tuning For Toxicity Mitigation Generalizes Across Languages",
            "original authors": "Xiaochen Li, Zheng Xin Yong, Stephen Bach",
            "EMNLP Paper ID": "2610",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "0e0ea3593dda3039cb93d2ec795a87420006ec08",
            "title": "R-Judge: Benchmarking Safety Risk Awareness for LLM Agents",
            "abstract": "Large language models (LLMs) have exhibited great potential in autonomously completing tasks across real-world applications. Despite this, these LLM agents introduce unexpected safety risks when operating in interactive environments. Instead of centering on the harmlessness of LLM-generated content in most prior studies, this work addresses the imperative need for benchmarking the behavioral safety of LLM agents within diverse environments. We introduce R-Judge, a benchmark crafted to evaluate the proficiency of LLMs in judging and identifying safety risks given agent interaction records. R-Judge comprises 569 records of multi-turn agent interaction, encompassing 27 key risk scenarios among 5 application categories and 10 risk types. It is of high-quality curation with annotated safety labels and risk descriptions. Evaluation of 11 LLMs on R-Judge shows considerable room for enhancing the risk awareness of LLMs: The best-performing model, GPT-4o, achieves 74.42% while no other models significantly exceed the random. Moreover, we reveal that risk awareness in open agent scenarios is a multi-dimensional capability involving knowledge and reasoning, thus challenging for LLMs. With further experiments, we find that fine-tuning on safety judgment significantly improve model performance while straightforward prompting mechanisms fail. R-Judge is publicly available at https://github.com/Lordog/R-Judge.",
            "link": "https://www.semanticscholar.org/paper/0e0ea3593dda3039cb93d2ec795a87420006ec08",
            "authors": "Tongxin Yuan, Zhiwei He, Lingzhong Dong, Yiming Wang, Ruijie Zhao, Tian Xia, Lizhen Xu, Binglin Zhou, Fangqi Li, Zhuosheng Zhang, Rui Wang, Gongshen Liu",
            "matchScore": 258.6052,
            "original title": "R-Judge: Benchmarking Safety Risk Awareness for LLM Agents",
            "original authors": "Tongxin Yuan, Zhiwei He, Lingzhong Dong, Yiming Wang, Ruijie Zhao, Tian Xia, Lizhen Xu, Binglin Zhou, Fangqi Li, Zhuosheng Zhang, Rui Wang, Gongshen Liu",
            "EMNLP Paper ID": "297",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Findings"
        },
        {
            "paperId": "636643e298da9b06f26e3ee0e937d9877a4cb01e",
            "title": "ToxiCraft: A Novel Framework for Synthetic Generation of Harmful Information",
            "abstract": "In different NLP tasks, detecting harmful content is crucial for online environments, especially with the growing influence of social media. However, previous research has two main issues: 1) a lack of data in low-resource settings, and 2) inconsistent definitions and criteria for judging harmful content, requiring classification models to be robust to spurious features and diverse. We propose Toxicraft, a novel framework for synthesizing datasets of harmful information to address these weaknesses. With only a small amount of seed data, our framework can generate a wide variety of synthetic, yet remarkably realistic, examples of toxic information. Experimentation across various datasets showcases a notable enhancement in detection model robustness and adaptability, surpassing or close to the gold labels. We release the generated data at Github upon acceptance.",
            "link": "https://www.semanticscholar.org/paper/636643e298da9b06f26e3ee0e937d9877a4cb01e",
            "authors": "Zheng Hui, Zhaoxiao Guo, Hang Zhao, Juanyong Duan, Congrui Huang",
            "matchScore": 238.95227,
            "original title": "ToxiCraft: A Novel Framework for Synthetic Generation of Harmful Information",
            "original authors": "Zheng Hui, Zhaoxiao Guo, Hang Zhao, Juanyong Duan, Congrui Huang",
            "EMNLP Paper ID": "3199",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        }
    ],
    "Advancements and Challenges in Causal and Reasoning Capabilities of Large Language Models": [
        {
            "paperId": "ae4d0695de237fa5f682e1de1811b9b17172d4ed",
            "title": "Can Large Language Models Learn Independent Causal Mechanisms?",
            "abstract": "Despite impressive performance on language modelling and complex reasoning tasks, Large Language Models (LLMs) fall short on the same tasks in uncommon settings or with distribution shifts, exhibiting a lack of generalisation ability. By contrast, systems such as causal models, that learn abstract variables and causal relationships, can demonstrate increased robustness against changes in the distribution. One reason for this success is the existence and use of Independent Causal Mechanisms (ICMs) representing high-level concepts that only sparsely interact. In this work, we apply two concepts from causality to learn ICMs within LLMs. We develop a new LLM architecture composed of multiple sparsely interacting language modelling modules. We show that such causal constraints can improve out-of-distribution performance on abstract and causal reasoning tasks. We also investigate the level of independence and domain specialisation and show that LLMs rely on pre-trained partially domain-invariant mechanisms resilient to fine-tuning.",
            "link": "https://www.semanticscholar.org/paper/ae4d0695de237fa5f682e1de1811b9b17172d4ed",
            "authors": "Ga\u00ebl Gendron, Bao Trung Nguyen, A. Peng, Michael Witbrock, Gillian Dobbie",
            "EMNLP Paper ID": "745",
            "Oral/Poster": "Oral",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "72e06972d652b53809a4d0934ee2e999393da0ce",
            "title": "Unveiling Factual Recall Behaviors of Large Language Models through Knowledge Neurons",
            "abstract": "In this paper, we investigate whether Large Language Models (LLMs) actively recall or retrieve their internal repositories of factual knowledge when faced with reasoning tasks. Through an analysis of LLMs' internal factual recall at each reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness the critical factual associations under certain circumstances. Instead, they tend to opt for alternative, shortcut-like pathways to answer reasoning questions. By manually manipulating the recall process of parametric knowledge in LLMs, we demonstrate that enhancing this recall process directly improves reasoning performance whereas suppressing it leads to notable degradation. Furthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a powerful technique for addressing complex reasoning tasks. Our findings indicate that CoT can intensify the recall of factual knowledge by encouraging LLMs to engage in orderly and reliable reasoning. Furthermore, we explored how contextual conflicts affect the retrieval of facts during the reasoning process to gain a comprehensive understanding of the factual recall behaviors of LLMs. Code and data will be available soon.",
            "link": "https://www.semanticscholar.org/paper/72e06972d652b53809a4d0934ee2e999393da0ce",
            "authors": "Yifei Wang, Yuheng Chen, Wanting Wen, Yu Sheng, Linjing Li, D. Zeng",
            "EMNLP Paper ID": "833",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "a8a2a15b5d51b51c62eeed6045ab1e67130e0867",
            "title": "LLMs Are Prone to Fallacies in Causal Inference",
            "abstract": "Recent work shows that causal facts can be effectively extracted from LLMs through prompting, facilitating the creation of causal graphs for causal inference tasks. However, it is unclear if this success is limited to explicitly-mentioned causal facts in the pretraining data which the model can memorize. Thus, this work investigates: Can LLMs infer causal relations from other relational data in text? To disentangle the role of memorized causal facts vs inferred causal relations, we finetune LLMs on synthetic data containing temporal, spatial and counterfactual relations, and measure whether the LLM can then infer causal relations. We find that: (a) LLMs are susceptible to inferring causal relations from the order of two entity mentions in text (e.g. X mentioned before Y implies X causes Y); (b) if the order is randomized, LLMs still suffer from the post hoc fallacy, i.e. X occurs before Y (temporal relation) implies X causes Y. We also find that while LLMs can correctly deduce the absence of causal relations from temporal and spatial relations, they have difficulty inferring causal relations from counterfactuals, questioning their understanding of causality.",
            "link": "https://www.semanticscholar.org/paper/a8a2a15b5d51b51c62eeed6045ab1e67130e0867",
            "authors": "Nitish Joshi, Abulhair Saparov, Yixin Wang, He He",
            "EMNLP Paper ID": "1193",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "e34783e6f83b2a1f0d40389ddccb300e0a788680",
            "title": "Experimental Contexts Can Facilitate Robust Semantic Property Inference in Language Models, but Inconsistently",
            "abstract": "Recent zero-shot evaluations have highlighted important limitations in the abilities of language models (LMs) to perform meaning extraction. However, it is now well known that LMs can demonstrate radical improvements in the presence of experimental contexts such as in-context examples and instructions. How well does this translate to previously studied meaning-sensitive tasks? We present a case-study on the extent to which experimental contexts can improve LMs' robustness in performing property inheritance -- predicting semantic properties of novel concepts, a task that they have been previously shown to fail on. Upon carefully controlling the nature of the in-context examples and the instructions, our work reveals that they can indeed lead to non-trivial property inheritance behavior in LMs. However, this ability is inconsistent: with a minimal reformulation of the task, some LMs were found to pick up on shallow, non-semantic heuristics from their inputs, suggesting that the computational principles of semantic property inference are yet to be mastered by LMs.",
            "link": "https://www.semanticscholar.org/paper/e34783e6f83b2a1f0d40389ddccb300e0a788680",
            "authors": "Kanishka Misra, Allyson Ettinger, Kyle Mahowald",
            "EMNLP Paper ID": "1442",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "b3c5da33f73b8d4b77c107134e05957b20d544ba",
            "title": "The Mystery of the Pathological Path-star Task for Language Models",
            "abstract": "The recently introduced path-star task is a minimal task designed to exemplify limitations to the abilities of language models (Bachmann and Nagarajan, 2024). It involves a path-star graph where multiple arms radiate from a single starting node and each node is unique. Given the start node and a specified target node that ends an arm, the task is to generate the arm containing that target node. This is straightforward for a human but surprisingly difficult for language models, which did not outperform the random baseline. The authors hypothesized this is due to a deficiency in teacher-forcing and the next-token prediction paradigm. We demonstrate the task is learnable using teacher-forcing in alternative settings and that the issue is partially due to representation. We introduce a regularization method using structured samples of the same graph but with differing target nodes, improving results across a variety of model types. We provide RASP proofs showing the task is theoretically solvable. Finally, we find settings where an encoder-only model can consistently solve the task.",
            "link": "https://www.semanticscholar.org/paper/b3c5da33f73b8d4b77c107134e05957b20d544ba",
            "authors": "Arvid Frydenlund",
            "EMNLP Paper ID": "1458",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "55f1cde49846c58b0bedebde15b8f7d939f39432",
            "title": "Are We Falling in a Middle-Intelligence Trap? An Analysis and Mitigation of the Reversal Curse",
            "abstract": "Recent studies have highlighted a phenomenon in large language models (LLMs) known as\"the reversal curse,\"in which the order of knowledge entities in the training data biases the models' comprehension. For example, if a model is trained on sentences where entity A consistently appears before entity B, it can respond to queries about A by providing B as the answer. However, it may encounter confusion when presented with questions concerning B. We contend that the reversal curse is partially a result of specific model training objectives, particularly evident in the prevalent use of the next-token prediction within most causal language models. For the next-token prediction, models solely focus on a token's preceding context, resulting in a restricted comprehension of the input. In contrast, we illustrate that the GLM, trained using the autoregressive blank infilling objective where tokens to be predicted have access to the entire context, exhibits better resilience against the reversal curse. We propose a novel training method, BIdirectional Casual language modeling Optimization (BICO), designed to mitigate the reversal curse when fine-tuning pretrained causal language models on new data. BICO modifies the causal attention mechanism to function bidirectionally and employs a mask denoising optimization. In the task designed to assess the reversal curse, our approach improves Llama's accuracy from the original 0% to around 70%. We hope that more attention can be focused on exploring and addressing these inherent weaknesses of the current LLMs, in order to achieve a higher level of intelligence.",
            "link": "https://www.semanticscholar.org/paper/55f1cde49846c58b0bedebde15b8f7d939f39432",
            "authors": "Ang Lv, Kaiyi Zhang, Shufang Xie, Quan Tu, Yuhan Chen, Ji-Rong Wen, Rui Yan",
            "EMNLP Paper ID": "1573",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "d610e066e5c3cc7eda13313554e25a340ce18f71",
            "title": "Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries",
            "abstract": "Large language models (LLMs) can solve complex multi-step problems, but little is known about how these computations are implemented internally. Motivated by this, we study how LLMs answer multi-hop queries such as\"The spouse of the performer of Imagine is\". These queries require two information extraction steps: a latent one for resolving the first hop (\"the performer of Imagine\") into the bridge entity (John Lennon), and another for resolving the second hop (\"the spouse of John Lennon\") into the target entity (Yoko Ono). Understanding how the latent step is computed internally is key to understanding the overall computation. By carefully analyzing the internal computations of transformer-based LLMs, we discover that the bridge entity is resolved in the early layers of the model. Then, only after this resolution, the two-hop query is solved in the later layers. Because the second hop commences in later layers, there could be cases where these layers no longer encode the necessary knowledge for correctly predicting the answer. Motivated by this, we propose a novel\"back-patching\"analysis method whereby a hidden representation from a later layer is patched back to an earlier layer. We find that in up to 66% of previously incorrect cases there exists a back-patch that results in the correct generation of the answer, showing that the later layers indeed sometimes lack the needed functionality. Overall, our methods and findings open further opportunities for understanding and improving latent reasoning in transformer-based LLMs.",
            "link": "https://www.semanticscholar.org/paper/d610e066e5c3cc7eda13313554e25a340ce18f71",
            "authors": "Eden Biran, Daniela Gottesman, Sohee Yang, Mor Geva, Amir Globerson",
            "EMNLP Paper ID": "1632",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "9f3aea106c281489d97d5891b1005f459b43a776",
            "title": "The Odyssey of Commonsense Causality: From Foundational Benchmarks to Cutting-Edge Reasoning",
            "abstract": "Understanding commonsense causality is a unique mark of intelligence for humans. It helps people understand the principles of the real world better and benefits the decision-making process related to causation. For instance, commonsense causality is crucial in judging whether a defendant's action causes the plaintiff's loss in determining legal liability. Despite its significance, a systematic exploration of this topic is notably lacking. Our comprehensive survey bridges this gap by focusing on taxonomies, benchmarks, acquisition methods, qualitative reasoning, and quantitative measurements in commonsense causality, synthesizing insights from over 200 representative articles. Our work aims to provide a systematic overview, update scholars on recent advancements, provide a pragmatic guide for beginners, and highlight promising future research directions in this vital field.",
            "link": "https://www.semanticscholar.org/paper/9f3aea106c281489d97d5891b1005f459b43a776",
            "authors": "Shaobo Cui, Zhijing Jin, Bernhard Sch\u00f6lkopf, Boi Faltings",
            "EMNLP Paper ID": "1973",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "d160e10dd95a8633bbd693e376a0f13d32549a77",
            "title": "CELLO: Causal Evaluation of Large Vision-Language Models",
            "abstract": "Causal reasoning is fundamental to human intelligence and crucial for effective decision-making in real-world environments. Despite recent advancements in large vision-language models (LVLMs), their ability to comprehend causality remains unclear. Previous work typically focuses on commonsense causality between events and/or actions, which is insufficient for applications like embodied agents and lacks the explicitly defined causal graphs required for formal causal reasoning. To overcome these limitations, we introduce a fine-grained and unified definition of causality involving interactions between humans and/or objects. Building on the definition, we construct a novel dataset, CELLO, consisting of 14,094 causal questions across all four levels of causality: discovery, association, intervention, and counterfactual. This dataset surpasses traditional commonsense causality by including explicit causal graphs that detail the interactions between humans and objects. Extensive experiments on CELLO reveal that current LVLMs still struggle with causal reasoning tasks, but they can benefit significantly from our proposed CELLO-CoT, a causally inspired chain-of-thought prompting strategy. Both quantitative and qualitative analyses from this study provide valuable insights for future research. Our project page is at https://github.com/OpenCausaLab/CELLO.",
            "link": "https://www.semanticscholar.org/paper/d160e10dd95a8633bbd693e376a0f13d32549a77",
            "authors": "Meiqi Chen, Bo Peng, Yan Zhang, Chaochao Lu",
            "EMNLP Paper ID": "3177",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "046835ef32c580cce3f7cfcb6cf756959bb537db",
            "title": "CLEAR: Can Language Models Really Understand Causal Graphs?",
            "abstract": "Causal reasoning is a cornerstone of how humans interpret the world. To model and reason about causality, causal graphs offer a concise yet effective solution. Given the impressive advancements in language models, a crucial question arises: can they really understand causal graphs? To this end, we pioneer an investigation into language models' understanding of causal graphs. Specifically, we develop a framework to define causal graph understanding, by assessing language models' behaviors through four practical criteria derived from diverse disciplines (e.g., philosophy and psychology). We then develop CLEAR, a novel benchmark that defines three complexity levels and encompasses 20 causal graph-based tasks across these levels. Finally, based on our framework and benchmark, we conduct extensive experiments on six leading language models and summarize five empirical findings. Our results indicate that while language models demonstrate a preliminary understanding of causal graphs, significant potential for improvement remains. Our project website is at https://github.com/OpenCausaLab/CLEAR.",
            "link": "https://www.semanticscholar.org/paper/046835ef32c580cce3f7cfcb6cf756959bb537db",
            "authors": "Sirui Chen, Mengying Xu, Kun Wang, Xingyu Zeng, Rui Zhao, Shengjie Zhao, Chaochao Lu",
            "matchScore": 263.87515,
            "original title": "CLEAR: Can Language Models Really Understand Causal Graphs?",
            "original authors": "Sirui Chen, Mengying Xu, Kun Wang, Xingyu Zeng, Rui Zhao, Shengjie Zhao, Chaochao Lu",
            "EMNLP Paper ID": "1284",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "fb2d19fbeb8f7caa89808895066d3c0dbd4dad32",
            "title": "The Mystery of Compositional Generalization in Graph-based Generative Commonsense Reasoning",
            "abstract": "While LLMs have emerged as performant architectures for reasoning tasks, their compositional generalization capabilities have been questioned. In this work, we introduce a Compositional Generalization Challenge for Graph-based Commonsense Reasoning (CGGC) that goes beyond previous evaluations that are based on sequences or tree structures - and instead involves a reasoning graph: It requires models to generate a natural sentence based on given concepts and a corresponding reasoning graph, where the presented graph involves a previously unseen combination of relation types. To master this challenge, models need to learn how to reason over relation tupels within the graph, and how to compose them when conceptualizing a verbalization. We evaluate seven well-known LLMs using in-context learning and find that performant LLMs still struggle in compositional generalization. We investigate potential causes of this gap by analyzing the structures of reasoning graphs, and find that different structures present varying levels of difficulty for compositional generalization. Arranging the order of demonstrations according to the structures' difficulty shows that organizing samples in an easy-to-hard schema enhances the compositional generalization ability of LLMs.",
            "link": "https://www.semanticscholar.org/paper/fb2d19fbeb8f7caa89808895066d3c0dbd4dad32",
            "authors": "Xiyan Fu, Anette Frank",
            "matchScore": 274.04886,
            "original title": "The Mystery of Compositional Generalization in Graph-based Generative Commonsense Reasoning",
            "original authors": "Xiyan Fu, Anette Frank",
            "EMNLP Paper ID": "1773",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "9320a4167b8d75b64936a63f5bd5dff228ec72df",
            "title": "Talking the Talk Does Not Entail Walking the Walk: On the Limits of Large Language Models in Lexical Entailment Recognition",
            "abstract": "Verbs form the backbone of language, providing the structure and meaning to sentences. Yet, their intricate semantic nuances pose a longstanding challenge. Understanding verb relations through the concept of lexical entailment is crucial for comprehending sentence meanings and grasping verb dynamics. This work investigates the capabilities of eight Large Language Models in recognizing lexical entailment relations among verbs through differently devised prompting strategies and zero-/few-shot settings over verb pairs from two lexical databases, namely WordNet and HyperLex. Our findings unveil that the models can tackle the lexical entailment recognition task with moderately good performance, although at varying degree of effectiveness and under different conditions. Also, utilizing few-shot prompting can enhance the models' performance. However, perfectly solving the task arises as an unmet challenge for all examined LLMs, which raises an emergence for further research developments on this topic.",
            "link": "https://www.semanticscholar.org/paper/9320a4167b8d75b64936a63f5bd5dff228ec72df",
            "authors": "C. M. Greco, Lucio La Cava, Andrea Tagarelli",
            "matchScore": 351.31577,
            "original title": "Talking the Talk Does Not Entail Walking the Walk: On the Limits of Large Language Models in Lexical Entailment Recognition",
            "original authors": "Candida Maria Greco, Lucio La Cava, Andrea Tagarelli",
            "EMNLP Paper ID": "2879",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "752f684371c9901791259dc4afd04b9754e803d1",
            "title": "Can LLM Graph Reasoning Generalize beyond Pattern Memorization?",
            "abstract": "Large language models (LLMs) demonstrate great potential for problems with implicit graphical structures, while recent works seek to enhance the graph reasoning capabilities of LLMs through specialized instruction tuning. The resulting 'graph LLMs' are evaluated with in-distribution settings only, thus it remains underexplored whether LLMs are learning generalizable graph reasoning skills or merely memorizing patterns in the synthetic training data. To this end, we propose the NLGift benchmark, an evaluation suite of LLM graph reasoning generalization: whether LLMs could go beyond semantic, numeric, structural, reasoning patterns in the synthetic training data and improve utility on real-world graph-based tasks. Extensive experiments with two LLMs across four graph reasoning tasks demonstrate that while generalization on simple patterns (semantic, numeric) is somewhat satisfactory, LLMs struggle to generalize across reasoning and real-world patterns, casting doubt on the benefit of synthetic graph tuning for real-world tasks with underlying network structures. We explore three strategies to improve LLM graph reasoning generalization, and we find that while post-training alignment is most promising for real-world tasks, empowering LLM graph reasoning to go beyond pattern memorization remains an open research question.",
            "link": "https://www.semanticscholar.org/paper/752f684371c9901791259dc4afd04b9754e803d1",
            "authors": "Yizhuo Zhang, Heng Wang, Shangbin Feng, Zhaoxuan Tan, Xiaochuang Han, Tianxing He, Yulia Tsvetkov",
            "matchScore": 283.89822,
            "original title": "Can LLM Graph Reasoning Generalize beyond Pattern Memorization?",
            "original authors": "Yizhuo Zhang, Heng Wang, Shangbin Feng, Zhaoxuan Tan, Xiaochuang Han, Tianxing He, Yulia Tsvetkov",
            "EMNLP Paper ID": "462",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Findings"
        }
    ],
    "Data Generation and Augmentation for Zero-shot Learning in NLP": [
        {
            "paperId": "b2e32806ea51b93bf5fb734540072edeab314f8a",
            "title": "UniGen: Universal Domain Generalization for Sentiment Classification via Zero-shot Dataset Generation",
            "abstract": "Although pre-trained language models have exhibited great flexibility and versatility with prompt-based few-shot learning, they suffer from the extensive parameter size and limited applicability for inference. Recent studies have suggested that PLMs be used as dataset generators and a tiny task-specific model be trained to achieve efficient inference. However, their applicability to various domains is limited because they tend to generate domain-specific datasets. In this work, we propose a novel approach to universal domain generalization that generates a dataset regardless of the target domain. This allows for generalization of the tiny task model to any domain that shares the label space, thus enhancing the real-world applicability of the dataset generation paradigm. Our experiments indicate that the proposed method accomplishes generalizability across various domains while using a parameter set that is orders of magnitude smaller than PLMs.",
            "link": "https://www.semanticscholar.org/paper/b2e32806ea51b93bf5fb734540072edeab314f8a",
            "authors": "Juhwan Choi, Yeonghwa Kim, Seunguk Yu, Jungmin Yun, Youngbin Kim",
            "EMNLP Paper ID": "2",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "727846d963c5568623def1fd36de37664e6f84ae",
            "title": "An Effective Deployment of Diffusion LM for Data Augmentation in Low-Resource Sentiment Classification",
            "abstract": "Sentiment classification (SC) often suffers from low-resource challenges such as domain-specific contexts, imbalanced label distributions, and few-shot scenarios. The potential of the diffusion language model (LM) for textual data augmentation (DA) remains unexplored, moreover, textual DA methods struggle to balance the diversity and consistency of new samples. Most DA methods either perform logical modifications or rephrase less important tokens in the original sequence with the language model. In the context of SC, strong emotional tokens could act critically on the sentiment of the whole sequence. Therefore, contrary to rephrasing less important context, we propose DiffusionCLS to leverage a diffusion LM to capture in-domain knowledge and generate pseudo samples by reconstructing strong label-related tokens. This approach ensures a balance between consistency and diversity, avoiding the introduction of noise and augmenting crucial features of datasets. DiffusionCLS also comprises a Noise-Resistant Training objective to help the model generalize. Experiments demonstrate the effectiveness of our method in various low-resource scenarios including domain-specific and domain-general problems. Ablation studies confirm the effectiveness of our framework's modules, and visualization studies highlight optimal deployment conditions, reinforcing our conclusions.",
            "link": "https://www.semanticscholar.org/paper/727846d963c5568623def1fd36de37664e6f84ae",
            "authors": "Zhuowei Chen, Lianxi Wang, Yuben Wu, Xinfeng Liao, Yujia Tian, Junyang Zhong",
            "EMNLP Paper ID": "206",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "88e7506a39f99412722e5e3751be3eebd35edbf5",
            "title": "FuseGen: PLM Fusion for Data-generation based Zero-shot Learning",
            "abstract": "Data generation-based zero-shot learning, although effective in training Small Task-specific Models (STMs) via synthetic datasets generated by Pre-trained Language Models (PLMs), is often limited by the low quality of such synthetic datasets. Previous solutions have primarily focused on single PLM settings, where synthetic datasets are typically restricted to specific sub-spaces and often deviate from real-world distributions, leading to severe distribution bias. To mitigate such bias, we propose FuseGen, a novel data generation-based zero-shot learning framework that introduces a new criteria for subset selection from synthetic datasets via utilizing multiple PLMs and trained STMs. The chosen subset provides in-context feedback to each PLM, enhancing dataset quality through iterative data generation. Trained STMs are then used for sample re-weighting as well, further improving data quality. Extensive experiments across diverse tasks demonstrate that FuseGen substantially outperforms existing methods, highly effective in boosting STM performance in a PLM-agnostic way. Code is provided in https://github.com/LindaLydia/FuseGen.",
            "link": "https://www.semanticscholar.org/paper/88e7506a39f99412722e5e3751be3eebd35edbf5",
            "authors": "Tianyuan Zou, Yang Liu, Peng Li, Jianqing Zhang, Jingjing Liu, Ya-Qin Zhang",
            "EMNLP Paper ID": "246",
            "Oral/Poster": "Oral",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "ce5496b880a15d05c2d046b475c20b6e26a68a9f",
            "title": "Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection",
            "abstract": "Grammatical Error Detection (GED) methods rely heavily on human annotated error corpora. However, these annotations are unavailable in many low-resource languages. In this paper, we investigate GED in this context. Leveraging the zero-shot cross-lingual transfer capabilities of multilingual pre-trained language models, we train a model using data from a diverse set of languages to generate synthetic errors in other languages. These synthetic error corpora are then used to train a GED model. Specifically we propose a two-stage fine-tuning pipeline where the GED model is first fine-tuned on multilingual synthetic data from target languages followed by fine-tuning on human-annotated GED corpora from source languages. This approach outperforms current state-of-the-art annotation-free GED methods. We also analyse the errors produced by our method and other strong baselines, finding that our approach produces errors that are more diverse and more similar to human errors.",
            "link": "https://www.semanticscholar.org/paper/ce5496b880a15d05c2d046b475c20b6e26a68a9f",
            "authors": "Gaetan Lopez Latouche, M. Carbonneau, Ben Swanson",
            "EMNLP Paper ID": "338",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "7dcc10262ecbd612019f74a7325ec1e106a5cd15",
            "title": "Zero-Shot Cross-Lingual NER Using Phonemic Representations for Low-Resource Languages",
            "abstract": "Existing zero-shot cross-lingual NER approaches require substantial prior knowledge of the target language, which is impractical for low-resource languages. In this paper, we propose a novel approach to NER using phonemic representation based on the International Phonetic Alphabet (IPA) to bridge the gap between representations of different languages. Our experiments show that our method significantly outperforms baseline models in extremely low-resource languages, with the highest average F1 score (46.38%) and lowest standard deviation (12.67), particularly demonstrating its robustness with non-Latin scripts. Our codes are available at https://github.com/Gabriel819/zeroshot_ner.git",
            "link": "https://www.semanticscholar.org/paper/7dcc10262ecbd612019f74a7325ec1e106a5cd15",
            "authors": "Jimin Sohn, Haeji Jung, Alex Cheng, Jooeon Kang, Yilin Du, David R. Mortensen",
            "EMNLP Paper ID": "1569",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "f0fb9d5794f69fec9bdce286f26a3cf15469476a",
            "title": "Cross-lingual Back-Parsing: Utterance Synthesis from Meaning Representation for Zero-Resource Semantic Parsing",
            "abstract": "Recent efforts have aimed to utilize multilingual pretrained language models (mPLMs) to extend semantic parsing (SP) across multiple languages without requiring extensive annotations. However, achieving zero-shot cross-lingual transfer for SP remains challenging, leading to a performance gap between source and target languages. In this study, we propose Cross-Lingual Back-Parsing (CBP), a novel data augmentation methodology designed to enhance cross-lingual transfer for SP. Leveraging the representation geometry of the mPLMs, CBP synthesizes target language utterances from source meaning representations. Our methodology effectively performs cross-lingual data augmentation in challenging zero-resource settings, by utilizing only labeled data in the source language and monolingual corpora. Extensive experiments on two cross-language SP benchmarks (Mschema2QA and Xspider) demonstrate that CBP brings substantial gains in the target language. Further analysis of the synthesized utterances shows that our method successfully generates target language utterances with high slot value alignment rates while preserving semantic integrity. Our codes and data are publicly available at https://github.com/deokhk/CBP.",
            "link": "https://www.semanticscholar.org/paper/f0fb9d5794f69fec9bdce286f26a3cf15469476a",
            "authors": "Deokhyung Kang, Seonjeong Hwang, Yunsu Kim, Gary Geunbae Lee",
            "EMNLP Paper ID": "1655",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "26dac83909af13932bfae9299c470fddb4e3d69f",
            "title": "Multilingual Topic Classification in X: Dataset and Analysis",
            "abstract": "In the dynamic realm of social media, diverse topics are discussed daily, transcending linguistic boundaries. However, the complexities of understanding and categorising this content across various languages remain an important challenge with traditional techniques like topic modelling often struggling to accommodate this multilingual diversity. In this paper, we introduce X-Topic, a multilingual dataset featuring content in four distinct languages (English, Spanish, Japanese, and Greek), crafted for the purpose of tweet topic classification. Our dataset includes a wide range of topics, tailored for social media content, making it a valuable resource for scientists and professionals working on cross-linguistic analysis, the development of robust multilingual models, and computational scientists studying online dialogue. Finally, we leverage X-Topic to perform a comprehensive cross-linguistic and multilingual analysis, and compare the capabilities of current general- and domain-specific language models.",
            "link": "https://www.semanticscholar.org/paper/26dac83909af13932bfae9299c470fddb4e3d69f",
            "authors": "Dimosthenis Antypas, Asahi Ushio, Francesco Barbieri, Jos\u00e9 Camacho-Collados",
            "EMNLP Paper ID": "2624",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "928612c34a27d8fed3dbe105df325d499773b664",
            "title": "Cross-lingual Contextualized Phrase Retrieval",
            "abstract": "Phrase-level dense retrieval has shown many appealing characteristics in downstream NLP tasks by leveraging the fine-grained information that phrases offer. In our work, we propose a new task formulation of dense retrieval, cross-lingual contextualized phrase retrieval, which aims to augment cross-lingual applications by addressing polysemy using context information. However, the lack of specific training data and models are the primary challenges to achieve our goal. As a result, we extract pairs of cross-lingual phrases using word alignment information automatically induced from parallel sentences. Subsequently, we train our Cross-lingual Contextualized Phrase Retriever (CCPR) using contrastive learning, which encourages the hidden representations of phrases with similar contexts and semantics to align closely. Comprehensive experiments on both the cross-lingual phrase retrieval task and a downstream task, i.e, machine translation, demonstrate the effectiveness of CCPR. On the phrase retrieval task, CCPR surpasses baselines by a significant margin, achieving a top-1 accuracy that is at least 13 points higher. When utilizing CCPR to augment the large-language-model-based translator, it achieves average gains of 0.7 and 1.5 in BERTScore for translations from X=>En and vice versa, respectively, on WMT16 dataset. Our code and data are available at \\url{https://github.com/ghrua/ccpr_release}.",
            "link": "https://www.semanticscholar.org/paper/928612c34a27d8fed3dbe105df325d499773b664",
            "authors": "Huayang Li, Deng Cai, Zhi Qu, Qu Cui, Hidetaka Kamigaito, Lemao Liu, Taro Watanabe",
            "matchScore": 197.35547,
            "original title": "Cross-lingual Contextualized Phrase Retrieval",
            "original authors": "Huayang Li, Deng Cai, Zhi Qu, Qu Cui, Hidetaka Kamigaito, Lemao Liu, Taro Watanabe",
            "EMNLP Paper ID": "1339",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "9ed1697f3f190788236d6f4eb6bf5830c64a222b",
            "title": "Diverse and Effective Synthetic Data Generation for Adaptable Zero-Shot Dialogue State Tracking",
            "abstract": "We demonstrate substantial performance gains in zero-shot dialogue state tracking (DST) by enhancing training data diversity through synthetic data generation. Existing DST datasets are severely limited in the number of application domains and slot types they cover due to the high costs of data collection, restricting their adaptability to new domains. This work addresses this challenge with a novel, fully automatic data generation approach that creates synthetic zero-shot DST datasets. Distinguished from previous methods, our approach can generate dialogues across a massive range of application domains, complete with silver-standard dialogue state annotations and slot descriptions. This technique is used to create the D0T dataset for training zero-shot DST models, encompassing an unprecedented 1,000+ domains. Experiments on the MultiWOZ benchmark show that training models on diverse synthetic data improves Joint Goal Accuracy by 6.7%, achieving results competitive with models 13.5 times larger than ours.",
            "link": "https://www.semanticscholar.org/paper/9ed1697f3f190788236d6f4eb6bf5830c64a222b",
            "authors": "James D. Finch, Jinho D. Choi",
            "matchScore": 287.26495,
            "original title": "Diverse and Effective Synthetic Data Generation for Adaptable Zero-Shot Dialogue State Tracking",
            "original authors": "James D. Finch, Jinho D. Choi",
            "EMNLP Paper ID": "2446",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "cce2dadb9abd09767438716ba96a6a4b1f74a97c",
            "title": "Generate then Refine: Data Augmentation for Zero-shot Intent Detection",
            "abstract": "In this short paper we propose a data augmentation method for intent detection in zero-resource domains. Existing data augmentation methods rely on few labelled examples for each intent category, which can be expensive in settings with many possible intents. We use a two-stage approach: First, we generate utterances for intent labels using an open-source large language model in a zero-shot setting. Second, we develop a smaller sequence-to-sequence model (the Refiner), to improve the generated utterances. The Refiner is fine-tuned on seen domains and then applied to unseen domains. We evaluate our method by training an intent classifier on the generated data, and evaluating it on real (human) data. We find that the Refiner significantly improves the data utility and diversity over the zero-shot LLM baseline for unseen domains and over common baseline approaches. Our results indicate that a two-step approach of a generative LLM in zero-shot setting and a smaller sequence-to-sequence model can provide high-quality data for intent detection.",
            "link": "https://www.semanticscholar.org/paper/cce2dadb9abd09767438716ba96a6a4b1f74a97c",
            "authors": "I-Fan Lin, Faegheh Hasibi, Suzan Verberne",
            "matchScore": 257.91412,
            "original title": "Generate then Refine: Data Augmentation for Zero-shot Intent Detection",
            "original authors": "I-Fan Lin, Faegheh Hasibi, Suzan Verberne",
            "EMNLP Paper ID": "2565",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "4e298242fd18904c09c67729d8f417546b0d02d0",
            "title": "SSP: Self-Supervised Prompting for Cross-Lingual Transfer to Low-Resource Languages using Large Language Models",
            "abstract": "Recently, very large language models (LLMs) have shown exceptional performance on several English NLP tasks with just in-context learning (ICL), but their utility in other languages is still underexplored. We investigate their effectiveness for NLP tasks in low-resource languages (LRLs), especially in the setting of zero-labelled cross-lingual transfer (0-CLT), where no labelled training data for the target language is available -- however training data from one or more related medium-resource languages (MRLs) is utilized, alongside the available unlabeled test data for a target language. We introduce Self-Supervised Prompting (SSP), a novel ICL approach tailored for the 0-CLT setting. SSP is based on the key observation that LLMs output more accurate labels if in-context exemplars are from the target language (even if their labels are slightly noisy). To operationalize this, since target language training data is not available in 0-CLT, SSP operates in two stages. In Stage I, using source MRL training data, target language's test data is noisily labeled. In Stage II, these noisy test data points are used as exemplars in ICL for further improved labelling. Additionally, our implementation of SSP uses a novel Integer Linear Programming (ILP)-based exemplar selection that balances similarity, prediction confidence (when available) and label coverage. Experiments on three tasks and eleven LRLs (from three regions) demonstrate that SSP strongly outperforms existing SOTA fine-tuned and prompting-based baselines in 0-CLT setup.",
            "link": "https://www.semanticscholar.org/paper/4e298242fd18904c09c67729d8f417546b0d02d0",
            "authors": "Vipul Rathore, Aniruddha Deb, Ankish Chandresh, Parag Singla, Mausam",
            "matchScore": 329.77798,
            "original title": "SSP: Self-Supervised Prompting for Cross-Lingual Transfer to Low-Resource Languages using Large Language Models",
            "original authors": "Vipul Kumar Rathore, Aniruddha Deb, Ankish Kumar Chandresh, Parag Singla, Mausam .",
            "EMNLP Paper ID": "2891",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "893e79842994c36971109a458d2150ec13e148e0",
            "title": "Text2Model: Text-based Model Induction for Zero-shot Image Classification",
            "abstract": "We address the challenge of building task-agnostic classifiers using only text descriptions, demonstrating a unified approach to image classification, 3D point cloud classification, and action recognition from scenes. Unlike approaches that learn a fixed representation of the output classes, we generate at inference time a model tailored to a query classification task. To generate task-based zero-shot classifiers, we train a hypernetwork that receives class descriptions and outputs a multi-class model. The hypernetwork is designed to be equivariant with respect to the set of descriptions and the classification layer, thus obeying the symmetries of the problem and improving generalization. Our approach generates non-linear classifiers, handles rich textual descriptions, and may be adapted to produce lightweight models efficient enough for on-device applications. We evaluate this approach in a series of zero-shot classification tasks, for image, point-cloud, and action recognition, using a range of text descriptions: From single words to rich descriptions. Our results demonstrate strong improvements over previous approaches, showing that zero-shot learning can be applied with little training data. Furthermore, we conduct an analysis with foundational vision and language models, demonstrating that they struggle to generalize when describing what attributes the class lacks.",
            "link": "https://www.semanticscholar.org/paper/893e79842994c36971109a458d2150ec13e148e0",
            "authors": "Ohad Amosy, Tomer Volk, Eilam Shapira, Eyal Ben-David, Roi Reichart, Gal Chechik",
            "matchScore": 254.69582,
            "original title": "Text2Model: Text-based Model Induction for Zero-shot Image Classification",
            "original authors": "Ohad Amosy, Tomer Volk, Eilam Shapira, Eyal Ben-David, Roi Reichart, Gal Chechik",
            "EMNLP Paper ID": "29",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        }
    ],
    "Improving Large Language Models for Agent-Based Decision Making and Interaction": [
        {
            "paperId": "e16273b28628bead1952dc56fdcae7ea85d35e8f",
            "title": "MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for Superior Planning and Decision-Making",
            "abstract": "Long-term memory is significant for agents, in which insights play a crucial role. However, the emergence of irrelevant insight and the lack of general insight can greatly undermine the effectiveness of insight. To solve this problem, in this paper, we introduce Multi-Scale Insight Agent (MSI-Agent), an embodied agent designed to improve LLMs' planning and decision-making ability by summarizing and utilizing insight effectively across different scales. MSI achieves this through the experience selector, insight generator, and insight selector. Leveraging a three-part pipeline, MSI can generate task-specific and high-level insight, store it in a database, and then use relevant insight from it to aid in decision-making. Our experiments show that MSI outperforms another insight strategy when planning by GPT3.5. Moreover, We delve into the strategies for selecting seed experience and insight, aiming to provide LLM with more useful and relevant insight for better decision-making. Our observations also indicate that MSI exhibits better robustness when facing domain-shifting scenarios.",
            "link": "https://www.semanticscholar.org/paper/e16273b28628bead1952dc56fdcae7ea85d35e8f",
            "authors": "Dayuan Fu, Biqing Qi, Yihuai Gao, Che Jiang, Guanting Dong, Bowen Zhou",
            "EMNLP Paper ID": "85",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "72baa65c6f99f60199f5938809540bcd5cf857e1",
            "title": "A Reflective LLM-based Agent to Guide Zero-shot Cryptocurrency Trading",
            "abstract": "The utilization of Large Language Models (LLMs) in financial trading has primarily been concentrated within the stock market, aiding in economic and financial decisions. Yet, the unique opportunities presented by the cryptocurrency market, noted for its on-chain data's transparency and the critical influence of off-chain signals like news, remain largely untapped by LLMs. This work aims to bridge the gap by developing an LLM-based trading agent, CryptoTrade, which uniquely combines the analysis of on-chain and off-chain data. This approach leverages the transparency and immutability of on-chain data, as well as the timeliness and influence of off-chain signals, providing a comprehensive overview of the cryptocurrency market. CryptoTrade incorporates a reflective mechanism specifically engineered to refine its daily trading decisions by analyzing the outcomes of prior trading decisions. This research makes two significant contributions. Firstly, it broadens the applicability of LLMs to the domain of cryptocurrency trading. Secondly, it establishes a benchmark for cryptocurrency trading strategies. Through extensive experiments, CryptoTrade has demonstrated superior performance in maximizing returns compared to traditional trading strategies and time-series baselines across various cryptocurrencies and market conditions. Our code and data are available at \\url{https://anonymous.4open.science/r/CryptoTrade-Public-92FC/}.",
            "link": "https://www.semanticscholar.org/paper/72baa65c6f99f60199f5938809540bcd5cf857e1",
            "authors": "Yuan Li, B. Luo, Qian Wang, Nuo Chen, Xu Liu, Bingsheng He",
            "EMNLP Paper ID": "136",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "394e14ae60bae4c41162056717d9e30a8168abaa",
            "title": "Watch Every Step! LLM Agent Learning via Iterative Step-Level Process Refinement",
            "abstract": "Large language model agents have exhibited exceptional performance across a range of complex interactive tasks. Recent approaches have utilized tuning with expert trajectories to enhance agent performance, yet they primarily concentrate on outcome rewards, which may lead to errors or suboptimal actions due to the absence of process supervision signals. In this paper, we introduce the Iterative step-level Process Refinement (IPR) framework, which provides detailed step-by-step guidance to enhance agent training. Specifically, we adopt the Monte Carlo method to estimate step-level rewards. During each iteration, the agent explores along the expert trajectory and generates new actions. These actions are then evaluated against the corresponding step of expert trajectory using step-level rewards. Such comparison helps identify discrepancies, yielding contrastive action pairs that serve as training data for the agent. Our experiments on three complex agent tasks demonstrate that our framework outperforms a variety of strong baselines. Moreover, our analytical findings highlight the effectiveness of IPR in augmenting action efficiency and its applicability to diverse models.",
            "link": "https://www.semanticscholar.org/paper/394e14ae60bae4c41162056717d9e30a8168abaa",
            "authors": "Weimin Xiong, Yifan Song, Xiutian Zhao, Wenhao Wu, Xun Wang, Ke Wang, Cheng Li, Wei Peng, Sujian Li",
            "EMNLP Paper ID": "184",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "833a2fb379e9d7faabcb55650fc6db39653277a1",
            "title": "UNO Arena for Evaluating Sequential Decision-Making Capability of Large Language Models",
            "abstract": "Sequential decision-making refers to algorithms that take into account the dynamics of the environment, where early decisions affect subsequent decisions. With large language models (LLMs) demonstrating powerful capabilities between tasks, we can't help but ask: Can Current LLMs Effectively Make Sequential Decisions? In order to answer this question, we propose the UNO Arena based on the card game UNO to evaluate the sequential decision-making capability of LLMs and explain in detail why we choose UNO. In UNO Arena, We evaluate the sequential decision-making capability of LLMs dynamically with novel metrics based Monte Carlo methods. We set up random players, DQN-based reinforcement learning players, and LLM players (e.g. GPT-4, Gemini-pro) for comparison testing. Furthermore, in order to improve the sequential decision-making capability of LLMs, we propose the TUTRI player, which can involves having LLMs reflect their own actions wtih the summary of game history and the game strategy. Numerous experiments demonstrate that the TUTRI player achieves a notable breakthrough in the performance of sequential decision-making compared to the vanilla LLM player.",
            "link": "https://www.semanticscholar.org/paper/833a2fb379e9d7faabcb55650fc6db39653277a1",
            "authors": "Zhanyue Qin, Haochuan Wang, Deyuan Liu, Ziyang Song, Cunhang Fan, Zhao Lv, Jinlin Wu, Zhen Lei, Zhiying Tu, Dianhui Chu, Xiaoyan Yu, Dianbo Sui",
            "EMNLP Paper ID": "867",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "38ad7b52fd9c5b7bf81c78fd54ee0de1113ce092",
            "title": "METAREFLECTION: Learning Instructions for Language Agents using Past Reflections",
            "abstract": "The popularity of Large Language Models (LLMs) have unleashed a new age ofLanguage Agents for solving a diverse range of tasks. While contemporary frontier LLMs are capable enough to power reasonably good Language agents, the closed-API model makes it hard to improve in cases they perform sub-optimally. To address this, recent works have explored ways to improve their performance using techniques like self-reflection and prompt optimization. Unfortunately, techniques like self-reflection can be used only in an online setup, while contemporary prompt optimization techniques are designed and tested to work on simple tasks. To this end, we introduce MetaReflection, a novel offline reinforcement learning technique that enhances the performance of Language Agents by augmenting a semantic memory based on experiential learnings from past trials. We demonstrate the efficacy of MetaReflection by evaluating across multiple domains, including complex logical reasoning, biomedical semantic similarity, open world question answering, and vulnerability threat detection, in Infrastructure-as-Code, spanning different agent designs. MetaReflection boosts Language agents' performance by 4% to 16.82% over the raw GPT-4 baseline and performs on par with existing state-of-the-art prompt optimization techniques while requiring fewer LLM calls.",
            "link": "https://www.semanticscholar.org/paper/38ad7b52fd9c5b7bf81c78fd54ee0de1113ce092",
            "authors": "Priyanshu Gupta, Shashank Kirtania, Ananya Singha, Sumit Gulwani, Arjun Radhakrishna, Sherry Shi, Gustavo Soares",
            "EMNLP Paper ID": "971",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "115aa901ec1cdadc956c5448eb20aa782d3f7944",
            "title": "Efficient Sequential Decision Making with Large Language Models",
            "abstract": "This paper focuses on extending the success of large language models (LLMs) to sequential decision making. Existing efforts either (i) re-train or finetune LLMs for decision making, or (ii) design prompts for pretrained LLMs. The former approach suffers from the computational burden of gradient updates, and the latter approach does not show promising results. In this paper, we propose a new approach that leverages online model selection algorithms to efficiently incorporate LLMs agents into sequential decision making. Statistically, our approach significantly outperforms both traditional decision making algorithms and vanilla LLM agents. Computationally, our approach avoids the need for expensive gradient updates of LLMs, and throughout the decision making process, it requires only a small number of LLM calls. We conduct extensive experiments to verify the effectiveness of our proposed approach. As an example, on a large-scale Amazon dataset, our approach achieves more than a $6$x performance gain over baselines while calling LLMs in only $1.5$\\% of the time steps.",
            "link": "https://www.semanticscholar.org/paper/115aa901ec1cdadc956c5448eb20aa782d3f7944",
            "authors": "Dingyang Chen, Qi Zhang, Yinglun Zhu",
            "EMNLP Paper ID": "1041",
            "Oral/Poster": "Oral",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "bae5e043025a6856637bdf6ebda2a6ac6c2ece30",
            "title": "Re-ReST: Reflection-Reinforced Self-Training for Language Agents",
            "abstract": "Finetuning language agents with reasoning-action trajectories is effective, but obtaining these trajectories from human annotations or stronger models is costly and sometimes impractical. In this paper, we investigate the use of self-training in language agents, which can generate supervision from the agent itself, offering a promising alternative without relying on human or stronger model demonstrations. Self-training, however, requires high-quality model-generated samples, which are hard to obtain for challenging language agent tasks. To address this, we present Reflection-Reinforced Self-Training (Re-ReST), which uses a \\textit{reflector} to refine low-quality generated samples during self-training. The reflector takes the agent's output and feedback from an external environment (e.g., unit test results in code generation) to produce improved samples. This technique enhances the quality of inferior samples and efficiently enriches the self-training dataset with higher-quality samples. We conduct extensive experiments on open-source language agents across tasks, including multi-hop question answering, sequential decision-making, code generation, visual question answering, and text-to-image generation. The results demonstrate the effectiveness of self-training and Re-ReST in language agent tasks, with self-training improving baselines by 7.6\\% on HotpotQA and 28.4\\% on AlfWorld, and Re-ReST further boosting performance by 2.0\\% and 14.1\\%, respectively. Our studies also confirm the efficiency of using a reflector to generate high-quality samples for self-training. Moreover, we demonstrate a method to employ reflection during inference without ground-truth feedback, addressing the limitation of previous reflection work. Our code is released at https://github.com/PlusLabNLP/Re-ReST.",
            "link": "https://www.semanticscholar.org/paper/bae5e043025a6856637bdf6ebda2a6ac6c2ece30",
            "authors": "Zi-Yi Dou, Cheng-Fu Yang, Xueqing Wu, Kai-Wei Chang, Nanyun Peng",
            "EMNLP Paper ID": "1791",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "d2132fcda99d16416a7068637be35424519357d3",
            "title": "NeBuLa: A discourse aware Minecraft Builder",
            "abstract": "When engaging in collaborative tasks, humans efficiently exploit the semantic structure of a conversation to optimize verbal and nonverbal interactions. But in recent\"language to code\"or\"language to action\"models, this information is lacking. We show how incorporating the prior discourse and nonlinguistic context of a conversation situated in a nonlinguistic environment can improve the\"language to action\"component of such interactions. We finetune an LLM to predict actions based on prior context; our model, Nebula, doubles the net-action F1 score over the baseline on this task of Jayannavar et al.(2020). We also investigate our model's ability to construct shapes and understand location descriptions using a synthetic dataset",
            "link": "https://www.semanticscholar.org/paper/d2132fcda99d16416a7068637be35424519357d3",
            "authors": "Akshay Chaturvedi, Kate Thompson, Nicholas Asher",
            "matchScore": 219.83153,
            "original title": "NeBuLa: A discourse aware Minecraft Builder",
            "original authors": "Akshay Chaturvedi, Kate Thompson, Nicholas Asher",
            "EMNLP Paper ID": "1312",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "b34c93096b2b3c5f6f5ac7aa52bf85757d74f7fa",
            "title": "Devil's Advocate: Anticipatory Reflection for LLM Agents",
            "abstract": "In this work, we introduce a novel approach that equips LLM agents with introspection, enhancing consistency and adaptability in solving complex tasks. Our approach prompts LLM agents to decompose a given task into manageable subtasks (i.e., to make a plan), and to continuously introspect upon the suitability and results of their actions. %; and when necessary, to explore ``the road not taken.'' We implement a three-fold introspective intervention: 1) anticipatory reflection on potential failures and alternative remedy before action execution, 2) post-action alignment with subtask objectives and backtracking with remedy to ensure utmost effort in plan execution, and 3) comprehensive review upon plan completion for future strategy refinement. By deploying and experimenting with this methodology -- a zero-shot approach -- within WebArena for practical tasks in web environments, our agent demonstrates superior performance with a success rate of 23.5% over existing zero-shot methods by 3.5%. The experimental results suggest that our introspection-driven approach not only enhances the agent's ability to navigate unanticipated challenges through a robust mechanism of plan execution, but also improves efficiency by reducing the number of trials and plan revisions by 45% needed to achieve a task.",
            "link": "https://www.semanticscholar.org/paper/b34c93096b2b3c5f6f5ac7aa52bf85757d74f7fa",
            "authors": "Haoyu Wang, Tao Li, Zhiwei Deng, Dan Roth, Yang Li",
            "matchScore": 230.34357,
            "original title": "Devil\u2019s Advocate: Anticipatory Reflection for LLM Agents",
            "original authors": "Haoyu Wang, Tao Li, Zhiwei Deng, Dan Roth, Yang Li",
            "EMNLP Paper ID": "195",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "e2b57df28c8edf25bf8adb86ca5a9e7fe61eea0b",
            "title": "Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft",
            "abstract": "In the Minecraft Collaborative Building Task, two players collaborate: an Architect (A) provides instructions to a Builder (B) to assemble a specified structure using 3D blocks. In this work, we investigate the use of large language models (LLMs) to predict the sequence of actions taken by the Builder. Leveraging LLMs' in-context learning abilities, we use few-shot prompting techniques, that significantly improve performance over baseline methods. Additionally, we present a detailed analysis of the gaps in performance for future work",
            "link": "https://www.semanticscholar.org/paper/e2b57df28c8edf25bf8adb86ca5a9e7fe61eea0b",
            "authors": "Kranti Chalamalasetti, Sherzod Hakimov, David Schlangen",
            "matchScore": 278.20917,
            "original title": "Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft",
            "original authors": "Kranti CH, Sherzod Hakimov, David Schlangen",
            "EMNLP Paper ID": "2218",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "b60a9a78caaf06fbdbf8ee91ed9416efa0e6c3c4",
            "title": "AgentBank: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories",
            "abstract": "Fine-tuning on agent-environment interaction trajectory data holds significant promise for surfacing generalized agent capabilities in open-source large language models (LLMs). In this work, we introduce AgentBank, by far the largest trajectory tuning data collection featuring more than 50k diverse high-quality interaction trajectories which comprises 16 tasks covering five distinct agent skill dimensions. Leveraging a novel annotation pipeline, we are able to scale the annotated trajectories and generate a trajectory dataset with minimized difficulty bias. Furthermore, we fine-tune LLMs on AgentBank to get a series of agent models, Samoyed. Our comparative experiments demonstrate the effectiveness of scaling the interaction trajectory data to acquire generalized agent capabilities. Additional studies also reveal some key observations regarding trajectory tuning and agent skill generalization.",
            "link": "https://www.semanticscholar.org/paper/b60a9a78caaf06fbdbf8ee91ed9416efa0e6c3c4",
            "authors": "Yifan Song, Weimin Xiong, Xiutian Zhao, Dawei Zhu, Wenhao Wu, Ke Wang, Cheng Li, Wei Peng, Sujian Li",
            "matchScore": 319.20764,
            "original title": "AgentBank: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories",
            "original authors": "Yifan Song, Weimin Xiong, Xiutian Zhao, Dawei Zhu, Wenhao Wu, Ke Wang, Cheng LI, Wei Peng, Sujian Li",
            "EMNLP Paper ID": "426",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "b73f66e19be7859f01478f49e1c031bc9a9f02ca",
            "title": "Enhancing Agent Learning through World Dynamics Modeling",
            "abstract": "Large language models (LLMs) have been increasingly applied to tasks in language understanding and interactive decision-making, with their impressive performance largely attributed to the extensive domain knowledge embedded within them. However, the depth and breadth of this knowledge can vary across domains. Many existing approaches assume that LLMs possess a comprehensive understanding of their environment, often overlooking potential gaps in their grasp of actual world dynamics. To address this, we introduce Discover, Verify, and Evolve (DiVE), a framework that discovers world dynamics from a small number of demonstrations, verifies the accuracy of these dynamics, and evolves new, advanced dynamics tailored to the current situation. Through extensive evaluations, we assess the impact of each component on performance and compare the dynamics generated by DiVE to human-annotated dynamics. Our results show that LLMs guided by DiVE make more informed decisions, achieving rewards comparable to human players in the Crafter environment and surpassing methods that require prior task-specific training in the MiniHack environment.",
            "link": "https://www.semanticscholar.org/paper/b73f66e19be7859f01478f49e1c031bc9a9f02ca",
            "authors": "Zhiyuan Sun, Haochen Shi, Marc-Alexandre Cot'e, Glen Berseth, Xingdi Yuan, Bang Liu",
            "matchScore": 202.11356,
            "original title": "Enhancing Agent Learning through World Dynamics Modeling",
            "original authors": "Zhiyuan Sun, Haochen Shi, Marc-Alexandre C\u00f4t\u00e9, Glen Berseth, Xingdi Yuan, Bang Liu",
            "EMNLP Paper ID": "716",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        }
    ],
    "Optimizing Large Language Models for Retrieval, Segmentation, and Keyphrase Generation Tasks": [
        {
            "paperId": "64c606726616ae4879864d740b59b1e1a50a525b",
            "title": "Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment",
            "abstract": "Pretrained language models like BERT and T5 serve as crucial backbone encoders for dense retrieval. However, these models often exhibit limited generalization capabilities and face challenges in improving in domain accuracy. Recent research has explored using large language models (LLMs) as retrievers, achieving SOTA performance across various tasks. Despite these advancements, the specific benefits of LLMs over traditional retrievers and the impact of different LLM configurations, such as parameter sizes, pretraining duration, and alignment processes on retrieval tasks remain unclear. In this work, we conduct a comprehensive empirical study on a wide range of retrieval tasks, including in domain accuracy, data efficiency, zero shot generalization, lengthy retrieval, instruction based retrieval, and multi task learning. We evaluate over 15 different backbone LLMs and non LLMs. Our findings reveal that larger models and extensive pretraining consistently enhance in domain accuracy and data efficiency. Additionally, larger models demonstrate significant potential in zero shot generalization, lengthy retrieval, instruction based retrieval, and multi task learning. These results underscore the advantages of LLMs as versatile and effective backbone encoders in dense retrieval, providing valuable insights for future research and development in this field.",
            "link": "https://www.semanticscholar.org/paper/64c606726616ae4879864d740b59b1e1a50a525b",
            "authors": "Kun Luo, Minghao Qin, Zheng Liu, Shitao Xiao, Jun Zhao, Kang Liu",
            "EMNLP Paper ID": "163",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "2e7faac64082325e70c28b054d4e7f7cd7e50592",
            "title": "How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?",
            "abstract": "The increase in parameter size of multimodal large language models (MLLMs) introduces significant capabilities, particularly in-context learning, where MLLMs enhance task performance without updating pre-trained parameters. This effectiveness, however, hinges on the appropriate selection of in-context examples, a process that is currently biased towards visual data, overlooking textual information. Furthermore, the area of supervised retrievers for MLLMs, crucial for optimal in-context example selection, continues to be uninvestigated. Our study offers an in-depth evaluation of the impact of textual information on the unsupervised selection of in-context examples in multimodal contexts, uncovering a notable sensitivity of retriever performance to the employed modalities. Responding to this, we introduce a novel supervised MLLM-retriever MSIER that employs a neural network to select examples that enhance multimodal in-context learning efficiency. This approach is validated through extensive testing across three distinct tasks, demonstrating the method's effectiveness. Additionally, we investigate the influence of modalities on our supervised retrieval method's training and pinpoint factors contributing to our model's success. This exploration paves the way for future advancements, highlighting the potential for refined in-context learning in MLLMs through the strategic use of multimodal data.",
            "link": "https://www.semanticscholar.org/paper/2e7faac64082325e70c28b054d4e7f7cd7e50592",
            "authors": "Yang Luo, Zangwei Zheng, Zirui Zhu, Yang You",
            "EMNLP Paper ID": "589",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "bd426c4eef736db20db093dc40c424614e73aacf",
            "title": "One2set + Large Language Model: Best Partners for Keyphrase Generation",
            "abstract": "Keyphrase generation (KPG) aims to automatically generate a collection of phrases representing the core concepts of a given document. The dominant paradigms in KPG include one2seq and one2set. Recently, there has been increasing interest in applying large language models (LLMs) to KPG. Our preliminary experiments reveal that it is challenging for a single model to excel in both recall and precision. Further analysis shows that: 1) the one2set paradigm owns the advantage of high recall, but suffers from improper assignments of supervision signals during training; 2) LLMs are powerful in keyphrase selection, but existing selection methods often make redundant selections. Given these observations, we introduce a generate-then-select framework decomposing KPG into two steps, where we adopt a one2set-based model as generator to produce candidates and then use an LLM as selector to select keyphrases from these candidates. Particularly, we make two important improvements on our generator and selector: 1) we design an Optimal Transport-based assignment strategy to address the above improper assignments; 2) we model the keyphrase selection as a sequence labeling task to alleviate redundant selections. Experimental results on multiple benchmark datasets show that our framework significantly surpasses state-of-the-art models, especially in absent keyphrase prediction.",
            "link": "https://www.semanticscholar.org/paper/bd426c4eef736db20db093dc40c424614e73aacf",
            "authors": "Liangying Shao, Liang Zhang, Minlong Peng, Guoqi Ma, Hao Yue, Mingming Sun, Jinsong Su",
            "EMNLP Paper ID": "1291",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 14): 13:00\u201314:00 (Afternoon)",
            "Type": "Main"
        },
        {
            "paperId": "8114c2ececb5d6fd960bbb152eff9fed37e920a3",
            "title": "Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation",
            "abstract": "Segmenting text into sentences plays an early and crucial role in many NLP systems. This is commonly achieved by using rule-based or statistical methods relying on lexical features such as punctuation. Although some recent works no longer exclusively rely on punctuation, we find that no prior method achieves all of (i) robustness to missing punctuation, (ii) effective adaptability to new domains, and (iii) high efficiency. We introduce a new model - Segment any Text (SaT) - to solve this problem. To enhance robustness, we propose a new pretraining scheme that ensures less reliance on punctuation. To address adaptability, we introduce an extra stage of parameter-efficient fine-tuning, establishing state-of-the-art performance in distinct domains such as verses from lyrics and legal documents. Along the way, we introduce architectural modifications that result in a threefold gain in speed over the previous state of the art and solve spurious reliance on context far in the future. Finally, we introduce a variant of our model with fine-tuning on a diverse, multilingual mixture of sentence-segmented data, acting as a drop-in replacement and enhancement for existing segmentation tools. Overall, our contributions provide a universal approach for segmenting any text. Our method outperforms all baselines - including strong LLMs - across 8 corpora spanning diverse domains and languages, especially in practically relevant situations where text is poorly formatted. Our models and code, including documentation, are available at https://github.com/segment-any-text/wtpsplit under the MIT license.",
            "link": "https://www.semanticscholar.org/paper/8114c2ececb5d6fd960bbb152eff9fed37e920a3",
            "authors": "Markus Frohmann, Igor Sterner, Ivan Vuli'c, Benjamin Minixhofer, Markus Schedl",
            "EMNLP Paper ID": "1388",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "0790403e08066b7bc86d2c07cdbb5c8b4675086e",
            "title": "SEGMENT+: Long Text Processing with Short-Context Language Models",
            "abstract": "There is a growing interest in expanding the input capacity of language models (LMs) across various domains. However, simply increasing the context window does not guarantee robust performance across diverse long-input processing tasks, such as understanding extensive documents and extracting detailed information from lengthy and noisy data. In response, we introduce SEGMENT+, a general framework that enables LMs to handle extended inputs within limited context windows efficiently. SEGMENT+ utilizes structured notes and a filtering module to manage information flow, resulting in a system that is both controllable and interpretable. Our extensive experiments across various model sizes, focusing on long-document question-answering and Needle-in-a-Haystack tasks, demonstrate the effectiveness of SEGMENT+ in improving performance.",
            "link": "https://www.semanticscholar.org/paper/0790403e08066b7bc86d2c07cdbb5c8b4675086e",
            "authors": "Weiyan Shi, Shuang Li, Kerun Yu, Jinglei Chen, Zujie Liang, Xinhui Wu, Yuxi Qian, Feng Wei, Bo Zheng, Jiaqing Liang, Jiangjie Chen, Yanghua Xiao",
            "EMNLP Paper ID": "1959",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "5bff6b3225ea189e2ab7c6516d2a51c98fee8c62",
            "title": "Seg2Act: Global Context-aware Action Generation for Document Logical Structuring",
            "abstract": "Document logical structuring aims to extract the underlying hierarchical structure of documents, which is crucial for document intelligence. Traditional approaches often fall short in handling the complexity and the variability of lengthy documents. To address these issues, we introduce Seg2Act, an end-to-end, generation-based method for document logical structuring, revisiting logical structure extraction as an action generation task. Specifically, given the text segments of a document, Seg2Act iteratively generates the action sequence via a global context-aware generative model, and simultaneously updates its global context and current logical structure based on the generated actions. Experiments on ChCatExt and HierDoc datasets demonstrate the superior performance of Seg2Act in both supervised and transfer learning settings.",
            "link": "https://www.semanticscholar.org/paper/5bff6b3225ea189e2ab7c6516d2a51c98fee8c62",
            "authors": "Zichao Li, Shaojie He, Meng Liao, Xuanang Chen, Yaojie Lu, Hongyu Lin, Yanxiong Lu, Xianpei Han, Le Sun",
            "EMNLP Paper ID": "2207",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "56c6186d6af5ff19ba56d5ae0c5fd206872c26be",
            "title": "Unsupervised Domain Adaptation for Keyphrase Generation using Citation Contexts",
            "abstract": "Adapting keyphrase generation models to new domains typically involves few-shot fine-tuning with in-domain labeled data. However, annotating documents with keyphrases is often prohibitively expensive and impractical, requiring expert annotators. This paper presents silk, an unsupervised method designed to address this issue by extracting silver-standard keyphrases from citation contexts to create synthetic labeled data for domain adaptation. Extensive experiments across three distinct domains demonstrate that our method yields high-quality synthetic samples, resulting in significant and consistent improvements in in-domain performance over strong baselines.",
            "link": "https://www.semanticscholar.org/paper/56c6186d6af5ff19ba56d5ae0c5fd206872c26be",
            "authors": "Florian Boudin, Akiko Aizawa",
            "matchScore": 244.29823,
            "original title": "Unsupervised Domain Adaptation for Keyphrase Generation using Citation Contexts",
            "original authors": "Florian Boudin, Akiko Aizawa",
            "EMNLP Paper ID": "110",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 14): 13:00\u201314:00 (Afternoon)",
            "Type": "Findings"
        },
        {
            "paperId": "83fad9edb0549c3bc912973abad2e9e98a2dedcd",
            "title": "MetaKP: On-Demand Keyphrase Generation",
            "abstract": "Traditional keyphrase prediction methods predict a single set of keyphrases per document, failing to cater to the diverse needs of users and downstream applications. To bridge the gap, we introduce on-demand keyphrase generation, a novel paradigm that requires keyphrases that conform to specific high-level goals or intents. For this task, we present MetaKP, a large-scale benchmark comprising four datasets, 7500 documents, and 3760 goals across news and biomedical domains with human-annotated keyphrases. Leveraging MetaKP, we design both supervised and unsupervised methods, including a multi-task fine-tuning approach and a self-consistency prompting method with large language models. The results highlight the challenges of supervised fine-tuning, whose performance is not robust to distribution shifts. By contrast, the proposed self-consistency prompting approach greatly improves the performance of large language models, enabling GPT-4o to achieve 0.548 SemF1, surpassing the performance of a fully fine-tuned BART-base model. Finally, we demonstrate the potential of our method to serve as a general NLP infrastructure, exemplified by its application in epidemic event detection from social media.",
            "link": "https://www.semanticscholar.org/paper/83fad9edb0549c3bc912973abad2e9e98a2dedcd",
            "authors": "Di Wu, Xiaoxian Shen, Kai-Wei Chang",
            "matchScore": 186.62325,
            "original title": "MetaKP: On-Demand Keyphrase Generation",
            "original authors": "Di Wu, Xiaoxian Shen, Kai-Wei Chang",
            "EMNLP Paper ID": "1775",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "1698b14ae86443e7959a38915684a1d486a78dd9",
            "title": "Scalable and Domain-General Abstractive Proposition Segmentation",
            "abstract": "Segmenting text into fine-grained units of meaning is important to a wide range of NLP applications. The default approach of segmenting text into sentences is often insufficient, especially since sentences are usually complex enough to include multiple units of meaning that merit separate treatment in the downstream task. We focus on the task of abstractive proposition segmentation: transforming text into simple, self-contained, well-formed sentences. Several recent works have demonstrated the utility of proposition segmentation with few-shot prompted LLMs for downstream tasks such as retrieval-augmented grounding and fact verification. However, this approach does not scale to large amounts of text and may not always extract all the facts from the input text. In this paper, we first introduce evaluation metrics for the task to measure several dimensions of quality. We then propose a scalable, yet accurate, proposition segmentation model. We model proposition segmentation as a supervised task by training LLMs on existing annotated datasets and show that training yields significantly improved results. We further show that by using the fine-tuned LLMs as teachers for annotating large amounts of multi-domain synthetic distillation data, we can train smaller student models with results similar to the teacher LLMs. We then demonstrate that our technique leads to effective domain generalization, by annotating data in two domains outside the original training data and evaluating on them. Finally, as a key contribution of the paper, we share an easy-to-use API for NLP practitioners to use.",
            "link": "https://www.semanticscholar.org/paper/1698b14ae86443e7959a38915684a1d486a78dd9",
            "authors": "Mohammad Javad Hosseini, Yang Gao, Tim Baumg\u00e4rtner, Alex Fabrikant, Reinald Kim Amplayo",
            "matchScore": 215.99234,
            "original title": "Scalable and Domain-General Abstractive Proposition Segmentation",
            "original authors": "Mohammad Javad Hosseini, Yang Gao, Tim Baumg\u00e4rtner, Alex Fabrikant, Reinald Kim Amplayo",
            "EMNLP Paper ID": "1842",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "aa91f7c08b8c9da6e1db153c0e5295c7bfd13159",
            "title": "Automating Easy Read Text Segmentation",
            "abstract": "Easy Read text is one of the main forms of access to information for people with reading difficulties. One of the key characteristics of this type of text is the requirement to split sentences into smaller grammatical segments, to facilitate reading. Automated segmentation methods could foster the creation of Easy Read content, but their viability has yet to be addressed. In this work, we study novel methods for the task, leveraging masked and generative language models, along with constituent parsing. We conduct comprehensive automatic and human evaluations in three languages, analysing the strengths and weaknesses of the proposed alternatives, under scarce resource limitations. Our results highlight the viability of automated Easy Read text segmentation and remaining deficiencies compared to expert-driven human segmentation.",
            "link": "https://www.semanticscholar.org/paper/aa91f7c08b8c9da6e1db153c0e5295c7bfd13159",
            "authors": "Jes\u00fas Calleja-Perez, Thierry Etchegoyhen, David Ponce",
            "matchScore": 198.17624,
            "original title": "Automating Easy Read Text Segmentation",
            "original authors": "Jesus Javier Calleja Perez, Thierry Etchegoyhen, Antonio David Ponce Mart\u00ednez",
            "EMNLP Paper ID": "2335",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "5840cd4edbfadee407a6195fbc111d07a5c78780",
            "title": "MINERS: Multilingual Language Models as Semantic Retrievers",
            "abstract": "Words have been represented in a high-dimensional vector space that encodes their semantic similarities, enabling downstream applications such as retrieving synonyms, antonyms, and relevant contexts. However, despite recent advances in multilingual language models (LMs), the effectiveness of these models' representations in semantic retrieval contexts has not been comprehensively explored. To fill this gap, this paper introduces the MINERS, a benchmark designed to evaluate the ability of multilingual LMs in semantic retrieval tasks, including bitext mining and classification via retrieval-augmented contexts. We create a comprehensive framework to assess the robustness of LMs in retrieving samples across over 200 diverse languages, including extremely low-resource languages in challenging cross-lingual and code-switching settings. Our results demonstrate that by solely retrieving semantically similar embeddings yields performance competitive with state-of-the-art approaches, without requiring any fine-tuning.",
            "link": "https://www.semanticscholar.org/paper/5840cd4edbfadee407a6195fbc111d07a5c78780",
            "authors": "Genta Indra Winata, Ruochen Zhang, D. Adelani",
            "matchScore": 194.03055,
            "original title": "MINERS: Multilingual Language Models as Semantic Retrievers",
            "original authors": "Genta Indra Winata, Ruochen Zhang, David Ifeoluwa Adelani",
            "EMNLP Paper ID": "560",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "4dedb0ba8ce61e0c9e09618b49d7a05a9459d858",
            "title": "Enhancing Incremental Summarization with Structured Representations",
            "abstract": "Large language models (LLMs) often struggle with processing extensive input contexts, which can lead to redundant, inaccurate, or incoherent summaries. Recent methods have used unstructured memory to incrementally process these contexts, but they still suffer from information overload due to the volume of unstructured data handled. In our study, we introduce structured knowledge representations ($GU_{json}$), which significantly improve summarization performance by 40% and 14% across two public datasets. Most notably, we propose the Chain-of-Key strategy ($CoK_{json}$) that dynamically updates or augments these representations with new information, rather than recreating the structured memory for each new source. This method further enhances performance by 7% and 4% on the datasets.",
            "link": "https://www.semanticscholar.org/paper/4dedb0ba8ce61e0c9e09618b49d7a05a9459d858",
            "authors": "Eunjeong Hwang, Yichao Zhou, James Bradley Wendt, Beliz Gunel, Nguyen Vo, Jing Xie, Sandeep Tata",
            "matchScore": 191.47906,
            "original title": "Enhancing Incremental Summarization with Structured Representations",
            "original authors": "EunJeong Hwang, Yichao Zhou, James Bradley Wendt, Beliz Gunel, Nguyen Vo, Jing Xie, Sandeep Tata",
            "EMNLP Paper ID": "765",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        }
    ],
    "Automatic Text Evaluation using LLMs": [
        {
            "paperId": "ecdd53eaab7455daea27609b07a418a21aa7ad35",
            "title": "Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models",
            "abstract": "Proprietary LMs such as GPT-4 are often employed to assess the quality of responses from various LMs. However, concerns including transparency, controllability, and affordability strongly motivate the development of open-source LMs specialized in evaluations. On the other hand, existing open evaluator LMs exhibit critical shortcomings: 1) they issue scores that significantly diverge from those assigned by humans, and 2) they lack the flexibility to perform both direct assessment and pairwise ranking, the two most prevalent forms of assessment. Additionally, they do not possess the ability to evaluate based on custom evaluation criteria, focusing instead on general attributes like helpfulness and harmlessness. To address these issues, we introduce Prometheus 2, a more powerful evaluator LM than its predecessor that closely mirrors human and GPT-4 judgements. Moreover, it is capable of processing both direct assessment and pair-wise ranking formats grouped with a user-defined evaluation criteria. On four direct assessment benchmarks and four pairwise ranking benchmarks, Prometheus 2 scores the highest correlation and agreement with humans and proprietary LM judges among all tested open evaluator LMs. Our models, code, and data are all publicly available at https://github.com/prometheus-eval/prometheus-eval.",
            "link": "https://www.semanticscholar.org/paper/ecdd53eaab7455daea27609b07a418a21aa7ad35",
            "authors": "Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, S. Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, Minjoon Seo",
            "EMNLP Paper ID": "476",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "4a70c45876cfc39a740ba9851e84608d10921660",
            "title": "Efficient LLM Comparative Assessment: a Product of Experts Framework for Pairwise Comparisons",
            "abstract": "LLM-as-a-judge approaches are a practical and effective way of assessing a range of text tasks, aligning with human judgements especially when applied in a comparative assessment fashion. However, when using pairwise comparisons to rank a set of candidates the computational costs scale quadratically with the number of candidates, which can have practical limitations. This paper introduces a Product of Expert (PoE) framework for efficient LLM Comparative Assessment. Here individual comparisons are considered experts that provide information on a pair's score difference. The PoE framework combines the information from these experts to yield an expression that can be maximized with respect to the underlying set of candidates, and is highly flexible where any form of expert can be assumed. When Gaussian experts are used one can derive simple closed-form solutions for the optimal candidate ranking, as well as expressions for selecting which comparisons should be made to maximize the probability of this ranking. Our approach enables efficient comparative assessment, where by using only a small subset of the possible comparisons, one can generate score predictions that correlate as well to human judgements as the predictions when all comparisons are used. We evaluate the approach on multiple NLG tasks and demonstrate that our framework can yield considerable computational savings when performing pairwise comparative assessment. When N is large, with as few as 2% of comparisons the PoE solution can achieve similar performance to when all comparisons are used.",
            "link": "https://www.semanticscholar.org/paper/4a70c45876cfc39a740ba9851e84608d10921660",
            "authors": "Adian Liusie, Vatsal Raina, Yassir Fathullah, Mark J. F. Gales",
            "EMNLP Paper ID": "762",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "c85c3ba2c34543d906c349ccca5bfc243f2ac3f4",
            "title": "RepEval: Effective Text Evaluation with LLM Representation",
            "abstract": "The era of Large Language Models (LLMs) raises new demands for automatic evaluation metrics, which should be adaptable to various application scenarios while maintaining low cost and effectiveness. Traditional metrics for automatic text evaluation are often tailored to specific scenarios, while LLM-based evaluation metrics are costly, requiring fine-tuning or rely heavily on the generation capabilities of LLMs. Besides, previous LLM-based metrics ignore the fact that, within the space of LLM representations, there exist direction vectors that indicate the estimation of text quality. To this end, we introduce RepEval, a metric that leverages the projection of LLM representations for evaluation. Through simple prompt modifications, RepEval can easily transition to various tasks, requiring only minimal sample pairs for direction vector construction. Results on fourteen datasets across two evaluation tasks demonstrate the high effectiveness of our method, which exhibits a higher correlation with human judgments than previous methods, even in complex evaluation scenarios involving pair-wise selection under nuanced aspects. Our work underscores the richness of information regarding text quality embedded within LLM representations, offering insights for the development of new metrics.",
            "link": "https://www.semanticscholar.org/paper/c85c3ba2c34543d906c349ccca5bfc243f2ac3f4",
            "authors": "Shuqian Sheng, Yi Xu, Tianhang Zhang, Zanwei Shen, Luoyi Fu, Jiaxin Ding, Lei Zhou, Xinbing Wang, Cheng Zhou",
            "EMNLP Paper ID": "784",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "2e0f66e626a69da131e486da451218bbabfe7d3b",
            "title": "Split and Merge: Aligning Position Biases in Large Language Model based Evaluators",
            "abstract": "Large language models (LLMs) have shown promise as automated evaluators for assessing the quality of answers generated by AI systems. However, these LLM-based evaluators exhibit position bias, or inconsistency, when used to evaluate candidate answers in pairwise comparisons, favoring either the first or second answer regardless of content. To address this limitation, we propose PORTIA, an alignment-based system designed to mimic human comparison strategies to calibrate position bias in a lightweight yet effective manner. Specifically, PORTIA splits the answers into multiple segments, aligns similar content across candidate answers, and then merges them back into a single prompt for evaluation by LLMs. We conducted extensive experiments with six diverse LLMs to evaluate 11,520 answer pairs. Our results show that PORTIA markedly enhances the consistency rates for all the models and comparison forms tested, achieving an average relative improvement of 47.46%. Remarkably, PORTIA enables less advanced GPT models to achieve 88% agreement with the state-of-the-art GPT-4 model at just 10% of the cost. Furthermore, it rectifies around 80% of the position bias instances within the GPT-4 model, elevating its consistency rate up to 98%. Subsequent human evaluations indicate that the PORTIA-enhanced GPT-3.5 model can even surpass the standalone GPT-4 in terms of alignment with human evaluators. These findings highlight PORTIA's ability to correct position bias, improve LLM consistency, and boost performance while keeping cost-efficiency. This represents a valuable step toward a more reliable and scalable use of LLMs for automated evaluations across diverse applications.",
            "link": "https://www.semanticscholar.org/paper/2e0f66e626a69da131e486da451218bbabfe7d3b",
            "authors": "Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Tianxiang Li, Shuai Wang, Cuiyun Gao, Yang Liu",
            "EMNLP Paper ID": "1268",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 14): 13:00\u201314:00 (Afternoon)",
            "Type": "Main"
        },
        {
            "paperId": "18561d2b52138b403e0844626e2524f94deb3be8",
            "title": "RepMatch: Quantifying Cross-Instance Similarities in Representation Space",
            "abstract": "Advances in dataset analysis techniques have enabled more sophisticated approaches to analyzing and characterizing training data instances, often categorizing data based on attributes such as ``difficulty''. In this work, we introduce RepMatch, a novel method that characterizes data through the lens of similarity. RepMatch quantifies the similarity between subsets of training instances by comparing the knowledge encoded in models trained on them, overcoming the limitations of existing analysis methods that focus solely on individual instances and are restricted to within-dataset analysis. Our framework allows for a broader evaluation, enabling similarity comparisons across arbitrary subsets of instances, supporting both dataset-to-dataset and instance-to-dataset analyses. We validate the effectiveness of RepMatch across multiple NLP tasks, datasets, and models. Through extensive experimentation, we demonstrate that RepMatch can effectively compare datasets, identify more representative subsets of a dataset (that lead to better performance than randomly selected subsets of equivalent size), and uncover heuristics underlying the construction of some challenge datasets.",
            "link": "https://www.semanticscholar.org/paper/18561d2b52138b403e0844626e2524f94deb3be8",
            "authors": "Mohammad Reza Modarres, Sina Abbasi, Mohammad Taher Pilehvar",
            "EMNLP Paper ID": "1710",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "5e088c921b558126d0246b28f95c9c402e87cc62",
            "title": "Themis: A Reference-free NLG Evaluation Language Model with Flexibility and Interpretability",
            "abstract": "The evaluation of natural language generation (NLG) tasks is a significant and longstanding research area. With the recent emergence of powerful large language models (LLMs), some studies have turned to LLM-based automatic evaluation methods, which demonstrate great potential to become a new evaluation paradigm following traditional string-based and model-based metrics. However, despite the improved performance of existing methods, they still possess some deficiencies, such as dependency on references and limited evaluation flexibility. Therefore, in this paper, we meticulously construct a large-scale NLG evaluation corpus NLG-Eval with annotations from both human and GPT-4 to alleviate the lack of relevant data in this field. Furthermore, we propose Themis, an LLM dedicated to NLG evaluation, which has been trained with our designed multi-perspective consistency verification and rating-oriented preference alignment methods. Themis can conduct flexible and interpretable evaluations without references, and it exhibits superior evaluation performance on various NLG tasks, simultaneously generalizing well to unseen tasks and surpassing other evaluation models, including GPT-4.",
            "link": "https://www.semanticscholar.org/paper/5e088c921b558126d0246b28f95c9c402e87cc62",
            "authors": "Xinyu Hu, Li Lin, Mingqi Gao, Xunjian Yin, Xiaojun Wan",
            "EMNLP Paper ID": "1869",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "03bae9c8d33a312987dc0b14fcfc7d31890a8829",
            "title": "Autoregressive Multi-trait Essay Scoring via Reinforcement Learning with Scoring-aware Multiple Rewards",
            "abstract": "Recent advances in automated essay scoring (AES) have shifted towards evaluating multiple traits to provide enriched feedback. Like typical AES systems, multi-trait AES employs the quadratic weighted kappa (QWK) to measure agreement with human raters, aligning closely with the rating schema; however, its non-differentiable nature prevents its direct use in neural network training. In this paper, we propose Scoring-aware Multi-reward Reinforcement Learning (SaMRL), which integrates actual evaluation schemes into the training process by designing QWK-based rewards with a mean-squared error penalty for multi-trait AES. Existing reinforcement learning (RL) applications in AES are limited to classification models despite associated performance degradation, as RL requires probability distributions; instead, we adopt an autoregressive score generation framework to leverage token generation probabilities for robust multi-trait score predictions. Empirical analyses demonstrate that SaMRL facilitates model training, notably enhancing scoring of previously inferior prompts.",
            "link": "https://www.semanticscholar.org/paper/03bae9c8d33a312987dc0b14fcfc7d31890a8829",
            "authors": "Heejin Do, Sangwon Ryu, Gary Geunbae Lee",
            "EMNLP Paper ID": "1935",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "6c5ae8a0bf1885c2dd62c4787db4ac0f5159141f",
            "title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
            "abstract": "As large language models (LLMs) advance, it becomes more challenging to reliably evaluate their output due to the high costs of human evaluation. To make progress towards better LLM autoraters, we introduce FLAMe, a family of Foundational Large Autorater Models. FLAMe is trained on our large and diverse collection of 100+ quality assessment tasks comprising 5M+ human judgments, curated and standardized using publicly released human evaluations from previous research. FLAMe significantly improves generalization to a wide variety of held-out tasks, outperforming LLMs trained on proprietary data like GPT-4 and Claude-3 on many tasks. We show that FLAMe can also serve as a powerful starting point for further downstream fine-tuning, using reward modeling evaluation as a case study (FLAMe-RM). Notably, on RewardBench, our FLAMe-RM-24B model (with an accuracy of 87.8%) is the top-performing generative model trained exclusively on permissively licensed data, outperforming both GPT-4-0125 (85.9%) and GPT-4o (84.7%). Additionally, we explore a more computationally efficient approach using a novel tail-patch fine-tuning strategy to optimize our FLAMe multitask mixture for reward modeling evaluation (FLAMe-Opt-RM), offering competitive RewardBench performance while requiring approximately 25x less training datapoints. Overall, our FLAMe variants outperform all popular proprietary LLM-as-a-Judge models we consider across 8 out of 12 autorater evaluation benchmarks, encompassing 53 quality assessment tasks, including RewardBench and LLM-AggreFact. Finally, our analysis reveals that FLAMe is significantly less biased than these LLM-as-a-Judge models on the CoBBLEr autorater bias benchmark, while effectively identifying high-quality responses for code generation.",
            "link": "https://www.semanticscholar.org/paper/6c5ae8a0bf1885c2dd62c4787db4ac0f5159141f",
            "authors": "Tu Vu, Kalpesh Krishna, Salaheddin Alzubi, Chris Tar, Manaal Faruqui, Yun-Hsuan Sung",
            "EMNLP Paper ID": "2018",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "b2df275f024c6c427698d3de865df2d4d0274aed",
            "title": "How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?",
            "abstract": "Work on instruction-tuned Large Language Models (LLMs) has used automatic methods based on text overlap and LLM judgments as cost-effective alternatives to human evaluation. In this paper, we perform a meta-evaluation of such methods and assess their reliability across a broad range of tasks. In evaluating how well automatic methods align with human evaluations, correlation metrics are the most commonly employed method despite their inherent limitations when dealing with ties and different scales. To address these shortcomings, we use Pairwise Accuracy as an alternative to standard correlation measures. We observe that while automatic evaluation methods can approximate human ratings under specific conditions, their validity is highly context-dependent. Specifically, the simple ROUGE-L metric correlates very well with human ratings for short-answer English tasks but is unreliable in free-form generation tasks and cross-lingual scenarios. The effectiveness of the more advanced method of using GPT-4 as a judge diminishes significantly if reference answers are not included in the prompt, which is the scenario where this method has the potential to provide the most value compared to other metrics. Our findings enhance the understanding of how automatic methods should be applied and interpreted when developing and evaluating instruction-tuned LLMs.",
            "link": "https://www.semanticscholar.org/paper/b2df275f024c6c427698d3de865df2d4d0274aed",
            "authors": "Ehsan Doostmohammadi, Oskar Holmstrom, Marco Kuhlmann",
            "matchScore": 231.41078,
            "original title": "How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?",
            "original authors": "Ehsan Doostmohammadi, Oskar Holmstr\u00f6m, Marco Kuhlmann",
            "EMNLP Paper ID": "1290",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "82a261f9f860c8e751c74e4de64ec0eeecac50f9",
            "title": "Beyond Agreement: Diagnosing the Rationale Alignment of Automated Essay Scoring Methods based on Linguistically-informed Counterfactuals",
            "abstract": "While current Automated Essay Scoring (AES) methods demonstrate high scoring agreement with human raters, their decision-making mechanisms are not fully understood. Our proposed method, using counterfactual intervention assisted by Large Language Models (LLMs), reveals that BERT-like models primarily focus on sentence-level features, whereas LLMs such as GPT-3.5, GPT-4 and Llama-3 are sensitive to conventions&accuracy, language complexity, and organization, indicating a more comprehensive rationale alignment with scoring rubrics. Moreover, LLMs can discern counterfactual interventions when giving feedback on essays. Our approach improves understanding of neural AES methods and can also apply to other domains seeking transparency in model-driven decisions.",
            "link": "https://www.semanticscholar.org/paper/82a261f9f860c8e751c74e4de64ec0eeecac50f9",
            "authors": "Yupei Wang, Renfen Hu, Zhe Zhao",
            "matchScore": 355.0625,
            "original title": "Beyond Agreement: Diagnosing the Rationale Alignment of Automated Essay Scoring Methods based on Linguistically-informed Counterfactuals",
            "original authors": "Yupei Wang, Renfen Hu, Zhe Zhao",
            "EMNLP Paper ID": "1859",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "13f2bef92cd5caac81cd1a41ff92df576bff3b90",
            "title": "Unleashing Large Language Models' Proficiency in Zero-shot Essay Scoring",
            "abstract": "Advances in automated essay scoring (AES) have traditionally relied on labeled essays, requiring tremendous cost and expertise for their acquisition. Recently, large language models (LLMs) have achieved great success in various tasks, but their potential is less explored in AES. In this paper, we show that our zero-shot prompting framework, Multi Trait Specialization (MTS), elicits LLMs' ample potential for essay scoring. In particular, we automatically decompose writing proficiency into distinct traits and generate scoring criteria for each trait. Then, an LLM is prompted to extract trait scores from several conversational rounds, each round scoring one of the traits based on the scoring criteria. Finally, we derive the overall score via trait averaging and min-max scaling. Experimental results on two benchmark datasets demonstrate that MTS consistently outperforms straightforward prompting (Vanilla) in average QWK across all LLMs and datasets, with maximum gains of 0.437 on TOEFL11 and 0.355 on ASAP. Additionally, with the help of MTS, the small-sized Llama2-13b-chat substantially outperforms ChatGPT, facilitating an effective deployment in real applications.",
            "link": "https://www.semanticscholar.org/paper/13f2bef92cd5caac81cd1a41ff92df576bff3b90",
            "authors": "Sanwoo Lee, Yida Cai, Desong Meng, Ziyang Wang, Yunfang Wu",
            "matchScore": 277.11533,
            "original title": "Unleashing Large Language Models\u2019 Proficiency in Zero-shot Essay Scoring",
            "original authors": "Sanwoo Lee, Yida Cai, Desong Meng, Ziyang Wang, Yunfang Wu",
            "EMNLP Paper ID": "33",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "385d3c4a5c571aafd42e51b7de2e226ddb478be4",
            "title": "AHP-Powered LLM Reasoning for Multi-Criteria Evaluation of Open-Ended Responses",
            "abstract": "Question answering (QA) tasks have been extensively studied in the field of natural language processing (NLP). Answers to open-ended questions are highly diverse and difficult to quantify, and cannot be simply evaluated as correct or incorrect, unlike close-ended questions with definitive answers. While large language models (LLMs) have demonstrated strong capabilities across various tasks, they exhibit relatively weaker performance in evaluating answers to open-ended questions. In this study, we propose a method that leverages LLMs and the analytic hierarchy process (AHP) to assess answers to open-ended questions. We utilized LLMs to generate multiple evaluation criteria for a question. Subsequently, answers were subjected to pairwise comparisons under each criterion with LLMs, and scores for each answer were calculated in the AHP. We conducted experiments on four datasets using both ChatGPT-3.5-turbo and GPT-4. Our results indicate that our approach more closely aligns with human judgment compared to the four baselines. Additionally, we explored the impact of the number of criteria, variations in models, and differences in datasets on the results.",
            "link": "https://www.semanticscholar.org/paper/385d3c4a5c571aafd42e51b7de2e226ddb478be4",
            "authors": "Xiaotian Lu, Jiyi Li, Koh Takeuchi, Hisashi Kashima",
            "matchScore": 259.9249,
            "original title": "AHP-Powered LLM Reasoning for Multi-Criteria Evaluation of Open-Ended Responses",
            "original authors": "Xiaotian Lu, Jiyi Li, Koh Takeuchi, Hisashi Kashima",
            "EMNLP Paper ID": "371",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        }
    ],
    "Evaluation and Enhancement of Language Models for Diverse Languages and Cultural Contexts": [
        {
            "paperId": "ba5284674face6cca3b678fd7a82d691ec29349b",
            "title": "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages",
            "abstract": "Southeast Asia (SEA) is a region rich in linguistic diversity and cultural variety, with over 1,300 indigenous languages and a population of 671 million people. However, prevailing AI models suffer from a significant lack of representation of texts, images, and audio datasets from SEA, compromising the quality of AI models for SEA languages. Evaluating models for SEA languages is challenging due to the scarcity of high-quality datasets, compounded by the dominance of English training data, raising concerns about potential cultural misrepresentation. To address these challenges, we introduce SEACrowd, a collaborative initiative that consolidates a comprehensive resource hub that fills the resource gap by providing standardized corpora in nearly 1,000 SEA languages across three modalities. Through our SEACrowd benchmarks, we assess the quality of AI models on 36 indigenous languages across 13 tasks, offering valuable insights into the current AI landscape in SEA. Furthermore, we propose strategies to facilitate greater AI advancements, maximizing potential utility and resource equity for the future of AI in SEA.",
            "link": "https://www.semanticscholar.org/paper/ba5284674face6cca3b678fd7a82d691ec29349b",
            "authors": "Holy Lovenia, Rahmad Mahendra, Salsabil Maulana Akbar, Lester James Validad Miranda, Jennifer Santoso, Elyanah Aco, Akhdan Fadhilah, Jonibek Mansurov, Joseph Marvin Imperial, Onno P. Kampman, Joel Ruben Antony Moniz, Muhammad Ravi Shulthan Habibi, Frederikus Hudi, Railey Montalan, Ryan Ignatius, Joanito Agili Lopo, William Nixon, B\u00f6rje F. Karlsson, James Jaya, Ryandito Diandaru, Yuze Gao, Patrick Amadeus Irawan, Bin Wang, Jan Christian Blaise Cruz, Chenxi Whitehouse, Ivan Halim Parmonangan, Maria Khelli, Wenyu Zhang, Lucky Susanto, Reynard Adha Ryanda, Sonny Lazuardi Hermawan, Dan John Velasco, Muhammad Dehan Al Kautsar, Willy Fitra Hendria, Yasmin Moslem, Noah Flynn, Muhammad Farid Adilazuarda, Haochen Li, Johanes Lee, R. Damanhuri, Shuo Sun, M. Qorib, Amirbek Djanibekov, Wei Qi Leong, Quyet V. Do, Niklas Muennighoff, T. Pansuwan, Ilham Firdausi Putra, Yan Xu, Ngee Chia Tai, Ayu Purwarianti, Sebastian Ruder, William-Chandra Tjhi, Peerat Limkonchotiwat, Alham Fikri Aji, Sedrick Scott Keh, Genta Indra Winata, Ruochen Zhang, Fajri Koto, Zheng-Xin Yong, Samuel Cahyawijaya",
            "EMNLP Paper ID": "570",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "0ad16e2c1c30d8ed5b63970e5fb3459a08218ea3",
            "title": "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian",
            "abstract": "Norwegian, spoken by only 5 million population, is under-representative within the most impressive breakthroughs in NLP tasks. To the best of our knowledge, there has not yet been a comprehensive evaluation of the existing language models (LMs) on Norwegian generation tasks during the article writing process. To fill this gap, we 1) compiled the existing Norwegian dataset and pre-trained 4 Norwegian Open Language Models varied from parameter scales and architectures, collectively called NorGLM; 2) introduced a comprehensive benchmark, NLEBench, for evaluating natural language generation capabilities in Norwegian, encompassing translation and human annotation. Based on the investigation, we find that: 1) the mainstream, English-dominated LM GPT-3.5 has limited capability in understanding the Norwegian context; 2) the increase in model parameter scales demonstrates limited impact on the performance of downstream tasks when the pre-training dataset is constrained in size; 3) smaller models also demonstrate the reasoning capability through Chain-of-Thought; 4) a multi-task dataset that includes synergy tasks can be used to verify the generalizability of LLMs on natural language understanding and, meanwhile, test the interconnectedness of these NLP tasks. We share our resources and code for reproducibility under a CC BY-NC 4.0 license.",
            "link": "https://www.semanticscholar.org/paper/0ad16e2c1c30d8ed5b63970e5fb3459a08218ea3",
            "authors": "Peng Liu, Lemei Zhang, Terje Nissen Farup, Even W. Lauvrak, Jon Espen Ingvaldsen, Simen Eide, J. Gulla, Zhirong Yang",
            "EMNLP Paper ID": "619",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "83b5fe68d4d46b238671bd37cd3f3acceaf5bacf",
            "title": "RuBLiMP: Russian Benchmark of Linguistic Minimal Pairs",
            "abstract": "Minimal pairs are a well-established approach to evaluating the grammatical knowledge of language models. However, existing resources for minimal pairs address a limited number of languages and lack diversity of language-specific grammatical phenomena. This paper introduces the Russian Benchmark of Linguistic Minimal Pairs (RuBLiMP), which includes 45k pairs of sentences that differ in grammaticality and isolate a morphological, syntactic, or semantic phenomenon. In contrast to existing benchmarks of linguistic minimal pairs, RuBLiMP is created by applying linguistic perturbations to automatically annotated sentences from open text corpora and carefully curating test data. We describe the data collection protocol and present the results of evaluating 25 language models in various scenarios. We find that the widely used language models for Russian are sensitive to morphological and agreement-oriented contrasts but fall behind humans on phenomena requiring understanding of structural relations, negation, transitivity, and tense. RuBLiMP, the codebase, and other materials are publicly available.",
            "link": "https://www.semanticscholar.org/paper/83b5fe68d4d46b238671bd37cd3f3acceaf5bacf",
            "authors": "Ekaterina Taktasheva, Maxim Bazhukov, Kirill Koncha, Alena Fenogenova, E. Artemova",
            "EMNLP Paper ID": "1050",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "a3357c562e46cbdad632737c73628869c02b95b2",
            "title": "Susu Box or Piggy Bank: Assessing Cultural Commonsense Knowledge between Ghana and the U.S",
            "abstract": "Recent work has highlighted the culturally-contingent nature of commonsense knowledge. We introduce AMAMMER${\\epsilon}$, a test set of 525 multiple-choice questions designed to evaluate the commonsense knowledge of English LLMs, relative to the cultural contexts of Ghana and the United States. To create AMAMMER${\\epsilon}$, we select a set of multiple-choice questions (MCQs) from existing commonsense datasets and rewrite them in a multi-stage process involving surveys of Ghanaian and U.S. participants. In three rounds of surveys, participants from both pools are solicited to (1) write correct and incorrect answer choices, (2) rate individual answer choices on a 5-point Likert scale, and (3) select the best answer choice from the newly-constructed MCQ items, in a final validation step. By engaging participants at multiple stages, our procedure ensures that participant perspectives are incorporated both in the creation and validation of test items, resulting in high levels of agreement within each pool. We evaluate several off-the-shelf English LLMs on AMAMMER${\\epsilon}$. Uniformly, models prefer answers choices that align with the preferences of U.S. annotators over Ghanaian annotators. Additionally, when test items specify a cultural context (Ghana or the U.S.), models exhibit some ability to adapt, but performance is consistently better in U.S. contexts than Ghanaian. As large resources are devoted to the advancement of English LLMs, our findings underscore the need for culturally adaptable models and evaluations to meet the needs of diverse English-speaking populations around the world.",
            "link": "https://www.semanticscholar.org/paper/a3357c562e46cbdad632737c73628869c02b95b2",
            "authors": "Christabel Acquaye, Haozhe An, Rachel Rudinger",
            "EMNLP Paper ID": "1063",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "5b61eb70cc89ba746d9aee363e4df1160838c666",
            "title": "SPEED++: A Multilingual Event Extraction Framework for Epidemic Prediction and Preparedness",
            "abstract": "Social media is often the first place where communities discuss the latest societal trends. Prior works have utilized this platform to extract epidemic-related information (e.g. infections, preventive measures) to provide early warnings for epidemic prediction. However, these works only focused on English posts, while epidemics can occur anywhere in the world, and early discussions are often in the local, non-English languages. In this work, we introduce the first multilingual Event Extraction (EE) framework SPEED++ for extracting epidemic event information for a wide range of diseases and languages. To this end, we extend a previous epidemic ontology with 20 argument roles; and curate our multilingual EE dataset SPEED++ comprising 5.1K tweets in four languages for four diseases. Annotating data in every language is infeasible; thus we develop zero-shot cross-lingual cross-disease models (i.e., training only on English COVID data) utilizing multilingual pre-training and show their efficacy in extracting epidemic-related events for 65 diverse languages across different diseases. Experiments demonstrate that our framework can provide epidemic warnings for COVID-19 in its earliest stages in Dec 2019 (3 weeks before global discussions) from Chinese Weibo posts without any training in Chinese. Furthermore, we exploit our framework's argument extraction capabilities to aggregate community epidemic discussions like symptoms and cure measures, aiding misinformation detection and public attention monitoring. Overall, we lay a strong foundation for multilingual epidemic preparedness.",
            "link": "https://www.semanticscholar.org/paper/5b61eb70cc89ba746d9aee363e4df1160838c666",
            "authors": "Tanmay Parekh, Jeffrey Kwan, Jiarui Yu, Sparsh Johri, Hyosang Ahn, Sreya Muppalla, Kai-Wei Chang, Wei Wang, Nanyun Peng",
            "EMNLP Paper ID": "1502",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "5a0573a3c15d094e8b3d488c11a660773f631070",
            "title": "Towards Measuring and Modeling \"Culture\" in LLMs: A Survey",
            "abstract": "We present a survey of more than 90 recent papers that aim to study cultural representation and inclusion in large language models (LLMs). We observe that none of the studies explicitly define\"culture, which is a complex, multifaceted concept; instead, they probe the models on some specially designed datasets which represent certain aspects of\"culture\". We call these aspects the proxies of culture, and organize them across two dimensions of demographic and semantic proxies. We also categorize the probing methods employed. Our analysis indicates that only certain aspects of ``culture,'' such as values and objectives, have been studied, leaving several other interesting and important facets, especially the multitude of semantic domains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps are the lack of robustness of probing techniques and situated studies on the impact of cultural mis- and under-representation in LLM-based applications.",
            "link": "https://www.semanticscholar.org/paper/5a0573a3c15d094e8b3d488c11a660773f631070",
            "authors": "Muhammad Farid Adilazuarda, Sagnik Mukherjee, Pradhyumna Lavania, Siddhant Singh, Ashutosh Dwivedi, Alham Fikri Aji, Jacki O'Neill, Ashutosh Modi, M. Choudhury",
            "EMNLP Paper ID": "1851",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "8cc5067c48aef6b019ba38f1a2bd612b999df16c",
            "title": "MT-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large Language Models",
            "abstract": "Large language models (LLMs) are increasingly relied upon for complex multi-turn conversations across diverse real-world applications. However, existing benchmarks predominantly focus on single-turn evaluations, overlooking the models' capabilities in multi-turn interactions. To address this gap, we introduce MT-Eval, a comprehensive benchmark designed to evaluate multi-turn conversational abilities. By analyzing human-LLM conversations, we categorize interaction patterns into four types: recollection, expansion, refinement, and follow-up. We construct multi-turn queries for each category either by augmenting existing datasets or by creating new examples with GPT-4 to avoid data leakage. To study the factors impacting multi-turn abilities, we create single-turn versions of the 1170 multi-turn queries and compare performance. Our evaluation of 11 well-known LLMs shows that while closed-source models generally surpass open-source ones, certain open-source models exceed GPT-3.5-Turbo in specific tasks. We observe significant performance degradation in multi-turn settings compared to single-turn settings in most models, which is not correlated with the models' fundamental capabilities. Moreover, we identify the distance to relevant content and susceptibility to error propagation as the key factors influencing multi-turn performance. MT-Eval is released publicly to encourage future research towards more robust conversational models.",
            "link": "https://www.semanticscholar.org/paper/8cc5067c48aef6b019ba38f1a2bd612b999df16c",
            "authors": "Wai-Chung Kwan, Xingshan Zeng, Yuxin Jiang, Yufei Wang, Liangyou Li, Lifeng Shang, Xin Jiang, Qun Liu, Kam-Fai Wong",
            "EMNLP Paper ID": "2628",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "1d83d2512bd9c7ceb8de1fa25ac7b8c4c00b5573",
            "title": "Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese",
            "abstract": "Large Language Models (LLMs) are increasingly being used to generate synthetic data for training and evaluating models. However, it is unclear whether they can generate a good quality of question answering (QA) dataset that incorporates knowledge and cultural nuance embedded in a language, especially for low-resource languages. In this study, we investigate the effectiveness of using LLMs in generating culturally relevant commonsense QA datasets for Indonesian and Sundanese languages. To do so, we create datasets for these languages using various methods involving both LLMs and human annotators, resulting in ~4.5K questions per language (~9K in total), making our dataset the largest of its kind. Our experiments show that automatic data adaptation from an existing English dataset is less effective for Sundanese. Interestingly, using the direct generation method on the target language, GPT-4 Turbo can generate questions with adequate general knowledge in both languages, albeit not as culturally 'deep' as humans. We also observe a higher occurrence of fluency errors in the Sundanese dataset, highlighting the discrepancy between medium- and lower-resource languages.",
            "link": "https://www.semanticscholar.org/paper/1d83d2512bd9c7ceb8de1fa25ac7b8c4c00b5573",
            "authors": "Rifki Afina Putri, Faiz Ghifari Haznitrama, Dea Adhista, Alice Oh",
            "EMNLP Paper ID": "2715",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "565387972326fb8088303e43e1104fb8d0da2f39",
            "title": "SpeciaLex: A Benchmark for In-Context Specialized Lexicon Learning",
            "abstract": "Specialized lexicons are collections of words with associated constraints such as special definitions, specific roles, and intended target audiences. These constraints are necessary for content generation and documentation tasks (e.g., writing technical manuals or children's reading materials), where the goal is to reduce the ambiguity of text content and increase its overall readability for a specific group of audience. Understanding how large language models can capture these constraints can help researchers build better, more impactful tools for wider use beyond the NLP community. Towards this end, we introduce SpeciaLex, a benchmark for evaluating a language model's ability to follow specialized lexicon-based constraints across 18 diverse subtasks with 1,785 test instances covering core tasks of Checking, Identification, Rewriting, and Open Generation. We present an empirical evaluation of 15 open and closed-source LLMs and discuss insights on how factors such as model scale, openness, setup, and recency affect performance upon evaluating with the benchmark.",
            "link": "https://www.semanticscholar.org/paper/565387972326fb8088303e43e1104fb8d0da2f39",
            "authors": "Joseph Marvin Imperial, Harish Tayyar Madabushi",
            "matchScore": 259.22385,
            "original title": "SpeciaLex: A Benchmark for In-Context Specialized Lexicon Learning",
            "original authors": "Joseph Marvin Imperial, Harish Tayyar Madabushi",
            "EMNLP Paper ID": "193",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "6cd53242323bbd115ba0bc03ed8a49ce60d59c21",
            "title": "Gazelle: An Instruction Dataset for Arabic Writing Assistance",
            "abstract": "Writing has long been considered a hallmark of human intelligence and remains a pinnacle task for artificial intelligence (AI) due to the intricate cognitive processes involved. Recently, rapid advancements in generative AI, particularly through the development of Large Language Models (LLMs), have significantly transformed the landscape of writing assistance. However, underrepresented languages like Arabic encounter significant challenges in the development of advanced AI writing tools, largely due to the limited availability of data. This scarcity constrains the training of effective models, impeding the creation of sophisticated writing assistance technologies. To address these issues, we present Gazelle, a comprehensive dataset for Arabic writing assistance. In addition, we offer an evaluation framework designed to enhance Arabic writing assistance tools. Our human evaluation of leading LLMs, including GPT-4, GPT-4o, Cohere Command R+, and Gemini 1.5 Pro, highlights their respective strengths and limitations in addressing the challenges of Arabic writing. Our findings underscore the need for continuous model training and dataset enrichment to manage the complexities of Arabic language processing, paving the way for more effective AI-powered Arabic writing tools.",
            "link": "https://www.semanticscholar.org/paper/6cd53242323bbd115ba0bc03ed8a49ce60d59c21",
            "authors": "S. Magdy, Fakhraddin Alwajih, S. Kwon, Reem Abdel-Salam, Muhammad Abdul-Mageed",
            "matchScore": 248.0332,
            "original title": "Gazelle: An Instruction Dataset for Arabic Writing Assistance",
            "original authors": "Samar Mohamed Magdy, Fakhraddin Alwajih, Sang Yun Kwon, Reem Abdel-Salam, Muhammad Abdul-Mageed",
            "EMNLP Paper ID": "3071",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "e672468c6b5c0ce10abf77378ae18a0b58d2cf45",
            "title": "Extrinsic Evaluation of Cultural Competence in Large Language Models",
            "abstract": "Productive interactions between diverse users and language technologies require outputs from the latter to be culturally relevant and sensitive. Prior works have evaluated models' knowledge of cultural norms, values, and artifacts, without considering how this knowledge manifests in downstream applications. In this work, we focus on extrinsic evaluation of cultural competence in two text generation tasks, open-ended question answering and story generation. We quantitatively and qualitatively evaluate model outputs when an explicit cue of culture, specifically nationality, is perturbed in the prompts. Although we find that model outputs do vary when varying nationalities and feature culturally relevant words, we also find weak correlations between text similarity of outputs for different countries and the cultural values of these countries. Finally, we discuss important considerations in designing comprehensive evaluation of cultural competence in user-facing tasks.",
            "link": "https://www.semanticscholar.org/paper/e672468c6b5c0ce10abf77378ae18a0b58d2cf45",
            "authors": "Shaily Bhatt, Fernando Diaz",
            "matchScore": 186.15948,
            "original title": "Extrinsic Evaluation of Cultural Competence in Large Language Models",
            "original authors": "Shaily Bhatt, Fernando Diaz",
            "EMNLP Paper ID": "3075",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "a670b9eeec59421e096056dd7eb48ced1ab83be1",
            "title": "CultureBank: An Online Community-Driven Knowledge Base Towards Culturally Aware Language Technologies",
            "abstract": "To enhance language models' cultural awareness, we design a generalizable pipeline to construct cultural knowledge bases from different online communities on a massive scale. With the pipeline, we construct CultureBank, a knowledge base built upon users' self-narratives with 12K cultural descriptors sourced from TikTok and 11K from Reddit. Unlike previous cultural knowledge resources, CultureBank contains diverse views on cultural descriptors to allow flexible interpretation of cultural knowledge, and contextualized cultural scenarios to help grounded evaluation. With CultureBank, we evaluate different LLMs' cultural awareness, and identify areas for improvement. We also fine-tune a language model on CultureBank: experiments show that it achieves better performances on two downstream cultural tasks in a zero-shot setting. Finally, we offer recommendations based on our findings for future culturally aware language technologies. The project page is https://culturebank.github.io . The code and model is at https://github.com/SALT-NLP/CultureBank . The released CultureBank dataset is at https://huggingface.co/datasets/SALT-NLP/CultureBank .",
            "link": "https://www.semanticscholar.org/paper/a670b9eeec59421e096056dd7eb48ced1ab83be1",
            "authors": "Weiyan Shi, Ryan Li, Yutong Zhang, Caleb Ziems, Chunhua yu, R. Horesh, Rog'erio Abreu de Paula, Diyi Yang",
            "matchScore": 274.15582,
            "original title": "CultureBank: An Online Community-Driven Knowledge Base Towards Culturally Aware Language Technologies",
            "original authors": "Weiyan Shi, Ryan Li, Yutong Zhang, Caleb Ziems, Sunny Yu, Raya Horesh, Rog\u00e9rio Abreu de Paula, Diyi Yang",
            "EMNLP Paper ID": "983",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        }
    ],
    "Social Intelligence and Interaction Capabilities of Large Language Models": [
        {
            "paperId": "670f9c71de3480d1ae4629b9e6b1775bb02e5ea4",
            "title": "An LLM Feature-based Framework for Dialogue Constructiveness Assessment",
            "abstract": "Research on dialogue constructiveness assessment focuses on (i) analysing conversational factors that influence individuals to take specific actions, win debates, change their perspectives or broaden their open-mindedness and (ii) predicting constructiveness outcomes following dialogues for such use cases. These objectives can be achieved by training either interpretable feature-based models (which often involve costly human annotations) or neural models such as pre-trained language models (which have empirically shown higher task accuracy but lack interpretability). In this paper we propose an LLM feature-based framework for dialogue constructiveness assessment that combines the strengths of feature-based and neural approaches, while mitigating their downsides. The framework first defines a set of dataset-independent and interpretable linguistic features, which can be extracted by both prompting an LLM and simple heuristics. Such features are then used to train LLM feature-based models. We apply this framework to three datasets of dialogue constructiveness and find that our LLM feature-based models outperform or performs at least as well as standard feature-based models and neural models. We also find that the LLM feature-based model learns more robust prediction rules instead of relying on superficial shortcuts, which often trouble neural models.",
            "link": "https://www.semanticscholar.org/paper/670f9c71de3480d1ae4629b9e6b1775bb02e5ea4",
            "authors": "Lexin Zhou, Youmna Farag, Andreas Vlachos",
            "EMNLP Paper ID": "600",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "7c649618873fb6e72261183507a09324c722e788",
            "title": "InterIntent: Investigating Social Intelligence of LLMs via Intention Understanding in an Interactive Game Context",
            "abstract": "Large language models (LLMs) have demonstrated the potential to mimic human social intelligence. However, most studies focus on simplistic and static self-report or performance-based tests, which limits the depth and validity of the analysis. In this paper, we developed a novel framework, InterIntent, to assess LLMs' social intelligence by mapping their ability to understand and manage intentions in a game setting. We focus on four dimensions of social intelligence: situational awareness, self-regulation, self-awareness, and theory of mind. Each dimension is linked to a specific game task: intention selection, intention following, intention summarization, and intention guessing. Our findings indicate that while LLMs exhibit high proficiency in selecting intentions, achieving an accuracy of 88%, their ability to infer the intentions of others is significantly weaker, trailing human performance by 20%. Additionally, game performance correlates with intention understanding, highlighting the importance of the four components towards success in this game. These findings underline the crucial role of intention understanding in evaluating LLMs' social intelligence and highlight the potential of using social deduction games as a complex testbed to enhance LLM evaluation. InterIntent contributes a structured approach to bridging the evaluation gap in social intelligence within multiplayer games.",
            "link": "https://www.semanticscholar.org/paper/7c649618873fb6e72261183507a09324c722e788",
            "authors": "Ziyi Liu, Abhishek Anand, Pei Zhou, Jen-tse Huang, Jieyu Zhao",
            "EMNLP Paper ID": "750",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "db441be2cad1a31d44e34f0f1dcc10ffbd6fc666",
            "title": "Do LLMs suffer from Multi-Party Hangover? A Diagnostic Approach to Addressee Recognition and Response Selection in Conversations",
            "abstract": "Assessing the performance of systems to classify Multi-Party Conversations (MPC) is challenging due to the interconnection between linguistic and structural characteristics of conversations. Conventional evaluation methods often overlook variances in model behavior across different levels of structural complexity on interaction graphs. In this work, we propose a methodological pipeline to investigate model performance across specific structural attributes of conversations. As a proof of concept we focus on Response Selection and Addressee Recognition tasks, to diagnose model weaknesses. To this end, we extract representative diagnostic subdatasets with a fixed number of users and a good structural variety from a large and open corpus of online MPCs. We further frame our work in terms of data minimization, avoiding the use of original usernames to preserve privacy, and propose alternatives to using original text messages. Results show that response selection relies more on the textual content of conversations, while addressee recognition requires capturing their structural dimension. Using an LLM in a zero-shot setting, we further highlight how sensitivity to prompt variations is task-dependent.",
            "link": "https://www.semanticscholar.org/paper/db441be2cad1a31d44e34f0f1dcc10ffbd6fc666",
            "authors": "Nicolo Penzo, Maryam Sajedinia, Bruno Lepri, Sara Tonelli, Marco Guerini",
            "EMNLP Paper ID": "1301",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "f1d7d80f26e469fa05faf85666c071d62561463b",
            "title": "ACE: A LLM-based Negotiation Coaching System",
            "abstract": "The growing prominence of LLMs has led to an increase in the development of AI tutoring systems. These systems are crucial in providing underrepresented populations with improved access to valuable education. One important area of education that is unavailable to many learners is strategic bargaining related to negotiation. To address this, we develop a LLM-based Assistant for Coaching nEgotiation (ACE). ACE not only serves as a negotiation partner for users but also provides them with targeted feedback for improvement. To build our system, we collect a dataset of negotiation transcripts between MBA students. These transcripts come from trained negotiators and emulate realistic bargaining scenarios. We use the dataset, along with expert consultations, to design an annotation scheme for detecting negotiation mistakes. ACE employs this scheme to identify mistakes and provide targeted feedback to users. To test the effectiveness of ACE-generated feedback, we conducted a user experiment with two consecutive trials of negotiation and found that it improves negotiation performances significantly compared to a system that doesn't provide feedback and one which uses an alternative method of providing feedback.",
            "link": "https://www.semanticscholar.org/paper/f1d7d80f26e469fa05faf85666c071d62561463b",
            "authors": "Ryan Shea, Aymen Kallala, Xin Lucy Liu, Michael W. Morris, Zhou Yu",
            "EMNLP Paper ID": "1479",
            "Oral/Poster": "Oral",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "6860e27d1731fd88b639589592afe1fb01633a9d",
            "title": "Advancing Social Intelligence in AI Agents: Technical Challenges and Open Questions",
            "abstract": "Building socially-intelligent AI agents (Social-AI) is a multidisciplinary, multimodal research goal that involves creating agents that can sense, perceive, reason about, learn from, and respond to affect, behavior, and cognition of other agents (human or artificial). Progress towards Social-AI has accelerated in the past decade across several computing communities, including natural language processing, machine learning, robotics, human-machine interaction, computer vision, and speech. Natural language processing, in particular, has been prominent in Social-AI research, as language plays a key role in constructing the social world. In this position paper, we identify a set of underlying technical challenges and open questions for researchers across computing communities to advance Social-AI. We anchor our discussion in the context of social intelligence concepts and prior progress in Social-AI research.",
            "link": "https://www.semanticscholar.org/paper/6860e27d1731fd88b639589592afe1fb01633a9d",
            "authors": "Leena Mathur, P. Liang, Louis-Philippe Morency",
            "EMNLP Paper ID": "2703",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "bfc2aee63d20fb19c9a851da9e97fec40c454124",
            "title": "Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs",
            "abstract": "Recent advances in large language models (LLM) have enabled richer social simulations, allowing for the study of various social phenomena. However, most recent work has used a more omniscient perspective on these simulations (e.g., single LLM to generate all interlocutors), which is fundamentally at odds with the non-omniscient, information asymmetric interactions that involve humans and AI agents in the real world. To examine these differences, we develop an evaluation framework to simulate social interactions with LLMs in various settings (omniscient, non-omniscient). Our experiments show that LLMs perform better in unrealistic, omniscient simulation settings but struggle in ones that more accurately reflect real-world conditions with information asymmetry. Our findings indicate that addressing information asymmetry remains a fundamental challenge for LLM-based agents.",
            "link": "https://www.semanticscholar.org/paper/bfc2aee63d20fb19c9a851da9e97fec40c454124",
            "authors": "Xuhui Zhou, Zhe Su, Tiwalayo Eisape, Hyunwoo Kim, Maarten Sap",
            "EMNLP Paper ID": "2986",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "bb5a01d4a73ecc3988f8605f7786be12f2dfee3f",
            "title": "A Fairness-Driven Method for Learning Human-Compatible Negotiation Strategies",
            "abstract": "Despite recent advancements in AI and NLP, negotiation remains a difficult domain for AI agents. Traditional game theoretic approaches that have worked well for two-player zero-sum games struggle in the context of negotiation due to their inability to learn human-compatible strategies. On the other hand, approaches that only use human data tend to be domain-specific and lack the theoretical guarantees provided by strategies grounded in game theory. Motivated by the notion of fairness as a criterion for optimality in general sum games, we propose a negotiation framework called FDHC which incorporates fairness into both the reward design and search to learn human-compatible negotiation strategies. Our method includes a novel, RL+search technique called LGM-Zero which leverages a pre-trained language model to retrieve human-compatible offers from large action spaces. Our results show that our method is able to achieve more egalitarian negotiation outcomes and improve negotiation quality.",
            "link": "https://www.semanticscholar.org/paper/bb5a01d4a73ecc3988f8605f7786be12f2dfee3f",
            "authors": "Ryan Shea, Zhou Yu",
            "matchScore": 251.91302,
            "original title": "A Fairness-Driven Method for Learning Human-Compatible Negotiation Strategies",
            "original authors": "Ryan Shea, Zhou Yu",
            "EMNLP Paper ID": "1075",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "4e61fa433eb1a905309f5955a75235e74f7eb6b2",
            "title": "Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues",
            "abstract": "A successful negotiation requires a range of capabilities, including comprehension of the conversation context, Theory-of-Mind (ToM) skills to infer the partner's motives, strategic reasoning, and effective communication, making it challenging for automated systems. Despite the remarkable performance of LLMs in various NLP tasks, there is no systematic evaluation of their capabilities in negotiation. Such an evaluation is critical for advancing AI negotiation agents and negotiation research, ranging from designing dialogue systems to providing pedagogical feedback and scaling up data collection practices. This work aims to systematically analyze the multifaceted capabilities of LLMs across diverse dialogue scenarios throughout the stages of a typical negotiation interaction. Our analysis highlights GPT-4's superior performance in many tasks while identifying specific challenges, such as making subjective assessments and generating contextually appropriate, strategically advantageous responses.",
            "link": "https://www.semanticscholar.org/paper/4e61fa433eb1a905309f5955a75235e74f7eb6b2",
            "authors": "Deuksin Kwon, Emily Weiss, Tara Kulshrestha, Kushal Chawla, Gale M. Lucas, Jonathan Gratch",
            "matchScore": 355.18652,
            "original title": "Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues",
            "original authors": "Deuksin Kwon, Emily Weiss, Tara Kulshrestha, Kushal Chawla, Gale Lucas, Jonathan Gratch",
            "EMNLP Paper ID": "1084",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "0d370b2dde53ca0fc43d3dfefd5f032952fcb3c5",
            "title": "Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues",
            "abstract": "We develop assistive agents based on Large Language Models (LLMs) that aid interlocutors in business negotiations. Specifically, we simulate business negotiations by letting two LLM-based agents engage in role play. A third LLM acts as a remediator agent to rewrite utterances violating norms for improving negotiation outcomes. We introduce a simple tuning-free and label-free In-Context Learning (ICL) method to identify high-quality ICL exemplars for the remediator, where we propose a novel select criteria, called value impact, to measure the quality of the negotiation outcomes. We provide rich empirical evidence to demonstrate its effectiveness in negotiations across three different negotiation topics. The source code and the generated dataset will be publicly available upon acceptance.",
            "link": "https://www.semanticscholar.org/paper/0d370b2dde53ca0fc43d3dfefd5f032952fcb3c5",
            "authors": "Yuncheng Hua, Lizhen Qu, Gholamreza Haffari",
            "matchScore": 252.82486,
            "original title": "Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues",
            "original authors": "YUNCHENG HUA, Lizhen Qu, Reza Haf",
            "EMNLP Paper ID": "1697",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "7e1cccd15c0f5aa511495abe0e0757a8f2962a89",
            "title": "How Personality Traits Influence Negotiation Outcomes? A Simulation based on Large Language Models",
            "abstract": "Psychological evidence reveals the influence of personality traits on decision-making. For instance, agreeableness is generally associated with positive outcomes in negotiations, whereas neuroticism is often linked to less favorable outcomes. This paper introduces a simulation framework centered on Large Language Model (LLM) agents endowed with synthesized personality traits. The agents negotiate within bargaining domains and possess customizable personalities and objectives. The experimental results show that the behavioral tendencies of LLM-based simulations could reproduce behavioral patterns observed in human negotiations. The contribution is twofold. First, we propose a simulation methodology that investigates the alignment between the linguistic and economic capabilities of LLM agents. Secondly, we offer empirical insights into the strategic impact of Big-Five personality traits on the outcomes of bilateral negotiations. We also provide a case study based on synthesized bargaining dialogues to reveal intriguing behaviors, including deceitful and compromising behaviors.",
            "link": "https://www.semanticscholar.org/paper/7e1cccd15c0f5aa511495abe0e0757a8f2962a89",
            "authors": "Yin Jou Huang, Rafik Hadfi",
            "matchScore": 242.52698,
            "original title": "How Personality Traits Influence Negotiation Outcomes? A Simulation based on Large Language Models",
            "original authors": "Yin Jou Huang, Rafik Hadfi",
            "EMNLP Paper ID": "2106",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "04e5c40f897098d1781e2d6ee721f5fafcbf0417",
            "title": "Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks",
            "abstract": "Creating human-like large language model (LLM) agents is crucial for faithful social simulation. Having LLMs role-play based on demographic information sometimes improves human likeness but often does not. This study assessed whether LLM alignment with human behavior can be improved by integrating information from empirically-derived human belief networks. Using data from a human survey, we estimated a belief network encompassing 64 topics loading on nine non-overlapping latent factors. We then seeded LLM-based agents with an opinion on one topic, and assessed the alignment of its expressed opinions on remaining test topics with corresponding human data. Role-playing based on demographic information alone did not align LLM and human opinions, but seeding the agent with a single belief greatly improved alignment for topics related in the belief network, and not for topics outside the network. These results suggest a novel path for human-LLM belief alignment in work seeking to simulate and understand patterns of belief distributions in society.",
            "link": "https://www.semanticscholar.org/paper/04e5c40f897098d1781e2d6ee721f5fafcbf0417",
            "authors": "Yun-Shiuan Chuang, Zach Studdiford, Krirk Nirunwiroj, Agam Goyal, Vincent V. Frigo, Sijia Yang, Dhavan Shah, Junjie Hu, Timothy T. Rogers",
            "matchScore": 289.00308,
            "original title": "Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks",
            "original authors": "Yun-Shiuan Chuang, Zach Studdiford, Krirk Nirunwiroj, Agam Goyal, Vincent V. Frigo, Sijia Yang, Dhavan V. Shah, Junjie Hu, Timothy T. Rogers",
            "EMNLP Paper ID": "2716",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "5dcc26649414295ec3d1d9a274d41b2759e53f8e",
            "title": "Language Models in Dialogue: Conversational Maxims for Human-AI Interactions",
            "abstract": "Modern language models, while sophisticated, exhibit some inherent shortcomings, particularly in conversational settings. We claim that many of the observed shortcomings can be attributed to violation of one or more conversational principles. By drawing upon extensive research from both the social science and AI communities, we propose a set of maxims -- quantity, quality, relevance, manner, benevolence, and transparency -- for describing effective human-AI conversation. We first justify the applicability of the first four maxims (from Grice) in the context of human-AI interactions. We then argue that two new maxims, benevolence (concerning the generation of, and engagement with, harmful content) and transparency (concerning recognition of one's knowledge boundaries, operational constraints, and intents), are necessary for addressing behavior unique to modern human-AI interactions. We evaluate the degree to which various language models are able to understand these maxims and find that models possess an internal prioritization of principles that can significantly impact their ability to interpret the maxims accurately.",
            "link": "https://www.semanticscholar.org/paper/5dcc26649414295ec3d1d9a274d41b2759e53f8e",
            "authors": "Erik Miehling, Manish Nagireddy, P. Sattigeri, Elizabeth M. Daly, David Piorkowski, John T. Richards",
            "matchScore": 243.1255,
            "original title": "Language Models in Dialogue: Conversational Maxims for Human-AI Interactions",
            "original authors": "Erik Miehling, Manish Nagireddy, Prasanna Sattigeri, Elizabeth M. Daly, David Piorkowski, John T. Richards",
            "EMNLP Paper ID": "2787",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Findings"
        }
    ],
    "Advanced Methods for Text Generation with Language Models": [
        {
            "paperId": "63161be2bd80005ffe5eff673994978b7ea9a4cb",
            "title": "RSA-Control: A Pragmatics-Grounded Lightweight Controllable Text Generation Framework",
            "abstract": "Despite significant advancements in natural language generation, controlling language models to produce texts with desired attributes remains a formidable challenge. In this work, we introduce RSA-Control, a training-free controllable text generation framework grounded in pragmatics. RSA-Control directs the generation process by recursively reasoning between imaginary speakers and listeners, enhancing the likelihood that target attributes are correctly interpreted by listeners amidst distractors. Additionally, we introduce a self-adjustable rationality parameter, which allows for automatic adjustment of control strength based on context. Our experiments, conducted with two task types and two types of language models, demonstrate that RSA-Control achieves strong attribute control while maintaining language fluency and content consistency. Our code is available at https://github.com/Ewanwong/RSA-Control.",
            "link": "https://www.semanticscholar.org/paper/63161be2bd80005ffe5eff673994978b7ea9a4cb",
            "authors": "Yifan Wang, Vera Demberg",
            "EMNLP Paper ID": "620",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "7348e050ea2f58494bad3389550c27f465f93b1d",
            "title": "Explaining and Improving Contrastive Decoding by Extrapolating the Probabilities of a Huge and Hypothetical LM",
            "abstract": "Contrastive decoding (CD) (Li et al., 2023) improves the next-token distribution of a large expert language model (LM) using a small amateur LM. Although CD is applied to various LMs and domains to enhance open-ended text generation, it is still unclear why CD often works well, when it could fail, and how we can make it better. To deepen our understanding of CD, we first theoretically prove that CD could be viewed as linearly extrapolating the next-token logits from a huge and hypothetical LM. We also highlight that the linear extrapolation could make CD unable to output the most obvious answers that have already been assigned high probabilities by the amateur LM. To overcome CD\u2019s limitation, we propose a new unsupervised decoding method called A symptotic P robability D ecoding (APD). 1 APD explicitly extrapolates the probability curves from the LMs of different sizes to infer the asymptotic probabilities from an infinitely large LM without inducing more inference costs than CD. In F ACTUALITY P ROMPTS , an open-ended text generation benchmark, sampling using APD significantly boosts factuality in comparison to the CD sampling and its variants, and achieves state-of-the-art results for Pythia 6.9B and OPT 6.7B. Furthermore, in five commonsense QA datasets, APD is often significantly better than CD and achieves a similar effect of using a larger LLM. For example, the perplexity of APD on top of Pythia 6.9B is even lower than the perplexity of Pythia 12B in CommonsenseQA and LAMBADA",
            "link": "https://www.semanticscholar.org/paper/7348e050ea2f58494bad3389550c27f465f93b1d",
            "authors": "Haw-Shiuan Chang, Nanyun Peng, Mohit Bansal, Anil Ramakrishna, Tagyoung Chung, Ouyang Long, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welin-der, Paul F. Christiano, Jan Leike, Ryan Lowe, Denis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazari-dou, Ngoc Quan Pham, R. Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda",
            "EMNLP Paper ID": "982",
            "Oral/Poster": "Oral",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "a41fc0be1e63dc1a272ba3fbf245efd6d3d04d69",
            "title": "KARL: Knowledge-Aware Retrieval and Representations aid Retention and Learning in Students",
            "abstract": "Flashcard schedulers rely on 1) student models to predict the flashcards a student knows; and 2) teaching policies to pick which cards to show next via these predictions. Prior student models, however, just use study data like the student's past responses, ignoring the text on cards. We propose content-aware scheduling, the first schedulers exploiting flashcard content. To give the first evidence that such schedulers enhance student learning, we build KARL, a simple but effective content-aware student model employing deep knowledge tracing (DKT), retrieval, and BERT to predict student recall. We train KARL by collecting a new dataset of 123,143 study logs on diverse trivia questions. KARL bests existing student models in AUC and calibration error. To ensure our improved predictions lead to better student learning, we create a novel delta-based teaching policy to deploy KARL online. Based on 32 study paths from 27 users, KARL improves learning efficiency over SOTA, showing KARL's strength and encouraging researchers to look beyond historical study data to fully capture student abilities.",
            "link": "https://www.semanticscholar.org/paper/a41fc0be1e63dc1a272ba3fbf245efd6d3d04d69",
            "authors": "Matthew Shu, Nishant Balepur, Shi Feng, Jordan L. Boyd-Graber",
            "EMNLP Paper ID": "1636",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "daa171a25956b537b222a564c1488b2b6cfbb6bb",
            "title": "Unlocking Anticipatory Text Generation: A Constrained Approach for Large Language Models Decoding",
            "abstract": "Large Language Models (LLMs) have demonstrated a powerful ability for text generation. However, achieving optimal results with a given prompt or instruction can be challenging, especially for billion-sized models. Additionally, undesired behaviors such as toxicity or hallucinations can manifest. While much larger models (e.g., ChatGPT) may demonstrate strength in mitigating these issues, there is still no guarantee of complete prevention. In this work, we propose formalizing text generation as a future-constrained generation problem to minimize undesirable behaviors and enforce faithfulness to instructions. The estimation of future constraint satisfaction, accomplished using LLMs, guides the text generation process. Our extensive experiments demonstrate the effectiveness of the proposed approach across three distinct text generation tasks: keyword-constrained generation (Lin et al., 2020), toxicity reduction (Gehman et al., 2020), and factual correctness in question-answering (Gao et al., 2023).",
            "link": "https://www.semanticscholar.org/paper/daa171a25956b537b222a564c1488b2b6cfbb6bb",
            "authors": "Lifu Tu, Semih Yavuz, Jin Qu, Jiacheng Xu, Rui Meng, Caiming Xiong, Yingbo Zhou",
            "EMNLP Paper ID": "1825",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "69aa6f6976d71e9f31b0488c5d77eaf5f7b8f173",
            "title": "Label Confidence Weighted Learning for Target-level Sentence Simplification",
            "abstract": "Multi-level sentence simplification generates simplified sentences with varying language proficiency levels. We propose Label Confidence Weighted Learning (LCWL), a novel approach that incorporates a label confidence weighting scheme in the training loss of the encoder-decoder model, setting it apart from existing confidence-weighting methods primarily designed for classification. Experimentation on English grade-level simplification dataset shows that LCWL outperforms state-of-the-art unsupervised baselines. Fine-tuning the LCWL model on in-domain data and combining with Symmetric Cross Entropy (SCE) consistently delivers better simplifications compared to strong supervised methods. Our results highlight the effectiveness of label confidence weighting techniques for text simplification tasks with encoder-decoder architectures.",
            "link": "https://www.semanticscholar.org/paper/69aa6f6976d71e9f31b0488c5d77eaf5f7b8f173",
            "authors": "Xinying Qiu, Jingshen Zhang",
            "EMNLP Paper ID": "2199",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "4c3a076ba646fcb851efe42f6e3c62c3b4e9c24b",
            "title": "FANTAstic SEquences and Where to Find Them: Faithful and Efficient API Call Generation through State-tracked Constrained Decoding and Reranking",
            "abstract": "API call generation is the cornerstone of large language models' tool-using ability that provides access to the larger world. However, existing supervised and in-context learning approaches suffer from high training costs, poor data efficiency, and generated API calls that can be unfaithful to the API documentation and the user's request. To address these limitations, we propose an output-side optimization approach called FANTASE. Two of the unique contributions of FANTASE are its State-Tracked Constrained Decoding (SCD) and Reranking components. SCD dynamically incorporates appropriate API constraints in the form of Token Search Trie for efficient and guaranteed generation faithfulness with respect to the API documentation. The Reranking component efficiently brings in the supervised signal by leveraging a lightweight model as the discriminator to rerank the beam-searched candidate generations of the large language model. We demonstrate the superior performance of FANTASE in API call generation accuracy, inference efficiency, and context efficiency with DSTC8 and API Bank datasets.",
            "link": "https://www.semanticscholar.org/paper/4c3a076ba646fcb851efe42f6e3c62c3b4e9c24b",
            "authors": "Zhuoer Wang, Leonardo F. R. Ribeiro, Alexandros Papangelis, Rohan Mukherjee, Tzu-Yen Wang, Xinyan Zhao, Arijit Biswas, James Caverlee, A. Metallinou",
            "matchScore": 433.76874,
            "original title": "FANTAstic SEquences and Where to Find Them: Faithful and Efficient API Call Generation through State-tracked Constrained Decoding and Reranking",
            "original authors": "Zhuoer Wang, Leonardo F. R. Ribeiro, Alexandros Papangelis, Rohan Mukherjee, Tzu-Yen Wang, Xinyan Zhao, Arijit Biswas, James Caverlee, Angeliki Metallinou",
            "EMNLP Paper ID": "1280",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "ed661f001edad1afc203844b225230bd8de4fb19",
            "title": "Edit-Constrained Decoding for Sentence Simplification",
            "abstract": "We propose edit operation based lexically constrained decoding for sentence simplification. In sentence simplification, lexical paraphrasing is one of the primary procedures for rewriting complex sentences into simpler correspondences. While previous studies have confirmed the efficacy of lexically constrained decoding on this task, their constraints can be loose and may lead to sub-optimal generation. We address this problem by designing constraints that replicate the edit operations conducted in simplification and defining stricter satisfaction conditions. Our experiments indicate that the proposed method consistently outperforms the previous studies on three English simplification corpora commonly used in this task.",
            "link": "https://www.semanticscholar.org/paper/ed661f001edad1afc203844b225230bd8de4fb19",
            "authors": "Tatsuya Zetsu, Yuki Arase, Tomoyuki Kajiwara",
            "matchScore": 204.27628,
            "original title": "Edit-Constrained Decoding for Sentence Simplification",
            "original authors": "Tatsuya Zetsu, Yuki Arase, Tomoyuki Kajiwara",
            "EMNLP Paper ID": "1452",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "40fb258232e5cd8e4c68b10064de45e448c2725a",
            "title": "Local and Global Decoding in Text Generation",
            "abstract": "Text generation, a key component in applications such as dialogue systems, relies on decoding algorithms that sample strings from a language model distribution. Traditional methods, such as top-$k$ and top-$\\pi$, apply local normalisation to the model's output distribution, which can distort it. In this paper, we investigate the effect of this distortion by introducing globally-normalised versions of these decoding methods. Additionally, we propose an independent Metropolis-Hastings algorithm to approximate sampling from globally-normalised distributions without explicitly computing them. Our empirical analysis compares the performance of local and global normalisation across two decoding algorithms (top-$k$ and top-$\\pi$) with various hyperparameters, using Pythia language models. Results show that, in most configurations, global decoding performs worse than the local decoding version of the same algorithms -- despite preserving the distribution's integrity. Our results suggest that distortion is an important feature of local decoding algorithms.",
            "link": "https://www.semanticscholar.org/paper/40fb258232e5cd8e4c68b10064de45e448c2725a",
            "authors": "Daniel Gareev, Thomas Hofmann, Ezhilmathi Krishnasamy, Tiago Pimentel",
            "matchScore": 175.48567,
            "original title": "Local and Global Decoding in Text Generation",
            "original authors": "Daniel Gareev, Thomas Hofmann, ezhilmathi krishnasamy, Tiago Pimentel",
            "EMNLP Paper ID": "2815",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "343581f6521603fb56e2f44ae6fcadea9cc54333",
            "title": "Adaptive Contrastive Search: Uncertainty-Guided Decoding for Open-Ended Text Generation",
            "abstract": "Decoding from the output distributions of large language models to produce high-quality text is a complex challenge in language modeling. Various approaches, such as beam search, sampling with temperature, $k-$sampling, nucleus $p-$sampling, typical decoding, contrastive decoding, and contrastive search, have been proposed to address this problem, aiming to improve coherence, diversity, as well as resemblance to human-generated text. In this study, we introduce adaptive contrastive search, a novel decoding strategy extending contrastive search by incorporating an adaptive degeneration penalty, guided by the estimated uncertainty of the model at each generation step. This strategy is designed to enhance both the creativity and diversity of the language modeling process while at the same time producing coherent and high-quality generated text output. Our findings indicate performance enhancement in both aspects, across different model architectures and datasets, underscoring the effectiveness of our method in text generation tasks. Our code base, datasets, and models are publicly available.",
            "link": "https://www.semanticscholar.org/paper/343581f6521603fb56e2f44ae6fcadea9cc54333",
            "authors": "Esteban Garces Arias, Julian Rodemann, Meimingwei Li, Christian Heumann, M. A\u00dfenmacher",
            "matchScore": 311.65796,
            "original title": "Adaptive Contrastive Search: Uncertainty-Guided Decoding for Open-Ended Text Generation",
            "original authors": "Esteban Garces Arias, Julian Rodemann, Meimingwei Li, Christian Heumann, Matthias A\u00dfenmacher",
            "EMNLP Paper ID": "2888",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "1d9c539eb54aee46fdb93a3f9798cb4cacd40bbe",
            "title": "Breaking the Ceiling of the LLM Community by Treating Token Generation as a Classification for Ensembling",
            "abstract": "Ensembling multiple models has always been an effective approach to push the limits of existing performance and is widely used in classification tasks by simply averaging the classification probability vectors from multiple classifiers to achieve better accuracy. However, in the thriving open-source Large Language Model (LLM) community, ensembling methods are rare and typically limited to ensembling the full-text outputs of LLMs, such as selecting the best output using a ranker, which leads to underutilization of token-level probability information. In this paper, we treat the Generation of each token by LLMs as a Classification (GaC) for ensembling. This approach fully exploits the probability information at each generation step and better prevents LLMs from producing early incorrect tokens that lead to snowballing errors. In experiments, we ensemble state-of-the-art LLMs on several benchmarks, including exams, mathematics and reasoning, and observe that our method breaks the existing community performance ceiling. Furthermore, we observed that most of the tokens in the answer are simple and do not affect the correctness of the final answer. Therefore, we also experimented with ensembling only key tokens, and the results showed better performance with lower latency across benchmarks.",
            "link": "https://www.semanticscholar.org/paper/1d9c539eb54aee46fdb93a3f9798cb4cacd40bbe",
            "authors": "Yao-Ching Yu, Chun-Chih Kuo, Ziqi Ye, Yu-Cheng Chang, Yueh-Se Li",
            "matchScore": 284.30426,
            "original title": "Breaking the Ceiling of the LLM Community by Treating Token Generation as a Classification for Ensembling",
            "original authors": "Yao-Ching Yu, Chun Chih Kuo, Ye Ziqi, CHANG YUCHENG, Yueh-Se Li",
            "EMNLP Paper ID": "368",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "915539c51252649cf7d09e25dea3af199235ebed",
            "title": "Adaptive Contrastive Decoding in Retrieval-Augmented Generation for Handling Noisy Contexts",
            "abstract": "When using large language models (LLMs) in knowledge-intensive tasks, such as open-domain question answering, external context can bridge the gap between external knowledge and the LLMs' parametric knowledge. Recent research has been developed to amplify contextual knowledge over the parametric knowledge of LLMs with contrastive decoding approaches. While these approaches could yield truthful responses when relevant context is provided, they are prone to vulnerabilities when faced with noisy contexts. We extend the scope of previous studies to encompass noisy contexts and propose adaptive contrastive decoding (ACD) to leverage contextual influence effectively. ACD demonstrates improvements in open-domain question answering tasks compared to baselines, especially in robustness by remaining undistracted by noisy contexts in retrieval-augmented generation.",
            "link": "https://www.semanticscholar.org/paper/915539c51252649cf7d09e25dea3af199235ebed",
            "authors": "Youna Kim, Hyuhng Joon Kim, Cheonbok Park, Choonghyun Park, Hyunsoo Cho, Junyeob Kim, Kang Min Yoo, Sang-goo Lee, Taeuk Kim",
            "matchScore": 301.0019,
            "original title": "Adaptive Contrastive Decoding in Retrieval-Augmented Generation for Handling Noisy Contexts",
            "original authors": "Youna Kim, Hyuhng Joon Kim, Cheonbok Park, Choonghyun Park, Hyunsoo Cho, Junyeob Kim, Kang Min Yoo, Sang-goo Lee, Taeuk Kim",
            "EMNLP Paper ID": "499",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "0ebbd9526f6b382c34b5a8dd97323de37ad07805",
            "title": "Ruler: A Model-Agnostic Method to Control Generated Length for Large Language Models",
            "abstract": "The instruction-following ability of large language models enables humans to interact with AI agents in a natural way. However, when required to generate responses of a specific length, large language models often struggle to meet users' needs due to their inherent difficulty in accurately perceiving numerical constraints. To explore the ability of large language models to control the length of generated responses, we propose the Target Length Generation Task (TLG) and design two metrics, Precise Match (PM) and Flexible Match (FM) to evaluate the model's performance in adhering to specified response lengths. Furthermore, we introduce a novel, model-agnostic approach called Ruler, which employs Meta Length Tokens (MLTs) to enhance the instruction-following ability of large language models under length-constrained instructions. Specifically, Ruler equips LLMs with the ability to generate responses of a specified length based on length constraints within the instructions. Moreover, Ruler can automatically generate appropriate MLT when length constraints are not explicitly provided, demonstrating excellent versatility and generalization. Comprehensive experiments show the effectiveness of Ruler across different LLMs on Target Length Generation Task, e.g., at All Level 27.97 average gain on PM, 29.57 average gain on FM. In addition, we conduct extensive ablation experiments to further substantiate the efficacy and generalization of Ruler. Our code and data is available at https://github.com/Geaming2002/Ruler.",
            "link": "https://www.semanticscholar.org/paper/0ebbd9526f6b382c34b5a8dd97323de37ad07805",
            "authors": "Jiaming Li, Lei Zhang, Yunshui Li, Ziqiang Liu, yuelin bai, Run Luo, Longze Chen, Min Yang",
            "matchScore": 257.04065,
            "original title": "Ruler: A Model-Agnostic Method to Control Generated Length for Large Language Models",
            "original authors": "Jiaming Li, Lei Zhang, Yunshui Li, Ziqiang Liu, yuelin bai, Run Luo, Longze Chen, Min Yang",
            "EMNLP Paper ID": "616",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        }
    ],
    "Advancing Legal Natural Language Processing with Large Language Models": [
        {
            "paperId": "14ceaf4fda4e7988c3c6c63436a8f0c67b9ec698",
            "title": "Learning Interpretable Legal Case Retrieval via Knowledge-Guided Case Reformulation",
            "abstract": "Legal case retrieval for sourcing similar cases is critical in upholding judicial fairness. Different from general web search, legal case retrieval involves processing lengthy, complex, and highly specialized legal documents. Existing methods in this domain often overlook the incorporation of legal expert knowledge, which is crucial for accurately understanding and modeling legal cases, leading to unsatisfactory retrieval performance. This paper introduces KELLER, a legal knowledge-guided case reformulation approach based on large language models (LLMs) for effective and interpretable legal case retrieval. By incorporating professional legal knowledge about crimes and law articles, we enable large language models to accurately reformulate the original legal case into concise sub-facts of crimes, which contain the essential information of the case. Extensive experiments on two legal case retrieval benchmarks demonstrate superior retrieval performance and robustness on complex legal case queries of KELLER over existing methods.",
            "link": "https://www.semanticscholar.org/paper/14ceaf4fda4e7988c3c6c63436a8f0c67b9ec698",
            "authors": "Chenlong Deng, Kelong Mao, Zhicheng Dou",
            "EMNLP Paper ID": "149",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "bdf2e3ea5a69a0fb1a4d8d1f08ac3df5191e022c",
            "title": "Enhancing Legal Case Retrieval via Scaling High-quality Synthetic Query-Candidate Pairs",
            "abstract": "Legal case retrieval (LCR) aims to provide similar cases as references for a given fact description. This task is crucial for promoting consistent judgments in similar cases, effectively enhancing judicial fairness and improving work efficiency for judges. However, existing works face two main challenges for real-world applications: existing works mainly focus on case-to-case retrieval using lengthy queries, which does not match real-world scenarios; and the limited data scale, with current datasets containing only hundreds of queries, is insufficient to satisfy the training requirements of existing data-hungry neural models. To address these issues, we introduce an automated method to construct synthetic query-candidate pairs and build the largest LCR dataset to date, LEAD, which is hundreds of times larger than existing datasets. This data construction method can provide ample training signals for LCR models. Experimental results demonstrate that model training with our constructed data can achieve state-of-the-art results on two widely-used LCR benchmarks. Besides, the construction method can also be applied to civil cases and achieve promising results. The data and codes can be found in https://github.com/thunlp/LEAD.",
            "link": "https://www.semanticscholar.org/paper/bdf2e3ea5a69a0fb1a4d8d1f08ac3df5191e022c",
            "authors": "Cheng Gao, Chaojun Xiao, Zhenghao Liu, Huimin Chen, Zhiyuan Liu, Maosong Sun",
            "EMNLP Paper ID": "796",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "9099ee08e59cc33ed1c88d4708cf5c931bf46dc4",
            "title": "LawBench: Benchmarking Legal Knowledge of Large Language Models",
            "abstract": "Large language models (LLMs) have demonstrated strong capabilities in various aspects. However, when applying them to the highly specialized, safe-critical legal domain, it is unclear how much legal knowledge they possess and whether they can reliably perform legal-related tasks. To address this gap, we propose a comprehensive evaluation benchmark LawBench. LawBench has been meticulously crafted to have precise assessment of the LLMs' legal capabilities from three cognitive levels: (1) Legal knowledge memorization: whether LLMs can memorize needed legal concepts, articles and facts; (2) Legal knowledge understanding: whether LLMs can comprehend entities, events and relationships within legal text; (3) Legal knowledge applying: whether LLMs can properly utilize their legal knowledge and make necessary reasoning steps to solve realistic legal tasks. LawBench contains 20 diverse tasks covering 5 task types: single-label classification (SLC), multi-label classification (MLC), regression, extraction and generation. We perform extensive evaluations of 51 LLMs on LawBench, including 20 multilingual LLMs, 22 Chinese-oriented LLMs and 9 legal specific LLMs. The results show that GPT-4 remains the best-performing LLM in the legal domain, surpassing the others by a significant margin. While fine-tuning LLMs on legal specific text brings certain improvements, we are still a long way from obtaining usable and reliable LLMs in legal tasks. All data, model predictions and evaluation code are released in https://github.com/open-compass/LawBench/. We hope this benchmark provides in-depth understanding of the LLMs' domain-specified capabilities and speed up the development of LLMs in the legal domain.",
            "link": "https://www.semanticscholar.org/paper/9099ee08e59cc33ed1c88d4708cf5c931bf46dc4",
            "authors": "Zhiwei Fei, Xiaoyu Shen, D. Zhu, Fengzhe Zhou, Zhuo Han, Songyang Zhang, Kai Chen, Zongwen Shen, Jidong Ge",
            "EMNLP Paper ID": "907",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 14): 13:00\u201314:00 (Afternoon)",
            "Type": "Main"
        },
        {
            "paperId": "b8d01e6fb377b1909ed1d7386bc322a580162cb0",
            "title": "Developing a Pragmatic Benchmark for Assessing Korean Legal Language Understanding in Large Language Models",
            "abstract": "Large language models (LLMs) have demonstrated remarkable performance in the legal domain, with GPT-4 even passing the Uniform Bar Exam in the U.S. However their efficacy remains limited for non-standardized tasks and tasks in languages other than English. This underscores the need for careful evaluation of LLMs within each legal system before application. Here, we introduce KBL, a benchmark for assessing the Korean legal language understanding of LLMs, consisting of (1) 7 legal knowledge tasks (510 examples), (2) 4 legal reasoning tasks (288 examples), and (3) the Korean bar exam (4 domains, 53 tasks, 2,510 examples). First two datasets were developed in close collaboration with lawyers to evaluate LLMs in practical scenarios in a certified manner. Furthermore, considering legal practitioners' frequent use of extensive legal documents for research, we assess LLMs in both a closed book setting, where they rely solely on internal knowledge, and a retrieval-augmented generation (RAG) setting, using a corpus of Korean statutes and precedents. The results indicate substantial room and opportunities for improvement.",
            "link": "https://www.semanticscholar.org/paper/b8d01e6fb377b1909ed1d7386bc322a580162cb0",
            "authors": "Yeeun Kim, Young Rok Choi, Eunkyung Choi, Jinhwan Choi, Hai Jin Park, Wonseok Hwang",
            "matchScore": 276.69067,
            "original title": "Developing a Pragmatic Benchmark for Assessing Korean Legal Language Understanding in Large Language Models",
            "original authors": "Kimyeeun, Choi Youngrok, Eunkyung Choi, JinHwan Choi, Hai Jin Park, Wonseok Hwang",
            "EMNLP Paper ID": "1117",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "1fb3b1d922925dace350581139bdaca209565966",
            "title": "HiCuLR: Hierarchical Curriculum Learning for Rhetorical Role Labeling of Legal Documents",
            "abstract": "Rhetorical Role Labeling (RRL) of legal documents is pivotal for various downstream tasks such as summarization, semantic case search and argument mining. Existing approaches often overlook the varying difficulty levels inherent in legal document discourse styles and rhetorical roles. In this work, we propose HiCuLR, a hierarchical curriculum learning framework for RRL. It nests two curricula: Rhetorical Role-level Curriculum (RC) on the outer layer and Document-level Curriculum (DC) on the inner layer. DC categorizes documents based on their difficulty, utilizing metrics like deviation from a standard discourse structure and exposes the model to them in an easy-to-difficult fashion. RC progressively strengthens the model to discern coarse-to-fine-grained distinctions between rhetorical roles. Our experiments on four RRL datasets demonstrate the efficacy of HiCuLR, highlighting the complementary nature of DC and RC.",
            "link": "https://www.semanticscholar.org/paper/1fb3b1d922925dace350581139bdaca209565966",
            "authors": "Santosh T.Y.S.S, Apolline Isaia, Shiyu Hong, Matthias Grabmair",
            "matchScore": 296.4812,
            "original title": "HiCuLR: Hierarchical Curriculum Learning for Rhetorical Role Labeling of Legal Documents",
            "original authors": "Santosh T.Y.S.S, Apolline Isaia, Shiyu Hong, Matthias Grabmair",
            "EMNLP Paper ID": "1515",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "66b40ffcd351f9486cfcfca0bb02198817cba39f",
            "title": "Enabling Discriminative Reasoning in LLMs for Legal Judgment Prediction",
            "abstract": "Legal judgment prediction is essential for enhancing judicial efficiency. In this work, we identify that existing large language models (LLMs) underperform in this domain due to challenges in understanding case complexities and distinguishing between similar charges. To adapt LLMs for effective legal judgment prediction, we introduce the Ask-Discriminate-Predict (ADAPT) reasoning framework inspired by human judicial reasoning. ADAPT involves decomposing case facts, discriminating among potential charges, and predicting the final judgment. We further enhance LLMs through fine-tuning with multi-task synthetic trajectories to improve legal judgment prediction accuracy and efficiency under our ADAPT framework. Extensive experiments conducted on two widely-used datasets demonstrate the superior performance of our framework in legal judgment prediction, particularly when dealing with complex and confusing charges.",
            "link": "https://www.semanticscholar.org/paper/66b40ffcd351f9486cfcfca0bb02198817cba39f",
            "authors": "Chenlong Deng, Kelong Mao, Yuyao Zhang, Zhicheng Dou",
            "matchScore": 258.6663,
            "original title": "Enabling Discriminative Reasoning in LLMs for Legal Judgment Prediction",
            "original authors": "Chenlong Deng, Kelong Mao, Yuyao Zhang, Zhicheng Dou",
            "EMNLP Paper ID": "153",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "6a4e9df8f0f06713282d1c4d63dc053de450d1e7",
            "title": "Can Large Language Models Grasp Legal Theories? Enhance Legal Reasoning with Insights from Multi-Agent Collaboration",
            "abstract": "Large Language Models (LLMs) could struggle to fully understand legal theories and perform complex legal reasoning tasks. In this study, we introduce a challenging task (confusing charge prediction) to better evaluate LLMs' understanding of legal theories and reasoning capabilities. We also propose a novel framework: Multi-Agent framework for improving complex Legal Reasoning capability (MALR). MALR employs non-parametric learning, encouraging LLMs to automatically decompose complex legal tasks and mimic human learning process to extract insights from legal rules, helping LLMs better understand legal theories and enhance their legal reasoning abilities. Extensive experiments on multiple real-world datasets demonstrate that the proposed framework effectively addresses complex reasoning issues in practical scenarios, paving the way for more reliable applications in the legal domain.",
            "link": "https://www.semanticscholar.org/paper/6a4e9df8f0f06713282d1c4d63dc053de450d1e7",
            "authors": "Weikang Yuan, Junjie Cao, Zhuoren Jiang, Yangyang Kang, Jun Lin, Kaisong Song, Tianqianjin Lin, Pengwei Yan, Changlong Sun, Xiaozhong Liu",
            "matchScore": 357.9787,
            "original title": "Can Large Language Models Grasp Legal Theories? Enhance Legal Reasoning with Insights from Multi-Agent Collaboration",
            "original authors": "Weikang Yuan, Junjie Cao, Zhuoren Jiang, Yangyang Kang, Jun Lin, Kaisong Song, tianqianjin lin, Pengwei Yan, Changlong Sun, Xiaozhong Liu",
            "EMNLP Paper ID": "1582",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "79d72cf13cc86dd46498384b32c8ac77898a23f9",
            "title": "AgentsCourt: Building Judicial Decision-Making Agents with Court Debate Simulation and Legal Knowledge Augmentation",
            "abstract": "With the development of deep learning, natural language processing technology has effectively improved the efficiency of various aspects of the traditional judicial industry. However, most current efforts focus on tasks within individual judicial stages, making it difficult to handle complex tasks that span multiple stages. As the autonomous agents powered by large language models are becoming increasingly smart and able to make complex decisions in real-world settings, offering new insights for judicial intelligence. In this paper, (1) we propose a novel multi-agent framework, AgentsCourt, for judicial decision-making. Our framework follows the classic court trial process, consisting of court debate simulation, legal resources retrieval and decision-making refinement to simulate the decision-making of judge. (2) we introduce SimuCourt, a judicial benchmark that encompasses 420 Chinese judgment documents, spanning the three most common types of judicial cases. Furthermore, to support this task, we construct a large-scale legal knowledge base, Legal-KB, with multi-resource legal knowledge. (3) Extensive experiments show that our framework outperforms the existing advanced methods in various aspects, especially in generating legal articles, where our model achieves significant improvements of 8.6% and 9.1% F1 score in the first and second instance settings, respectively.",
            "link": "https://www.semanticscholar.org/paper/79d72cf13cc86dd46498384b32c8ac77898a23f9",
            "authors": "Zhitao He, Pengfei Cao, Chenhao Wang, Zhuoran Jin, Yubo Chen, Jiexin Xu, Huaijun Li, Xiaojian Jiang, Kang Liu, Jun Zhao",
            "matchScore": 322.9887,
            "original title": "AgentsCourt: Building Judicial Decision-Making Agents with Court Debate Simulation and Legal Knowledge Augmentation",
            "original authors": "Zhitao He, Pengfei Cao, Chenhao Wang, Zhuoran Jin, Yubo Chen, Jiexin Xu, Huaijun Li, Kang Liu, Jun Zhao",
            "EMNLP Paper ID": "1966",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "262ac80703a24005a20edd72229371fe39c8bee7",
            "title": "LEGOBench: Scientific Leaderboard Generation Benchmark",
            "abstract": "The ever-increasing volume of paper submissions makes it difficult to stay informed about the latest state-of-the-art research. To address this challenge, we introduce LEGOBench, a benchmark for evaluating systems that generate scientific leaderboards. LEGOBench is curated from 22 years of preprint submission data on arXiv and more than 11k machine learning leaderboards on the PapersWithCode portal. We present four graph-based and two language model-based leaderboard generation task configurations. We evaluate popular encoder-only scientific language models as well as decoder-only large language models across these task configurations. State-of-the-art models showcase significant performance gaps in automatic leaderboard generation on LEGOBench. The code is available on GitHub ( https://github.com/lingo-iitgn/LEGOBench ) and the dataset is hosted on OSF ( https://osf.io/9v2py/?view_only=6f91b0b510df498ba01595f8f278f94c ).",
            "link": "https://www.semanticscholar.org/paper/262ac80703a24005a20edd72229371fe39c8bee7",
            "authors": "Shruti Singh, Shoaib Alam, Husain Malwat, Mayank Singh",
            "matchScore": 217.41792,
            "original title": "LEGOBench: Scientific Leaderboard Generation Benchmark",
            "original authors": "Shruti Singh, Shoaib Alam, Husain Malwat, Mayank Singh",
            "EMNLP Paper ID": "2822",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "72147da92e1a27c981f936b43db7a4acbf09cfcb",
            "title": "The Craft of Selective Prediction: Towards Reliable Case Outcome Classification - An Empirical Study on European Court of Human Rights Cases",
            "abstract": "In high-stakes decision-making tasks within legal NLP, such as Case Outcome Classification (COC), quantifying a model's predictive confidence is crucial. Confidence estimation enables humans to make more informed decisions, particularly when the model's certainty is low, or where the consequences of a mistake are significant. However, most existing COC works prioritize high task performance over model reliability. This paper conducts an empirical investigation into how various design choices including pre-training corpus, confidence estimator and fine-tuning loss affect the reliability of COC models within the framework of selective prediction. Our experiments on the multi-label COC task, focusing on European Court of Human Rights (ECtHR) cases, highlight the importance of a diverse yet domain-specific pre-training corpus for better calibration. Additionally, we demonstrate that larger models tend to exhibit overconfidence, Monte Carlo dropout methods produce reliable confidence estimates, and confident error regularization effectively mitigates overconfidence. To our knowledge, this is the first systematic exploration of selective prediction in legal NLP. Our findings underscore the need for further research on enhancing confidence measurement and improving the trustworthiness of models in the legal domain.",
            "link": "https://www.semanticscholar.org/paper/72147da92e1a27c981f936b43db7a4acbf09cfcb",
            "authors": "Santosh T.Y.S.S, Irtiza Chowdhury, Shanshan Xu, Matthias Grabmair",
            "matchScore": 342.58383,
            "original title": "The Craft of Selective Prediction: Towards Reliable Case Outcome Classification - An Empirical Study on European Court of Human Rights Cases",
            "original authors": "Santosh T.Y.S.S, Irtiza Chowdhury, Shanshan Xu, Matthias Grabmair",
            "EMNLP Paper ID": "737",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "19dbe8f8905e4e1cbbc7cda5713079eaf586da0f",
            "title": "Incorporating Precedents for Legal Judgement Prediction on European Court of Human Rights Cases",
            "abstract": "Inspired by the legal doctrine of stare decisis, which leverages precedents (prior cases) for informed decision-making, we explore methods to integrate them into LJP models. To facilitate precedent retrieval, we train a retriever with a fine-grained relevance signal based on the overlap ratio of alleged articles between cases. We investigate two strategies to integrate precedents: direct incorporation at inference via label interpolation based on case proximity and during training via a precedent fusion module using a stacked-cross attention model. We employ joint training of the retriever and LJP models to address latent space divergence between them. Our experiments on LJP tasks from the ECHR jurisdiction reveal that integrating precedents during training coupled with joint training of the retriever and LJP model, outperforms models without precedents or with precedents incorporated only at inference, particularly benefiting sparser articles.",
            "link": "https://www.semanticscholar.org/paper/19dbe8f8905e4e1cbbc7cda5713079eaf586da0f",
            "authors": "Santosh T.Y.S.S, Mohamed Hesham Elganayni, Stanislaw S'ojka, Matthias Grabmair",
            "matchScore": 266.55884,
            "original title": "Incorporating Precedents for Legal Judgement Prediction on European Court of Human Rights Cases",
            "original authors": "Santosh T.Y.S.S, Mohamed Hesham Elganayni, Stanis\u0142aw S\u00f3jka, Matthias Grabmair",
            "EMNLP Paper ID": "753",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        }
    ],
    "Advancements in Large Language Models for Code Generation and Analysis": [
        {
            "paperId": "2589e7435b478b0389b78cafea75910c2e9460e0",
            "title": "LLM4Decompile: Decompiling Binary Code with Large Language Models",
            "abstract": "Decompilation aims to convert binary code to high-level source code, but traditional tools like Ghidra often produce results that are difficult to read and execute. Motivated by the advancements in Large Language Models (LLMs), we propose LLM4Decompile, the first and largest open-source LLM series (1.3B to 33B) trained to decompile binary code. We optimize the LLM training process and introduce the LLM4Decompile-End models to decompile binary directly. The resulting models significantly outperform GPT-4o and Ghidra on the HumanEval and ExeBench benchmarks by over 100% in terms of re-executability rate. Additionally, we improve the standard refinement approach to fine-tune the LLM4Decompile-Ref models, enabling them to effectively refine the decompiled code from Ghidra and achieve a further 16.2% improvement over the LLM4Decompile-End. LLM4Decompile demonstrates the potential of LLMs to revolutionize binary code decompilation, delivering remarkable improvements in readability and executability while complementing conventional tools for optimal results. Our code, dataset, and models are released at https://github.com/albertan017/LLM4Decompile",
            "link": "https://www.semanticscholar.org/paper/2589e7435b478b0389b78cafea75910c2e9460e0",
            "authors": "Hanzhuo Tan, Qi Luo, Jing Li, Yuqun Zhang",
            "EMNLP Paper ID": "396",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "635577dd7434946f04f0913b093e223c29e4d9b9",
            "title": "How Do Your Code LLMs Perform? Empowering Code Instruction Tuning with High-Quality Data",
            "abstract": "Recently, there has been a growing interest in studying how to construct better code instruction tuning data. However, we observe Code models trained with these datasets exhibit high performance on HumanEval but perform worse on other benchmarks such as LiveCodeBench. Upon further investigation, we find that many datasets suffer from severe data leakage. After cleaning up most of the leaked data, some well-known high-quality datasets perform poorly. This discovery reveals a new challenge: identifying which dataset genuinely qualify as high-quality code instruction data. To address this, we propose an efficient code data pruning strategy for selecting good samples. Our approach is based on three dimensions: instruction complexity, response quality, and instruction diversity. Based on our selected data, we present XCoder, a family of models finetuned from LLaMA3. Our experiments show XCoder achieves new state-of-the-art performance using fewer training data, which verify the effectiveness of our data strategy. Moreover, we perform a comprehensive analysis on the data composition and find existing code datasets have different characteristics according to their construction methods, which provide new insights for future code LLMs. Our models and dataset are released in https://github.com/banksy23/XCoder",
            "link": "https://www.semanticscholar.org/paper/635577dd7434946f04f0913b093e223c29e4d9b9",
            "authors": "Yejie Wang, Keqing He, Dayuan Fu, Zhuoma Gongque, Heyang Xu, Yanxu Chen, Zhexu Wang, Yujia Fu, Guanting Dong, Muxi Diao, Jingang Wang, Mengdi Zhang, Xunliang Cai, Weiran Xu",
            "EMNLP Paper ID": "1619",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "eb0db181546bd8493246d8f767b63d57ca4e686f",
            "title": "ECCO: Can We Improve Model-Generated Code Efficiency Without Sacrificing Functional Correctness?",
            "abstract": "Although large language models (LLMs) have been largely successful in generating functionally correct programs, conditioning models to produce efficient solutions while ensuring correctness remains a challenge. Further, unreliability in benchmarking code efficiency is a hurdle across varying hardware specifications for popular interpreted languages such as Python. In this paper, we present ECCO, a reproducible benchmark for evaluating program efficiency via two paradigms: natural language (NL) based code generation and history-based code editing. On ECCO, we adapt and thoroughly investigate the three most promising existing LLM-based approaches: in-context learning, iterative refinement with execution or NL feedback, and fine-tuning conditioned on execution and editing history. While most methods degrade functional correctness and moderately increase program efficiency, we find that adding execution information often helps maintain functional correctness, and NL feedback enhances more on efficiency. We release our benchmark to support future work on LLM-based generation of efficient code.",
            "link": "https://www.semanticscholar.org/paper/eb0db181546bd8493246d8f767b63d57ca4e686f",
            "authors": "Siddhant Waghjale, Vishruth Veerendranath, Zora Zhiruo Wang, Daniel Fried",
            "EMNLP Paper ID": "1788",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "a5eda09ecb1dd3fd795edbbbb46b7f7e2dbd1f32",
            "title": "DocCGen: Document-based Controlled Code Generation",
            "abstract": "Recent developments show that Large Language Models (LLMs) produce state-of-the-art performance on natural language (NL) to code generation for resource-rich general-purpose languages like C++, Java, and Python. However, their practical usage for structured domain-specific languages (DSLs) such as YAML, JSON is limited due to domain-specific schema, grammar, and customizations generally unseen by LLMs during pre-training. Efforts have been made to mitigate this challenge via in-context learning through relevant examples or by fine-tuning. However, it suffers from problems, such as limited DSL samples and prompt sensitivity but enterprises maintain good documentation of the DSLs. Therefore, we propose DocCGen, a framework that can leverage such rich knowledge by breaking the NL-to-Code generation task for structured code languages into a two-step process. First, it detects the correct libraries using the library documentation that best matches the NL query. Then, it utilizes schema rules extracted from the documentation of these libraries to constrain the decoding. We evaluate our framework for two complex structured languages, Ansible YAML and Bash command, consisting of two settings: Out-of-domain (OOD) and In-domain (ID). Our extensive experiments show that DocCGen consistently improves different-sized language models across all six evaluation metrics, reducing syntactic and semantic errors in structured code. We plan to open-source the datasets and code to motivate research in constrained code generation.",
            "link": "https://www.semanticscholar.org/paper/a5eda09ecb1dd3fd795edbbbb46b7f7e2dbd1f32",
            "authors": "Sameer Pimparkhede, Mehant Kammakomati, Srikanth G. Tamilselvam, Prince Kumar, Ashok Pon Kumar, Pushpak Bhattacharyya",
            "EMNLP Paper ID": "2329",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "242188b68aaa5b3cb8db99bb543a70971d49d5ba",
            "title": "CoCoST: Automatic Complex Code Generation with Online Searching and Correctness Testing",
            "abstract": "Large Language Models have revolutionized code generation ability by converting natural language descriptions into executable code. However, generating complex code within real-world scenarios remains challenging due to intricate structures, subtle bugs, understanding of advanced data types, and lack of supplementary contents. To address these challenges, we introduce the CoCoST framework, which enhances complex code generation by online searching for more information with planned queries and correctness testing for code refinement. Moreover, CoCoST serializes the complex inputs and outputs to improve comprehension and generates test cases to ensure the adaptability for real-world applications. CoCoST is validated through rigorous experiments on the DS-1000 and ClassEval datasets. Experimental results show that CoCoST substantially improves the quality of complex code generation, highlighting its potential to enhance the practicality of LLMs in generating complex code.",
            "link": "https://www.semanticscholar.org/paper/242188b68aaa5b3cb8db99bb543a70971d49d5ba",
            "authors": "Xinyi He, Jiaru Zou, Yun Lin, Mengyu Zhou, Shi Han, Zejian Yuan, Dongmei Zhang",
            "EMNLP Paper ID": "2473",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "d7aacfac565d0cde3ce511fb915cec42fecfd9fd",
            "title": "CodeJudge: Evaluating Code Generation with Large Language Models",
            "abstract": "Large Language Models (LLMs) have shown promising performance in code generation. However, how to reliably evaluate code generated by LLMs remains an unresolved problem. This paper presents CodeJudge, a code evaluation framework that leverages LLMs to evaluate the semantic correctness of generated code without the need for test cases. We investigate different ways to guide the LLM in performing\"slow thinking\"to arrive at an in-depth and reliable evaluation. We experimented with four LLMs as evaluators on four code generation datasets and five programming languages. The results show that CodeJudge significantly outperformed existing methods in most settings. Furthermore, compared with a SOTA GPT-3.5-based code evaluation method, CodeJudge achieved better results even when using a much smaller model, Llama-3-8B-Instruct. Our code and datasets are available on GitHub https://github.com/VichyTong/CodeJudge.",
            "link": "https://www.semanticscholar.org/paper/d7aacfac565d0cde3ce511fb915cec42fecfd9fd",
            "authors": "Weixi Tong, Tianyi Zhang",
            "EMNLP Paper ID": "2611",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "6d73697837801b82744667f1659d01f027961b31",
            "title": "Coffee-Gym: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code",
            "abstract": "This paper presents Coffee-Gym, a comprehensive RL environment for training models that provide feedback on code editing. Coffee-Gym includes two major components: (1) Coffee, a dataset containing humans' code edit traces for coding questions and machine-written feedback for editing erroneous code; (2) CoffeeEval, a reward function that faithfully reflects the helpfulness of feedback by assessing the performance of the revised code in unit tests. With them, Coffee-Gym addresses the unavailability of high-quality datasets for training feedback models with RL, and provides more accurate rewards than the SOTA reward model (i.e., GPT-4). By applying Coffee-Gym, we elicit feedback models that outperform baselines in enhancing open-source code LLMs' code editing, making them comparable with closed-source LLMs. We make the dataset and the model checkpoint publicly available.",
            "link": "https://www.semanticscholar.org/paper/6d73697837801b82744667f1659d01f027961b31",
            "authors": "Hyungjoo Chae, Taeyoon Kwon, Seungjun Moon, Yongho Song, Dongjin Kang, Kai Tzu-iunn Ong, Beong-woo Kwak, Seonghyeon Bae, Seung-won Hwang, Jinyoung Yeo",
            "EMNLP Paper ID": "3246",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "24368187b30ed3475556f39fcdc73c168be079ac",
            "title": "Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement",
            "abstract": "Decompilation transforms compiled code back into a high-level programming language for analysis when source code is unavailable. Previous work has primarily focused on enhancing decompilation performance by increasing the scale of model parameters or training data for pre-training. Based on the characteristics of the decompilation task, we propose two methods: (1) Without fine-tuning, the Self-Constructed Context Decompilation (sc$^2$dec) method recompiles the LLM's decompilation results to construct pairs for in-context learning, helping the model improve decompilation performance. (2) Fine-grained Alignment Enhancement (FAE), which meticulously aligns assembly code with source code at the statement level by leveraging debugging information, is employed during the fine-tuning phase to achieve further improvements in decompilation. By integrating these two methods, we achieved a Re-Executability performance improvement of approximately 3.90% on the Decompile-Eval benchmark, establishing a new state-of-the-art performance of 52.41%. The code, data, and models are available at https://github.com/AlongWY/sccdec.",
            "link": "https://www.semanticscholar.org/paper/24368187b30ed3475556f39fcdc73c168be079ac",
            "authors": "ylfeng, Yang Xu, Dechuan Teng, Honglin Mu, Xiao Xu, Libo Qin, Wanxiang Che, Qingfu Zhu",
            "matchScore": 259.1272,
            "original title": "Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement",
            "original authors": "Yunlong Feng, Dechuan Teng, Yang Xu, Xiao Xu, Honglin Mu, Libo Qin, Qingfu Zhu, Wanxiang Che",
            "EMNLP Paper ID": "1343",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "ec187030dfbef8d3daaa80d70f84a99ea9fe0565",
            "title": "Revisiting the Impact of Pursuing Modularity for Code Generation",
            "abstract": "Modular programming, which aims to construct the final program by integrating smaller, independent building blocks, has been regarded as a desirable practice in software development. However, with the rise of recent code generation agents built upon large language models (LLMs), a question emerges: is this traditional practice equally effective for these new tools? In this work, we assess the impact of modularity in code generation by introducing a novel metric for its quantitative measurement. Surprisingly, unlike conventional wisdom on the topic, we find that modularity is not a core factor for improving the performance of code generation models. We also explore potential explanations for why LLMs do not exhibit a preference for modular code compared to non-modular code.",
            "link": "https://www.semanticscholar.org/paper/ec187030dfbef8d3daaa80d70f84a99ea9fe0565",
            "authors": "Deokyeong Kang, Ki Jung Seo, Taeuk Kim",
            "matchScore": 198.8354,
            "original title": "Revisiting the Impact of Pursuing Modularity for Code Generation",
            "original authors": "Deokyeong Kang, KiJung Seo, Taeuk Kim",
            "EMNLP Paper ID": "2279",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "fbf3659c0967703035cfed17481276a576d6f88a",
            "title": "How You Prompt Matters! Even Task-Oriented Constraints in Instructions Affect LLM-Generated Text Detection",
            "abstract": "To combat the misuse of Large Language Models (LLMs), many recent studies have presented LLM-generated-text detectors with promising performance. When users instruct LLMs to generate texts, the instruction can include different constraints depending on the user's need. However, most recent studies do not cover such diverse instruction patterns when creating datasets for LLM detection. In this paper, we reveal that even task-oriented constraints -- constraints that would naturally be included in an instruction and are not related to detection-evasion -- cause existing powerful detectors to have a large variance in detection performance. We focus on student essay writing as a realistic domain and manually create task-oriented constraints based on several factors for essay quality. Our experiments show that the standard deviation (SD) of current detector performance on texts generated by an instruction with such a constraint is significantly larger (up to an SD of 14.4 F1-score) than that by generating texts multiple times or paraphrasing the instruction. We also observe an overall trend where the constraints can make LLM detection more challenging than without them. Finally, our analysis indicates that the high instruction-following ability of LLMs fosters the large impact of such constraints on detection performance.",
            "link": "https://www.semanticscholar.org/paper/fbf3659c0967703035cfed17481276a576d6f88a",
            "authors": "Ryuto Koike, Masahiro Kaneko, Naoaki Okazaki",
            "matchScore": 373.8048,
            "original title": "How You Prompt Matters! Even Task-Oriented Constraints in Instructions Affect LLM-Generated Text Detection",
            "original authors": "Ryuto Koike, Masahiro Kaneko, Naoaki Okazaki",
            "EMNLP Paper ID": "2779",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "7760bb962353b2a086b5fc3453676c3dd903946f",
            "title": "PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLMs",
            "abstract": "Driven by the surge in code generation using large language models (LLMs), numerous benchmarks have emerged to evaluate these LLMs capabilities. We conducted a large-scale human evaluation of HumanEval and MBPP, two popular benchmarks for Python code generation, analyzing their diversity and difficulty. Our findings unveil a critical bias towards a limited set of programming concepts, neglecting most of the other concepts entirely. Furthermore, we uncover a worrying prevalence of easy tasks, potentially inflating model performance estimations. To address these limitations, we propose a novel benchmark, PythonSaga, featuring 185 hand-crafted prompts on a balanced representation of 38 programming concepts across diverse difficulty levels. The robustness of our benchmark is demonstrated by the poor performance of existing Code-LLMs.",
            "link": "https://www.semanticscholar.org/paper/7760bb962353b2a086b5fc3453676c3dd903946f",
            "authors": "Ankit Yadav, Himanshu Beniwal, Mayank Singh",
            "matchScore": 253.26997,
            "original title": "PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLMs",
            "original authors": "Ankit Yadav, Mayank Singh, Himanshu Beniwal",
            "EMNLP Paper ID": "3286",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        }
    ],
    "Large Language Models for Planning and Task Automation": [
        {
            "paperId": "cfbdf67fc11977637d4cb13ed7e1abce75623796",
            "title": "Unlocking the Future: Exploring Look-Ahead Planning Mechanistic Interpretability in Large Language Models",
            "abstract": "Planning, as the core module of agents, is crucial in various fields such as embodied agents, web navigation, and tool using. With the development of large language models (LLMs), some researchers treat large language models as intelligent agents to stimulate and evaluate their planning capabilities. However, the planning mechanism is still unclear. In this work, we focus on exploring the look-ahead planning mechanism in large language models from the perspectives of information flow and internal representations. First, we study how planning is done internally by analyzing the multi-layer perception (MLP) and multi-head self-attention (MHSA) components at the last token. We find that the output of MHSA in the middle layers at the last token can directly decode the decision to some extent. Based on this discovery, we further trace the source of MHSA by information flow, and we reveal that MHSA mainly extracts information from spans of the goal states and recent steps. According to information flow, we continue to study what information is encoded within it. Specifically, we explore whether future decisions have been encoded in advance in the representation of flow. We demonstrate that the middle and upper layers encode a few short-term future decisions to some extent when planning is successful. Overall, our research analyzes the look-ahead planning mechanisms of LLMs, facilitating future research on LLMs performing planning tasks.",
            "link": "https://www.semanticscholar.org/paper/cfbdf67fc11977637d4cb13ed7e1abce75623796",
            "authors": "Tianyi Men, Pengfei Cao, Zhuoran Jin, Yubo Chen, Kang Liu, Jun Zhao",
            "EMNLP Paper ID": "874",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "9ed390a25c0734dd1af57d0cc401bdd95aba3e4d",
            "title": "ActPlan-1K: Benchmarking the Procedural Planning Ability of Visual Language Models in Household Activities",
            "abstract": "Large language models~(LLMs) have been adopted to process textual task description and accomplish procedural planning in embodied AI tasks because of their powerful reasoning ability. However, there is still lack of study on how vision language models~(VLMs) behave when multi-modal task inputs are considered. Counterfactual planning that evaluates the model's reasoning ability over alternative task situations are also under exploited. In order to evaluate the planning ability of both multi-modal and counterfactual aspects, we propose ActPlan-1K. ActPlan-1K is a multi-modal planning benchmark constructed based on ChatGPT and household activity simulator iGibson2. The benchmark consists of 153 activities and 1,187 instances. Each instance describing one activity has a natural language task description and multiple environment images from the simulator. The gold plan of each instance is action sequences over the objects in provided scenes. Both the correctness and commonsense satisfaction are evaluated on typical VLMs. It turns out that current VLMs are still struggling at generating human-level procedural plans for both normal activities and counterfactual activities. We further provide automatic evaluation metrics by finetuning over BLEURT model to facilitate future research on our benchmark.",
            "link": "https://www.semanticscholar.org/paper/9ed390a25c0734dd1af57d0cc401bdd95aba3e4d",
            "authors": "Ying Su, Zhan Ling, Haochen Shi, Jiayang Cheng, Yauwai Yim, Yangqiu Song",
            "EMNLP Paper ID": "1728",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "52daa3f9bef746cc51696c5588b8589444a67c4e",
            "title": "AppBench: Planning of Multiple APIs from Various APPs for Complex User Instruction",
            "abstract": "Large Language Models (LLMs) can interact with the real world by connecting with versatile external APIs, resulting in better problem-solving and task automation capabilities. Previous research primarily focuses on APIs with limited arguments from a single source or overlooks the complex dependency relationship between different APIs. However, it is essential to utilize multiple APIs collaboratively from various sources (e.g., different Apps in the iPhone), especially for complex user instructions. In this paper, we introduce \\texttt{AppBench}, the first benchmark to evaluate LLMs' ability to plan and execute multiple APIs from various sources in order to complete the user's task. Specifically, we consider two significant challenges in multiple APIs: \\textit{1) graph structures:} some APIs can be executed independently while others need to be executed one by one, resulting in graph-like execution order; and \\textit{2) permission constraints:} which source is authorized to execute the API call. We have experimental results on 9 distinct LLMs; e.g., GPT-4o achieves only a 2.0\\% success rate at the most complex instruction, revealing that the existing state-of-the-art LLMs still cannot perform well in this situation even with the help of in-context learning and finetuning. Our code and data are publicly available at https://github.com/ruleGreen/AppBench.",
            "link": "https://www.semanticscholar.org/paper/52daa3f9bef746cc51696c5588b8589444a67c4e",
            "authors": "Hongru Wang, Rui Wang, Boyang Xue, Heming Xia, Jingtao Cao, Zeming Liu, Jeff Z. Pan, Kam-Fai Wong",
            "EMNLP Paper ID": "1784",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "ab0fa7d219908c04a7b8dde51c1bb5a2a4caec3b",
            "title": "CaT-BENCH: Benchmarking Language Model Understanding of Causal and Temporal Dependencies in Plans",
            "abstract": "Understanding the abilities of LLMs to reason about natural language plans, such as instructional text and recipes, is critical to reliably using them in decision-making systems. A fundamental aspect of plans is the temporal order in which their steps needs to be executed, which reflects the underlying causal dependencies between them. We introduce CaT-Bench, a benchmark of Step Order Prediction questions, which test whether a step must necessarily occur before or after another in cooking recipe plans. We use this to evaluate how well frontier LLMs understand causal and temporal dependencies. We find that SOTA LLMs are underwhelming (best zero-shot is only 0.59 in F1), and are biased towards predicting dependence more often, perhaps relying on temporal order of steps as a heuristic. While prompting for explanations and using few-shot examples improve performance, the best F1 result is only 0.73. Further, human evaluation of explanations along with answer correctness show that, on average, humans do not agree with model reasoning. Surprisingly, we also find that explaining after answering leads to better performance than normal chain-of-thought prompting, and LLM answers are not consistent across questions about the same step pairs. Overall, results show that LLMs' ability to detect dependence between steps has significant room for improvement.",
            "link": "https://www.semanticscholar.org/paper/ab0fa7d219908c04a7b8dde51c1bb5a2a4caec3b",
            "authors": "Yash Kumar Lal, Vanya Cohen, Nathanael Chambers, Niranjan Balasubramanian, Raymond J. Mooney",
            "EMNLP Paper ID": "2452",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "23cd60c214d2f76c6f80a35d077cea337433b084",
            "title": "A Training Data Recipe to Accelerate A* Search with Language Models",
            "abstract": "Combining Large Language Models (LLMs) with heuristic search algorithms like A* holds the promise of enhanced LLM reasoning and scalable inference. To accelerate training and reduce computational demands, we investigate the coreset selection problem for the training data of LLM heuristic learning. Few methods to learn the heuristic functions consider the interaction between the search algorithm and the machine learning model. In this work, we empirically disentangle the requirements of A* search algorithm from the requirements of the LLM to generalise on this task. Surprisingly, we find an overlap between their requirements; A* requires more accurate predictions on search nodes near the goal, and LLMs need the same set of nodes for effective generalisation. With these insights, we derive a data-selection distribution for learning LLM-based heuristics. On three classical planning domains, maze navigation, Sokoban and sliding tile puzzles, our technique reduces the number of iterations required to find the solutions by up to 15x, with a wall-clock speed-up of search up to 5x. The codebase is at https://github.com/devaansh100/a_star.",
            "link": "https://www.semanticscholar.org/paper/23cd60c214d2f76c6f80a35d077cea337433b084",
            "authors": "Devaansh Gupta, Boyang Li",
            "matchScore": 236.07208,
            "original title": "A Training Data Recipe to Accelerate A* Search with Language Models",
            "original authors": "Devaansh Gupta, Boyang Li",
            "EMNLP Paper ID": "1360",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 14): 13:00\u201314:00 (Afternoon)",
            "Type": "Findings"
        },
        {
            "paperId": "367e43d1561fce27c919e2d370e42399a40846bd",
            "title": "Ask-before-Plan: Proactive Language Agents for Real-World Planning",
            "abstract": "The evolution of large language models (LLMs) has enhanced the planning capabilities of language agents in diverse real-world scenarios. Despite these advancements, the potential of LLM-powered agents to comprehend ambiguous user instructions for reasoning and decision-making is still under exploration. In this work, we introduce a new task, Proactive Agent Planning, which requires language agents to predict clarification needs based on user-agent conversation and agent-environment interaction, invoke external tools to collect valid information, and generate a plan to fulfill the user's demands. To study this practical problem, we establish a new benchmark dataset, Ask-before-Plan. To tackle the deficiency of LLMs in proactive planning, we propose a novel multi-agent framework, Clarification-Execution-Planning (\\texttt{CEP}), which consists of three agents specialized in clarification, execution, and planning. We introduce the trajectory tuning scheme for the clarification agent and static execution agent, as well as the memory recollection mechanism for the dynamic execution agent. Extensive evaluations and comprehensive analyses conducted on the Ask-before-Plan dataset validate the effectiveness of our proposed framework.",
            "link": "https://www.semanticscholar.org/paper/367e43d1561fce27c919e2d370e42399a40846bd",
            "authors": "Xuan Zhang, Yang Deng, Zifeng Ren, See-Kiong Ng, Tat-Seng Chua",
            "matchScore": 281.79883,
            "original title": "Ask-before-Plan: Proactive Language Agents for Real-World Planning",
            "original authors": "Xuan Zhang, Yang Deng, Zifeng Ren, See-Kiong Ng, Tat-Seng Chua",
            "EMNLP Paper ID": "2176",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "b49707c75c9f29ea020111e7a6f3af28c7061729",
            "title": "FlowBench: Revisiting and Benchmarking Workflow-Guided Planning for LLM-based Agents",
            "abstract": "LLM-based agents have emerged as promising tools, which are crafted to fulfill complex tasks by iterative planning and action. However, these agents are susceptible to undesired planning hallucinations when lacking specific knowledge for expertise-intensive tasks. To address this, preliminary attempts are made to enhance planning reliability by incorporating external workflow-related knowledge. Despite the promise, such infused knowledge is mostly disorganized and diverse in formats, lacking rigorous formalization and comprehensive comparisons. Motivated by this, we formalize different formats of workflow knowledge and present FlowBench, the first benchmark for workflow-guided planning. FlowBench covers 51 different scenarios from 6 domains, with knowledge presented in diverse formats. To assess different LLMs on FlowBench, we design a multi-tiered evaluation framework. We evaluate the efficacy of workflow knowledge across multiple formats, and the results indicate that current LLM agents need considerable improvements for satisfactory planning. We hope that our challenging benchmark can pave the way for future agent planning research.",
            "link": "https://www.semanticscholar.org/paper/b49707c75c9f29ea020111e7a6f3af28c7061729",
            "authors": "Rui Xiao, Wen-Cheng Ma, Ke Wang, Yuchuan Wu, Junbo Zhao, Haobo Wang, Fei Huang, Yongbin Li",
            "matchScore": 307.09967,
            "original title": "FlowBench: Revisiting and Benchmarking Workflow-Guided Planning for LLM-based Agents",
            "original authors": "Ruixuan Xiao, Wentao Ma, Ke Wang, Yuchuan Wu, Junbo Zhao, Haobo Wang, Fei Huang, Yongbin Li",
            "EMNLP Paper ID": "2182",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "8bb5b517012530244497beb4d1b7257d3c76661c",
            "title": "LLM-A*: Large Language Model Enhanced Incremental Heuristic Search on Path Planning",
            "abstract": "Path planning is a fundamental scientific problem in robotics and autonomous navigation, requiring the derivation of efficient routes from starting to destination points while avoiding obstacles. Traditional algorithms like A* and its variants are capable of ensuring path validity but suffer from significant computational and memory inefficiencies as the state space grows. Conversely, large language models (LLMs) excel in broader environmental analysis through contextual understanding, providing global insights into environments. However, they fall short in detailed spatial and temporal reasoning, often leading to invalid or inefficient routes. In this work, we propose LLM-A*, an new LLM based route planning method that synergistically combines the precise pathfinding capabilities of A* with the global reasoning capability of LLMs. This hybrid approach aims to enhance pathfinding efficiency in terms of time and space complexity while maintaining the integrity of path validity, especially in large-scale scenarios. By integrating the strengths of both methodologies, LLM-A* addresses the computational and memory limitations of conventional algorithms without compromising on the validity required for effective pathfinding.",
            "link": "https://www.semanticscholar.org/paper/8bb5b517012530244497beb4d1b7257d3c76661c",
            "authors": "Silin Meng, Yiwei Wang, Cheng-Fu Yang, Nanyun Peng, Kai-Wei Chang",
            "matchScore": 250.58496,
            "original title": "LLM-A*: Large Language Model Enhanced Incremental Heuristic Search on Path Planning",
            "original authors": "Silin Meng, Yiwei Wang, Cheng-Fu Yang, Nanyun Peng, Kai-Wei Chang",
            "EMNLP Paper ID": "232",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Findings"
        },
        {
            "paperId": "2199a5ac145edbc46db9fcbf97b01461a2367cda",
            "title": "Android in the Zoo: Chain-of-Action-Thought for GUI Agents",
            "abstract": "Large language model (LLM) leads to a surge of autonomous GUI agents for smartphone, which completes a task triggered by natural language through predicting a sequence of actions of API. Even though the task highly relies on past actions and visual observations, existing studies typically consider little semantic information carried out by intermediate screenshots and screen operations. To address this, this work presents Chain-of-Action-Thought (dubbed CoAT), which takes the description of the previous actions, the current screen, and more importantly the action thinking of what actions should be performed and the outcomes led by the chosen action. We demonstrate that, in a zero-shot setting upon three off-the-shelf LMMs, CoAT significantly improves the action prediction compared to previous proposed context modeling. To further facilitate the research in this line, we construct a dataset Android-In-The-Zoo (AitZ), which contains 18,643 screen-action pairs together with chain-of-action-thought annotations. Experiments show that fine-tuning a 1B model (i.e. AUTO-UI-base) on our AitZ dataset achieves on-par performance with CogAgent-Chat-18B.",
            "link": "https://www.semanticscholar.org/paper/2199a5ac145edbc46db9fcbf97b01461a2367cda",
            "authors": "Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, Duyu Tang",
            "matchScore": 262.52347,
            "original title": "Android in the Zoo: Chain-of-Action-Thought for GUI Agents",
            "original authors": "Jiwen Zhang, Jihao Wu, Teng Yihua, Minghui Liao, Nuo Xu, Xiao Xiao, zhongyu wei, Duyu Tang",
            "EMNLP Paper ID": "2368",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "1f029211b0294fbc063d1fd95e3f171d71b5cf0c",
            "title": "Dynamic Planning for LLM-based Graphical User Interface Automation",
            "abstract": "The advent of large language models (LLMs) has spurred considerable interest in advancing autonomous LLMs-based agents, particularly in intriguing applications within smartphone graphical user interfaces (GUIs). When presented with a task goal, these agents typically emulate human actions within a GUI environment until the task is completed. However, a key challenge lies in devising effective plans to guide action prediction in GUI tasks, though planning have been widely recognized as effective for decomposing complex tasks into a series of steps. Specifically, given the dynamic nature of environmental GUIs following action execution, it is crucial to dynamically adapt plans based on environmental feedback and action history.We show that the widely-used ReAct approach fails due to the excessively long historical dialogues. To address this challenge, we propose a novel approach called Dynamic Planning of Thoughts (D-PoT) for LLM-based GUI agents.D-PoT involves the dynamic adjustment of planning based on the environmental feedback and execution history. Experimental results reveal that the proposed D-PoT significantly surpassed the strong GPT-4V baseline by +12.7% (34.66% $\\rightarrow$ 47.36%) in accuracy. The analysis highlights the generality of dynamic planning in different backbone LLMs, as well as the benefits in mitigating hallucinations and adapting to unseen tasks. Code is available at https://github.com/sqzhang-lazy/D-PoT.",
            "link": "https://www.semanticscholar.org/paper/1f029211b0294fbc063d1fd95e3f171d71b5cf0c",
            "authors": "Shaoqing Zhang, Zhuosheng Zhang, Kehai Chen, Xinbei Ma, Muyun Yang, Tiejun Zhao, Min Zhang",
            "matchScore": 238.19891,
            "original title": "Dynamic Planning for LLM-based Graphical User Interface Automation",
            "original authors": "Shaoqing Zhang, Zhuosheng Zhang, Kehai Chen, Xinbei Ma, Muyun Yang, Tiejun Zhao, Min Zhang",
            "EMNLP Paper ID": "263",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "a4083e0cd128b92df8805d99b3d1a82688a3efd3",
            "title": "UrbanLLM: Autonomous Urban Activity Planning and Management with Large Language Models",
            "abstract": "Location-based services play an critical role in improving the quality of our daily lives. Despite the proliferation of numerous specialized AI models within spatio-temporal context of location-based services, these models struggle to autonomously tackle problems regarding complex urban planing and management. To bridge this gap, we introduce UrbanLLM, a fine-tuned large language model (LLM) designed to tackle diverse problems in urban scenarios. UrbanLLM functions as a problem-solver by decomposing urban-related queries into manageable sub-tasks, identifying suitable spatio-temporal AI models for each sub-task, and generating comprehensive responses to the given queries. Our experimental results indicate that UrbanLLM significantly outperforms other established LLMs, such as Llama and the GPT series, in handling problems concerning complex urban activity planning and management. UrbanLLM exhibits considerable potential in enhancing the effectiveness of solving problems in urban scenarios, reducing the workload and reliance for human experts.",
            "link": "https://www.semanticscholar.org/paper/a4083e0cd128b92df8805d99b3d1a82688a3efd3",
            "authors": "Yue Jiang, Qin Chao, Yile Chen, Xiucheng Li, Shuai Liu, Gao Cong",
            "matchScore": 256.38983,
            "original title": "UrbanLLM: Autonomous Urban Activity Planning and Management with Large Language Models",
            "original authors": "YUE JIANG, Qin Chao, Yile Chen, Xiucheng Li, SHUAI LIU, Gao Cong",
            "EMNLP Paper ID": "366",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        }
    ],
    "Error Correction and Handling in AI Systems Using Language Models": [
        {
            "paperId": "62d4a78f34315e8fb0b670988bd28a935cb2a60f",
            "title": "Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections",
            "abstract": "Language models will inevitably err in situations with which they are unfamiliar. However, by effectively communicating uncertainties, they can still guide humans toward making sound decisions in those contexts. We demonstrate this idea by developing HEAR, a system that can successfully guide humans in simulated residential environments despite generating potentially inaccurate instructions. Diverging from systems that provide users with only the instructions they generate, HEAR warns users of potential errors in its instructions and suggests corrections. This rich uncertainty information effectively prevents misguidance and reduces the search space for users. Evaluation with 80 users shows that HEAR achieves a 13% increase in success rate and a 29% reduction in final location error distance compared to only presenting instructions to users. Interestingly, we find that offering users possibilities to explore, HEAR motivates them to make more attempts at the task, ultimately leading to a higher success rate. To our best knowledge, this work is the first to show the practical benefits of uncertainty communication in a long-horizon sequential decision-making problem.",
            "link": "https://www.semanticscholar.org/paper/62d4a78f34315e8fb0b670988bd28a935cb2a60f",
            "authors": "Lingjun Zhao, Khanh Nguyen, Hal Daum'e",
            "EMNLP Paper ID": "92",
            "Oral/Poster": "Oral",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "4973f75a35ed7ecb436bc2a42eb6062d3be5bce8",
            "title": "Can Active Label Correction Improve LLM-based Modular AI Systems?",
            "abstract": "Modular AI systems can be developed using LLM-prompts-based modules to minimize deployment time even for complex tasks. However, these systems do not always perform well and improving them using the data traces collected from a deployment remains an open challenge. The data traces contain LLM inputs and outputs, but the annotations from LLMs are noisy. We hypothesize that Active Label Correction (ALC) can be use on the collected data to train smaller task-specific improved models that can replace LLM-based modules. In this paper, we study the noise in three GPT-3.5-annotated datasets and their denoising with human feedback. We also propose a novel method ALC3 that iteratively applies three updates to the training dataset: auto-correction, correction using human feedback and filtering. Our results show that ALC3 can lead to oracle performance with feedback on 17-24% fewer examples than the number of noisy examples in the dataset across three different NLP tasks.",
            "link": "https://www.semanticscholar.org/paper/4973f75a35ed7ecb436bc2a42eb6062d3be5bce8",
            "authors": "Karan Taneja, Ashok K. Goel",
            "EMNLP Paper ID": "1029",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "60396f5dbc79d0295e440f1a1a289a49029c1c3a",
            "title": "Repairs in a Block World: A New Benchmark for Handling User Corrections with Multi-Modal Language Models",
            "abstract": "In dialogue, the addressee may initially misunderstand the speaker and respond erroneously, often prompting the speaker to correct the misunderstanding in the next turn with a Third Position Repair (TPR). The ability to process and respond appropriately to such repair sequences is thus crucial in conversational AI systems. In this paper, we first collect, analyse, and publicly release BlockWorld-Repairs: a dataset of multi-modal TPR sequences in an instruction-following manipulation task that is, by design, rife with referential ambiguity. We employ this dataset to evaluate several state-of-the-art Vision and Language Models (VLM) across multiple settings, focusing on their capability to process and accurately respond to TPRs and thus recover from miscommunication. We find that, compared to humans, all models significantly underperform in this task. We then show that VLMs can benefit from specialised losses targeting relevant tokens during fine-tuning, achieving better performance and generalising better to new scenarios. Our results suggest that these models are not yet ready to be deployed in multi-modal collaborative settings where repairs are common, and highlight the need to design training regimes and objectives that facilitate learning from interaction. Our code and data are available at www.github.com/JChiyah/blockworld-repairs",
            "link": "https://www.semanticscholar.org/paper/60396f5dbc79d0295e440f1a1a289a49029c1c3a",
            "authors": "Javier Chiyah-Garcia, Alessandro Suglia, Arash Eshghi",
            "EMNLP Paper ID": "1341",
            "Oral/Poster": "Oral",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "2a7bbc62bcfb944d1352eea72096bfc7cb1e2fe0",
            "title": "Enhancing AI Assisted Writing with One-Shot Implicit Negative Feedback",
            "abstract": "AI-mediated communication enables users to communicate more quickly and efficiently. Various systems have been proposed such as smart reply and AI-assisted writing. Yet, the heterogeneity of the forms of inputs and architectures often renders it challenging to combine insights from user behaviour in one system to improve performance in another. In this work, we consider the case where the user does not select any of the suggested replies from a smart reply system, and how this can be used as one-shot implicit negative feedback to enhance the accuracy of an AI writing model. We introduce Nifty, an approach that uses classifier guidance to controllably integrate implicit user feedback into the text generation process. Empirically, we find up to 34% improvement in Rouge-L, 89% improvement in generating the correct intent, and an 86% win-rate according to human evaluators compared to a vanilla AI writing system on the MultiWOZ and Schema-Guided Dialog datasets.",
            "link": "https://www.semanticscholar.org/paper/2a7bbc62bcfb944d1352eea72096bfc7cb1e2fe0",
            "authors": "Benjamin Towle, Ke Zhou",
            "EMNLP Paper ID": "1470",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "8894116bab87027044d96496e98944dd47e14ad6",
            "title": "CoGen: Learning from Feedback with Coupled Comprehension and Generation",
            "abstract": "Systems with both language comprehension and generation capabilities can benefit from the tight connection between the two. This work studies coupling comprehension and generation with focus on continually learning from interaction with users. We propose techniques to tightly integrate the two capabilities for both learning and inference. We situate our studies in two-player reference games, and deploy various models for thousands of interactions with human users, while learning from interaction feedback signals. We show dramatic improvements in performance over time, with comprehension-generation coupling leading to performance improvements up to 26% in absolute terms and up to 17% higher accuracies compared to a non-coupled system. Our analysis also shows coupling has substantial qualitative impact on the system's language, making it significantly more human-like.",
            "link": "https://www.semanticscholar.org/paper/8894116bab87027044d96496e98944dd47e14ad6",
            "authors": "Mustafa Omer Gul, Yoav Artzi",
            "EMNLP Paper ID": "1505",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "a3d177f1ac6984d305b0a0af89f6bab4c17cf817",
            "title": "E2CL: Exploration-based Error Correction Learning for Embodied Agents",
            "abstract": "Language models are exhibiting increasing capability in knowledge utilization and reasoning. However, when applied as agents in embodied environments, they often suffer from misalignment between their intrinsic knowledge and environmental knowledge, leading to infeasible actions. Traditional environment alignment methods, such as supervised learning on expert trajectories and reinforcement learning, encounter limitations in covering environmental knowledge and achieving efficient convergence, respectively. Inspired by human learning, we propose Exploration-based Error Correction Learning (E2CL), a novel framework that leverages exploration-induced errors and environmental feedback to enhance environment alignment for embodied agents. E2CL incorporates teacher-guided and teacher-free explorations to gather environmental feedback and correct erroneous actions. The agent learns to provide feedback and self-correct, thereby enhancing its adaptability to target environments. Extensive experiments in the VirtualHome environment demonstrate that E2CL-trained agents outperform those trained by baseline methods and exhibit superior self-correction capabilities.",
            "link": "https://www.semanticscholar.org/paper/a3d177f1ac6984d305b0a0af89f6bab4c17cf817",
            "authors": "Hanlin Wang, Chak Tou Leong, Jian Wang, Wenjie Li",
            "matchScore": 167.3325,
            "original title": "Exploration-based Error Correction Learning in Embodied Language Models",
            "original authors": "Hanlin Wang, Chak Tou Leong, Jian Wang, Wenjie Li",
            "EMNLP Paper ID": "1587",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "70b3ae0480e795608ce0b2a4b555d002c5d6f4d7",
            "title": "LLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement",
            "abstract": "This paper introduces the innovative\"LLMs-as-Instructors\"framework, which leverages the advanced Large Language Models (LLMs) to autonomously enhance the training of smaller target models. Inspired by the theory of\"Learning from Errors\", this framework employs an instructor LLM to meticulously analyze the specific errors within a target model, facilitating targeted and efficient training cycles. Within this framework, we implement two strategies:\"Learning from Error,\"which focuses solely on incorrect responses to tailor training data, and\"Learning from Error by Contrast\", which uses contrastive learning to analyze both correct and incorrect responses for a deeper understanding of errors. Our empirical studies, conducted with several open-source models, demonstrate significant improvements across multiple benchmarks, including mathematical reasoning, coding abilities, and factual knowledge. Notably, the refined Llama-3-8b-Instruction has outperformed ChatGPT, illustrating the effectiveness of our approach. By leveraging the strengths of both strategies, we have attained a more balanced performance improvement on both in-domain and out-of-domain benchmarks. Our code can be found at https://yingjiahao14.github.io/LLMs-as-Instructors-pages/.",
            "link": "https://www.semanticscholar.org/paper/70b3ae0480e795608ce0b2a4b555d002c5d6f4d7",
            "authors": "Jiahao Ying, Mingbao Lin, Yixin Cao, Wei Tang, Bo Wang, Qianru Sun, Xuanjing Huang, Shuicheng Yan",
            "matchScore": 278.1972,
            "original title": "LLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement",
            "original authors": "Jiahao Ying, Mingbao Lin, Yixin Cao, Wei Tang, Bo Wang, Qianru Sun, Xuanjing Huang, Shuicheng YAN",
            "EMNLP Paper ID": "2222",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "1ef95907484526088c4370c5a60164fe56399551",
            "title": "Resilience of Large Language Models for Noisy Instructions",
            "abstract": "As the rapidly advancing domain of natural language processing (NLP), large language models (LLMs) have emerged as powerful tools for interpreting human commands and generating text across various tasks. Nonetheless, the resilience of LLMs to handle text containing inherent errors, stemming from human interactions and collaborative systems, has not been thoroughly explored. Our study investigates the resilience of LLMs against five common types of disruptions including 1) ASR (Automatic Speech Recognition) errors, 2) OCR (Optical Character Recognition) errors, 3) grammatical mistakes, 4) typographical errors, and 5) distractive content. We aim to investigate how these models react by deliberately embedding these errors into instructions. Our findings reveal that while some LLMs show a degree of resistance to certain types of noise, their overall performance significantly suffers. This emphasizes the importance of further investigation into enhancing model resilience. In response to the observed decline in performance, our study also evaluates a\"re-pass\"strategy, designed to purify the instructions of noise before the LLMs process them. Our analysis indicates that correcting noisy instructions, particularly for open-source LLMs, presents significant challenges.",
            "link": "https://www.semanticscholar.org/paper/1ef95907484526088c4370c5a60164fe56399551",
            "authors": "Bin Wang, Chengwei Wei, Zhengyuan Liu, Geyu Lin, Nancy F. Chen",
            "matchScore": 180.73059,
            "original title": "Resilience of Large Language Models for Noisy Instructions",
            "original authors": "Bin Wang, Chengwei Wei, Zhengyuan Liu, Geyu Lin, Nancy F. Chen",
            "EMNLP Paper ID": "2356",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "69c8421f110dd8133ac45c2d77652fc85aebd18e",
            "title": "Learning to Refine with Fine-Grained Natural Language Feedback",
            "abstract": "Recent work has explored the capability of large language models (LLMs) to identify and correct errors in LLM-generated responses. These refinement approaches frequently evaluate what sizes of models are able to do refinement for what problems, but less attention is paid to what effective feedback for refinement looks like. In this work, we propose looking at refinement with feedback as a composition of three distinct LLM competencies: (1) detection of bad generations; (2) fine-grained natural language critique generation; (3) refining with fine-grained feedback. The first step can be implemented with a high-performing discriminative model and steps 2 and 3 can be implemented either via prompted or fine-tuned LLMs. A key property of the proposed Detect, Critique, Refine (\"DCR\") method is that the step 2 critique model can give fine-grained feedback about errors, made possible by offloading the discrimination to a separate model in step 1. We show that models of different capabilities benefit from refining with DCR on the task of improving factual consistency of document grounded summaries. Overall, DCR consistently outperforms existing end-to-end refinement approaches and current trained models not fine-tuned for factuality critiquing.",
            "link": "https://www.semanticscholar.org/paper/69c8421f110dd8133ac45c2d77652fc85aebd18e",
            "authors": "Manya Wadhwa, Xinyu Zhao, Junyi Jessy Li, Greg Durrett",
            "matchScore": 214.66283,
            "original title": "Learning to Refine with Fine-Grained Natural Language Feedback",
            "original authors": "Manya Wadhwa, Xinyu Zhao, Junyi Jessy Li, Greg Durrett",
            "EMNLP Paper ID": "2411",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "7ea2b0d6bab8e5a6ca0660c8737d7938cd3b711d",
            "title": "To Err Is Human, but Llamas Can Learn It Too",
            "abstract": "This study explores enhancing grammatical error correction (GEC) through artificial error generation (AEG) using language models (LMs). Specifically, we fine-tune Llama 2-based LMs for error generation and find that this approach yields synthetic errors akin to human errors. Next, we train GEC Llama models with the help of these artificial errors and outperform previous state-of-the-art error correction models, with gains ranging between 0.8 and 6 F0.5 points across all tested languages (German, Ukrainian, and Estonian). Moreover, we demonstrate that generating errors by fine-tuning smaller sequence-to-sequence models and prompting large commercial LMs (GPT-3.5 and GPT-4) also results in synthetic errors beneficially affecting error generation models.",
            "link": "https://www.semanticscholar.org/paper/7ea2b0d6bab8e5a6ca0660c8737d7938cd3b711d",
            "authors": "Agnes Luhtaru, Taido Purason, Martin Vainikko, Maksym Del, Mark Fishel",
            "matchScore": 249.06255,
            "original title": "To Err Is Human, but Llamas Can Learn It Too",
            "original authors": "Agnes Luhtaru, Taido Purason, Martin Vainikko, Maksym Del, Mark Fishel",
            "EMNLP Paper ID": "2434",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        }
    ],
    "Enhancements and Applications of Large Language Models": [
        {
            "paperId": "d3f52ab6abc86b269380fbfd8b2a77b69013af3f",
            "title": "Standardize: Aligning Language Models with Expert-Defined Standards for Content Generation",
            "abstract": "Domain experts across engineering, healthcare, and education follow strict standards for producing quality content such as technical manuals, medication instructions, and children's reading materials. However, current works in controllable text generation have yet to explore using these standards as references for control. Towards this end, we introduce Standardize, a retrieval-style in-context learning-based framework to guide large language models to align with expert-defined standards. Focusing on English language standards in the education domain as a use case, we consider the Common European Framework of Reference for Languages (CEFR) and Common Core Standards (CCS) for the task of open-ended content generation. Our findings show that models can gain a 45% to 100% increase in precise accuracy across open and commercial LLMs evaluated, demonstrating that the use of knowledge artifacts extracted from standards and integrating them in the generation process can effectively guide models to produce better standard-aligned content.",
            "link": "https://www.semanticscholar.org/paper/d3f52ab6abc86b269380fbfd8b2a77b69013af3f",
            "authors": "Joseph Marvin Imperial, Gail Forey, Harish Tayyar Madabushi",
            "EMNLP Paper ID": "186",
            "Oral/Poster": "Oral",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "e03648463405a77515c6af6cae4947a029b465ae",
            "title": "Teaching LLMs to Abstain across Languages via Multilingual Feedback",
            "abstract": "Multilingual LLMs often have knowledge disparities across languages, with larger gaps in under-resourced languages. Teaching LLMs to abstain in the face of knowledge gaps is thus a promising strategy to mitigate hallucinations in multilingual settings. However, previous studies on LLM abstention primarily focus on English; we find that directly applying existing solutions beyond English results in up to 20.5% performance gaps between high and low-resource languages, potentially due to LLMs' drop in calibration and reasoning beyond a few resource-rich languages. To this end, we propose strategies to enhance LLM abstention by learning from multilingual feedback, where LLMs self-reflect on proposed answers in one language by generating multiple feedback items in related languages: we show that this helps identifying the knowledge gaps across diverse languages, cultures, and communities. Extensive experiments demonstrate that our multilingual feedback approach outperforms various strong baselines, achieving up to 9.2% improvement for low-resource languages across three black-box and open models on three datasets, featuring open-book, closed-book, and commonsense QA. Further analysis reveals that multilingual feedback is both an effective and a more equitable abstain strategy to serve diverse language speakers, and cultural factors have great impact on language selection and LLM abstention behavior, highlighting future directions for multilingual and multi-cultural reliable language modeling.",
            "link": "https://www.semanticscholar.org/paper/e03648463405a77515c6af6cae4947a029b465ae",
            "authors": "Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Orevaoghene Ahia, Shuyue Stella Li, Vidhisha Balachandran, Sunayana Sitaram, Yulia Tsvetkov",
            "EMNLP Paper ID": "460",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "4b6749c981de4c3f519ef18749ecd3059abbff32",
            "title": "C-LLM: Learn to Check Chinese Spelling Errors Character by Character",
            "abstract": "Chinese Spell Checking (CSC) aims to detect and correct spelling errors in sentences. Despite Large Language Models (LLMs) exhibit robust capabilities and are widely applied in various tasks, their performance on CSC is often unsatisfactory. We find that LLMs fail to meet the Chinese character-level constraints of the CSC task, namely equal length and phonetic similarity, leading to a performance bottleneck. Further analysis reveal that this issue stems from the granularity of tokenization, as current mixed character-word tokenization struggles to satisfy these character-level constraints. To address this issue, we propose C-LLM, a Large Language Model-based Chinese Spell Checking method that learns to check errors Character by Character. Character-level tokenization enables the model to learn character-level alignment, effectively mitigating issues related to character-level constraints. Furthermore, CSC is simplified to replication-dominated and substitution-supplemented tasks. Experiments on two CSC benchmarks demonstrate that C-LLM achieves an average improvement of 10% over existing methods. Specifically, it shows a 2.1% improvement in general scenarios and a significant 12% improvement in vertical domain scenarios, establishing state-of-the-art performance. The source code can be accessed at https://github.com/ktlKTL/C-LLM.",
            "link": "https://www.semanticscholar.org/paper/4b6749c981de4c3f519ef18749ecd3059abbff32",
            "authors": "Kunting Li, Yong Hu, Liang He, Fandong Meng, Jie Zhou",
            "EMNLP Paper ID": "661",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 14): 13:00\u201314:00 (Afternoon)",
            "Type": "Main"
        },
        {
            "paperId": "1f874c94d531f6776f65e0aa807adbf432ea35cf",
            "title": "Understanding and Mitigating Language Confusion in LLMs",
            "abstract": "We investigate a surprising limitation of LLMs: their inability to consistently generate text in a user's desired language. We create the Language Confusion Benchmark (LCB) to evaluate such failures, covering 15 typologically diverse languages with existing and newly-created English and multilingual prompts. We evaluate a range of LLMs on monolingual and cross-lingual generation reflecting practical use cases, finding that Llama Instruct and Mistral models exhibit high degrees of language confusion and even the strongest models fail to consistently respond in the correct language. We observe that base and English-centric instruct models are more prone to language confusion, which is aggravated by complex prompts and high sampling temperatures. We find that language confusion can be partially mitigated via few-shot prompting, multilingual SFT and preference tuning. We release our language confusion benchmark, which serves as a first layer of efficient, scalable multilingual evaluation at https://github.com/for-ai/language-confusion.",
            "link": "https://www.semanticscholar.org/paper/1f874c94d531f6776f65e0aa807adbf432ea35cf",
            "authors": "Kelly Marchisio, Wei-Yin Ko, Alexandre B'erard, Th'eo Dehaze, Sebastian Ruder",
            "EMNLP Paper ID": "743",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "a4e3fcc394193737d9925fb74ddb520fa6045a6d",
            "title": "I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses",
            "abstract": "This paper explores an intriguing observation: fine-tuning a large language model (LLM) with responses generated by a LLM often yields better results than using responses generated by humans, particularly in reasoning tasks. We conduct an in-depth investigation to understand why this occurs. Contrary to the common belief that these instances is due to the more detailed nature of LLM-generated content, our study identifies another contributing factor: an LLM is inherently more\"familiar\"with LLM generated responses. This familiarity is evidenced by lower perplexity before fine-tuning. We design a series of experiments to understand the impact of the\"familiarity\"and our conclusion reveals that this\"familiarity\"significantly impacts learning performance. Training with LLM-generated responses not only enhances performance but also helps maintain the model's capabilities in other reasoning tasks after fine-tuning on a specific task.",
            "link": "https://www.semanticscholar.org/paper/a4e3fcc394193737d9925fb74ddb520fa6045a6d",
            "authors": "Xuan Ren, Biao Wu, Lingqiao Liu",
            "EMNLP Paper ID": "1144",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "73e98c367b5e2b95a2965cc5d25f64155e2da887",
            "title": "LLM-based Code-Switched Text Generation for Grammatical Error Correction",
            "abstract": "With the rise of globalisation, code-switching (CSW) has become a ubiquitous part of multilingual conversation, posing new challenges for natural language processing (NLP), especially in Grammatical Error Correction (GEC). This work explores the complexities of applying GEC systems to CSW texts. Our objectives include evaluating the performance of state-of-the-art GEC systems on an authentic CSW dataset from English as a Second Language (ESL) learners, exploring synthetic data generation as a solution to data scarcity, and developing a model capable of correcting grammatical errors in monolingual and CSW texts. We generated synthetic CSW GEC data, resulting in one of the first substantial datasets for this task, and showed that a model trained on this data is capable of significant improvements over existing systems. This work targets ESL learners, aiming to provide educational technologies that aid in the development of their English grammatical correctness without constraining their natural multilingualism.",
            "link": "https://www.semanticscholar.org/paper/73e98c367b5e2b95a2965cc5d25f64155e2da887",
            "authors": "Tom Potter, Zheng Yuan",
            "EMNLP Paper ID": "1994",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "619346184ae157741fc1c4ac74447ab2455cd6f3",
            "title": "A Simple yet Effective Training-free Prompt-free Approach to Chinese Spelling Correction Based on Large Language Models",
            "abstract": "This work proposes a simple training-free prompt-free approach to leverage large language models (LLMs) for the Chinese spelling correction (CSC) task, which is totally different from all previous CSC approaches. The key idea is to use an LLM as a pure language model in a conventional manner. The LLM goes through the input sentence from the beginning, and at each inference step, produces a distribution over its vocabulary for deciding the next token, given a partial sentence. To ensure that the output sentence remains faithful to the input sentence, we design a minimal distortion model that utilizes pronunciation or shape similarities between the original and replaced characters. Furthermore, we propose two useful reward strategies to address practical challenges specific to the CSC task. Experiments on five public datasets demonstrate that our approach significantly improves LLM performance, enabling them to compete with state-of-the-art domain-general CSC models.",
            "link": "https://www.semanticscholar.org/paper/619346184ae157741fc1c4ac74447ab2455cd6f3",
            "authors": "Houquan Zhou, Zhenghua Li, Bo Zhang, Chen Li, Shaopeng Lai, Ji Zhang, Fei Huang, Min Zhang",
            "EMNLP Paper ID": "2080",
            "Oral/Poster": "Oral",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "a9e89f28a085556683933dabe0d8e0da66022147",
            "title": "LLM-TOPLA: Efficient LLM Ensemble by Maximising Diversity",
            "abstract": "Combining large language models during training or at inference time has shown substantial performance gain over component LLMs. This paper presents LLM-TOPLA, a diversity-optimized LLM ensemble method with three unique properties: (i) We introduce the focal diversity metric to capture the diversity-performance correlation among component LLMs of an ensemble. (ii) We develop a diversity-optimized ensemble pruning algorithm to select the top-k sub-ensembles from a pool of $N$ base LLMs. Our pruning method recommends top-performing LLM subensembles of size $S$, often much smaller than $N$. (iii) We generate new output for each prompt query by utilizing a learn-to-ensemble approach, which learns to detect and resolve the output inconsistency among all component LLMs of an ensemble. Extensive evaluation on four different benchmarks shows good performance gain over the best LLM ensemble methods: (i) In constrained solution set problems, LLM-TOPLA outperforms the best-performing ensemble (Mixtral) by 2.2\\% in accuracy on MMLU and the best-performing LLM ensemble (MoreAgent) on GSM8k by 2.1\\%. (ii) In generative tasks, LLM-TOPLA outperforms the top-2 performers (Llama70b/Mixtral) on SearchQA by $3.9\\mathrm{x}$ in F1, and on XSum by more than $38$ in ROUGE-1. Our code and dataset, which contains outputs of 8 modern LLMs on 4 benchmarks is available at https://github.com/git-disl/llm-topla",
            "link": "https://www.semanticscholar.org/paper/a9e89f28a085556683933dabe0d8e0da66022147",
            "authors": "S. Tekin, Fatih Ilhan, Tiansheng Huang, Sihao Hu, Ling Liu",
            "matchScore": 304.56232,
            "original title": "LLM-TOPLA: Efficient LLM Ensemble by Maximising Diversity",
            "original authors": "Selim Furkan Tekin, Fatih Ilhan, Tiansheng Huang, Sihao Hu, Ling Liu",
            "EMNLP Paper ID": "2359",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "e48b5bbd881098b4dcee07e037d243b6177566e6",
            "title": "The Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking Strategies from Benchmark Performance",
            "abstract": "Cloze testing is a common method for measuring the behavior of large language models on a number of benchmark tasks. Using the MMLU dataset, we show that the base-rate probability (BRP) differences across answer tokens are significant and affect task performance ie. guess A if uncertain. We find that counterfactual prompting does sufficiently mitigate the BRP effect. The BRP effect is found to have a similar effect to test taking strategies employed by humans leading to the conflation of task performance and test-taking ability. We propose the Nvr-X-MMLU task, a variation of MMLU, which helps to disambiguate test-taking ability from task performance and reports the latter.",
            "link": "https://www.semanticscholar.org/paper/e48b5bbd881098b4dcee07e037d243b6177566e6",
            "authors": "Kyle Moore, Jesse Roberts, Thao Pham, Oseremhen Ewaleifoh, Douglas H. Fisher",
            "matchScore": 375.03693,
            "original title": "The Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking Strategies from Benchmark Performance",
            "original authors": "Kyle Moore, Jesse Roberts, Thao Pham, Oseremhen Ewaleifoh, Douglas Fisher",
            "EMNLP Paper ID": "458",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "a77f2fc7a4dfd847439c6bbf391c0550822b7cd2",
            "title": "Bi-DCSpell: A Bi-directional Detector-Corrector Interactive Framework for Chinese Spelling Check",
            "abstract": "Chinese Spelling Check (CSC) aims to detect and correct potentially misspelled characters in Chinese sentences. Naturally, it involves the detection and correction subtasks, which interact with each other dynamically. Such interactions are bi-directional, i.e., the detection result would help reduce the risk of over-correction and under-correction while the knowledge learnt from correction would help prevent false detection. Current CSC approaches are of two types: correction-only or single-directional detection-to-correction interactive frameworks. Nonetheless, they overlook the bi-directional interactions between detection and correction. This paper aims to fill the gap by proposing a Bi-directional Detector-Corrector framework for CSC (Bi-DCSpell). Notably, Bi-DCSpell contains separate detection and correction encoders, followed by a novel interactive learning module facilitating bi-directional feature interactions between detection and correction to improve each other's representation learning. Extensive experimental results demonstrate a robust correction performance of Bi-DCSpell on widely used benchmarking datasets while possessing a satisfactory detection ability.",
            "link": "https://www.semanticscholar.org/paper/a77f2fc7a4dfd847439c6bbf391c0550822b7cd2",
            "authors": "Haiming Wu, Hanqing Zhang, Richeng Xuan, Dawei Song",
            "matchScore": 329.5395,
            "original title": "Bi-DCSpell: A Bi-directional Detector-Corrector Interactive Framework for Chinese Spelling Check",
            "original authors": "Haiming Wu, Hanqing Zhang, richeng xuan, Dawei Song",
            "EMNLP Paper ID": "789",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 12): 17:45\u201318:45 (Evening)",
            "Type": "Findings"
        }
    ],
    "Improving LLM Efficiency and Performance in Text-to-SQL and Document Understanding": [
        {
            "paperId": "e9d58b92de2b6c6d8ddba09e561de3b996aa3c1c",
            "title": "DocKD: Knowledge Distillation from LLMs for Open-World Document Understanding Models",
            "abstract": "Visual document understanding (VDU) is a challenging task that involves understanding documents across various modalities (text and image) and layouts (forms, tables, etc.). This study aims to enhance generalizability of small VDU models by distilling knowledge from LLMs. We identify that directly prompting LLMs often fails to generate informative and useful data. In response, we present a new framework (called DocKD) that enriches the data generation process by integrating external document knowledge. Specifically, we provide an LLM with various document elements like key-value pairs, layouts, and descriptions, to elicit open-ended answers. Our experiments show that DocKD produces high-quality document annotations and surpasses the direct knowledge distillation approach that does not leverage external document knowledge. Moreover, student VDU models trained with solely DocKD-generated data are not only comparable to those trained with human-annotated data on in-domain tasks but also significantly excel them on out-of-domain tasks.",
            "link": "https://www.semanticscholar.org/paper/e9d58b92de2b6c6d8ddba09e561de3b996aa3c1c",
            "authors": "Sungnyun Kim, Haofu Liao, Srikar Appalaraju, Peng Tang, Zhuowen Tu, R. Satzoda, R. Manmatha, Vijay Mahadevan, Stefano Soatto",
            "EMNLP Paper ID": "353",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "647f2aa9c120a200b9b91363c5677f7d89b21d4d",
            "title": "PTD-SQL: Partitioning and Targeted Drilling with LLMs in Text-to-SQL",
            "abstract": "Large Language Models (LLMs) have emerged as powerful tools for Text-to-SQL tasks, exhibiting remarkable reasoning capabilities. Different from tasks such as math word problems and commonsense reasoning, SQL solutions have a relatively fixed pattern. This facilitates the investigation of whether LLMs can benefit from categorical thinking, mirroring how humans acquire knowledge through inductive reasoning based on comparable examples. In this study, we propose that employing query group partitioning allows LLMs to focus on learning the thought processes specific to a single problem type, consequently enhancing their reasoning abilities across diverse difficulty levels and problem categories. Our experiments reveal that multiple advanced LLMs, when equipped with PTD-SQL, can either surpass or match previous state-of-the-art (SOTA) methods on the Spider and BIRD datasets. Intriguingly, models with varying initial performances have exhibited significant improvements, mainly at the boundary of their capabilities after targeted drilling, suggesting a parallel with human progress. Code is available at https://github.com/lrlbbzl/PTD-SQL.",
            "link": "https://www.semanticscholar.org/paper/647f2aa9c120a200b9b91363c5677f7d89b21d4d",
            "authors": "Ruilin Luo, Liyuan Wang, Binghuai Lin, Zicheng Lin, Yujiu Yang",
            "EMNLP Paper ID": "430",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "8a9b8f73c1beb9b32571802d27ea30921a1a0f60",
            "title": "Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and Schema Pruning",
            "abstract": "We focus on Text-to-SQL semantic parsing from the perspective of Large Language Models. Motivated by challenges related to the size of commercial database schemata and the deployability of business intelligence solutions, we propose an approach that dynamically retrieves input database information and uses abstract syntax trees to select few-shot examples for in-context learning. Furthermore, we investigate the extent to which an in-parallel semantic parser can be leveraged for generating $\\textit{approximated}$ versions of the expected SQL queries, to support our retrieval. We take this approach to the extreme--we adapt a model consisting of less than $500$M parameters, to act as an extremely efficient approximator, enhancing it with the ability to process schemata in a parallelised manner. We apply our approach to monolingual and cross-lingual benchmarks for semantic parsing, showing improvements over state-of-the-art baselines. Comprehensive experiments highlight the contribution of modules involved in this retrieval-augmented generation setting, revealing interesting directions for future work.",
            "link": "https://www.semanticscholar.org/paper/8a9b8f73c1beb9b32571802d27ea30921a1a0f60",
            "authors": "Zhili Shen, P. Vougiouklis, Chenxin Diao, Kaustubh Vyas, Yuanyi Ji, Jeff Z. Pan",
            "EMNLP Paper ID": "897",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "84c567316b2eb804fb673871ae89748c3990a558",
            "title": "Dual-Space Knowledge Distillation for Large Language Models",
            "abstract": "Knowledge distillation (KD) is known as a promising solution to compress large language models (LLMs) via transferring their knowledge to smaller models. During this process, white-box KD methods usually minimize the distance between the output distributions of the two models so that more knowledge can be transferred. However, in the current white-box KD framework, the output distributions are from the respective output spaces of the two models, using their own prediction heads. We argue that the space discrepancy will lead to low similarity between the teacher model and the student model on both representation and distribution levels. Furthermore, this discrepancy also hinders the KD process between models with different vocabularies, which is common for current LLMs. To address these issues, we propose a dual-space knowledge distillation (DSKD) framework that unifies the output spaces of the two models for KD. On the basis of DSKD, we further develop a cross-model attention mechanism, which can automatically align the representations of the two models with different vocabularies. Thus, our framework is not only compatible with various distance functions for KD (e.g., KL divergence) like the current framework, but also supports KD between any two LLMs regardless of their vocabularies. Experiments on task-agnostic instruction-following benchmarks show that DSKD significantly outperforms the current white-box KD framework with various distance functions, and also surpasses existing KD methods for LLMs with different vocabularies.",
            "link": "https://www.semanticscholar.org/paper/84c567316b2eb804fb673871ae89748c3990a558",
            "authors": "Songming Zhang, Xue Zhang, Zengkui Sun, Yufeng Chen, Jinan Xu",
            "EMNLP Paper ID": "2234",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "ccb505e9672548fd930a979a5dd6cbf5a8b40ece",
            "title": "Learning from Imperfect Data: Towards Efficient Knowledge Distillation of Autoregressive Language Models for Text-to-SQL",
            "abstract": "Large Language Models (LLMs) have shown promising performance in text-to-SQL, which involves translating natural language questions into SQL queries. However, current text-to-SQL LLMs are computationally expensive and challenging to deploy in real-world applications, highlighting the importance of compressing them. To achieve this goal, knowledge distillation (KD) is a common approach, which aims to distill the larger teacher model into a smaller student model. While numerous KD methods for autoregressive LLMs have emerged recently, it is still under-explored whether they work well in complex text-to-SQL scenarios. To this end, we conduct a series of analyses and reveal that these KD methods generally fall short in balancing performance and efficiency. In response to this problem, we propose to improve the KD with Imperfect Data, namely KID, which effectively boosts the performance without introducing much training budget. The core of KID is to efficiently mitigate the training-inference mismatch by simulating the cascading effect of inference in the imperfect training data. Extensive experiments on 5 text-to-SQL benchmarks show that, KID can not only achieve consistent and significant performance gains (up to +5.83% average score) across all model types and sizes, but also effectively improve the training efficiency.",
            "link": "https://www.semanticscholar.org/paper/ccb505e9672548fd930a979a5dd6cbf5a8b40ece",
            "authors": "Qihuang Zhong, Kunfeng Chen, Liang Ding, Juhua Liu, Bo Du, D. Tao",
            "matchScore": 328.78973,
            "original title": "Learning from Imperfect Data: Towards Efficient Knowledge Distillation of Autoregressive Language Models for Text-to-SQL",
            "original authors": "Qihuang Zhong, Kunfeng Chen, Liang Ding, Juhua Liu, Bo Du, Dacheng Tao",
            "EMNLP Paper ID": "1410",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "07de76b2862cf7738b3fd28aabd708186231a527",
            "title": "DTS-SQL: Decomposed Text-to-SQL with Small Large Language Models",
            "abstract": "Leading models for the text-to-SQL task heavily rely on proprietary Large Language Models (LLMs), posing concerns over data privacy. Closing the performance gap between small open-source models and large proprietary models is crucial to mitigate this reliance. To this end, we introduce a novel two-stage fine-tuning approach that decomposes the task into two simpler tasks. Through comprehensive evaluation on two large cross-domain datasets and two small LLMs, we show that this approach improves execution accuracy by 3 to 7 percent, effectively aligning the performance of open-source models with their proprietary counterparts.",
            "link": "https://www.semanticscholar.org/paper/07de76b2862cf7738b3fd28aabd708186231a527",
            "authors": "Mohammadreza Pourreza, Davood Rafiei",
            "matchScore": 300.69223,
            "original title": "DTS-SQL: Decomposed Text-to-SQL with Small Large Language Models",
            "original authors": "Mohammadreza Pourreza, Davood Rafiei",
            "EMNLP Paper ID": "1730",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "2d125b26788e41586fa1594da60169f33fb481f0",
            "title": "Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL",
            "abstract": "Currently, the in-context learning method based on large language models (LLMs) has become the mainstream of text-to-SQL research. Previous works have discussed how to select demonstrations related to the user question from a human-labeled demonstration pool. However, human labeling suffers from the limitations of insufficient diversity and high labeling overhead. Therefore, in this paper, we discuss how to measure and improve the diversity of the demonstrations for text-to-SQL. We present a metric to measure the diversity of the demonstrations and analyze the insufficient of the existing labeled data by experiments. Based on the above discovery, we propose fusing iteratively for demonstrations (Fused) to build a high-diversity demonstration pool through human-free multiple-iteration synthesis, improving diversity and lowering label cost. Our method achieves an average improvement of 3.2% and 5.0% with and without human labeling on several mainstream datasets, which proves the effectiveness of Fused.",
            "link": "https://www.semanticscholar.org/paper/2d125b26788e41586fa1594da60169f33fb481f0",
            "authors": "Dingzirui Wang, Longxu Dou, Xuanliang Zhang, Qingfu Zhu, Wanxiang Che",
            "matchScore": 275.7282,
            "original title": "Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL",
            "original authors": "Dingzirui Wang, Longxu Dou, Xuanliang Zhang, Qingfu Zhu, Wanxiang Che",
            "EMNLP Paper ID": "250",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "51b3597c8ed0cd1588b7c5995adf7def69920fab",
            "title": "\"What is the value of {templates}?\"Rethinking Document Information Extraction Datasets for LLMs",
            "abstract": "The rise of large language models (LLMs) for visually rich document understanding (VRDU) has kindled a need for prompt-response, document-based datasets. As annotating new datasets from scratch is labor-intensive, the existing literature has generated prompt-response datasets from available resources using simple templates. For the case of key information extraction (KIE), one of the most common VRDU tasks, past work has typically employed the template\"What is the value for the {key}?\". However, given the variety of questions encountered in the wild, simple and uniform templates are insufficient for creating robust models in research and industrial contexts. In this work, we present K2Q, a diverse collection of five datasets converted from KIE to a prompt-response format using a plethora of bespoke templates. The questions in K2Q can span multiple entities and be extractive or boolean. We empirically compare the performance of seven baseline generative models on K2Q with zero-shot prompting. We further compare three of these models when training on K2Q versus training on simpler templates to motivate the need of our work. We find that creating diverse and intricate KIE questions enhances the performance and robustness of VRDU models. We hope this work encourages future studies on data quality for generative model training.",
            "link": "https://www.semanticscholar.org/paper/51b3597c8ed0cd1588b7c5995adf7def69920fab",
            "authors": "Ran Zmigrod, Pranav Shetty, Mathieu Sibue, Zhiqiang Ma, Armineh Nourbakhsh, Xiaomo Liu, Manuela Veloso",
            "matchScore": 257.3973,
            "original title": "\u201cWhat is the value of {templates}?\u201d Rethinking Document Information Extraction Datasets for LLMs",
            "original authors": "Ran Zmigrod, Pranav Shetty, Mathieu Sibue, Zhiqiang Ma, Armineh Nourbakhsh, Xiaomo Liu, Manuela Veloso",
            "EMNLP Paper ID": "2570",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "b85d8b2bd6caa0e1e62af0e4e75836055c457dfc",
            "title": "Knowledge-Centric Templatic Views of Documents",
            "abstract": "Authors seeking to communicate with broader audiences often share their ideas in various document formats, such as slide decks, newsletters, reports, and posters. Prior work on document generation has generally tackled the creation of each separate format to be a different task, leading to fragmented learning processes, redundancy in models and methods, and disjointed evaluation. We consider each of these documents as templatic views of the same underlying knowledge/content, and we aim to unify the generation and evaluation of these templatic views. We begin by showing that current LLMs are capable of generating various document formats with little to no supervision. Further, a simple augmentation involving a structured intermediate representation can improve performance, especially for smaller models. We then introduce a novel unified evaluation framework that can be adapted to measuring the quality of document generators for heterogeneous downstream applications. This evaluation is adaptable to a range of user defined criteria and application scenarios, obviating the need for task specific evaluation metrics. Finally, we conduct a human evaluation, which shows that people prefer 82% of the documents generated with our method, while correlating more highly with our unified evaluation framework than prior metrics in the literature.",
            "link": "https://www.semanticscholar.org/paper/b85d8b2bd6caa0e1e62af0e4e75836055c457dfc",
            "authors": "Isabel Cachola, Silviu Cucerzan, Allen Herring, Vuksan Mijovic, Erik Oveson, S. Jauhar",
            "matchScore": 188.76984,
            "original title": "Knowledge-Centric Templatic Views of Documents",
            "original authors": "Isabel Alyssa Cachola, Silviu Cucerzan, Allen herring, Vuksan Mijovic, Erik Oveson, Sujay Kumar Jauhar",
            "EMNLP Paper ID": "2978",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "6fa3544f42bc026ac684cf6c7a8cd50f59b3ee7d",
            "title": "Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation",
            "abstract": "Large language models (LLMs) have significantly advanced various natural language processing tasks, but deploying them remains computationally expensive. Knowledge distillation (KD) is a promising solution, enabling the transfer of capabilities from larger teacher LLMs to more compact student models. Particularly, sequence-level KD, which distills rationale-based reasoning processes instead of merely final outcomes, shows great potential in enhancing students' reasoning capabilities. However, current methods struggle with sequence level KD under long-tailed data distributions, adversely affecting generalization on sparsely represented domains. We introduce the Multi-Stage Balanced Distillation (BalDistill) framework, which iteratively balances training data within a fixed computational budget. By dynamically selecting representative head domain examples and synthesizing tail domain examples, BalDistill achieves state-of-the-art performance across diverse long-tailed datasets, enhancing both the efficiency and efficacy of the distilled models.",
            "link": "https://www.semanticscholar.org/paper/6fa3544f42bc026ac684cf6c7a8cd50f59b3ee7d",
            "authors": "Yuhang Zhou, Jing Zhu, Paiheng Xu, Xiaoyu Liu, Xiyao Wang, Danai Koutra, Wei Ai, Furong Huang",
            "matchScore": 314.33307,
            "original title": "Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation",
            "original authors": "Yuhang Zhou, Jing Zhu, Paiheng Xu, Xiaoyu Liu, Xiyao Wang, Danai Koutra, Wei Ai, Furong Huang",
            "EMNLP Paper ID": "677",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        }
    ],
    "Temporal Reasoning and Performance Evaluation of LLMs": [
        {
            "paperId": "9a2a0fcc0366346d4c23689d80dc50c429da3f66",
            "title": "Will LLMs Replace the Encoder-Only Models in Temporal Relation Classification?",
            "abstract": "The automatic detection of temporal relations among events has been mainly investigated with encoder-only models such as RoBERTa. Large Language Models (LLM) have recently shown promising performance in temporal reasoning tasks such as temporal question answering. Nevertheless, recent studies have tested the LLMs' performance in detecting temporal relations of closed-source models only, limiting the interpretability of those results. In this work, we investigate LLMs' performance and decision process in the Temporal Relation Classification task. First, we assess the performance of seven open and closed-sourced LLMs experimenting with in-context learning and lightweight fine-tuning approaches. Results show that LLMs with in-context learning significantly underperform smaller encoder-only models based on RoBERTa. Then, we delve into the possible reasons for this gap by applying explainable methods. The outcome suggests a limitation of LLMs in this task due to their autoregressive nature, which causes them to focus only on the last part of the sequence. Additionally, we evaluate the word embeddings of these two models to better understand their pre-training differences. The code and the fine-tuned models can be found respectively on GitHub.",
            "link": "https://www.semanticscholar.org/paper/9a2a0fcc0366346d4c23689d80dc50c429da3f66",
            "authors": "G. Roccabruna, Massimo Rizzoli, Giuseppe Riccardi",
            "EMNLP Paper ID": "2662",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "80b06bb6b5ab0e6e6de9eecf8d5829dec2f6df57",
            "title": "Evaluating Large Language Models on Time Series Feature Understanding: A Comprehensive Taxonomy and Benchmark",
            "abstract": "Large Language Models (LLMs) offer the potential for automatic time series analysis and reporting, which is a critical task across many domains, spanning healthcare, finance, climate, energy, and many more. In this paper, we propose a framework for rigorously evaluating the capabilities of LLMs on time series understanding, encompassing both univariate and multivariate forms. We introduce a comprehensive taxonomy of time series features, a critical framework that delineates various characteristics inherent in time series data. Leveraging this taxonomy, we have systematically designed and synthesized a diverse dataset of time series, embodying the different outlined features, each accompanied by textual descriptions. This dataset acts as a solid foundation for assessing the proficiency of LLMs in comprehending time series. Our experiments shed light on the strengths and limitations of state-of-the-art LLMs in time series understanding, revealing which features these models readily comprehend effectively and where they falter. In addition, we uncover the sensitivity of LLMs to factors including the formatting of the data, the position of points queried within a series and the overall time series length.",
            "link": "https://www.semanticscholar.org/paper/80b06bb6b5ab0e6e6de9eecf8d5829dec2f6df57",
            "authors": "Elizabeth Fons, Rachneet Kaur, Soham Palande, Zhen Zeng, Svitlana Vyetrenko, T. Balch",
            "EMNLP Paper ID": "2973",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "345ab14ccc88798f9787e8b0ed2108052f3f054a",
            "title": "Explaining Graph Neural Networks with Large Language Models: A Counterfactual Perspective for Molecular Property Prediction",
            "abstract": "In recent years, Graph Neural Networks (GNNs) have become successful in molecular property prediction tasks such as toxicity analysis. However, due to the black-box nature of GNNs, their outputs can be concerning in high-stakes decision-making scenarios, e.g., drug discovery. Facing such an issue, Graph Counterfactual Explanation (GCE) has emerged as a promising approach to improve GNN transparency. However, current GCE methods usually fail to take domain-specific knowledge into consideration, which can result in outputs that are not easily comprehensible by humans. To address this challenge, we propose a novel GCE method, LLM-GCE, to unleash the power of large language models (LLMs) in explaining GNNs for molecular property prediction. Specifically, we utilize an autoencoder to generate the counterfactual graph topology from a set of counterfactual text pairs (CTPs) based on an input graph. Meanwhile, we also incorporate a CTP dynamic feedback module to mitigate LLM hallucination, which provides intermediate feedback derived from the generated counterfactuals as an attempt to give more faithful guidance. Extensive experiments demonstrate the superior performance of LLM-GCE. Our code is released on https://github.com/YinhanHe123/new\\_LLM4GNNExplanation.",
            "link": "https://www.semanticscholar.org/paper/345ab14ccc88798f9787e8b0ed2108052f3f054a",
            "authors": "Yinhan He, Zaiyi Zheng, Patrick Soga, Yaozhen Zhu, Yushun Dong, Jundong Li",
            "matchScore": 210.50922,
            "original title": "Explaining Graph Neural Networks with Large Language Models: A Counterfactual Perspective on Molecule Graphs",
            "original authors": "Yinhan He, Zaiyi Zheng, Patrick Soga, Yaochen Zhu, Yushun Dong, Jundong Li",
            "EMNLP Paper ID": "1437",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "cd41fddd1c5427fcf6f6783507e4b67afa17cf87",
            "title": "Detecting Temporal Ambiguity in Questions",
            "abstract": "Detecting and answering ambiguous questions has been a challenging task in open-domain question answering. Ambiguous questions have different answers depending on their interpretation and can take diverse forms. Temporally ambiguous questions are one of the most common types of such questions. In this paper, we introduce TEMPAMBIQA, a manually annotated temporally ambiguous QA dataset consisting of 8,162 open-domain questions derived from existing datasets. Our annotations focus on capturing temporal ambiguity to study the task of detecting temporally ambiguous questions. We propose a novel approach by using diverse search strategies based on disambiguated versions of the questions. We also introduce and test non-search, competitive baselines for detecting temporal ambiguity using zero-shot and few-shot approaches.",
            "link": "https://www.semanticscholar.org/paper/cd41fddd1c5427fcf6f6783507e4b67afa17cf87",
            "authors": "Bhawna Piryani, Abdelrahman Abdallah, Jamshid Mozafari, Adam Jatowt",
            "matchScore": 159.07643,
            "original title": "Detecting Temporal Ambiguity in Questions",
            "original authors": "Bhawna Piryani, Abdelrahman Abdallah, Jamshid Mozafari, Adam Jatowt",
            "EMNLP Paper ID": "1990",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "94a8cb4f8b54d5fdfa3bad8ea729fdff8e2fd1e6",
            "title": "Enhancing Temporal Sensitivity and Reasoning for Time-Sensitive Question Answering",
            "abstract": "Time-Sensitive Question Answering (TSQA) demands the effective utilization of specific temporal contexts, encompassing multiple time-evolving facts, to address time-sensitive questions. This necessitates not only the parsing of temporal information within questions but also the identification and understanding of time-evolving facts to generate accurate answers. However, current large language models still have limited sensitivity to temporal information and their inadequate temporal reasoning capabilities. In this paper, we propose a novel framework that enhances temporal awareness and reasoning through Temporal Information-Aware Embedding and Granular Contrastive Reinforcement Learning. Experimental results on four TSQA datasets demonstrate that our framework significantly outperforms existing LLMs in TSQA tasks, marking a step forward in bridging the performance gap between machine and human temporal understanding and reasoning.",
            "link": "https://www.semanticscholar.org/paper/94a8cb4f8b54d5fdfa3bad8ea729fdff8e2fd1e6",
            "authors": "Wanqi Yang, Yanda Li, Meng Fang, Ling Chen",
            "matchScore": 242.53384,
            "original title": "Enhancing Temporal Sensitivity and Reasoning for Time-Sensitive Question Answering",
            "original authors": "Wanqi Yang, Yanda Li, Meng Fang, Ling Chen",
            "EMNLP Paper ID": "2795",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "8621010eaf6f5aa28bdeaa06745935e91627e7e3",
            "title": "Remember This Event That Year? Assessing Temporal Information and Reasoning in Large Language Models",
            "abstract": "Large Language Models (LLMs) are increasingly ubiquitous, yet their ability to retain and reason about temporal information remains limited, hindering their application in real-world scenarios where understanding the sequential nature of events is crucial. Our study experiments with 12 state-of-the-art models (ranging from 2B to 70B+ parameters) on a novel numerical-temporal dataset, \\textbf{TempUN}, spanning from 10,000 BCE to 2100 CE, to uncover significant temporal retention and comprehension limitations. We propose six metrics to assess three learning paradigms to enhance temporal knowledge acquisition. Our findings reveal that open-source models exhibit knowledge gaps more frequently, suggesting a trade-off between limited knowledge and incorrect responses. Additionally, various fine-tuning approaches significantly improved performance, reducing incorrect outputs and impacting the identification of 'information not available' in the generations. The associated dataset and code are available at (https://github.com/lingoiitgn/TempUN).",
            "link": "https://www.semanticscholar.org/paper/8621010eaf6f5aa28bdeaa06745935e91627e7e3",
            "authors": "Himanshu Beniwal, Dishant Patel, D. KowsikNandagopan, Hritik Ladia, Ankit Yadav, Mayank Singh",
            "matchScore": 229.71289,
            "original title": "Remember This Event That Year? Assessing Temporal Information and Understanding in Large Language Models",
            "original authors": "Himanshu Beniwal, Dishant Patel, Kowsik Nandagopan D, Hritik Ladia, Ankit Yadav, Mayank Singh",
            "EMNLP Paper ID": "3131",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "0aeeba8ddd3cbf9d9d913c26594bc1c19053b9f7",
            "title": "Narrative-of-Thought: Improving Temporal Reasoning of Large Language Models via Recounted Narratives",
            "abstract": "Reasoning about time and temporal relations is an integral aspect of human cognition, essential for perceiving the world and navigating our experiences. Though large language models (LLMs) have demonstrated impressive performance in many reasoning tasks, temporal reasoning remains challenging due to its intrinsic complexity. In this work, we first study an essential task of temporal reasoning -- temporal graph generation, to unveil LLMs' inherent, global reasoning capabilities. We show that this task presents great challenges even for the most powerful LLMs, such as GPT-3.5/4. We also notice a significant performance gap by small models (<10B) that lag behind LLMs by 50%. Next, we study how to close this gap with a budget constraint, e.g., not using model finetuning. We propose a new prompting technique tailored for temporal reasoning, Narrative-of-Thought (NoT), that first converts the events set to a Python class, then prompts a small model to generate a temporally grounded narrative, guiding the final generation of a temporal graph. Extensive experiments showcase the efficacy of NoT in improving various metrics. Notably, NoT attains the highest F1 on the Schema-11 evaluation set, while securing an overall F1 on par with GPT-3.5. NoT also achieves the best structural similarity across the board, even compared with GPT-3.5/4. Our code is available at https://github.com/launchnlp/NoT.",
            "link": "https://www.semanticscholar.org/paper/0aeeba8ddd3cbf9d9d913c26594bc1c19053b9f7",
            "authors": "Xinliang Frederick Zhang, Nick Beauchamp, Lu Wang",
            "matchScore": 183.41577,
            "original title": "Improving Temporal Reasoning of Language Models via Recounted Narratives",
            "original authors": "Xinliang Frederick Zhang, Nicholas Beauchamp, Lu Wang",
            "EMNLP Paper ID": "3175",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "dcb4bea3887ec6429de17ece804ca9ce847a935b",
            "title": "EconLogicQA: A Question-Answering Benchmark for Evaluating Large Language Models in Economic Sequential Reasoning",
            "abstract": "In this paper, we introduce EconLogicQA, a rigorous benchmark designed to assess the sequential reasoning capabilities of large language models (LLMs) within the intricate realms of economics, business, and supply chain management. Diverging from traditional benchmarks that predict subsequent events individually, EconLogicQA poses a more challenging task: it requires models to discern and sequence multiple interconnected events, capturing the complexity of economic logics. EconLogicQA comprises an array of multi-event scenarios derived from economic articles, which necessitate an insightful understanding of both temporal and logical event relationships. Through comprehensive evaluations, we exhibit that EconLogicQA effectively gauges a LLM's proficiency in navigating the sequential complexities inherent in economic contexts. We provide a detailed description of EconLogicQA dataset and shows the outcomes from evaluating the benchmark across various leading-edge LLMs, thereby offering a thorough perspective on their sequential reasoning potential in economic contexts. Our benchmark dataset is available at https://huggingface.co/datasets/yinzhu-quan/econ_logic_qa.",
            "link": "https://www.semanticscholar.org/paper/dcb4bea3887ec6429de17ece804ca9ce847a935b",
            "authors": "Yinzhu Quan, Zefang Liu",
            "matchScore": 284.66034,
            "original title": "EconLogicQA: A Question-Answering Benchmark for Evaluating Large Language Models in Economic Sequential Reasoning",
            "original authors": "Yinzhu Quan, Zefang Liu",
            "EMNLP Paper ID": "455",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "8991a260faad1a80f4f1f73e9f1fc6a63b247b0f",
            "title": "Language Models Still Struggle to Zero-shot Reason about Time Series",
            "abstract": "Time series are critical for decision-making in fields like finance and healthcare. Their importance has driven a recent influx of works passing time series into language models, leading to non-trivial forecasting on some datasets. But it remains unknown whether non-trivial forecasting implies that language models can reason about time series. To address this gap, we generate a first-of-its-kind evaluation framework for time series reasoning, including formal tasks and a corresponding dataset of multi-scale time series paired with text captions across ten domains. Using these data, we probe whether language models achieve three forms of reasoning: (1) Etiological Reasoning - given an input time series, can the language model identify the scenario that most likely created it? (2) Question Answering - can a language model answer factual questions about time series? (3) Context-Aided Forecasting - does highly relevant textual context improve a language model's time series forecasts? We find that otherwise highly-capable language models demonstrate surprisingly limited time series reasoning: they score marginally above random on etiological and question answering tasks (up to 30 percentage points worse than humans) and show modest success in using context to improve forecasting. These weakness showcase that time series reasoning is an impactful, yet deeply underdeveloped direction for language model research. We also make our datasets and code public at to support further research in this direction at https://github.com/behavioral-data/TSandLanguage",
            "link": "https://www.semanticscholar.org/paper/8991a260faad1a80f4f1f73e9f1fc6a63b247b0f",
            "authors": "Mike A. Merrill, Mingtian Tan, Vinayak Gupta, Tom Hartvigsen, Tim Althoff",
            "matchScore": 290.65204,
            "original title": "Language Models Still Struggle to Zero-shot Reason about Time Series",
            "original authors": "Mike A Merrill, Mingtian Tan, Vinayak Gupta, Thomas Hartvigsen, Tim Althoff",
            "EMNLP Paper ID": "711",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "3857c71b91a3330921bb019af47c23ad1a942beb",
            "title": "When and Where Did it Happen? An Encoder-Decoder Model to Identify Scenario Context",
            "abstract": "We introduce a neural architecture finetuned for the task of scenario context generation: The relevant location and time of an event or entity mentioned in text. Contextualizing information extraction helps to scope the validity of automated finings when aggregating them as knowledge graphs. Our approach uses a high-quality curated dataset of time and location annotations in a corpus of epidemiology papers to train an encoder-decoder architecture. We also explored the use of data augmentation techniques during training. Our findings suggest that a relatively small fine-tuned encoder-decoder model performs better than out-of-the-box LLMs and semantic role labeling parsers to accurate predict the relevant scenario information of a particular entity or event.",
            "link": "https://www.semanticscholar.org/paper/3857c71b91a3330921bb019af47c23ad1a942beb",
            "authors": "Enrique Noriega-Atala, Robert Vacareanu, Salena Torres Ashton, A. Pyarelal, Clayton T. Morrison, Mihai Surdeanu",
            "matchScore": 294.83884,
            "original title": "When and Where Did it Happen? An Encoder-Decoder Model to Identify Scenario Context",
            "original authors": "Enrique Noriega-Atala, Robert Vacareanu, Salena Torres Ashton, Adarsh Pyarelal, Clayton T Morrison, Mihai Surdeanu",
            "EMNLP Paper ID": "764",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        }
    ],
    "Misinformation Detection and Mitigation Using Advanced Computational Methods": [
        {
            "paperId": "26a3de5b063719a50a8bfd578811387b7929539c",
            "title": "On Fake News Detection with LLM Enhanced Semantics Mining",
            "abstract": "Large language models (LLMs) have emerged 001 as valuable tools for enhancing textual fea-002 tures in various text-related tasks. In this pa-003 per, we assess the effectiveness of news em-004 beddings from ChatGPT for detecting fake 005 news and showcase that despite their initial 006 performance slightly surpassing the pre-trained 007 BERT model, they still lag behind the state-of-008 the-arts. This shortfall is attributed to the re-009 liance on tokenized training text, which misses 010 the complex narratives and subtleties that are 011 crucial for identifying fake news. To capture 012 these nuances, we probe the high-level seman-013 tic relations among the news pieces, real enti-014 ties, and topics, which are modeled as a het-015 erogeneous graph with nodes denoting differ-016 ent items and the relations are represented as 017 edges. We then propose a Generalized Page-018 Rank model and a consistent learning criteria 019 for mining the local and global semantics cen-020 tered on each news piece through the adaptive 021 propagation of features across the graph. Our 022 model shows new state-of-the-art performance 023 on five benchmark datasets and the effective-024 ness of the key ingredients is supported by 025 extensive analysis. Our code is available at 026 https://github.com/LEG4FD/LEG4FD . 027",
            "link": "https://www.semanticscholar.org/paper/26a3de5b063719a50a8bfd578811387b7929539c",
            "authors": "Ziwei Chen, Linmei Hu, Weixin Li, Yingxia Shao, J. Devlin, Ming-Wei Chang, Kenton Lee, Yingtong Dou, Kai Shu, Congying Xia, Philip S. Yu, Yaqian Dun, Kefei Tu, Chen Chen, Chunyan Hou, Tianchi Yang, Luhao Zhang, Wanjun Zhong, Duyu Tang, Chuan Shi, Nan Duan, Mingxia Zhou, Ming Jin, Qingsong Wen, Yuxuan Liang, Chaoli Zhang, Rohit Kumar Kaliyar, Anurag Goswami, Dhruv Khattar, Jaipal Singh Goud, Manish Gupta, Yoon Kim, Convolutional",
            "EMNLP Paper ID": "72",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "9e88aa0b8e117108b817b88ac436bdb70f3c2ba0",
            "title": "Generative AI in the Era of 'Alternative Facts'",
            "abstract": "The spread of misinformation on social media platforms threatens democratic processes, contributes to massive economic losses, and endangers public health. Many efforts to address misinformation focus on a knowledge deficit model and propose interventions for improving users\u2019 critical thinking through improved access to facts. Such efforts are often hampered by challenges with scalability on the part of platform providers, and by confirmation bias on the part of platform users. The emergence of generative AI presents promising opportunities for countering misinformation at scale across ideological barriers. In this paper, we present (1) an experiment with a simulated social media environment to examine the effectiveness of interventions generated by large language models (LLMs) against misinformation, (2) a second experiment with personalized explanations tailored to the demographics and beliefs of users with the goal of alleviating confirmation bias, and (3) an analysis of potential harms posed by personalized generative AI when exploited for automated creation of disinformation. Our findings confirm that LLM-based interventions are highly effective at correcting user behavior (improving overall user accuracy at reliability labeling by up to 47.6%). Furthermore, we find that users favor more personalized interventions when making decisions about news reliability.",
            "link": "https://www.semanticscholar.org/paper/9e88aa0b8e117108b817b88ac436bdb70f3c2ba0",
            "authors": "Saadia Gabriel, Liang Lyu, James Siderius, Marzyeh Ghassemi, Jacob Andreas, Asu Ozdaglar",
            "EMNLP Paper ID": "994",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "120cb9d04afb25a5a3a47ccd47a2efc99158bae0",
            "title": "Multimodal Clickbait Detection by De-confounding Biases Using Causal Representation Inference",
            "abstract": "This paper focuses on detecting clickbait posts on the Web. These posts often use eye-catching disinformation in mixed modalities to mislead users to click for profit. That affects the user experience and thus would be blocked by content provider. To escape detection, malicious creators use tricks to add some irrelevant non-bait content into bait posts, dressing them up as legal to fool the detector. This content often has biased relations with non-bait labels, yet traditional detectors tend to make predictions based on simple co-occurrence rather than grasping inherent factors that lead to malicious behavior. This spurious bias would easily cause misjudgments. To address this problem, we propose a new debiased method based on causal inference. We first employ a set of features in multiple modalities to characterize the posts. Considering these features are often mixed up with unknown biases, we then disentangle three kinds of latent factors from them, including the invariant factor that indicates intrinsic bait intention; the causal factor which reflects deceptive patterns in a certain scenario, and non-causal noise. By eliminating the noise that causes bias, we can use invariant and causal factors to build a robust model with good generalization ability. Experiments on three popular datasets show the effectiveness of our approach.",
            "link": "https://www.semanticscholar.org/paper/120cb9d04afb25a5a3a47ccd47a2efc99158bae0",
            "authors": "Jianxing Yu, Shiqi Wang, Han Yin, Zhen Sun, Ruobing Xie, Bo Zhang, Yanghui Rao",
            "EMNLP Paper ID": "1163",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "43cbfe896a3b3e5194f43a2161f40e29463ff29d",
            "title": "Decoding Susceptibility: Modeling Misbelief to Misinformation Through a Computational Approach (preprint)",
            "abstract": "Susceptibility to misinformation describes the degree of belief in unverifiable claims, a latent aspect of individuals' mental processes that is not observable. Existing susceptibility studies heavily rely on self-reported beliefs, which can be subject to bias, expensive to collect, and challenging to scale for downstream applications. To address these limitations, in this work, we propose a computational approach to model users' latent susceptibility levels. As shown in previous research, susceptibility is influenced by various factors (e.g., demographic factors, political ideology), and directly influences people's reposting behavior on social media. To represent the underlying mental process, our susceptibility modeling incorporates these factors as inputs, guided by the supervision of people's sharing behavior. Using COVID-19 as a testbed domain, our experiments demonstrate a significant alignment between the susceptibility scores estimated by our computational modeling and human judgments, confirming the effectiveness of this latent modeling approach. Furthermore, we apply our model to annotate susceptibility scores on a large-scale dataset and analyze the relationships between susceptibility with various factors. Our analysis reveals that political leanings and psychological factors exhibit varying degrees of association with susceptibility to COVID-19 misinformation.",
            "link": "https://www.semanticscholar.org/paper/43cbfe896a3b3e5194f43a2161f40e29463ff29d",
            "authors": "Yanchen Liu, Mingyu Derek Ma, Wenna Qin, Azure Zhou, Jiaao Chen, Weiyan Shi, Wei Wang, Diyi Yang",
            "EMNLP Paper ID": "1767",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "570aefa173b255a083c6c5e625a8f8393e446ecd",
            "title": "Multimodal Misinformation Detection by Learning from Synthetic Data with Multimodal LLMs",
            "abstract": "Detecting multimodal misinformation, especially in the form of image-text pairs, is crucial. Obtaining large-scale, high-quality real-world fact-checking datasets for training detectors is costly, leading researchers to use synthetic datasets generated by AI technologies. However, the generalizability of detectors trained on synthetic data to real-world scenarios remains unclear due to the distribution gap. To address this, we propose learning from synthetic data for detecting real-world multimodal misinformation through two model-agnostic data selection methods that match synthetic and real-world data distributions. Experiments show that our method enhances the performance of a small MLLM (13B) on real-world fact-checking datasets, enabling it to even surpass GPT-4V~\\cite{GPT-4V}.",
            "link": "https://www.semanticscholar.org/paper/570aefa173b255a083c6c5e625a8f8393e446ecd",
            "authors": "Fengzhu Zeng, Wenqian Li, Wei Gao, Yan Pang",
            "matchScore": 265.51544,
            "original title": "Multimodal Misinformation Detection by Learning from Synthetic Data with Multimodal LLMs",
            "original authors": "Fengzhu ZENG, Wenqian Li, Wei Gao, Yan Pang",
            "EMNLP Paper ID": "2136",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "471c4be087848b03a6d2492e27eb02d3ce4818b3",
            "title": "Seeing Through VisualBERT: A Causal Adventure on Memetic Landscapes",
            "abstract": "Detecting offensive memes is crucial, yet standard deep neural network systems often remain opaque. Various input attribution-based methods attempt to interpret their behavior, but they face challenges with implicitly offensive memes and non-causal attributions. To address these issues, we propose a framework based on a Structural Causal Model (SCM). In this framework, VisualBERT is trained to predict the class of an input meme based on both meme input and causal concepts, allowing for transparent interpretation. Our qualitative evaluation demonstrates the framework's effectiveness in understanding model behavior, particularly in determining whether the model was right due to the right reason, and in identifying reasons behind misclassification. Additionally, quantitative analysis assesses the significance of proposed modelling choices, such as de-confounding, adversarial learning, and dynamic routing, and compares them with input attribution methods. Surprisingly, we find that input attribution methods do not guarantee causality within our framework, raising questions about their reliability in safety-critical applications. The project page is at: https://newcodevelop.github.io/causality_adventure/",
            "link": "https://www.semanticscholar.org/paper/471c4be087848b03a6d2492e27eb02d3ce4818b3",
            "authors": "Dibyanayan Bandyopadhyay, Mohammed Hasanuzzaman, Asif Ekbal",
            "matchScore": 282.07748,
            "original title": "Seeing Through VisualBERT: A Causal Adventure on Memetic Landscapes",
            "original authors": "Dibyanayan Bandyopadhyay, Mohammed Hasanuzzaman, Asif Ekbal",
            "EMNLP Paper ID": "2163",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "582054d50242a08e3a551defac83ceed94872d1f",
            "title": "CoCoHD: Congress Committee Hearing Dataset",
            "abstract": "U.S. congressional hearings significantly influence the national economy and social fabric, impacting individual lives. Despite their importance, there is a lack of comprehensive datasets for analyzing these discourses. To address this, we propose the Congress Committee Hearing Dataset (CoCoHD), covering hearings from 1997 to 2024 across 86 committees, with 32,697 records. This dataset enables researchers to study policy language on critical issues like healthcare, LGBTQ+ rights, and climate justice. We demonstrate its potential with a case study on 1,000 energy-related sentences, analyzing the Energy and Commerce Committee's stance on fossil fuel consumption. By fine-tuning pre-trained language models, we create energy-relevant measures for each hearing. Our market analysis shows that natural language analysis using CoCoHD can predict and highlight trends in the energy sector.",
            "link": "https://www.semanticscholar.org/paper/582054d50242a08e3a551defac83ceed94872d1f",
            "authors": "Arnav Hiray, Yunsong Liu, Ming-da Song, Agam Shah, S. Chava",
            "matchScore": 218.41924,
            "original title": "CoCoHD: Congress Committee Hearing Dataset",
            "original authors": "Arnav Hiray, Yunsong Liu, Mingxiao Song, Agam Shah, Sudheer Chava",
            "EMNLP Paper ID": "2992",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "17eaf5c5924e935c138000613fdac83cdd50d968",
            "title": "Misinformation with Legal Consequences (MisLC): A New Task Towards Harnessing Societal Harm of Misinformation",
            "abstract": "Misinformation, defined as false or inaccurate information, can result in significant societal harm when it is spread with malicious or even innocuous intent. The rapid online information exchange necessitates advanced detection mechanisms to mitigate misinformation-induced harm. Existing research, however, has predominantly focused on assessing veracity, overlooking the legal implications and social consequences of misinformation. In this work, we take a novel angle to consolidate the definition of misinformation detection using legal issues as a measurement of societal ramifications, aiming to bring interdisciplinary efforts to tackle misinformation and its consequence. We introduce a new task: Misinformation with Legal Consequence (MisLC), which leverages definitions from a wide range of legal domains covering 4 broader legal topics and 11 fine-grained legal issues, including hate speech, election laws, and privacy regulations. For this task, we advocate a two-step dataset curation approach that utilizes crowd-sourced checkworthiness and expert evaluations of misinformation. We provide insights about the MisLC task through empirical evidence, from the problem definition to experiments and expert involvement. While the latest large language models and retrieval-augmented generation are effective baselines for the task, we find they are still far from replicating expert performance.",
            "link": "https://www.semanticscholar.org/paper/17eaf5c5924e935c138000613fdac83cdd50d968",
            "authors": "Chunyan Luo, Radin Shayanfar, R. Bhambhoria, Samuel Dahan, Xiaodan Zhu",
            "matchScore": 349.38867,
            "original title": "Misinformation with Legal Consequences (MisLC): A New Task Towards Harnessing Societal Harm of Misinformation",
            "original authors": "Chu Fei Luo, Radin Shayanfar, Rohan V Bhambhoria, Samuel Dahan, Xiaodan Zhu",
            "EMNLP Paper ID": "3019",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        },
        {
            "paperId": "5c5c1ff802975b6d4efebfbb3848c308289a2064",
            "title": "MiRAGeNews: Multimodal Realistic AI-Generated News Detection",
            "abstract": "The proliferation of inflammatory or misleading\"fake\"news content has become increasingly common in recent years. Simultaneously, it has become easier than ever to use AI tools to generate photorealistic images depicting any scene imaginable. Combining these two -- AI-generated fake news content -- is particularly potent and dangerous. To combat the spread of AI-generated fake news, we propose the MiRAGeNews Dataset, a dataset of 12,500 high-quality real and AI-generated image-caption pairs from state-of-the-art generators. We find that our dataset poses a significant challenge to humans (60% F-1) and state-of-the-art multi-modal LLMs (<24% F-1). Using our dataset we train a multi-modal detector (MiRAGe) that improves by +5.1% F-1 over state-of-the-art baselines on image-caption pairs from out-of-domain image generators and news publishers. We release our code and data to aid future work on detecting AI-generated content.",
            "link": "https://www.semanticscholar.org/paper/5c5c1ff802975b6d4efebfbb3848c308289a2064",
            "authors": "Runsheng Huang, Liam Dugan, Yue Yang, Christopher Callison-Burch",
            "matchScore": 257.9107,
            "original title": "MiRAGeNews: Multimodal Realistic AI-Generated News Detection",
            "original authors": "Runsheng Huang, Liam Dugan, Chris Callison-Burch",
            "EMNLP Paper ID": "3152",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        }
    ],
    "Advanced Techniques in Natural Language Processing": [
        {
            "paperId": "5df3397ff6007dc37e524272f42e54279982b54d",
            "title": "Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing AANNs",
            "abstract": "Language models learn rare syntactic phenomena, but the extent to which this is attributable to generalization vs. memorization is a major open question. To that end, we iteratively trained transformer language models on systematically manipulated corpora which were human-scale in size, and then evaluated their learning of a rare grammatical phenomenon: the English Article+Adjective+Numeral+Noun (AANN) construction (``a beautiful five days''). We compared how well this construction was learned on the default corpus relative to a counterfactual corpus in which AANN sentences were removed. We found that AANNs were still learned better than systematically perturbed variants of the construction. Using additional counterfactual corpora, we suggest that this learning occurs through generalization from related constructions (e.g., ``a few days''). An additional experiment showed that this learning is enhanced when there is more variability in the input. Taken together, our results provide an existence proof that LMs can learn rare grammatical phenomena by generalization from less rare phenomena. Data and code: https://github.com/kanishkamisra/aannalysis.",
            "link": "https://www.semanticscholar.org/paper/5df3397ff6007dc37e524272f42e54279982b54d",
            "authors": "Kanishka Misra, Kyle Mahowald",
            "EMNLP Paper ID": "119",
            "Oral/Poster": "Oral",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "78ca20ef7420354ff3c16d02d67094e6f91a515a",
            "title": "A Generic Method for Fine-grained Category Discovery in Natural Language Texts",
            "abstract": "Fine-grained category discovery using only coarse-grained supervision is a cost-effective yet challenging task. Previous training methods focus on aligning query samples with positive samples and distancing them from negatives. They often neglect intra-category and inter-category semantic similarities of fine-grained categories when navigating sample distributions in the embedding space. Furthermore, some evaluation techniques that rely on pre-collected test samples are inadequate for real-time applications. To address these shortcomings, we introduce a method that successfully detects fine-grained clusters of semantically similar texts guided by a novel objective function. The method uses semantic similarities in a logarithmic space to guide sample distributions in the Euclidean space and to form distinct clusters that represent fine-grained categories. We also propose a centroid inference mechanism to support real-time applications. The efficacy of the method is both theoretically justified and empirically confirmed on three benchmark tasks. The proposed objective function is integrated in multiple contrastive learning based neural models. Its results surpass existing state-of-the-art approaches in terms of Accuracy, Adjusted Rand Index and Normalized Mutual Information of the detected fine-grained categories. Code and data will be available at https://github.com/XX upon publication.",
            "link": "https://www.semanticscholar.org/paper/78ca20ef7420354ff3c16d02d67094e6f91a515a",
            "authors": "Chang Tian, M. Blaschko, Wenpeng Yin, Mingzhe Xing, Yinliang Yue, Marie-Francine Moens",
            "EMNLP Paper ID": "401",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "9b565bbcbfeb9ce00264c4f524b4a4f76067b852",
            "title": "Text Grafting: Near-Distribution Weak Supervision for Minority Classes in Text Classification",
            "abstract": "For extremely weak-supervised text classification, pioneer research generates pseudo labels by mining texts similar to the class names from the raw corpus, which may end up with very limited or even no samples for the minority classes. Recent works have started to generate the relevant texts by prompting LLMs using the class names or definitions; however, there is a high risk that LLMs cannot generate in-distribution (i.e., similar to the corpus where the text classifier will be applied) data, leading to ungeneralizable classifiers. In this paper, we combine the advantages of these two approaches and propose to bridge the gap via a novel framework, \\emph{text grafting}, which aims to obtain clean and near-distribution weak supervision for minority classes. Specifically, we first use LLM-based logits to mine masked templates from the raw corpus, which have a high potential for data synthesis into the target minority class. Then, the templates are filled by state-of-the-art LLMs to synthesize near-distribution texts falling into minority classes. Text grafting shows significant improvement over direct mining or synthesis on minority classes. We also use analysis and case studies to comprehend the property of text grafting.",
            "link": "https://www.semanticscholar.org/paper/9b565bbcbfeb9ce00264c4f524b4a4f76067b852",
            "authors": "Letian Peng, Yi Gu, Chengyu Dong, Zihan Wang, Jingbo Shang",
            "EMNLP Paper ID": "427",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "b35493008484f8a558fb5b86bf9874f38b2da1f3",
            "title": "Incubating Text Classifiers Following User Instruction with Nothing but LLM",
            "abstract": "In this paper, we aim to generate text classification data given arbitrary class definitions (i.e., user instruction), so one can train a small text classifier without any human annotation or raw corpus. Compared with pioneer attempts, our proposed Incubator is the first framework that can handle complicated and even mutually dependent classes (e.g.,\"TED Talk given by Educator\"and\"Other\"). Specifically, Incubator is an LLM firstly tuned on the instruction-to-data mappings that we obtained from classification datasets and descriptions on HuggingFace together with in-context augmentation by GPT-4. We then refine Incubator by learning on the cluster centers of semantic textual embeddings to emphasize the uniformity and semantic diversity in generations. We compare Incubator on various classification tasks with strong baselines such as direct LLM-based inference and training data generation by prompt engineering. Experiments show Incubator is able to (1) perform well on traditional benchmarks, (2) take label dependency and user preference into consideration, and (3) enable logical text mining by incubating multiple classifiers.",
            "link": "https://www.semanticscholar.org/paper/b35493008484f8a558fb5b86bf9874f38b2da1f3",
            "authors": "Letian Peng, Jingbo Shang",
            "EMNLP Paper ID": "428",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "e009e7bd762551c3fd1e88b16e8faac6ad295ff1",
            "title": "Concept-skill Transferability-based Data Selection for Large Vision-Language Models",
            "abstract": "Instruction tuning, or supervised finetuning on extensive task-specific data, is necessary for Large Vision-Language Models (LVLMs) to generalize well across a broad range of vision-language (VL) tasks. However, training on large VL datasets can become prohibitively expensive. In this work, we introduce COINCIDE, an effective and scalable data selection technique that uses a small model as a reference model to select visual instruction tuning data for efficient finetuning of a target LVLM, focusing on diversity and transferability. Specifically, we cluster the training data using internal activations from a small model, which identifies VL concept-skill compositions needed by a target LVLM. We then sample data from these diverse clusters by considering their density and transferability, or the ability to transfer well to other concept-skill compositions. This approach ensures the diversity of these compositions, which is vital for LVLM generalization. Extensive experiments demonstrate that COINCIDE achieves superior performance and data selection efficiency against 8 strong baselines on two distinct datasets: LLaVA-1.5 and Vision-Flan. Using only 20% of the LLaVA-1.5 dataset, COINCIDE achieves performance comparable to the LVLM finetuned on the whole dataset, with 70% reduction of the wall-clock running time. On the Vision-Flan dataset, our method achieves superior results with only 16.7% of the training data.",
            "link": "https://www.semanticscholar.org/paper/e009e7bd762551c3fd1e88b16e8faac6ad295ff1",
            "authors": "Jaewoo Lee, Boyang Li, Sung Ju Hwang",
            "EMNLP Paper ID": "558",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "03092a3f26ed774af11cf05082a3d48e6ae1429d",
            "title": "SLANG: New Concept Comprehension of Large Language Models",
            "abstract": "The dynamic nature of language, particularly evident in the realm of slang and memes on the Internet, poses serious challenges to the adaptability of large language models (LLMs). Traditionally anchored to static datasets, these models often struggle to keep up with the rapid linguistic evolution characteristic of online communities. This research aims to bridge this gap by enhancing LLMs' comprehension of the evolving new concepts on the Internet, without the high cost of continual retraining. In pursuit of this goal, we introduce $\\textbf{SLANG}$, a benchmark designed to autonomously integrate novel data and assess LLMs' ability to comprehend emerging concepts, alongside $\\textbf{FOCUS}$, an approach uses causal inference to enhance LLMs to understand new phrases and their colloquial context. Our benchmark and approach involves understanding real-world instances of linguistic shifts, serving as contextual beacons, to form more precise and contextually relevant connections between newly emerging expressions and their meanings. The empirical analysis shows that our causal inference-based approach outperforms the baseline methods in terms of precision and relevance in the comprehension of Internet slang and memes.",
            "link": "https://www.semanticscholar.org/paper/03092a3f26ed774af11cf05082a3d48e6ae1429d",
            "authors": "Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Xueqi Chen",
            "EMNLP Paper ID": "1461",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "beab90db1104da045823838fbf902a9495b06af5",
            "title": "Open-world Multi-label Text Classification with Extremely Weak Supervision",
            "abstract": "We study open-world multi-label text classification under extremely weak supervision (XWS), where the user only provides a brief description for classification objectives without any labels or ground-truth label space. Similar single-label XWS settings have been explored recently, however, these methods cannot be easily adapted for multi-label. We observe that (1) most documents have a dominant class covering the majority of content and (2) long-tail labels would appear in some documents as a dominant class. Therefore, we first utilize the user description to prompt a large language model (LLM) for dominant keyphrases of a subset of raw documents, and then construct a (initial) label space via clustering. We further apply a zero-shot multi-label classifier to locate the documents with small top predicted scores, so we can revisit their dominant keyphrases for more long-tail labels. We iterate this process to discover a comprehensive label space and construct a multi-label classifier as a novel method, X-MLClass. X-MLClass exhibits a remarkable increase in ground-truth label space coverage on various datasets, for example, a 40% improvement on the AAPD dataset over topic modeling and keyword extraction methods. Moreover, X-MLClass achieves the best end-to-end multi-label classification accuracy.",
            "link": "https://www.semanticscholar.org/paper/beab90db1104da045823838fbf902a9495b06af5",
            "authors": "Xintong Li, Jinya Jiang, Ria Dharmani, Jayanth Srinivasa, Gaowen Liu, Jingbo Shang",
            "EMNLP Paper ID": "1755",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "66839275377eb13c242d68507dcb82f9c5954715",
            "title": "Why do objects have many names? A study on word informativeness in language use and lexical systems",
            "abstract": "Human lexicons contain many different words that speakers can use to refer to the same object, e.g.,\"purple\"or\"magenta\"for the same shade of color. On the one hand, studies on language use have explored how speakers adapt their referring expressions to successfully communicate in context, without focusing on properties of the lexical system. On the other hand, studies in language evolution have discussed how competing pressures for informativeness and simplicity shape lexical systems, without tackling in-context communication. We aim at bridging the gap between these traditions, and explore why a soft mapping between referents and words is a good solution for communication, by taking into account both in-context communication and the structure of the lexicon. We propose a simple measure of informativeness for words and lexical systems, grounded in a visual space, and analyze color naming data for English and Mandarin Chinese. We conclude that optimal lexical systems are those where multiple words can apply to the same referent, conveying different amounts of information. Such systems allow speakers to maximize communication accuracy and minimize the amount of information they convey when communicating about referents in contexts.",
            "link": "https://www.semanticscholar.org/paper/66839275377eb13c242d68507dcb82f9c5954715",
            "authors": "Eleonora Gualdoni, Gemma Boleda",
            "EMNLP Paper ID": "2232",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "7d73ab3b2ceaec0136474ab41c8c43dea510961c",
            "title": "Improve Meta-learning for Few-Shot Text Classification with All You Can Acquire from the Tasks",
            "abstract": "Meta-learning has emerged as a prominent technology for few-shot text classification and has achieved promising performance. However, existing methods often encounter difficulties in drawing accurate class prototypes from support set samples, primarily due to probable large intra-class differences and small inter-class differences within the task. Recent approaches attempt to incorporate external knowledge or pre-trained language models to augment data, but this requires additional resources and thus does not suit many few-shot scenarios. In this paper, we propose a novel solution to address this issue by adequately leveraging the information within the task itself. Specifically, we utilize label information to construct a task-adaptive metric space, thereby adaptively reducing the intra-class differences and magnifying the inter-class differences. We further employ the optimal transport technique to estimate class prototypes with query set samples together, mitigating the problem of inaccurate and ambiguous support set samples caused by large intra-class differences. We conduct extensive experiments on eight benchmark datasets, and our approach shows obvious advantages over state-of-the-art models across all the tasks on all the datasets. For reproducibility, all the datasets and codes are available at https://github.com/YvoGao/LAQDA.",
            "link": "https://www.semanticscholar.org/paper/7d73ab3b2ceaec0136474ab41c8c43dea510961c",
            "authors": "Xinyue Liu, Yunlong Gao, Linlin Zong, Bo Xu",
            "matchScore": 312.73117,
            "original title": "Improve Meta-learning for Few-Shot Text Classification with All You Can Acquire from the Tasks",
            "original authors": "Xinyue Liu, Yunlong Gao, Linlin Zong, Bo Xu",
            "EMNLP Paper ID": "38",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        }
    ],
    "Innovations in Question Answering Systems": [
        {
            "paperId": "e1ef2124c6e403e3c760f60a98ca49e291d6a808",
            "title": "QUDSELECT: Selective Decoding for Questions Under Discussion Parsing",
            "abstract": "Question Under Discussion (QUD) is a discourse framework that uses implicit questions to reveal discourse relationships between sentences. In QUD parsing, each sentence is viewed as an answer to a question triggered by an anchor sentence in prior context. The resulting QUD structure is required to conform to several theoretical criteria like answer compatibility (how well the question is answered), making QUD parsing a challenging task. Previous works construct QUD parsers in a pipelined manner (i.e. detect the trigger sentence in context and then generate the question). However, these parsers lack a holistic view of the task and can hardly satisfy all the criteria. In this work, we introduce QUDSELECT, a joint-training framework that selectively decodes the QUD dependency structures considering the QUD criteria. Using instruction-tuning, we train models to simultaneously predict the anchor sentence and generate the associated question. To explicitly incorporate the criteria, we adopt a selective decoding strategy of sampling multiple QUD candidates during inference, followed by selecting the best one with criteria scorers. Our method outperforms the state-of-the-art baseline models by 9% in human evaluation and 4% in automatic evaluation, demonstrating the effectiveness of our framework.",
            "link": "https://www.semanticscholar.org/paper/e1ef2124c6e403e3c760f60a98ca49e291d6a808",
            "authors": "Ashima Suvarna, Xiao Liu, Tanmay Parekh, Kai-Wei Chang, Nanyun Peng",
            "EMNLP Paper ID": "158",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "275ca35b8adce1bb6238981f65e8b9f00c414a20",
            "title": "Advancing Large Language Model Attribution through Self-Improving",
            "abstract": "Teaching large language models (LLMs) to generate text with citations to evidence sources can mitigate hallucinations and enhance verifiability in information-seeking systems. However, improving this capability requires high-quality attribution data, which is costly and labor-intensive. Inspired by recent advances in self-improvement that enhance LLMs without manual annotation, we present START, a Self-Taught AttRibuTion framework for iteratively improving the attribution capability of LLMs. First, to prevent models from stagnating due to initially insufficient supervision signals, START leverages the model to self-construct synthetic training data for warming up. To further self-improve the model's attribution ability, START iteratively utilizes fine-grained preference supervision signals constructed from its sampled responses to encourage robust, comprehensive, and attributable generation. Experiments on three open-domain question-answering datasets, covering long-form QA and multi-step reasoning, demonstrate significant performance gains of 25.13% on average without relying on human annotations and more advanced models. Further analysis reveals that START excels in aggregating information across multiple sources.",
            "link": "https://www.semanticscholar.org/paper/275ca35b8adce1bb6238981f65e8b9f00c414a20",
            "authors": "Lei Huang, Xiaocheng Feng, Weitao Ma, Liang Zhao, Yuchun Fan, Weihong Zhong, Dongliang Xu, Qing Yang, Hongtao Liu, Bing Qin",
            "EMNLP Paper ID": "436",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Main"
        },
        {
            "paperId": "ed4460f7c7903720b3f52cfcc7947371eee2336d",
            "title": "Attribute or Abstain: Large Language Models as Long Document Assistants",
            "abstract": "LLMs can help humans working with long documents, but are known to hallucinate. Attribution can increase trust in LLM responses: The LLM provides evidence that supports its response, which enhances verifiability. Existing approaches to attribution have only been evaluated in RAG settings, where the initial retrieval confounds LLM performance. This is crucially different from the long document setting, where retrieval is not needed, but could help. Thus, a long document specific evaluation of attribution is missing. To fill this gap, we present LAB, a benchmark of 6 diverse long document tasks with attribution, and experiments with different approaches to attribution on 5 LLMs of different sizes. We find that citation, i.e. response generation and evidence extraction in one step, performs best for large and fine-tuned models, while additional retrieval can help for small, prompted models. We investigate whether the\"Lost in the Middle'' phenomenon exists for attribution, but do not find this. We also find that evidence quality can predict response quality on datasets with simple responses, but not so for complex responses, as models struggle with providing evidence for complex claims.",
            "link": "https://www.semanticscholar.org/paper/ed4460f7c7903720b3f52cfcc7947371eee2336d",
            "authors": "Jan Buchmann, Xiao Liu, Iryna Gurevych",
            "EMNLP Paper ID": "932",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "00fec110cb998e344af8fa9ddb37aca2caec4c71",
            "title": "PCQPR: Proactive Conversational Question Planning with Reflection",
            "abstract": "Conversational Question Generation (CQG) enhances the interactivity of conversational question-answering systems in fields such as education, customer service, and entertainment. However, traditional CQG, focusing primarily on the immediate context, lacks the conversational foresight necessary to guide conversations toward specified conclusions. This limitation significantly restricts their ability to achieve conclusion-oriented conversational outcomes. In this work, we redefine the CQG task as Conclusion-driven Conversational Question Generation (CCQG) by focusing on proactivity, not merely reacting to the unfolding conversation but actively steering it towards a conclusion-oriented question-answer pair. To address this, we propose a novel approach, called Proactive Conversational Question Planning with self-Refining (PCQPR). Concretely, by integrating a planning algorithm inspired by Monte Carlo Tree Search (MCTS) with the analytical capabilities of large language models (LLMs), PCQPR predicts future conversation turns and continuously refines its questioning strategies. This iterative self-refining mechanism ensures the generation of contextually relevant questions strategically devised to reach a specified outcome. Our extensive evaluations demonstrate that PCQPR significantly surpasses existing CQG methods, marking a paradigm shift towards conclusion-oriented conversational question-answering systems.",
            "link": "https://www.semanticscholar.org/paper/00fec110cb998e344af8fa9ddb37aca2caec4c71",
            "authors": "Shasha Guo, Lizi Liao, Jing Zhang, Cuiping Li, Hong Chen",
            "EMNLP Paper ID": "1317",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "05e1abfc38d25c706e7e0efe43954f571002b1f4",
            "title": "Adaptive Question Answering: Enhancing Language Model Proficiency for Addressing Knowledge Conflicts with Source Citations",
            "abstract": "Resolving knowledge conflicts is a crucial challenge in Question Answering (QA) tasks, as the internet contains numerous conflicting facts and opinions. While some research has made progress in tackling ambiguous settings where multiple valid answers exist, these approaches often neglect to provide source citations, leaving users to evaluate the factuality of each answer. On the other hand, existing work on citation generation has focused on unambiguous settings with single answers, failing to address the complexity of real-world scenarios. Despite the importance of both aspects, no prior research has combined them, leaving a significant gap in the development of QA systems. In this work, we bridge this gap by proposing the novel task of QA with source citation in ambiguous settings, where multiple valid answers exist. To facilitate research in this area, we create a comprehensive framework consisting of: (1) five novel datasets, obtained by augmenting three existing reading comprehension datasets with citation meta-data across various ambiguous settings, such as distractors and paraphrasing; (2) the first ambiguous multi-hop QA dataset featuring real-world, naturally occurring contexts; (3) two new metrics to evaluate models' performances; and (4) several strong baselines using rule-based, prompting, and finetuning approaches over five large language models. We hope that this new task, datasets, metrics, and baselines will inspire the community to push the boundaries of QA research and develop more trustworthy and interpretable systems.",
            "link": "https://www.semanticscholar.org/paper/05e1abfc38d25c706e7e0efe43954f571002b1f4",
            "authors": "Sagi Shaier, Ari Kobren, Philip Ogren",
            "EMNLP Paper ID": "2039",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "21c530f348f9dee0b967d05ca324d8268c4a183a",
            "title": "Enhancing Post-Hoc Attributions in Long Document Comprehension via Coarse Grained Answer Decomposition",
            "abstract": "Accurately attributing answer text to its source document is crucial for developing a reliable question-answering system. However, attribution for long documents remains largely unexplored. Post-hoc attribution systems are designed to map answer text back to the source document, yet the granularity of this mapping has not been addressed. Furthermore, a critical question arises: What exactly should be attributed? This involves identifying the specific information units within an answer that require grounding. In this paper, we propose and investigate a novel approach to the factual decomposition of generated answers for attribution, employing template-based in-context learning. To accomplish this, we utilize the question and integrate negative sampling during few-shot in-context learning for decomposition. This approach enhances the semantic understanding of both abstractive and extractive answers. We examine the impact of answer decomposition by providing a thorough examination of various attribution approaches, ranging from retrieval-based techniques to LLM-based attributors.",
            "link": "https://www.semanticscholar.org/paper/21c530f348f9dee0b967d05ca324d8268c4a183a",
            "authors": "Pritika Ramu, Koustava Goswami, Apoorv Saxena, Balaji Vasan Srinivavsan",
            "EMNLP Paper ID": "2147",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "5fb8997c8cc9f4eaab102d54c2c86cfc61f30445",
            "title": "Which questions should I answer? Salience Prediction of Inquisitive Questions",
            "abstract": "Inquisitive questions -- open-ended, curiosity-driven questions people ask as they read -- are an integral part of discourse processing (Kehler and Rohde, 2017; Onea, 2016) and comprehension (Prince, 2004). Recent work in NLP has taken advantage of question generation capabilities of LLMs to enhance a wide range of applications. But the space of inquisitive questions is vast: many questions can be evoked from a given context. So which of those should be prioritized to find answers? Linguistic theories, unfortunately, have not yet provided an answer to this question. This paper presents QSALIENCE, a salience predictor of inquisitive questions. QSALIENCE is instruction-tuned over our dataset of linguist-annotated salience scores of 1,766 (context, question) pairs. A question scores high on salience if answering it would greatly enhance the understanding of the text (Van Rooy, 2003). We show that highly salient questions are empirically more likely to be answered in the same article, bridging potential questions (Onea, 2016) with Questions Under Discussion (Roberts, 2012). We further validate our findings by showing that answering salient questions is an indicator of summarization quality in news.",
            "link": "https://www.semanticscholar.org/paper/5fb8997c8cc9f4eaab102d54c2c86cfc61f30445",
            "authors": "Yating Wu, Ritika Mangla, A. Dimakis, Greg Durrett, Junyi Jessy Li",
            "EMNLP Paper ID": "2602",
            "Oral/Poster": "Oral",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "6ad3717ecb5741a4ef07f8e1b9537594771886ff",
            "title": "Correct after Answer: Enhancing Multi-Span Question Answering with Post-Processing Method",
            "abstract": "Multi-Span Question Answering (MSQA) requires models to extract one or multiple answer spans from a given context to answer a question. Prior work mainly focuses on designing specific methods or applying heuristic strategies to encourage models to predict more correct predictions. However, these models are trained on gold answers and fail to consider the incorrect predictions. Through a statistical analysis, we observe that models with stronger abilities do not predict less incorrect predictions compared with other models. In this work, we propose Answering-Classifying-Correcting (ACC) framework, which employs a post-processing strategy to handle incorrect predictions. Specifically, the ACC framework first introduces a classifier to classify the predictions into three types and exclude\"wrong predictions\", then introduces a corrector to modify\"partially correct predictions\". Experiments on several MSQA datasets show that ACC framework significantly improves the Exact Match (EM) scores, and further analysis demostrates that ACC framework efficiently reduces the number of incorrect predictions, improving the quality of predictions.",
            "link": "https://www.semanticscholar.org/paper/6ad3717ecb5741a4ef07f8e1b9537594771886ff",
            "authors": "Jiayi Lin, Chenyang Zhang, Haibo Tong, Dongyu Zhang, Qingqing Hong, Bingxuan Hou, Junli Wang",
            "matchScore": 275.7381,
            "original title": "Correct after Answer: Enhancing Multi-Span Question Answering with Post-Processing Method",
            "original authors": "Jiayi Lin, Chenyang Zhang, Haibo Tong, Dongyu Zhang, Qingqing Hong, Bingxuan Hou, Junli Wang",
            "EMNLP Paper ID": "553",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Findings"
        },
        {
            "paperId": "f0932282d573ed6a5745e5498fc1437fa66d47eb",
            "title": "Learning to Ask Informative Questions: Enhancing LLMs with Preference Optimization and Expected Information Gain",
            "abstract": "Questions are essential tools for acquiring the necessary information to complete information-seeking tasks. However, large language models (LLMs), especially open-source models, often perform poorly in generating informative questions, as measured by expected information gain (EIG). In this paper, we propose a method to enhance the informativeness of LLM-generated questions in 20-question game dialogues. We sample multiple questions from the same model (LLAMA 2-CHAT 7B) for each game and create pairs of low-EIG and high-EIG questions to apply a Direct Preference Optimization (DPO) algorithm. Our results show that this method produces more effective questions (in terms of EIG), even in domains different from those used to train the DPO model.",
            "link": "https://www.semanticscholar.org/paper/f0932282d573ed6a5745e5498fc1437fa66d47eb",
            "authors": "Davide Mazzaccara, A. Testoni, Raffaella Bernardi",
            "matchScore": 317.8051,
            "original title": "Learning to Ask Informative Questions: Enhancing LLMs with Preference Optimization and Expected Information Gain",
            "original authors": "Davide Mazzaccara, Alberto Testoni, Raffaella Bernardi",
            "EMNLP Paper ID": "991",
            "Oral/Poster": "Poster",
            "Session": "Session 09",
            "Session Time": "Nov 13 (Wed) 16:00-17:30",
            "Type": "Findings"
        }
    ],
    "Cognitive Abilities and Reasoning in Language Models": [
        {
            "paperId": "0c97435611169f5d63ce3e2f06ccd08bbdcdb46e",
            "title": "Development of Cognitive Intelligence in Pre-trained Language Models",
            "abstract": "Recent studies show evidence for emergent cognitive abilities in Large Pre-trained Language Models (PLMs). The increasing cognitive alignment of these models has made them candidates for cognitive science theories. Prior research into the emergent cognitive abilities of PLMs has largely been path-independent to model training, i.e., has focused on the final model weights and not the intermediate steps. However, building plausible models of human cognition using PLMs would benefit from considering the developmental alignment of their performance during training to the trajectories of children's thinking. Guided by psychometric tests of human intelligence, we choose four sets of tasks to investigate the alignment of ten popular families of PLMs and evaluate their available intermediate and final training steps. These tasks are Numerical ability, Linguistic abilities, Conceptual understanding, and Fluid reasoning. We find a striking regularity: regardless of model size, the developmental trajectories of PLMs consistently exhibit a window of maximal alignment to human cognitive development. Before that window, training appears to endow\"blank slate\"models with the requisite structure to be poised to rapidly learn from experience. After that window, training appears to serve the engineering goal of reducing loss but not the scientific goal of increasing alignment with human cognition.",
            "link": "https://www.semanticscholar.org/paper/0c97435611169f5d63ce3e2f06ccd08bbdcdb46e",
            "authors": "Raj Sanjay Shah, Khushi Bhardwaj, Sashank Varma",
            "EMNLP Paper ID": "1077",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "5c24dd41f46fd7107997b0a46e1207e0fed63b34",
            "title": "AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies",
            "abstract": "Humans regularly engage in analogical thinking, relating personal experiences to current situations (X is analogous to Y because of Z). Analogical thinking allows humans to solve problems in creative ways, grasp difficult concepts, and articulate ideas more effectively. Can language models (LMs) do the same? To answer this question, we propose AnaloBench, a benchmark to determine analogical reasoning ability in LMs. Our benchmarking approach focuses on aspects of this ability that are common among humans: (i) recalling related experiences from a large amount of information, and (ii) applying analogical reasoning to complex and lengthy scenarios. We test a broad collection of proprietary models (e.g., GPT family, Claude V2) and open source models such as LLaMA2. As in prior results, scaling up LMs results in some performance boosts. Surprisingly, scale offers minimal gains when, (i) analogies involve lengthy scenarios, or (ii) recalling relevant scenarios from a large pool of information, a process analogous to finding a needle in a haystack. We hope these observations encourage further research in this field.",
            "link": "https://www.semanticscholar.org/paper/5c24dd41f46fd7107997b0a46e1207e0fed63b34",
            "authors": "Xiao Ye, Andrew Wang, Jacob Choi, Yining Lu, Shreya Sharma, Lingfeng Shen, Vijay Tiyyala, Nicholas Andrews, Daniel Khashabi",
            "EMNLP Paper ID": "1511",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "a4cecf5c1037bc15f7713b95695e49dd61fde254",
            "title": "Distractor Generation in Multiple-Choice Tasks: A Survey of Methods, Datasets, and Evaluation",
            "abstract": "The distractor generation task focuses on generating incorrect but plausible options for objective questions such as fill-in-the-blank and multiple-choice questions. This task is widely utilized in educational settings across various domains and subjects. The effectiveness of these questions in assessments relies on the quality of the distractors, as they challenge examinees to select the correct answer from a set of misleading options. The evolution of artificial intelligence (AI) has transitioned the task from traditional methods to the use of neural networks and pre-trained language models. This shift has established new benchmarks and expanded the use of advanced deep learning methods in generating distractors. This survey explores distractor generation tasks, datasets, methods, and current evaluation metrics for English objective questions, covering both text-based and multi-modal domains. It also evaluates existing AI models and benchmarks and discusses potential future research directions.",
            "link": "https://www.semanticscholar.org/paper/a4cecf5c1037bc15f7713b95695e49dd61fde254",
            "authors": "Elaf Alhazmi, Quan Z. Sheng, W. Zhang, Munazza Zaib, A. Alhazmi",
            "EMNLP Paper ID": "1666",
            "Oral/Poster": "Oral",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "31aa31257a50530f817a9d35b971758da55d72d0",
            "title": "Perceptions to Beliefs: Exploring Precursory Inferences for Theory of Mind in Large Language Models",
            "abstract": "While humans naturally develop theory of mind (ToM), the capability to understand other people's mental states and beliefs, state-of-the-art large language models (LLMs) underperform on simple ToM benchmarks. We posit that we can extend our understanding of LLMs' ToM abilities by evaluating key human ToM precursors -- perception inference and perception-to-belief inference -- in LLMs. We introduce two datasets, Percept-ToMi and Percept-FANToM, to evaluate these precursory inferences for ToM in LLMs by annotating characters' perceptions on ToMi and FANToM, respectively. Our evaluation of eight state-of-the-art LLMs reveals that the models generally perform well in perception inference while exhibiting limited capability in perception-to-belief inference (e.g., lack of inhibitory control). Based on these results, we present PercepToM, a novel ToM method leveraging LLMs' strong perception inference capability while supplementing their limited perception-to-belief inference. Experimental results demonstrate that PercepToM significantly enhances LLM's performance, especially in false belief scenarios.",
            "link": "https://www.semanticscholar.org/paper/31aa31257a50530f817a9d35b971758da55d72d0",
            "authors": "Chani Jung, Dongkwan Kim, Jiho Jin, Jiseon Kim, Yeon Seonwoo, Yejin Choi, Alice Oh, Hyunwoo Kim",
            "EMNLP Paper ID": "2571",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "98b99da4acad062f00650ce4f0ca42cd3657db5c",
            "title": "Do great minds think alike? Investigating Human-AI Complementarity for Question Answering",
            "abstract": "This study examines question-answering ( QA ) abilities across human and AI agents. Our framework CAIMIRA addresses limitations in traditional item response theory, by incorporating multidimensional analysis, identifiability, and content awareness, enabling nuanced comparison of QA agents. Analyzing responses from ~ 30 AI systems and 155 humans over thousands of questions, we identify distinct knowledge domains and reasoning skills where these agents demonstrate differential proficien-cies. Humans outperform AI systems in scientific reasoning and understanding nuanced language, while large-scale LLM s like GPT - 4 and LLAMA - 2 - 70 B excel in retrieving specific factual information. The study identifies key areas for future QA tasks and model development, emphasizing the importance of semantic understanding and scientific reasoning in creating more effective and discriminating benchmarks.",
            "link": "https://www.semanticscholar.org/paper/98b99da4acad062f00650ce4f0ca42cd3657db5c",
            "authors": "Maharshi Gor, Tianyi Zhou, Hal Daum\u00e9, Jordan Boyd-Graber",
            "EMNLP Paper ID": "2968",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "c2eeef03f0c0d85237fe64b8da3a44d6170dbf32",
            "title": "CogGPT: Unleashing the Power of Cognitive Dynamics on Large Language Models",
            "abstract": "Cognitive dynamics are pivotal to advance human understanding of the world. Recent advancements in large language models (LLMs) reveal their potential for cognitive simulation. However, these LLM-based cognitive studies primarily focus on static modeling, overlooking the dynamic nature of cognition. To bridge this gap, we propose the concept of the cognitive dynamics of LLMs and present a corresponding task with the inspiration of longitudinal studies. Towards the task, we develop CogBench, a novel benchmark to assess the cognitive dynamics of LLMs and validate it through participant surveys. We also design two evaluation metrics for CogBench, including Authenticity and Rationality. Recognizing the inherent static nature of LLMs, we introduce CogGPT for the task, which features an innovative iterative cognitive mechanism aimed at enhancing lifelong cognitive dynamics. Empirical results demonstrate the superiority of CogGPT over existing methods, particularly in its ability to facilitate role-specific cognitive dynamics under continuous information flows.",
            "link": "https://www.semanticscholar.org/paper/c2eeef03f0c0d85237fe64b8da3a44d6170dbf32",
            "authors": "Yaojia Lv, Haojie Pan, Ruiji Fu, Ming Liu, Zhongyuan Wang, Bing Qin",
            "matchScore": 236.09406,
            "original title": "CogGPT: Unleashing the Power of Cognitive Dynamics on Large Language Models",
            "original authors": "Yaojia Lv, Haojie Pan, Zekun Wang, Jiafeng Liang, Yuanxing Liu, Ruiji Fu, Ming Liu, Zhongyuan Wang, Bing Qin",
            "EMNLP Paper ID": "1258",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "0133aba17b2967e2343df3f0db061743f71cca62",
            "title": "DisGeM: Distractor Generation for Multiple Choice Questions with Span Masking",
            "abstract": "Recent advancements in Natural Language Processing (NLP) have impacted numerous sub-fields such as natural language generation, natural language inference, question answering, and more. However, in the field of question generation, the creation of distractors for multiple-choice questions (MCQ) remains a challenging task. In this work, we present a simple, generic framework for distractor generation using readily available Pre-trained Language Models (PLMs). Unlike previous methods, our framework relies solely on pre-trained language models and does not require additional training on specific datasets. Building upon previous research, we introduce a two-stage framework consisting of candidate generation and candidate selection. Our proposed distractor generation framework outperforms previous methods without the need for training or fine-tuning. Human evaluations confirm that our approach produces more effective and engaging distractors. The related codebase is publicly available at https://github.com/obss/disgem.",
            "link": "https://www.semanticscholar.org/paper/0133aba17b2967e2343df3f0db061743f71cca62",
            "authors": "Devrim Cavusoglu, Secil Sen, Ulas Sert",
            "matchScore": 267.99677,
            "original title": "DisGeM: Distractor Generation for Multiple Choice Questions with Span Masking",
            "original authors": "Devrim \u00c7avu\u015fo\u011flu, Se\u00e7il \u015een, Ula\u015f Sert",
            "EMNLP Paper ID": "2008",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        },
        {
            "paperId": "e1b320f5afc2c7189c1eba6ab1ce546979fb43bb",
            "title": "TuringQ: Benchmarking AI Comprehension in Theory of Computation",
            "abstract": "We present TuringQ, the first benchmark designed to evaluate the reasoning capabilities of large language models (LLMs) in the theory of computation. TuringQ consists of 4,006 undergraduate and graduate-level question-answer pairs, categorized into four difficulty levels and covering seven core theoretical areas. We evaluate several open-source LLMs, as well as GPT-4, using Chain of Thought prompting and expert human assessment. Additionally, we propose an automated LLM-based evaluation system that demonstrates competitive accuracy when compared to human evaluation. Fine-tuning a Llama3-8B model on TuringQ shows measurable improvements in reasoning ability and out-of-domain tasks such as algebra. TuringQ serves as both a benchmark and a resource for enhancing LLM performance in complex computational reasoning tasks. Our analysis offers insights into LLM capabilities and advances in AI comprehension of theoretical computer science.",
            "link": "https://www.semanticscholar.org/paper/e1b320f5afc2c7189c1eba6ab1ce546979fb43bb",
            "authors": "Pardis Sadat Zahraei, Ehsaneddin Asgari",
            "matchScore": 229.28073,
            "original title": "TuringQ: Benchmarking AI Comprehension in Theory of Computation",
            "original authors": "Pardis Sadat Zahraei, Ehsaneddin Asgari",
            "EMNLP Paper ID": "2408",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 14): 13:00\u201314:00 (Afternoon)",
            "Type": "Findings"
        },
        {
            "paperId": "6dd415a07a6c304a78e191bbb09f50ee04c03d89",
            "title": "A Notion of Complexity for Theory of Mind via Discrete World Models",
            "abstract": "Theory of Mind (ToM) can be used to assess the capabilities of Large Language Models (LLMs) in complex scenarios where social reasoning is required. While the research community has proposed many ToM benchmarks, their hardness varies greatly, and their complexity is not well defined. This work proposes a framework inspired by cognitive load theory to measure the complexity of ToM tasks. We quantify a problem's complexity as the number of states necessary to solve it correctly. Our complexity measure also accounts for spurious states of a ToM problem designed to make it apparently harder. We use our method to assess the complexity of five widely adopted ToM benchmarks. On top of this framework, we design a prompting technique that augments the information available to a model with a description of how the environment changes with the agents' interactions. We name this technique Discrete World Models (DWM) and show how it elicits superior performance on ToM tasks.",
            "link": "https://www.semanticscholar.org/paper/6dd415a07a6c304a78e191bbb09f50ee04c03d89",
            "authors": "X. A. Huang, Emanuele La Malfa, Samuele Marro, A. Asperti, Anthony Cohn, Michael Wooldridge",
            "matchScore": 235.93877,
            "original title": "A Notion of Complexity for Theory of Mind via Discrete World Models",
            "original authors": "X. Angelo Huang, Emanuele La Malfa, Samuele Marro, Andrea Asperti, Anthony G. Cohn, Michael J. Wooldridge",
            "EMNLP Paper ID": "595",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Findings"
        }
    ],
    "Active Learning and Annotation Techniques in NLP": [
        {
            "paperId": "30bfb0a8801c8e8ba65a6009027839ac34cd374e",
            "title": "On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices",
            "abstract": "While learning with limited labelled data can improve performance when the labels are lacking, it is also sensitive to the effects of uncontrolled randomness introduced by so-called randomness factors (e.g., varying order of data). We propose a method to systematically investigate the effects of randomness factors while taking the interactions between them into consideration. To measure the true effects of an individual randomness factor, our method mitigates the effects of other factors and observes how the performance varies across multiple runs. Applying our method to multiple randomness factors across in-context learning and fine-tuning approaches on 7 representative text classification tasks and meta-learning on 3 tasks, we show that: 1) disregarding interactions between randomness factors in existing works caused inconsistent findings due to incorrect attribution of the effects of randomness factors, such as disproving the consistent sensitivity of in-context learning to sample order even with random sample selection; and 2) besides mutual interactions, the effects of randomness factors, especially sample order, are also dependent on more systematic choices unexplored in existing works, such as number of classes, samples per class or choice of prompt format.",
            "link": "https://www.semanticscholar.org/paper/30bfb0a8801c8e8ba65a6009027839ac34cd374e",
            "authors": "Branislav Pecher, Ivan Srba, M. Bielikov\u00e1",
            "EMNLP Paper ID": "74",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "9ea1493cae4219a822738b8814467a283ebe9b1f",
            "title": "From Insights to Actions: The Impact of Interpretability and Analysis Research on NLP",
            "abstract": "Interpretability and analysis (IA) research is a growing subfield within NLP with the goal of developing a deeper understanding of the behavior or inner workings of NLP systems and methods. Despite growing interest in the subfield, a criticism of this work is that it lacks actionable insights and therefore has little impact on NLP. In this paper, we seek to quantify the impact of IA research on the broader field of NLP. We approach this with a mixed-methods analysis of: (1) a citation graph of 185K+ papers built from all papers published at ACL and EMNLP conferences from 2018 to 2023, and their references and citations, and (2) a survey of 138 members of the NLP community. Our quantitative results show that IA work is well-cited outside of IA, and central in the NLP citation graph. Through qualitative analysis of survey responses and manual annotation of 556 papers, we find that NLP researchers build on findings from IA work and perceive it as important for progress in NLP, multiple subfields, and rely on its findings and terminology for their own work. Many novel methods are proposed based on IA findings and highly influenced by them, but highly influential non-IA work cites IA findings without being driven by them. We end by summarizing what is missing in IA work today and provide a call to action, to pave the way for a more impactful future of IA research.",
            "link": "https://www.semanticscholar.org/paper/9ea1493cae4219a822738b8814467a283ebe9b1f",
            "authors": "Marius Mosbach, Vagrant Gautam, Tom'as Vergara-Browne, Dietrich Klakow, Mor Geva",
            "EMNLP Paper ID": "347",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "8c913c0f6a6133469200384849d8db9cc5b5f6ad",
            "title": "Self-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models",
            "abstract": "Active learning is an iterative labeling process that is used to obtain a small labeled subset, despite the absence of labeled data, thereby enabling to train a model for supervised tasks such as text classification. While active learning has made considerable progress in recent years due to improvements provided by pre-trained language models, there is untapped potential in the often neglected unlabeled portion of the data, although it is available in considerably larger quantities than the usually small set of labeled data. In this work, we investigate how self-training, a semi-supervised approach that uses a model to obtain pseudo-labels for unlabeled data, can be used to improve the efficiency of active learning for text classification. Building on a comprehensive reproduction of four previous self-training approaches, some of which are evaluated for the first time in the context of active learning or natural language processing, we introduce HAST, a new and effective self-training strategy, which is evaluated on four text classification benchmarks. Our results show that it outperforms the reproduced self-training approaches and reaches classification results comparable to previous experiments for three out of four datasets, using as little as 25% of the data. The code is publicly available at https://github.com/chschroeder/self-training-for-sample-efficient-active-learning .",
            "link": "https://www.semanticscholar.org/paper/8c913c0f6a6133469200384849d8db9cc5b5f6ad",
            "authors": "Christopher Schr\u00f6der, Gerhard Heyer",
            "EMNLP Paper ID": "1402",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 13): 7:45\u20138:45 (Morning)",
            "Type": "Main"
        },
        {
            "paperId": "0bd68693587fbf7d9190bcc373fbafa4a0af177d",
            "title": "Annotator-Centric Active Learning for Subjective NLP Tasks",
            "abstract": "Active Learning (AL) addresses the high costs of collecting human annotations by strategically annotating the most informative samples. However, for subjective NLP tasks, incorporating a wide range of perspectives in the annotation process is crucial to capture the variability in human judgments. We introduce Annotator-Centric Active Learning (ACAL), which incorporates an annotator selection strategy following data sampling. Our objective is two-fold: 1) to efficiently approximate the full diversity of human judgments, and 2) to assess model performance using annotator-centric metrics, which value minority and majority perspectives equally. We experiment with multiple annotator selection strategies across seven subjective NLP tasks, employing both traditional and novel, human-centered evaluation metrics. Our findings indicate that ACAL improves data efficiency and excels in annotator-centric performance evaluations. However, its success depends on the availability of a sufficiently large and diverse pool of annotators to sample from.",
            "link": "https://www.semanticscholar.org/paper/0bd68693587fbf7d9190bcc373fbafa4a0af177d",
            "authors": "Michiel van der Meer, Neele Falk, P. Murukannaiah, Enrico Liscio",
            "EMNLP Paper ID": "2308",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "a24650d2caf7a3bee9b17d2dc436f63f1629d205",
            "title": "On the Fragility of Active Learners for Text Classification",
            "abstract": "Active learning (AL) techniques optimally utilize a labeling budget by iteratively selecting instances that are most valuable for learning. However, they lack ``prerequisite checks'', i.e., there are no prescribed criteria to pick an AL algorithm best suited for a dataset. A practitioner must pick a technique they \\emph{trust} would beat random sampling, based on prior reported results, and hope that it is resilient to the many variables in their environment: dataset, labeling budget and prediction pipelines. The important questions then are: how often on average, do we expect any AL technique to reliably beat the computationally cheap and easy-to-implement strategy of random sampling? Does it at least make sense to use AL in an ``Always ON'' mode in a prediction pipeline, so that while it might not always help, it never under-performs random sampling? How much of a role does the prediction pipeline play in AL's success? We examine these questions in detail for the task of text classification using pre-trained representations, which are ubiquitous today. Our primary contribution here is a rigorous evaluation of AL techniques, old and new, across setups that vary wrt datasets, text representations and classifiers. This unlocks multiple insights around warm-up times, i.e., number of labels before gains from AL are seen, viability of an ``Always ON'' mode and the relative significance of different factors. Additionally, we release a framework for rigorous benchmarking of AL techniques for text classification.",
            "link": "https://www.semanticscholar.org/paper/a24650d2caf7a3bee9b17d2dc436f63f1629d205",
            "authors": "Abhishek Ghose, Emma Nguyen",
            "EMNLP Paper ID": "3150",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "bf7d39e13b60adcfd79ea704055bdefb7a17aee1",
            "title": "ALVIN: Active Learning Via INterpolation",
            "abstract": "Active Learning aims to minimize annotation effort by selecting the most useful instances from a pool of unlabeled data. However, typical active learning methods overlook the presence of distinct example groups within a class, whose prevalence may vary, e.g., in occupation classification datasets certain demographics are disproportionately represented in specific classes. This oversight causes models to rely on shortcuts for predictions, i.e., spurious correlations between input attributes and labels occurring in well-represented groups. To address this issue, we propose Active Learning Via INterpolation (ALVIN), which conducts intra-class interpolations between examples from under-represented and well-represented groups to create anchors, i.e., artificial points situated between the example groups in the representation space. By selecting instances close to the anchors for annotation, ALVIN identifies informative examples exposing the model to regions of the representation space that counteract the influence of shortcuts. Crucially, since the model considers these examples to be of high certainty, they are likely to be ignored by typical active learning methods. Experimental results on six datasets encompassing sentiment analysis, natural language inference, and paraphrase detection demonstrate that ALVIN outperforms state-of-the-art active learning methods in both in-distribution and out-of-distribution generalization.",
            "link": "https://www.semanticscholar.org/paper/bf7d39e13b60adcfd79ea704055bdefb7a17aee1",
            "authors": "Michalis Korakakis, Andreas Vlachos, Adrian Weller",
            "EMNLP Paper ID": "3280",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "198122afad5ee5bdb808ba703858f308d260ff19",
            "title": "Selective Annotation via Data Allocation: These Data Should Be Triaged to Experts for Annotation Rather Than the Model",
            "abstract": "To obtain high-quality annotations under limited budget, semi-automatic annotation methods are commonly used, where a portion of the data is annotated by experts and a model is then trained to complete the annotations for the remaining data. However, these methods mainly focus on selecting informative data for expert annotations to improve the model predictive ability (i.e., triage-to-human data), while the rest of the data is indiscriminately assigned to model annotation (i.e., triage-to-model data). This may lead to inefficiencies in budget allocation for annotations, as easy data that the model could accurately annotate may be unnecessarily assigned to the expert, and hard data may be misclassified by the model. As a result, the overall annotation quality may be compromised. To address this issue, we propose a selective annotation framework called SANT. It effectively takes advantage of both the triage-to-human and triage-to-model data through the proposed error-aware triage and bi-weighting mechanisms. As such, informative or hard data is assigned to the expert for annotation, while easy data is handled by the model. Experimental results show that SANT consistently outperforms other baselines, leading to higher-quality annotation through its proper allocation of data to both expert and model workers. We provide pioneering work on data annotation within budget constraints, establishing a landmark for future triage-based annotation studies.",
            "link": "https://www.semanticscholar.org/paper/198122afad5ee5bdb808ba703858f308d260ff19",
            "authors": "Chen Huang, Yang Deng, Wenqiang Lei, Jiancheng Lv, Ido Dagan",
            "matchScore": 342.29913,
            "original title": "Selective Annotation via Data Allocation: These Data Should Be Triaged to Experts for Annotation Rather Than the Model",
            "original authors": "Chen Huang, Yang Deng, Wenqiang Lei, Jiancheng Lv, Ido Dagan",
            "EMNLP Paper ID": "56",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "8d687f8edca283308d024fa3016cc18ec4a21c1e",
            "title": "Cost-Efficient Subjective Task Annotation and Modeling through Few-Shot Annotator Adaptation",
            "abstract": "In subjective NLP tasks, where a single ground truth does not exist, the inclusion of diverse annotators becomes crucial as their unique perspectives significantly influence the annotations. In realistic scenarios, the annotation budget often becomes the main determinant of the number of perspectives (i.e., annotators) included in the data and subsequent modeling. We introduce a novel framework for annotation collection and modeling in subjective tasks that aims to minimize the annotation budget while maximizing the predictive performance for each annotator. Our framework has a two-stage design: first, we rely on a small set of annotators to build a multitask model, and second, we augment the model for a new perspective by strategically annotating a few samples per annotator. To test our framework at scale, we introduce and release a unique dataset, Moral Foundations Subjective Corpus, of 2000 Reddit posts annotated by 24 annotators for moral sentiment. We demonstrate that our framework surpasses the previous SOTA in capturing the annotators' individual perspectives with as little as 25% of the original annotation budget on two datasets. Furthermore, our framework results in more equitable models, reducing the performance disparity among annotators.",
            "link": "https://www.semanticscholar.org/paper/8d687f8edca283308d024fa3016cc18ec4a21c1e",
            "authors": "Preni Golazizian, Ali Omrani, Alireza S. Ziabari, Morteza Dehghani",
            "matchScore": 283.99957,
            "original title": "Cost-Efficient Subjective Task Annotation and Modeling through Few-Shot Annotator Adaptation",
            "original authors": "Preni Golazizian, Alireza Salkhordeh Ziabari, Ali Omrani, Morteza Dehghani",
            "EMNLP Paper ID": "708",
            "Oral/Poster": "Poster",
            "Session": "Session 12",
            "Session Time": "Nov 14 (Thu) 14:00-15:30",
            "Type": "Findings"
        }
    ],
    "Knowledge Dynamics in Large Language Models": [
        {
            "paperId": "881af971d00621709b4c772750cd3ea9d0fb11fd",
            "title": "Estimating Knowledge in Large Language Models Without Generating a Single Token",
            "abstract": "To evaluate knowledge in large language models (LLMs), current methods query the model and then evaluate its generated responses. In this work, we ask whether evaluation can be done $\\textit{before}$ the model has generated any text. Concretely, is it possible to estimate how knowledgeable a model is about a certain entity, only from its internal computation? We study this question with two tasks: given a subject entity, the goal is to predict (a) the ability of the model to answer common questions about the entity, and (b) the factuality of responses generated by the model about the entity. Experiments with a variety of LLMs show that KEEN, a simple probe trained over internal subject representations, succeeds at both tasks - strongly correlating with both the QA accuracy of the model per-subject and FActScore, a recent factuality metric in open-ended generation. Moreover, KEEN naturally aligns with the model's hedging behavior and faithfully reflects changes in the model's knowledge after fine-tuning. Lastly, we show a more interpretable yet equally performant variant of KEEN, which highlights a small set of tokens that correlates with the model's lack of knowledge. Being simple and lightweight, KEEN can be leveraged to identify gaps and clusters of entity knowledge in LLMs, and guide decisions such as augmenting queries with retrieval.",
            "link": "https://www.semanticscholar.org/paper/881af971d00621709b4c772750cd3ea9d0fb11fd",
            "authors": "Daniela Gottesman, Mor Geva",
            "EMNLP Paper ID": "449",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "82a3228dcda7fd2a8e5d1fc9bb2bf43bfb6ac531",
            "title": "Discovering Knowledge-Critical Subnetworks in Pretrained Language Models",
            "abstract": "Pretrained language models (LMs) encode implicit representations of knowledge in their parameters. However, localizing these representations and disentangling them from each other remains an open problem. In this work, we investigate whether pretrained language models contain various knowledge-critical subnetworks: particular sparse computational subgraphs that can, if removed, precisely suppress specific knowledge the model has memorized. We propose a multi-objective differentiable masking scheme that can be applied to both weights and neurons to discover such subnetworks and show that we can use them to precisely remove specific knowledge from models while minimizing adverse effects on the behavior of the original model. We demonstrate our method on multiple GPT2 variants, uncovering highly sparse subnetworks (98%+ sparsity) that are critical for expressing specific collections of relational knowledge. When these subnetworks are removed, the remaining network maintains most of its initial abilities but struggles to represent the suppressed knowledge.",
            "link": "https://www.semanticscholar.org/paper/82a3228dcda7fd2a8e5d1fc9bb2bf43bfb6ac531",
            "authors": "Deniz Bayazit, Negar Foroutan, Zeming Chen, Gail Weiss, Antoine Bosselut",
            "EMNLP Paper ID": "732",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "ab8e6df5001dbb9b48445220099425aff536b3e8",
            "title": "Knowledge Conflicts for LLMs: A Survey",
            "abstract": "This survey provides an in-depth analysis of knowledge conflicts for large language models (LLMs), highlighting the complex challenges they encounter when blending contextual and parametric knowledge. Our focus is on three categories of knowledge conflicts: context-memory, inter-context, and intra-memory conflict. These conflicts can significantly impact the trustworthiness and performance of LLMs, especially in real-world applications where noise and misinformation are common. By categorizing these conflicts, exploring the causes, examining the behaviors of LLMs under such conflicts, and reviewing available solutions, this survey aims to shed light on strategies for improving the robustness of LLMs, thereby serving as a valuable resource for advancing research in this evolving area.",
            "link": "https://www.semanticscholar.org/paper/ab8e6df5001dbb9b48445220099425aff536b3e8",
            "authors": "Rongwu Xu, Zehan Qi, Zhijiang Guo, Cunxiang Wang, Hongru Wang, Yue Zhang, Wei Xu",
            "EMNLP Paper ID": "990",
            "Oral/Poster": "VPoster",
            "Session": "VPoster",
            "Session Time": "(Nov 14): 13:00\u201314:00 (Afternoon)",
            "Type": "Main"
        },
        {
            "paperId": "dce90e2cfe1538697bb1004a5b338e0944ac3659",
            "title": "Cluster-norm for Unsupervised Probing of Knowledge",
            "abstract": "The deployment of language models brings challenges in generating reliable information, especially when these models are fine-tuned using human preferences. To extract encoded knowledge without (potentially) biased human labels, unsupervised probing techniques like Contrast-Consistent Search (CCS) have been developed (Burns et al., 2022). However, salient but unrelated features in a given dataset can mislead these probes (Farquhar et al., 2023). Addressing this, we propose a cluster normalization method to minimize the impact of such features by clustering and normalizing activations of contrast pairs before applying unsupervised probing techniques. While this approach does not address the issue of differentiating between knowledge in general and simulated knowledge - a major issue in the literature of latent knowledge elicitation (Christiano et al., 2021) - it significantly improves the ability of unsupervised probes to identify the intended knowledge amidst distractions.",
            "link": "https://www.semanticscholar.org/paper/dce90e2cfe1538697bb1004a5b338e0944ac3659",
            "authors": "Walter Laurito, Sharan Maiya, Gr\u00e9goire Dhimo\u00efla, Owen Yeung, Kaarel H\u00e4nni",
            "EMNLP Paper ID": "1631",
            "Oral/Poster": "Poster",
            "Session": "Session 03",
            "Session Time": "Nov 12 (Tue) 14:00-15:30",
            "Type": "Main"
        },
        {
            "paperId": "f3b36bbfe20156a336e2fb3ae9736fd49ca68679",
            "title": "Defining Knowledge: Bridging Epistemology and Large Language Models",
            "abstract": "Knowledge claims are abundant in the literature on large language models (LLMs); but can we say that GPT-4 truly\"knows\"the Earth is round? To address this question, we review standard definitions of knowledge in epistemology and we formalize interpretations applicable to LLMs. In doing so, we identify inconsistencies and gaps in how current NLP research conceptualizes knowledge with respect to epistemological frameworks. Additionally, we conduct a survey of 100 professional philosophers and computer scientists to compare their preferences in knowledge definitions and their views on whether LLMs can really be said to know. Finally, we suggest evaluation protocols for testing knowledge in accordance to the most relevant definitions.",
            "link": "https://www.semanticscholar.org/paper/f3b36bbfe20156a336e2fb3ae9736fd49ca68679",
            "authors": "Constanza Fierro, Ruchira Dhar, Filippos Stamatiou, Nicolas Garneau, Anders S\u00f8gaard",
            "EMNLP Paper ID": "1893",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "65f20ab2481203e3699792d1f00a78df14ee95a8",
            "title": "Knowledge Mechanisms in Large Language Models: A Survey and Perspective",
            "abstract": "Understanding knowledge mechanisms in Large Language Models (LLMs) is crucial for advancing towards trustworthy AGI. This paper reviews knowledge mechanism analysis from a novel taxonomy including knowledge utilization and evolution. Knowledge utilization delves into the mechanism of memorization, comprehension and application, and creation. Knowledge evolution focuses on the dynamic progression of knowledge within individual and group LLMs. Moreover, we discuss what knowledge LLMs have learned, the reasons for the fragility of parametric knowledge, and the potential dark knowledge (hypothesis) that will be challenging to address. We hope this work can help understand knowledge in LLMs and provide insights for future research.",
            "link": "https://www.semanticscholar.org/paper/65f20ab2481203e3699792d1f00a78df14ee95a8",
            "authors": "Meng Wang, Yunzhi Yao, Ziwen Xu, Shuofei Qiao, Shumin Deng, Peng Wang, Xiang Chen, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen, Ningyu Zhang",
            "matchScore": 179.69151,
            "original title": "Knowledge Mechanisms in Large Language Models: A Survey and Perspective",
            "original authors": "Mengru Wang, Yunzhi Yao, Ziwen Xu, Shuofei Qiao, Shumin Deng, Peng Wang, Xiang Chen, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen, Ningyu Zhang",
            "EMNLP Paper ID": "1448",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "d5ebd84b996491d8ffadefd05a32f8f25085935d",
            "title": "Self-training Large Language Models through Knowledge Detection",
            "abstract": "Large language models (LLMs) often necessitate extensive labeled datasets and training compute to achieve impressive performance across downstream tasks. This paper explores a self-training paradigm, where the LLM autonomously curates its own labels and selectively trains on unknown data samples identified through a reference-free consistency method. Empirical evaluations demonstrate significant improvements in reducing hallucination in generation across multiple subjects. Furthermore, the selective training framework mitigates catastrophic forgetting in out-of-distribution benchmarks, addressing a critical limitation in training LLMs. Our findings suggest that such an approach can substantially reduce the dependency on large labeled datasets, paving the way for more scalable and cost-effective language model training.",
            "link": "https://www.semanticscholar.org/paper/d5ebd84b996491d8ffadefd05a32f8f25085935d",
            "authors": "Wei Jie Yeo, Teddy Ferdinan, Przemys\u0142aw Kazienko, Ranjan Satapathy, Erik Cambria",
            "matchScore": 225.68346,
            "original title": "Self-training Large Language Models through Knowledge Detection",
            "original authors": "Yeo Wei Jie, Teddy Ferdinan, Przemyslaw Kazienko, Ranjan Satapathy, Erik Cambria",
            "EMNLP Paper ID": "2886",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Findings"
        },
        {
            "paperId": "fdb244375205054e70e61f4fcb4d0352a4b28694",
            "title": "To Know or Not To Know? Analyzing Self-Consistency of Large Language Models under Ambiguity",
            "abstract": "One of the major aspects contributing to the striking performance of large language models (LLMs) is the vast amount of factual knowledge accumulated during pre-training. Yet, many LLMs suffer from self-inconsistency, which raises doubts about their trustworthiness and reliability. This paper focuses on entity type ambiguity, analyzing the proficiency and consistency of state-of-the-art LLMs in applying factual knowledge when prompted with ambiguous entities. To do so, we propose an evaluation protocol that disentangles knowing from applying knowledge, and test state-of-the-art LLMs on 49 ambiguous entities. Our experiments reveal that LLMs struggle with choosing the correct entity reading, achieving an average accuracy of only 85%, and as low as 75% with underspecified prompts. The results also reveal systematic discrepancies in LLM behavior, showing that while the models may possess knowledge, they struggle to apply it consistently, exhibit biases toward preferred readings, and display self-inconsistencies. This highlights the need to address entity ambiguity in the future for more trustworthy LLMs.",
            "link": "https://www.semanticscholar.org/paper/fdb244375205054e70e61f4fcb4d0352a4b28694",
            "authors": "Anastasiia Sedova, Robert Litschko, Diego Frassinelli, Benjamin Roth, Barbara Plank",
            "matchScore": 292.2533,
            "original title": "To Know or Not To Know? Analyzing Self-Consistency of Large Language Models under Ambiguity",
            "original authors": "Anastasiia Sedova, Robert Litschko, Diego Frassinelli, Benjamin Roth, Barbara Plank",
            "EMNLP Paper ID": "3306",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        }
    ],
    "Advancements in Text and Speech Style Transfer": [
        {
            "paperId": "436dfb4dc626e064232e2da3e8cb4469f0c585aa",
            "title": "EmphAssess : a Prosodic Benchmark on Assessing Emphasis Transfer in Speech-to-Speech Models",
            "abstract": "We introduce EmphAssess, a prosodic benchmark designed to evaluate the capability of speech-to-speech models to encode and reproduce prosodic emphasis. We apply this to two tasks: speech resynthesis and speech-to-speech translation. In both cases, the benchmark evaluates the ability of the model to encode emphasis in the speech input and accurately reproduce it in the output, potentially across a change of speaker and language. As part of the evaluation pipeline, we introduce EmphaClass, a new model that classifies emphasis at the frame or word level.",
            "link": "https://www.semanticscholar.org/paper/436dfb4dc626e064232e2da3e8cb4469f0c585aa",
            "authors": "Maureen de Seyssel, Antony D'Avirro, Adina Williams, Emmanuel Dupoux",
            "EMNLP Paper ID": "69",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "12a52a817bf1495d872cfde09ce071c1a995ba3d",
            "title": "StyleRemix: Interpretable Authorship Obfuscation via Distillation and Perturbation of Style Elements",
            "abstract": "Authorship obfuscation, rewriting a text to intentionally obscure the identity of the author, is an important but challenging task. Current methods using large language models (LLMs) lack interpretability and controllability, often ignoring author-specific stylistic features, resulting in less robust performance overall. To address this, we develop StyleRemix, an adaptive and interpretable obfuscation method that perturbs specific, fine-grained style elements of the original input text. StyleRemix uses pre-trained Low Rank Adaptation (LoRA) modules to rewrite an input specifically along various stylistic axes (e.g., formality and length) while maintaining low computational cost. StyleRemix outperforms state-of-the-art baselines and much larger LLMs in a variety of domains as assessed by both automatic and human evaluation. Additionally, we release AuthorMix, a large set of 30K high-quality, long-form texts from a diverse set of 14 authors and 4 domains, and DiSC, a parallel corpus of 1,500 texts spanning seven style axes in 16 unique directions",
            "link": "https://www.semanticscholar.org/paper/12a52a817bf1495d872cfde09ce071c1a995ba3d",
            "authors": "Jillian R. Fisher, Skyler Hallinan, Ximing Lu, Mitchell Gordon, Zaid Harchaoui, Yejin Choi",
            "EMNLP Paper ID": "464",
            "Oral/Poster": "Poster",
            "Session": "Session 02",
            "Session Time": "Nov 12 (Tue) 11:00-12:30",
            "Type": "Main"
        },
        {
            "paperId": "3250f1358e89d850db2f9d3cb1eb4fc72ff42db9",
            "title": "EmoKnob: Enhance Voice Cloning with Fine-Grained Emotion Control",
            "abstract": "While recent advances in Text-to-Speech (TTS) technology produce natural and expressive speech, they lack the option for users to select emotion and control intensity. We propose EmoKnob, a framework that allows fine-grained emotion control in speech synthesis with few-shot demonstrative samples of arbitrary emotion. Our framework leverages the expressive speaker representation space made possible by recent advances in foundation voice cloning models. Based on the few-shot capability of our emotion control framework, we propose two methods to apply emotion control on emotions described by open-ended text, enabling an intuitive interface for controlling a diverse array of nuanced emotions. To facilitate a more systematic emotional speech synthesis field, we introduce a set of evaluation metrics designed to rigorously assess the faithfulness and recognizability of emotion control frameworks. Through objective and subjective evaluations, we show that our emotion control framework effectively embeds emotions into speech and surpasses emotion expressiveness of commercial TTS services.",
            "link": "https://www.semanticscholar.org/paper/3250f1358e89d850db2f9d3cb1eb4fc72ff42db9",
            "authors": "Haozhe Chen, Run Chen, Julia Hirschberg",
            "EMNLP Paper ID": "945",
            "Oral/Poster": "Poster",
            "Session": "Session 06",
            "Session Time": "Nov 13 (Wed) 10:30-12:00",
            "Type": "Main"
        },
        {
            "paperId": "45401767b42fe94a6e22154f64500ead01bcab36",
            "title": "Style-Specific Neurons for Steering LLMs in Text Style Transfer",
            "abstract": "Text style transfer (TST) aims to modify the style of a text without altering its original meaning. Large language models (LLMs) demonstrate superior performance across multiple tasks, including TST. However, in zero-shot setups, they tend to directly copy a significant portion of the input text to the output without effectively changing its style. To enhance the stylistic variety and fluency of the text, we present sNeuron-TST, a novel approach for steering LLMs using style-specific neurons in TST. Specifically, we identify neurons associated with the source and target styles and deactivate source-style-only neurons to give target-style words a higher probability, aiming to enhance the stylistic diversity of the generated text. However, we find that this deactivation negatively impacts the fluency of the generated text, which we address by proposing an improved contrastive decoding method that accounts for rapid token probability shifts across layers caused by deactivated source-style neurons. Empirical experiments demonstrate the effectiveness of the proposed method on six benchmarks, encompassing formality, toxicity, politics, politeness, authorship, and sentiment.",
            "link": "https://www.semanticscholar.org/paper/45401767b42fe94a6e22154f64500ead01bcab36",
            "authors": "Wen Lai, Viktor Hangya, Alexander Fraser",
            "EMNLP Paper ID": "1555",
            "Oral/Poster": "Poster",
            "Session": "Session 04",
            "Session Time": "Nov 12 (Tue) 16:00-17:30",
            "Type": "Main"
        },
        {
            "paperId": "9646153a4b49abd2e269fae5666b60e568a1999f",
            "title": "PSST: A Benchmark for Evaluation-driven Text Public-Speaking Style Transfer",
            "abstract": "Language style is necessary for AI systems to understand and generate diverse human language accurately. However, previous text style transfer primarily focused on sentence-level data-driven approaches, limiting exploration of potential problems in large language models (LLMs) and the ability to meet complex application needs. To overcome these limitations, we introduce a novel task called Public-Speaking Style Transfer (PSST), which aims to simulate humans to transform passage-level, official texts into a public-speaking style. Grounded in the analysis of real-world data from a linguistic perspective, we decompose public-speaking style into key sub-styles to pose challenges and quantify the style modeling capability of LLMs. For such intricate text style transfer, we further propose a fine-grained evaluation framework to analyze the characteristics and identify the problems of stylized texts. Comprehensive experiments suggest that current LLMs struggle to generate public speaking texts that align with human preferences, primarily due to excessive stylization and loss of semantic information.",
            "link": "https://www.semanticscholar.org/paper/9646153a4b49abd2e269fae5666b60e568a1999f",
            "authors": "Huashan Sun, Yixiao Wu, Yuhao Ye, Yizhe Yang, Yinghao Li, Jiawei Li, Yang Gao",
            "matchScore": 304.06335,
            "original title": "PSST: A Benchmark for Evaluation-driven Text Public-Speaking Style Transfer",
            "original authors": "Huashan Sun, Yixiao Wu, Yizhe Yang, Yinghao Li, Jiawei Li, Yuhao Ye, Yang Gao",
            "EMNLP Paper ID": "1778",
            "Oral/Poster": "Not Presenting",
            "Session": "Not Presenting",
            "Session Time": "None",
            "Type": "Findings"
        },
        {
            "paperId": "664c6d0e468fc569d561c97395111c09b5a6a439",
            "title": "TinyStyler: Efficient Few-Shot Text Style Transfer with Authorship Embeddings",
            "abstract": "The goal of text style transfer is to transform the style of texts while preserving their original meaning, often with only a few examples of the target style. Existing style transfer methods generally rely on the few-shot capabilities of large language models or on complex controllable text generation approaches that are inefficient and underperform on fluency metrics. We introduce TinyStyler, a lightweight but effective approach, which leverages a small language model (800M params) and pre-trained authorship embeddings to perform efficient, few-shot text style transfer. We evaluate on the challenging task of authorship style transfer and find TinyStyler outperforms strong approaches such as GPT-4. We also evaluate TinyStyler's ability to perform text attribute style transfer (formal $\\leftrightarrow$ informal) with automatic and human evaluations and find that the approach outperforms recent controllable text generation methods. Our model has been made publicly available at https://huggingface.co/tinystyler/tinystyler .",
            "link": "https://www.semanticscholar.org/paper/664c6d0e468fc569d561c97395111c09b5a6a439",
            "authors": "Zachary Horvitz, Ajay Patel, Kanishk Singh, Christopher Callison-Burch, Kathleen McKeown, Zhou Yu",
            "matchScore": 297.90118,
            "original title": "TinyStyler: Efficient Few-Shot Text Style Transfer with Authorship Embeddings",
            "original authors": "Zachary Horvitz, Ajay Patel, Kanishk Singh, Chris Callison-Burch, Kathleen McKeown, Zhou Yu",
            "EMNLP Paper ID": "2601",
            "Oral/Poster": "Poster",
            "Session": "Session 11",
            "Session Time": "Nov 14 (Thu) 10:30-12:00",
            "Type": "Findings"
        }
    ]
}